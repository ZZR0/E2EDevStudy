[
    {
        "idx": 51680,
        "repo_name": "openscilab_memor",
        "url": "https://github.com/openscilab/memor",
        "description": "A Python Library for Managing and Transferring Conversational Memory Across LLMs",
        "stars": 29,
        "forks": 1,
        "language": "python",
        "size": 234,
        "created_at": "2024-12-27T12:31:37+00:00",
        "updated_at": "2025-04-25T19:34:51+00:00",
        "pypi_info": {
            "name": "memor",
            "version": "0.5",
            "url": "https://files.pythonhosted.org/packages/d8/7b/aa43bb303a5074bdc4e1ab265fdaf7905799f6be9bc13313dfc96c36c27d/memor-0.5.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 18,
            "comment_ratio": 0.14378456359701852,
            "pyfile_content_length": 154835,
            "pyfile_code_lines": 4159,
            "test_file_exist": true,
            "test_file_content_length": 62307,
            "pytest_framework": true,
            "test_case_num": 217,
            "metadata_path": [
                "setup.py"
            ],
            "readme_content_length": 9837,
            "llm_reason": "The 'Memor' project is a very good candidate for an AI 'Build from Scratch' benchmark.\n\nPositive Aspects:\n*   **Self-Contained & Independent:** The core library logic (managing conversation data structures like Prompt, Response, Session; templating; custom token estimation) is self-contained. It manipulates data and formats it, without requiring active internet or external APIs for its primary operations. Serialization is to local files. Dependencies like `tiktoken` (likely used for `openai_tokens_estimator` given the test behavior) are standard, pip-installable, and work offline after initial setup, which is acceptable for this type of benchmark.\n*   **Clear & Well-Defined Functionality:** The README clearly explains the project's purpose. The code structure, with classes like `Prompt`, `Response`, `Session`, and `PromptTemplate`, along with their methods (evident from the structure and tests), provides a clear specification for what needs to be rebuilt.\n*   **Testable & Verifiable Output:** The project includes a comprehensive suite of unit tests for its core components (`test_response.py`, `test_prompt.py`, `test_session.py`, `test_prompt_template.py`, `test_token_estimators.py`). These tests cover attribute validation, method behavior, serialization, rendering, and equality, making verification of the AI's output robust and straightforward.\n*   **No Graphical User Interface (GUI):** It is a Python library, intended for programmatic use, which is ideal.\n*   **Appropriate Complexity, Scope & Difficulty:** The project is non-trivial, involving multiple interacting OOP classes, data validation, serialization (JSON), string templating, and custom algorithmic logic (e.g., `universal_tokens_estimator`). The scope (around 10 core Python files in the `memor` package) is manageable for an AI to rebuild within a reasonable timeframe (estimated human effort: a few days). It's more complex than a toy example but not overwhelmingly large.\n*   **Well-Understood Problem Domain:** Managing and formatting data for LLM interactions is a common and understandable problem domain.\n*   **Predominantly Code-Based Solution:** The task is entirely about generating Python code.\n\nNegative Aspects or Concerns:\n*   **Token Estimators:** The `openai_tokens_estimator_*` functions likely rely on a library like `tiktoken` to pass the provided tests. The benchmark specification should clarify that using `tiktoken` (if it's used in the original) is allowed. Replicating `tiktoken`'s behavior from scratch would be too hard. The `universal_tokens_estimator` is custom and a good challenge, but its accurate replication will depend on the AI's ability to understand and implement the specific logic, guided by tests.\n*   **Inter-class Dependencies:** The classes (`Prompt`, `Response`, `Session`) are interconnected. The AI needs to correctly implement these relationships and ensure data flows and updates correctly across them.\n\nOverall, the project's clear structure, extensive tests, and appropriate complexity make it a strong candidate. The 'Medium' difficulty aligns well with the desired target for such benchmarks.",
            "llm_project_type": "Python utility library for LLM conversation data management",
            "llm_rating": 85,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "openscilab_memor",
            "finish_test": true,
            "test_case_result": {
                "tests/test_prompt.py::test_message1": "passed",
                "tests/test_prompt.py::test_message2": "passed",
                "tests/test_prompt.py::test_message3": "passed",
                "tests/test_prompt.py::test_tokens1": "passed",
                "tests/test_prompt.py::test_tokens2": "passed",
                "tests/test_prompt.py::test_tokens3": "passed",
                "tests/test_prompt.py::test_tokens4": "passed",
                "tests/test_prompt.py::test_estimated_tokens1": "passed",
                "tests/test_prompt.py::test_estimated_tokens2": "passed",
                "tests/test_prompt.py::test_estimated_tokens3": "passed",
                "tests/test_prompt.py::test_role1": "passed",
                "tests/test_prompt.py::test_role2": "passed",
                "tests/test_prompt.py::test_role3": "passed",
                "tests/test_prompt.py::test_role4": "passed",
                "tests/test_prompt.py::test_responses1": "passed",
                "tests/test_prompt.py::test_responses2": "passed",
                "tests/test_prompt.py::test_responses3": "passed",
                "tests/test_prompt.py::test_responses4": "passed",
                "tests/test_prompt.py::test_responses5": "passed",
                "tests/test_prompt.py::test_add_response1": "passed",
                "tests/test_prompt.py::test_add_response2": "passed",
                "tests/test_prompt.py::test_add_response3": "passed",
                "tests/test_prompt.py::test_remove_response": "passed",
                "tests/test_prompt.py::test_select_response": "passed",
                "tests/test_prompt.py::test_template1": "passed",
                "tests/test_prompt.py::test_template2": "passed",
                "tests/test_prompt.py::test_template3": "passed",
                "tests/test_prompt.py::test_template4": "passed",
                "tests/test_prompt.py::test_template5": "passed",
                "tests/test_prompt.py::test_copy1": "passed",
                "tests/test_prompt.py::test_copy2": "passed",
                "tests/test_prompt.py::test_str": "passed",
                "tests/test_prompt.py::test_repr": "passed",
                "tests/test_prompt.py::test_json1": "passed",
                "tests/test_prompt.py::test_json2": "passed",
                "tests/test_prompt.py::test_json3": "passed",
                "tests/test_prompt.py::test_save1": "passed",
                "tests/test_prompt.py::test_save2": "passed",
                "tests/test_prompt.py::test_load1": "passed",
                "tests/test_prompt.py::test_load2": "passed",
                "tests/test_prompt.py::test_save3": "passed",
                "tests/test_prompt.py::test_render1": "passed",
                "tests/test_prompt.py::test_render2": "passed",
                "tests/test_prompt.py::test_render3": "passed",
                "tests/test_prompt.py::test_render4": "passed",
                "tests/test_prompt.py::test_render5": "passed",
                "tests/test_prompt.py::test_render6": "passed",
                "tests/test_prompt.py::test_init_check": "passed",
                "tests/test_prompt.py::test_check_render1": "passed",
                "tests/test_prompt.py::test_check_render2": "passed",
                "tests/test_prompt.py::test_equality1": "passed",
                "tests/test_prompt.py::test_equality2": "passed",
                "tests/test_prompt.py::test_equality3": "passed",
                "tests/test_prompt.py::test_equality4": "passed",
                "tests/test_prompt.py::test_length1": "passed",
                "tests/test_prompt.py::test_length2": "passed",
                "tests/test_prompt.py::test_length3": "passed",
                "tests/test_prompt.py::test_date_modified": "passed",
                "tests/test_prompt.py::test_date_created": "passed",
                "tests/test_prompt_template.py::test_title1": "passed",
                "tests/test_prompt_template.py::test_title2": "passed",
                "tests/test_prompt_template.py::test_title3": "passed",
                "tests/test_prompt_template.py::test_title4": "passed",
                "tests/test_prompt_template.py::test_content1": "passed",
                "tests/test_prompt_template.py::test_content2": "passed",
                "tests/test_prompt_template.py::test_content3": "passed",
                "tests/test_prompt_template.py::test_custom_map1": "passed",
                "tests/test_prompt_template.py::test_custom_map2": "passed",
                "tests/test_prompt_template.py::test_custom_map3": "passed",
                "tests/test_prompt_template.py::test_date_modified": "passed",
                "tests/test_prompt_template.py::test_date_created": "passed",
                "tests/test_prompt_template.py::test_json1": "passed",
                "tests/test_prompt_template.py::test_json2": "passed",
                "tests/test_prompt_template.py::test_save1": "passed",
                "tests/test_prompt_template.py::test_save2": "passed",
                "tests/test_prompt_template.py::test_load1": "passed",
                "tests/test_prompt_template.py::test_load2": "passed",
                "tests/test_prompt_template.py::test_load3": "passed",
                "tests/test_prompt_template.py::test_copy1": "passed",
                "tests/test_prompt_template.py::test_copy2": "passed",
                "tests/test_prompt_template.py::test_str": "passed",
                "tests/test_prompt_template.py::test_repr": "passed",
                "tests/test_prompt_template.py::test_equality1": "passed",
                "tests/test_prompt_template.py::test_equality2": "passed",
                "tests/test_prompt_template.py::test_equality3": "passed",
                "tests/test_prompt_template.py::test_equality4": "passed",
                "tests/test_response.py::test_message1": "passed",
                "tests/test_response.py::test_message2": "passed",
                "tests/test_response.py::test_message3": "passed",
                "tests/test_response.py::test_tokens1": "passed",
                "tests/test_response.py::test_tokens2": "passed",
                "tests/test_response.py::test_tokens3": "passed",
                "tests/test_response.py::test_estimated_tokens1": "passed",
                "tests/test_response.py::test_estimated_tokens2": "passed",
                "tests/test_response.py::test_estimated_tokens3": "passed",
                "tests/test_response.py::test_tokens4": "passed",
                "tests/test_response.py::test_inference_time1": "passed",
                "tests/test_response.py::test_inference_time2": "passed",
                "tests/test_response.py::test_inference_time3": "passed",
                "tests/test_response.py::test_inference_time4": "passed",
                "tests/test_response.py::test_score1": "passed",
                "tests/test_response.py::test_score2": "passed",
                "tests/test_response.py::test_score3": "passed",
                "tests/test_response.py::test_role1": "passed",
                "tests/test_response.py::test_role2": "passed",
                "tests/test_response.py::test_role3": "passed",
                "tests/test_response.py::test_role4": "passed",
                "tests/test_response.py::test_temperature1": "passed",
                "tests/test_response.py::test_temperature2": "passed",
                "tests/test_response.py::test_temperature3": "passed",
                "tests/test_response.py::test_model1": "passed",
                "tests/test_response.py::test_model2": "passed",
                "tests/test_response.py::test_model3": "passed",
                "tests/test_response.py::test_date1": "passed",
                "tests/test_response.py::test_date2": "passed",
                "tests/test_response.py::test_date3": "passed",
                "tests/test_response.py::test_date4": "passed",
                "tests/test_response.py::test_json1": "passed",
                "tests/test_response.py::test_json2": "passed",
                "tests/test_response.py::test_save1": "passed",
                "tests/test_response.py::test_save2": "passed",
                "tests/test_response.py::test_load1": "passed",
                "tests/test_response.py::test_load2": "passed",
                "tests/test_response.py::test_load3": "passed",
                "tests/test_response.py::test_copy1": "passed",
                "tests/test_response.py::test_copy2": "passed",
                "tests/test_response.py::test_str": "passed",
                "tests/test_response.py::test_repr": "passed",
                "tests/test_response.py::test_render1": "passed",
                "tests/test_response.py::test_render2": "passed",
                "tests/test_response.py::test_render3": "passed",
                "tests/test_response.py::test_render4": "passed",
                "tests/test_response.py::test_render5": "passed",
                "tests/test_response.py::test_equality1": "passed",
                "tests/test_response.py::test_equality2": "passed",
                "tests/test_response.py::test_equality3": "passed",
                "tests/test_response.py::test_equality4": "passed",
                "tests/test_response.py::test_length1": "passed",
                "tests/test_response.py::test_length2": "passed",
                "tests/test_response.py::test_date_modified": "passed",
                "tests/test_response.py::test_date_created": "passed",
                "tests/test_session.py::test_title1": "passed",
                "tests/test_session.py::test_title2": "passed",
                "tests/test_session.py::test_title3": "passed",
                "tests/test_session.py::test_messages1": "passed",
                "tests/test_session.py::test_messages2": "passed",
                "tests/test_session.py::test_messages3": "passed",
                "tests/test_session.py::test_messages4": "passed",
                "tests/test_session.py::test_messages_status1": "passed",
                "tests/test_session.py::test_messages_status2": "passed",
                "tests/test_session.py::test_messages_status3": "passed",
                "tests/test_session.py::test_messages_status4": "passed",
                "tests/test_session.py::test_enable_message": "passed",
                "tests/test_session.py::test_disable_message": "passed",
                "tests/test_session.py::test_mask_message": "passed",
                "tests/test_session.py::test_unmask_message": "passed",
                "tests/test_session.py::test_masks": "passed",
                "tests/test_session.py::test_add_message1": "passed",
                "tests/test_session.py::test_add_message2": "passed",
                "tests/test_session.py::test_add_message3": "passed",
                "tests/test_session.py::test_add_message4": "passed",
                "tests/test_session.py::test_remove_message": "passed",
                "tests/test_session.py::test_clear_messages": "passed",
                "tests/test_session.py::test_copy1": "passed",
                "tests/test_session.py::test_copy2": "passed",
                "tests/test_session.py::test_str": "passed",
                "tests/test_session.py::test_repr": "passed",
                "tests/test_session.py::test_json": "passed",
                "tests/test_session.py::test_save1": "passed",
                "tests/test_session.py::test_save2": "passed",
                "tests/test_session.py::test_load1": "passed",
                "tests/test_session.py::test_load2": "passed",
                "tests/test_session.py::test_render1": "passed",
                "tests/test_session.py::test_render2": "passed",
                "tests/test_session.py::test_render3": "passed",
                "tests/test_session.py::test_render4": "passed",
                "tests/test_session.py::test_render5": "passed",
                "tests/test_session.py::test_check_render1": "passed",
                "tests/test_session.py::test_check_render2": "passed",
                "tests/test_session.py::test_init_check": "passed",
                "tests/test_session.py::test_equality1": "passed",
                "tests/test_session.py::test_equality2": "passed",
                "tests/test_session.py::test_equality3": "passed",
                "tests/test_session.py::test_equality4": "passed",
                "tests/test_session.py::test_date_modified": "passed",
                "tests/test_session.py::test_date_created": "passed",
                "tests/test_session.py::test_length": "passed",
                "tests/test_session.py::test_iter": "passed",
                "tests/test_session.py::test_addition1": "passed",
                "tests/test_session.py::test_addition2": "passed",
                "tests/test_session.py::test_addition3": "passed",
                "tests/test_session.py::test_addition4": "passed",
                "tests/test_session.py::test_addition5": "passed",
                "tests/test_session.py::test_addition6": "passed",
                "tests/test_session.py::test_addition7": "passed",
                "tests/test_session.py::test_addition8": "passed",
                "tests/test_session.py::test_contains1": "passed",
                "tests/test_session.py::test_contains2": "passed",
                "tests/test_session.py::test_contains3": "passed",
                "tests/test_session.py::test_getitem1": "passed",
                "tests/test_session.py::test_getitem2": "passed",
                "tests/test_session.py::test_estimated_tokens1": "passed",
                "tests/test_session.py::test_estimated_tokens2": "passed",
                "tests/test_session.py::test_estimated_tokens3": "passed",
                "tests/test_token_estimators.py::test_universal_tokens_estimator_with_contractions": "passed",
                "tests/test_token_estimators.py::test_universal_tokens_estimator_with_code_snippets": "passed",
                "tests/test_token_estimators.py::test_universal_tokens_estimator_with_loops": "passed",
                "tests/test_token_estimators.py::test_universal_tokens_estimator_with_long_sentences": "passed",
                "tests/test_token_estimators.py::test_universal_tokens_estimator_with_variable_names": "passed",
                "tests/test_token_estimators.py::test_universal_tokens_estimator_with_function_definitions": "passed",
                "tests/test_token_estimators.py::test_universal_tokens_estimator_with_numbers": "passed",
                "tests/test_token_estimators.py::test_universal_tokens_estimator_with_print_statements": "passed",
                "tests/test_token_estimators.py::test_openai_tokens_estimator_with_function_definition": "passed",
                "tests/test_token_estimators.py::test_openai_tokens_estimator_with_url": "passed",
                "tests/test_token_estimators.py::test_openai_tokens_estimator_with_long_words": "passed",
                "tests/test_token_estimators.py::test_openai_tokens_estimator_with_newlines": "passed",
                "tests/test_token_estimators.py::test_openai_tokens_estimator_with_gpt4_model": "passed"
            },
            "success_count": 217,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "unknown_count": 0,
            "total_count": 217,
            "success_rate": 1.0,
            "coverage_report": {
                "covered_lines": 989,
                "num_statements": 1015,
                "percent_covered": 97.11934156378601,
                "percent_covered_display": "97",
                "missing_lines": 26,
                "excluded_lines": 0,
                "num_branches": 200,
                "num_partial_branches": 9,
                "covered_branches": 191,
                "missing_branches": 9
            },
            "coverage_result": {}
        },
        "codelines_count": 4159,
        "codefiles_count": 18,
        "code_length": 154835,
        "test_files_count": 5,
        "test_code_length": 62307,
        "class_diagram": "@startuml\nclass Prompt {\n    __init__(message, responses, role, tokens, template, file_path, init_check): Unknown\n    __eq__(other_prompt): bool\n    __str__(): str\n    __repr__(): str\n    __len__(): int\n    __copy__(): Prompt\n    copy(): Prompt\n    add_response(response, index): Unknown\n    remove_response(index): Unknown\n    select_response(index): Unknown\n    update_responses(responses): Unknown\n    update_message(message): Unknown\n    update_role(role): Unknown\n    update_tokens(tokens): Unknown\n    update_template(template): Unknown\n    save(file_path, save_template): Dict[str, Any]\n    load(file_path): Unknown\n    from_json(json_object): Unknown\n    to_json(save_template): Dict[str, Any]\n    to_dict(save_template): Dict[str, Any]\n    message(): str\n    responses(): List[Response]\n    role(): Role\n    tokens(): int\n    date_created(): datetime.datetime\n    date_modified(): datetime.datetime\n    template(): PromptTemplate\n    selected_response(): Response\n    render(render_format): Union[str, Dict[str, Any], List[Tuple[str, Any]]]\n    check_render(): bool\n    estimate_tokens(method): int\n}\nclass Role {\n    SYSTEM: Unknown\n    USER: Unknown\n    ASSISTANT: Unknown\n    DEFAULT: USER\n}\nclass RenderFormat {\n    STRING: Unknown\n    OPENAI: Unknown\n    DICTIONARY: Unknown\n    ITEMS: Unknown\n    DEFAULT: STRING\n}\nclass PromptTemplate {\n    __init__(content, file_path, title, custom_map): Unknown\n    __eq__(other_template): bool\n    __str__(): str\n    __repr__(): str\n    __copy__(): PromptTemplate\n    copy(): PromptTemplate\n    update_title(title): Unknown\n    update_content(content): Unknown\n    update_map(custom_map): Unknown\n    save(file_path): Dict[str, Any]\n    load(file_path): Unknown\n    from_json(json_object): Unknown\n    to_json(): Dict[str, Any]\n    to_dict(): Dict[str, Any]\n    content(): str\n    title(): str\n    date_created(): datetime.datetime\n    date_modified(): datetime.datetime\n    custom_map(): Dict[str, str]\n}\nclass _BasicPresetPromptTemplate {\n    PROMPT: PromptTemplate\n    RESPONSE: PromptTemplate\n    RESPONSE0: PromptTemplate\n    RESPONSE1: PromptTemplate\n    RESPONSE2: PromptTemplate\n    RESPONSE3: PromptTemplate\n    PROMPT_WITH_LABEL: PromptTemplate\n    RESPONSE_WITH_LABEL: PromptTemplate\n    RESPONSE0_WITH_LABEL: PromptTemplate\n    RESPONSE1_WITH_LABEL: PromptTemplate\n    RESPONSE2_WITH_LABEL: PromptTemplate\n    RESPONSE3_WITH_LABEL: PromptTemplate\n    PROMPT_RESPONSE_STANDARD: PromptTemplate\n    PROMPT_RESPONSE_FULL: PromptTemplate\n}\nclass _Instruction1PresetPromptTemplate {\n    PROMPT: PromptTemplate\n    RESPONSE: PromptTemplate\n    RESPONSE0: PromptTemplate\n    RESPONSE1: PromptTemplate\n    RESPONSE2: PromptTemplate\n    RESPONSE3: PromptTemplate\n    PROMPT_WITH_LABEL: PromptTemplate\n    RESPONSE_WITH_LABEL: PromptTemplate\n    RESPONSE0_WITH_LABEL: PromptTemplate\n    RESPONSE1_WITH_LABEL: PromptTemplate\n    RESPONSE2_WITH_LABEL: PromptTemplate\n    RESPONSE3_WITH_LABEL: PromptTemplate\n    PROMPT_RESPONSE_STANDARD: PromptTemplate\n    PROMPT_RESPONSE_FULL: PromptTemplate\n}\nclass _Instruction2PresetPromptTemplate {\n    PROMPT: PromptTemplate\n    RESPONSE: PromptTemplate\n    RESPONSE0: PromptTemplate\n    RESPONSE1: PromptTemplate\n    RESPONSE2: PromptTemplate\n    RESPONSE3: PromptTemplate\n    PROMPT_WITH_LABEL: PromptTemplate\n    RESPONSE_WITH_LABEL: PromptTemplate\n    RESPONSE0_WITH_LABEL: PromptTemplate\n    RESPONSE1_WITH_LABEL: PromptTemplate\n    RESPONSE2_WITH_LABEL: PromptTemplate\n    RESPONSE3_WITH_LABEL: PromptTemplate\n    PROMPT_RESPONSE_STANDARD: PromptTemplate\n    PROMPT_RESPONSE_FULL: PromptTemplate\n}\nclass _Instruction3PresetPromptTemplate {\n    PROMPT: PromptTemplate\n    RESPONSE: PromptTemplate\n    RESPONSE0: PromptTemplate\n    RESPONSE1: PromptTemplate\n    RESPONSE2: PromptTemplate\n    RESPONSE3: PromptTemplate\n    PROMPT_WITH_LABEL: PromptTemplate\n    RESPONSE_WITH_LABEL: PromptTemplate\n    RESPONSE0_WITH_LABEL: PromptTemplate\n    RESPONSE1_WITH_LABEL: PromptTemplate\n    RESPONSE2_WITH_LABEL: PromptTemplate\n    RESPONSE3_WITH_LABEL: PromptTemplate\n    PROMPT_RESPONSE_STANDARD: PromptTemplate\n    PROMPT_RESPONSE_FULL: PromptTemplate\n}\nclass PresetPromptTemplate {\n    BASIC: _BasicPresetPromptTemplate\n    INSTRUCTION1: _Instruction1PresetPromptTemplate\n    INSTRUCTION2: _Instruction2PresetPromptTemplate\n    INSTRUCTION3: _Instruction3PresetPromptTemplate\n    DEFAULT: Unknown\n}\nclass MemorValidationError {\n}\nclass MemorRenderError {\n}\nclass Session {\n    __init__(title, messages, file_path, init_check): Unknown\n    __eq__(other_session): bool\n    __str__(): str\n    __repr__(): str\n    __len__(): int\n    __iter__(): Generator[Union[Prompt, Response], Unknown, Unknown]\n    __add__(other_object): Session\n    __radd__(other_object): Session\n    __contains__(message): bool\n    __getitem__(index): Union[Prompt, Response]\n    __copy__(): Session\n    copy(): Session\n    add_message(message, status, index): Unknown\n    remove_message(index): Unknown\n    clear_messages(): Unknown\n    enable_message(index): Unknown\n    disable_message(index): Unknown\n    mask_message(index): Unknown\n    unmask_message(index): Unknown\n    update_title(title): Unknown\n    update_messages(messages, status): Unknown\n    update_messages_status(status): Unknown\n    save(file_path): Dict[str, Any]\n    load(file_path): Unknown\n    from_json(json_object): Unknown\n    to_json(): Dict[str, Any]\n    to_dict(): Dict[str, Any]\n    render(render_format): Union[str, Dict[str, Any], List[Tuple[str, Any]]]\n    check_render(): bool\n    estimate_tokens(method): int\n    date_created(): datetime.datetime\n    date_modified(): datetime.datetime\n    title(): str\n    messages(): List[Union[Prompt, Response]]\n    messages_status(): List[bool]\n    masks(): List[bool]\n}\nclass TokensEstimator {\n    UNIVERSAL: universal_tokens_estimator\n    OPENAI_GPT_3_5: openai_tokens_estimator_gpt_3_5\n    OPENAI_GPT_4: openai_tokens_estimator_gpt_4\n    DEFAULT: UNIVERSAL\n}\nclass Response {\n    __init__(message, score, role, temperature, tokens, inference_time, model, date, file_path): Unknown\n    __eq__(other_response): bool\n    __str__(): str\n    __repr__(): str\n    __len__(): int\n    __copy__(): Response\n    copy(): Response\n    update_message(message): Unknown\n    update_score(score): Unknown\n    update_role(role): Unknown\n    update_temperature(temperature): Unknown\n    update_tokens(tokens): Unknown\n    update_inference_time(inference_time): Unknown\n    update_model(model): Unknown\n    save(file_path): Dict[str, Any]\n    load(file_path): Unknown\n    from_json(json_object): Unknown\n    to_json(): Dict[str, Any]\n    to_dict(): Dict[str, Any]\n    render(render_format): Union[str, Dict[str, Any], List[Tuple[str, Any]]]\n    estimate_tokens(method): int\n    message(): str\n    score(): float\n    temperature(): float\n    tokens(): int\n    inference_time(): float\n    role(): Role\n    model(): str\n    date_created(): datetime.datetime\n    date_modified(): datetime.datetime\n}\n_Instruction2PresetPromptTemplate --> PromptTemplate\nPresetPromptTemplate --> _BasicPresetPromptTemplate\nPrompt --> PromptTemplate\nSession --> Response\nResponse --> TokensEstimator\nPromptTemplate --> PromptTemplate\nPrompt --> RenderFormat\n_Instruction3PresetPromptTemplate --> PromptTemplate\nPrompt --> Role\nResponse --> RenderFormat\nResponse --> Response\n_BasicPresetPromptTemplate --> PromptTemplate\n_Instruction1PresetPromptTemplate --> PromptTemplate\nPrompt --> Prompt\nPrompt --> TokensEstimator\nPresetPromptTemplate --> _Instruction3PresetPromptTemplate\nPresetPromptTemplate --> _Instruction1PresetPromptTemplate\nSession --> TokensEstimator\nPresetPromptTemplate --> _Instruction2PresetPromptTemplate\nSession --> Session\nSession --> Prompt\nResponse --> Role\nSession --> RenderFormat\nPrompt --> Response\nPresetPromptTemplate ..> _Instruction3PresetPromptTemplate\nPresetPromptTemplate ..> _Instruction1PresetPromptTemplate\nSession ..> Prompt\nPromptTemplate ..> PromptTemplate\nPresetPromptTemplate ..> _Instruction2PresetPromptTemplate\nPrompt ..> Role\nPrompt ..> PromptTemplate\nPrompt ..> TokensEstimator\nPresetPromptTemplate ..> _BasicPresetPromptTemplate\n_Instruction1PresetPromptTemplate ..> PromptTemplate\n_BasicPresetPromptTemplate ..> PromptTemplate\nSession ..> Response\n_Instruction3PresetPromptTemplate ..> PromptTemplate\nResponse ..> Role\nResponse ..> TokensEstimator\nResponse ..> Response\nResponse ..> RenderFormat\n_Instruction2PresetPromptTemplate ..> PromptTemplate\nPrompt ..> Prompt\nSession ..> Session\nSession ..> TokensEstimator\nPrompt ..> RenderFormat\nPrompt ..> Response\nSession ..> RenderFormat\n@enduml",
        "structure": [
            {
                "file": "setup.py",
                "functions": [
                    {
                        "name": "get_requires",
                        "docstring": "Read requirements.txt.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "read_description",
                        "docstring": "Read README.md and CHANGELOG.md.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_response.py",
                "functions": [
                    {
                        "name": "test_message1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_message2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_message3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_tokens1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_tokens2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_tokens3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_tokens4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_inference_time1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_inference_time2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_inference_time3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_inference_time4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_score1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_score2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_score3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_role1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_role2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_role3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_role4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_temperature1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_temperature2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_temperature3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_model1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_model2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_model3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_json1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_json2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_copy1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_copy2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_str",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_repr",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render5",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_length1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_length2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date_modified",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date_created",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_token_estimators.py",
                "functions": [
                    {
                        "name": "test_universal_tokens_estimator_with_contractions",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_universal_tokens_estimator_with_code_snippets",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_universal_tokens_estimator_with_loops",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_universal_tokens_estimator_with_long_sentences",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_universal_tokens_estimator_with_variable_names",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_universal_tokens_estimator_with_function_definitions",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_universal_tokens_estimator_with_numbers",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_universal_tokens_estimator_with_print_statements",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_openai_tokens_estimator_with_function_definition",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_openai_tokens_estimator_with_url",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_openai_tokens_estimator_with_long_words",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_openai_tokens_estimator_with_newlines",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_openai_tokens_estimator_with_gpt4_model",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_prompt.py",
                "functions": [
                    {
                        "name": "test_message1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_message2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_message3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_tokens1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_tokens2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_tokens3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_tokens4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_role1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_role2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_role3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_role4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_responses1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_responses2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_responses3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_responses4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_responses5",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_add_response1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_add_response2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_add_response3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_remove_response",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_select_response",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_template1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_template2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_template3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_template4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_template5",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_copy1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_copy2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_str",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_repr",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_json1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_json2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_json3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render5",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render6",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_init_check",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_check_render1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_check_render2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_length1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_length2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_length3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date_modified",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date_created",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_prompt_template.py",
                "functions": [
                    {
                        "name": "test_title1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_title2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_title3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_title4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_content1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_content2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_content3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_custom_map1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_custom_map2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_custom_map3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date_modified",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date_created",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_json1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_json2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_copy1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_copy2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_str",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_repr",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_session.py",
                "functions": [
                    {
                        "name": "test_title1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_title2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_title3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_messages1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_messages2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_messages3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_messages4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_messages_status1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_messages_status2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_messages_status3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_messages_status4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_enable_message",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_disable_message",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_mask_message",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_unmask_message",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_masks",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_add_message1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_add_message2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_add_message3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_add_message4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_remove_message",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_clear_messages",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_copy1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_copy2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_str",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_repr",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_json",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_save2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_load2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_render5",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_check_render1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_check_render2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_init_check",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_equality4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date_modified",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_date_created",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_length",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_iter",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_addition1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_addition2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_addition3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_addition4",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_addition5",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_addition6",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_addition7",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_addition8",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_contains1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_contains2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_contains3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_getitem1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_getitem2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens1",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens2",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_estimated_tokens3",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "otherfiles/requirements-splitter.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "otherfiles/version_check.py",
                "functions": [
                    {
                        "name": "print_result",
                        "docstring": "Print final result.\n\n:param failed: failed flag",
                        "comments": null,
                        "args": [
                            "failed"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "memor/keywords.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "memor/prompt.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Prompt",
                        "docstring": "Prompt class.\n\n>>> from memor import Prompt, Role, Response\n>>> responses = [Response(message=\"I am fine.\"), Response(message=\"I am not fine.\"), Response(message=\"I am okay.\")]\n>>> prompt = Prompt(message=\"Hello, how are you?\", responses=responses)\n>>> prompt.message\n'Hello, how are you?'\n>>> prompt.responses[1].message\n'I am not fine.'",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Prompt object initiator.\n\n:param message: prompt message\n:param responses: prompt responses\n:param role: prompt role\n:param tokens: tokens\n:param template: prompt template\n:param file_path: prompt file path\n:param init_check: initial check flag",
                                "comments": null,
                                "args": [
                                    "self",
                                    "message",
                                    "responses",
                                    "role",
                                    "tokens",
                                    "template",
                                    "file_path",
                                    "init_check"
                                ]
                            },
                            {
                                "name": "__eq__",
                                "docstring": "Check prompts equality.\n\n:param other_prompt: another prompt",
                                "comments": null,
                                "args": [
                                    "self",
                                    "other_prompt"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": "Return string representation of Prompt.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__repr__",
                                "docstring": "Return string representation of Prompt.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__len__",
                                "docstring": "Return the length of the Prompt object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__copy__",
                                "docstring": "Return a copy of the Prompt object.\n\n:return: a copy of Prompt object",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "copy",
                                "docstring": "Return a copy of the Prompt object.\n\n:return: a copy of Prompt object",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "add_response",
                                "docstring": "Add a response to the prompt object.\n\n:param response: response\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "response",
                                    "index"
                                ]
                            },
                            {
                                "name": "remove_response",
                                "docstring": "Remove a response from the prompt object.\n\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "index"
                                ]
                            },
                            {
                                "name": "select_response",
                                "docstring": "Select a response as selected response.\n\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "index"
                                ]
                            },
                            {
                                "name": "update_responses",
                                "docstring": "Update the prompt responses.\n\n:param responses: responses",
                                "comments": null,
                                "args": [
                                    "self",
                                    "responses"
                                ]
                            },
                            {
                                "name": "update_message",
                                "docstring": "Update the prompt message.\n\n:param message: message",
                                "comments": null,
                                "args": [
                                    "self",
                                    "message"
                                ]
                            },
                            {
                                "name": "update_role",
                                "docstring": "Update the prompt role.\n\n:param role: role",
                                "comments": null,
                                "args": [
                                    "self",
                                    "role"
                                ]
                            },
                            {
                                "name": "update_tokens",
                                "docstring": "Update the tokens.\n\n:param tokens: tokens",
                                "comments": null,
                                "args": [
                                    "self",
                                    "tokens"
                                ]
                            },
                            {
                                "name": "update_template",
                                "docstring": "Update the prompt template.\n\n:param template: template",
                                "comments": null,
                                "args": [
                                    "self",
                                    "template"
                                ]
                            },
                            {
                                "name": "save",
                                "docstring": "Save method.\n\n:param file_path: prompt file path\n:param save_template: save template flag",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path",
                                    "save_template"
                                ]
                            },
                            {
                                "name": "load",
                                "docstring": "Load method.\n\n:param file_path: prompt file path",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "from_json",
                                "docstring": "Load attributes from the JSON object.\n\n:param json_object: JSON object",
                                "comments": null,
                                "args": [
                                    "self",
                                    "json_object"
                                ]
                            },
                            {
                                "name": "to_json",
                                "docstring": "Convert the prompt to a JSON object.\n\n:param save_template: save template flag",
                                "comments": null,
                                "args": [
                                    "self",
                                    "save_template"
                                ]
                            },
                            {
                                "name": "to_dict",
                                "docstring": "Convert the prompt to a dictionary.\n\n:param save_template: save template flag",
                                "comments": null,
                                "args": [
                                    "self",
                                    "save_template"
                                ]
                            },
                            {
                                "name": "message",
                                "docstring": "Get the prompt message.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "responses",
                                "docstring": "Get the prompt responses.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "role",
                                "docstring": "Get the prompt role.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tokens",
                                "docstring": "Get the prompt tokens.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "date_created",
                                "docstring": "Get the prompt creation date.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "date_modified",
                                "docstring": "Get the prompt object modification date.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "template",
                                "docstring": "Get the prompt template.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "selected_response",
                                "docstring": "Get the prompt selected response.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "render",
                                "docstring": "Render method.\n\n:param render_format: render format",
                                "comments": null,
                                "args": [
                                    "self",
                                    "render_format"
                                ]
                            },
                            {
                                "name": "check_render",
                                "docstring": "Check render.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "estimate_tokens",
                                "docstring": "Estimate the number of tokens in the prompt message.\n\n:param method: token estimator method",
                                "comments": null,
                                "args": [
                                    "self",
                                    "method"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "memor/params.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Role",
                        "docstring": "Role enum.",
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "SYSTEM",
                            "USER",
                            "ASSISTANT",
                            "DEFAULT"
                        ]
                    },
                    {
                        "name": "RenderFormat",
                        "docstring": "Render format.",
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "STRING",
                            "OPENAI",
                            "DICTIONARY",
                            "ITEMS",
                            "DEFAULT"
                        ]
                    }
                ]
            },
            {
                "file": "memor/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "memor/template.py",
                "functions": [],
                "classes": [
                    {
                        "name": "PromptTemplate",
                        "docstring": "Prompt template.\n\n>>> template = PromptTemplate(content=\"Take a deep breath\\n{prompt_message}!\", title=\"Greeting\")\n>>> template.title\n'Greeting'",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Prompt template object initiator.\n\n:param content: template content\n:param file_path: template file path\n:param title: template title\n:param custom_map: custom map",
                                "comments": null,
                                "args": [
                                    "self",
                                    "content",
                                    "file_path",
                                    "title",
                                    "custom_map"
                                ]
                            },
                            {
                                "name": "__eq__",
                                "docstring": "Check templates equality.\n\n:param other_template: another template",
                                "comments": null,
                                "args": [
                                    "self",
                                    "other_template"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": "Return string representation of PromptTemplate.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__repr__",
                                "docstring": "Return string representation of PromptTemplate.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__copy__",
                                "docstring": "Return a copy of the PromptTemplate object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "copy",
                                "docstring": "Return a copy of the PromptTemplate object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "update_title",
                                "docstring": "Update title.\n\n:param title: title",
                                "comments": null,
                                "args": [
                                    "self",
                                    "title"
                                ]
                            },
                            {
                                "name": "update_content",
                                "docstring": "Update content.\n\n:param content: content",
                                "comments": null,
                                "args": [
                                    "self",
                                    "content"
                                ]
                            },
                            {
                                "name": "update_map",
                                "docstring": "Update custom map.\n\n:param custom_map: custom map",
                                "comments": null,
                                "args": [
                                    "self",
                                    "custom_map"
                                ]
                            },
                            {
                                "name": "save",
                                "docstring": "Save method.\n\n:param file_path: template file path",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "load",
                                "docstring": "Load method.\n\n:param file_path: template file path",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "from_json",
                                "docstring": "Load attributes from the JSON object.\n\n:param json_object: JSON object",
                                "comments": null,
                                "args": [
                                    "self",
                                    "json_object"
                                ]
                            },
                            {
                                "name": "to_json",
                                "docstring": "Convert PromptTemplate to json.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "to_dict",
                                "docstring": "Convert PromptTemplate to dict.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "content",
                                "docstring": "Get the PromptTemplate content.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "title",
                                "docstring": "Get the PromptTemplate title.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "date_created",
                                "docstring": "Get the PromptTemplate creation date.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "date_modified",
                                "docstring": "Get the PromptTemplate modification date.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "custom_map",
                                "docstring": "Get the PromptTemplate custom map.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "_BasicPresetPromptTemplate",
                        "docstring": "Preset basic-prompt templates.",
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "PROMPT",
                            "RESPONSE",
                            "RESPONSE0",
                            "RESPONSE1",
                            "RESPONSE2",
                            "RESPONSE3",
                            "PROMPT_WITH_LABEL",
                            "RESPONSE_WITH_LABEL",
                            "RESPONSE0_WITH_LABEL",
                            "RESPONSE1_WITH_LABEL",
                            "RESPONSE2_WITH_LABEL",
                            "RESPONSE3_WITH_LABEL",
                            "PROMPT_RESPONSE_STANDARD",
                            "PROMPT_RESPONSE_FULL"
                        ]
                    },
                    {
                        "name": "_Instruction1PresetPromptTemplate",
                        "docstring": "Preset instruction1-prompt templates.",
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "PROMPT",
                            "RESPONSE",
                            "RESPONSE0",
                            "RESPONSE1",
                            "RESPONSE2",
                            "RESPONSE3",
                            "PROMPT_WITH_LABEL",
                            "RESPONSE_WITH_LABEL",
                            "RESPONSE0_WITH_LABEL",
                            "RESPONSE1_WITH_LABEL",
                            "RESPONSE2_WITH_LABEL",
                            "RESPONSE3_WITH_LABEL",
                            "PROMPT_RESPONSE_STANDARD",
                            "PROMPT_RESPONSE_FULL"
                        ]
                    },
                    {
                        "name": "_Instruction2PresetPromptTemplate",
                        "docstring": "Preset instruction2-prompt templates.",
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "PROMPT",
                            "RESPONSE",
                            "RESPONSE0",
                            "RESPONSE1",
                            "RESPONSE2",
                            "RESPONSE3",
                            "PROMPT_WITH_LABEL",
                            "RESPONSE_WITH_LABEL",
                            "RESPONSE0_WITH_LABEL",
                            "RESPONSE1_WITH_LABEL",
                            "RESPONSE2_WITH_LABEL",
                            "RESPONSE3_WITH_LABEL",
                            "PROMPT_RESPONSE_STANDARD",
                            "PROMPT_RESPONSE_FULL"
                        ]
                    },
                    {
                        "name": "_Instruction3PresetPromptTemplate",
                        "docstring": "Preset instruction3-prompt templates.",
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "PROMPT",
                            "RESPONSE",
                            "RESPONSE0",
                            "RESPONSE1",
                            "RESPONSE2",
                            "RESPONSE3",
                            "PROMPT_WITH_LABEL",
                            "RESPONSE_WITH_LABEL",
                            "RESPONSE0_WITH_LABEL",
                            "RESPONSE1_WITH_LABEL",
                            "RESPONSE2_WITH_LABEL",
                            "RESPONSE3_WITH_LABEL",
                            "PROMPT_RESPONSE_STANDARD",
                            "PROMPT_RESPONSE_FULL"
                        ]
                    },
                    {
                        "name": "PresetPromptTemplate",
                        "docstring": "Preset prompt templates.",
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "BASIC",
                            "INSTRUCTION1",
                            "INSTRUCTION2",
                            "INSTRUCTION3",
                            "DEFAULT"
                        ]
                    }
                ]
            },
            {
                "file": "memor/errors.py",
                "functions": [],
                "classes": [
                    {
                        "name": "MemorValidationError",
                        "docstring": "Base class for validation errors in Memor.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "MemorRenderError",
                        "docstring": "Base class for render error in Memor.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "memor/functions.py",
                "functions": [
                    {
                        "name": "get_time_utc",
                        "docstring": "Get time in UTC format.\n\n:return: UTC format time as a datetime object",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "_validate_string",
                        "docstring": "Validate string.\n\n:param value: value\n:param parameter_name: parameter name",
                        "comments": null,
                        "args": [
                            "value",
                            "parameter_name"
                        ]
                    },
                    {
                        "name": "_validate_bool",
                        "docstring": "Validate boolean.\n\n:param value: value\n:param parameter_name: parameter name",
                        "comments": null,
                        "args": [
                            "value",
                            "parameter_name"
                        ]
                    },
                    {
                        "name": "_can_convert_to_string",
                        "docstring": "Check if value can be converted to string.\n\n:param value: value",
                        "comments": null,
                        "args": [
                            "value"
                        ]
                    },
                    {
                        "name": "_validate_pos_int",
                        "docstring": "Validate positive integer.\n\n:param value: value\n:param parameter_name: parameter name",
                        "comments": null,
                        "args": [
                            "value",
                            "parameter_name"
                        ]
                    },
                    {
                        "name": "_validate_pos_float",
                        "docstring": "Validate positive float.\n\n:param value: value\n:param parameter_name: parameter name",
                        "comments": null,
                        "args": [
                            "value",
                            "parameter_name"
                        ]
                    },
                    {
                        "name": "_validate_probability",
                        "docstring": "Validate probability (a float between 0 and 1).\n\n:param value: value\n:param parameter_name: parameter name",
                        "comments": null,
                        "args": [
                            "value",
                            "parameter_name"
                        ]
                    },
                    {
                        "name": "_validate_list_of",
                        "docstring": "Validate list of values.\n\n:param value: value\n:param parameter_name: parameter name\n:param type_: type\n:param type_name: type name",
                        "comments": null,
                        "args": [
                            "value",
                            "parameter_name",
                            "type_",
                            "type_name"
                        ]
                    },
                    {
                        "name": "_validate_date_time",
                        "docstring": "Validate date time.\n\n:param date_time: date time\n:param parameter_name: parameter name",
                        "comments": null,
                        "args": [
                            "date_time",
                            "parameter_name"
                        ]
                    },
                    {
                        "name": "_validate_path",
                        "docstring": "Validate path property.\n\n:param path: path",
                        "comments": null,
                        "args": [
                            "path"
                        ]
                    },
                    {
                        "name": "_validate_custom_map",
                        "docstring": "Validate custom map a dictionary with keys and values that can be converted to strings.\n\n:param custom_map: custom map",
                        "comments": null,
                        "args": [
                            "custom_map"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "memor/session.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Session",
                        "docstring": "Session class.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Session object initiator.\n\n:param title: title\n:param messages: messages\n:param file_path: file path\n:param init_check: initial check flag",
                                "comments": null,
                                "args": [
                                    "self",
                                    "title",
                                    "messages",
                                    "file_path",
                                    "init_check"
                                ]
                            },
                            {
                                "name": "__eq__",
                                "docstring": "Check sessions equality.\n\n:param other_session: other session",
                                "comments": null,
                                "args": [
                                    "self",
                                    "other_session"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": "Return string representation of Session.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__repr__",
                                "docstring": "Return string representation of Session.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__len__",
                                "docstring": "Return the length of the Session object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__iter__",
                                "docstring": "Iterate through the Session object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__add__",
                                "docstring": "Addition method.\n\n:param other_object: other object",
                                "comments": null,
                                "args": [
                                    "self",
                                    "other_object"
                                ]
                            },
                            {
                                "name": "__radd__",
                                "docstring": "Reverse addition method.\n\n:param other_object: other object",
                                "comments": null,
                                "args": [
                                    "self",
                                    "other_object"
                                ]
                            },
                            {
                                "name": "__contains__",
                                "docstring": "Check if the Session contains the given message.\n\n:param message: message",
                                "comments": null,
                                "args": [
                                    "self",
                                    "message"
                                ]
                            },
                            {
                                "name": "__getitem__",
                                "docstring": "Return the Session message(s).\n\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "index"
                                ]
                            },
                            {
                                "name": "__copy__",
                                "docstring": "Return a copy of the Session object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "copy",
                                "docstring": "Return a copy of the Session object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "add_message",
                                "docstring": "Add a message to the session object.\n\n:param message: message\n:param status: status\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "message",
                                    "status",
                                    "index"
                                ]
                            },
                            {
                                "name": "remove_message",
                                "docstring": "Remove a message from the session object.\n\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "index"
                                ]
                            },
                            {
                                "name": "clear_messages",
                                "docstring": "Remove all messages.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "enable_message",
                                "docstring": "Enable a message.\n\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "index"
                                ]
                            },
                            {
                                "name": "disable_message",
                                "docstring": "Disable a message.\n\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "index"
                                ]
                            },
                            {
                                "name": "mask_message",
                                "docstring": "Mask a message.\n\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "index"
                                ]
                            },
                            {
                                "name": "unmask_message",
                                "docstring": "Unmask a message.\n\n:param index: index",
                                "comments": null,
                                "args": [
                                    "self",
                                    "index"
                                ]
                            },
                            {
                                "name": "update_title",
                                "docstring": "Update the session title.\n\n:param title: title",
                                "comments": null,
                                "args": [
                                    "self",
                                    "title"
                                ]
                            },
                            {
                                "name": "update_messages",
                                "docstring": "Update the session messages.\n\n:param messages: messages\n:param status: status",
                                "comments": null,
                                "args": [
                                    "self",
                                    "messages",
                                    "status"
                                ]
                            },
                            {
                                "name": "update_messages_status",
                                "docstring": "Update the session messages status.\n\n:param status: status",
                                "comments": null,
                                "args": [
                                    "self",
                                    "status"
                                ]
                            },
                            {
                                "name": "save",
                                "docstring": "Save method.\n\n:param file_path: session file path",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "load",
                                "docstring": "Load method.\n\n:param file_path: session file path",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "from_json",
                                "docstring": "Load attributes from the JSON object.\n\n:param json_object: JSON object",
                                "comments": null,
                                "args": [
                                    "self",
                                    "json_object"
                                ]
                            },
                            {
                                "name": "to_json",
                                "docstring": "Convert the session to a JSON object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "to_dict",
                                "docstring": "Convert the session to a dictionary.\n\n:return: dict",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "render",
                                "docstring": "Render method.\n\n:param render_format: render format",
                                "comments": null,
                                "args": [
                                    "self",
                                    "render_format"
                                ]
                            },
                            {
                                "name": "check_render",
                                "docstring": "Check render.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "estimate_tokens",
                                "docstring": "Estimate the number of tokens in the session.\n\n:param method: token estimator method",
                                "comments": null,
                                "args": [
                                    "self",
                                    "method"
                                ]
                            },
                            {
                                "name": "date_created",
                                "docstring": "Get the session creation date.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "date_modified",
                                "docstring": "Get the session object modification date.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "title",
                                "docstring": "Get the session title.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "messages",
                                "docstring": "Get the session messages.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "messages_status",
                                "docstring": "Get the session messages status.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "masks",
                                "docstring": "Get the session masks.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "memor/tokens_estimator.py",
                "functions": [
                    {
                        "name": "_is_code_snippet",
                        "docstring": "Check if the message is a code snippet based on common coding symbols.\n\n:param message: The input message to check.\n:return: Boolean indicating if the message is a code snippet.",
                        "comments": null,
                        "args": [
                            "message"
                        ]
                    },
                    {
                        "name": "_preprocess_message",
                        "docstring": "Preprocess message by replacing contractions in non-code text.\n\n:param message: The input message to preprocess.\n:param is_code: Boolean indicating if the message is a code.\n:return: Preprocessed message.",
                        "comments": null,
                        "args": [
                            "message",
                            "is_code"
                        ]
                    },
                    {
                        "name": "_tokenize_message",
                        "docstring": "Tokenize the message based on words, symbols, and numbers.\n\n:param message: The input message to tokenize.\n:return: List of tokens.",
                        "comments": null,
                        "args": [
                            "message"
                        ]
                    },
                    {
                        "name": "_count_code_tokens",
                        "docstring": "Count tokens in code snippets considering different token types.\n\n:param token: The token to count.\n:param common_keywords: Set of common keywords in programming languages.\n:return: Count of tokens.",
                        "comments": null,
                        "args": [
                            "token",
                            "common_keywords"
                        ]
                    },
                    {
                        "name": "_count_text_tokens",
                        "docstring": "Count tokens in text based on prefixes, suffixes, and subwords.\n\n:param token: The token to count.\n:param prefixes: Set of common prefixes.\n:param suffixes: Set of common suffixes.\n:return: Token count.",
                        "comments": null,
                        "args": [
                            "token",
                            "prefixes",
                            "suffixes"
                        ]
                    },
                    {
                        "name": "universal_tokens_estimator",
                        "docstring": "Estimate the number of tokens in a given text or code snippet.\n\n:param message: The input text or code snippet to estimate tokens for.\n:return: Estimated number of tokens.",
                        "comments": null,
                        "args": [
                            "message"
                        ]
                    },
                    {
                        "name": "_openai_tokens_estimator",
                        "docstring": "Estimate the number of tokens in a given text for OpenAI's models.\n\n:param text: The input text to estimate tokens for.\n:return: Estimated number of tokens.",
                        "comments": null,
                        "args": [
                            "text"
                        ]
                    },
                    {
                        "name": "openai_tokens_estimator_gpt_3_5",
                        "docstring": "Estimate the number of tokens in a given text for OpenAI's GPT-3.5 Turbo model.\n\n:param text: The input text to estimate tokens for.\n:return: Estimated number of tokens.",
                        "comments": null,
                        "args": [
                            "text"
                        ]
                    },
                    {
                        "name": "openai_tokens_estimator_gpt_4",
                        "docstring": "Estimate the number of tokens in a given text for OpenAI's GPT-4 model.\n\n:param text: The input text to estimate tokens for.\n:return: Estimated number of tokens.",
                        "comments": null,
                        "args": [
                            "text"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "TokensEstimator",
                        "docstring": "Token estimator enum.",
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "UNIVERSAL",
                            "OPENAI_GPT_3_5",
                            "OPENAI_GPT_4",
                            "DEFAULT"
                        ]
                    }
                ]
            },
            {
                "file": "memor/response.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Response",
                        "docstring": "Response class.\n\n>>> from memor import Response, Role\n>>> response = Response(message=\"Hello!\", score=0.9, role=Role.ASSISTANT, temperature=0.5, model=\"gpt-3.5\")\n>>> response.message\n'Hello!'",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Response object initiator.\n\n:param message: response message\n:param score: response score\n:param role: response role\n:param temperature: temperature\n:param tokens: tokens\n:param inference_time: inference time\n:param model: agent model\n:param date: response date\n:param file_path: response file path",
                                "comments": null,
                                "args": [
                                    "self",
                                    "message",
                                    "score",
                                    "role",
                                    "temperature",
                                    "tokens",
                                    "inference_time",
                                    "model",
                                    "date",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "__eq__",
                                "docstring": "Check responses equality.\n\n:param other_response: another response",
                                "comments": null,
                                "args": [
                                    "self",
                                    "other_response"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": "Return string representation of Response.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__repr__",
                                "docstring": "Return string representation of Response.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__len__",
                                "docstring": "Return the length of the Response object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__copy__",
                                "docstring": "Return a copy of the Response object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "copy",
                                "docstring": "Return a copy of the Response object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "update_message",
                                "docstring": "Update the response message.\n\n:param message: message",
                                "comments": null,
                                "args": [
                                    "self",
                                    "message"
                                ]
                            },
                            {
                                "name": "update_score",
                                "docstring": "Update the response score.\n\n:param score: score",
                                "comments": null,
                                "args": [
                                    "self",
                                    "score"
                                ]
                            },
                            {
                                "name": "update_role",
                                "docstring": "Update the response role.\n\n:param role: role",
                                "comments": null,
                                "args": [
                                    "self",
                                    "role"
                                ]
                            },
                            {
                                "name": "update_temperature",
                                "docstring": "Update the temperature.\n\n:param temperature: temperature",
                                "comments": null,
                                "args": [
                                    "self",
                                    "temperature"
                                ]
                            },
                            {
                                "name": "update_tokens",
                                "docstring": "Update the tokens.\n\n:param tokens: tokens",
                                "comments": null,
                                "args": [
                                    "self",
                                    "tokens"
                                ]
                            },
                            {
                                "name": "update_inference_time",
                                "docstring": "Update inference time.\n\n:param inference_time: inference time",
                                "comments": null,
                                "args": [
                                    "self",
                                    "inference_time"
                                ]
                            },
                            {
                                "name": "update_model",
                                "docstring": "Update the agent model.\n\n:param model: model",
                                "comments": null,
                                "args": [
                                    "self",
                                    "model"
                                ]
                            },
                            {
                                "name": "save",
                                "docstring": "Save method.\n\n:param file_path: response file path",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "load",
                                "docstring": "Load method.\n\n:param file_path: response file path",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "from_json",
                                "docstring": "Load attributes from the JSON object.\n\n:param json_object: JSON object",
                                "comments": null,
                                "args": [
                                    "self",
                                    "json_object"
                                ]
                            },
                            {
                                "name": "to_json",
                                "docstring": "Convert the response to a JSON object.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "to_dict",
                                "docstring": "Convert the response to a dictionary.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "render",
                                "docstring": "Render the response.\n\n:param render_format: render format",
                                "comments": null,
                                "args": [
                                    "self",
                                    "render_format"
                                ]
                            },
                            {
                                "name": "estimate_tokens",
                                "docstring": "Estimate the number of tokens in the response message.\n\n:param method: token estimator method",
                                "comments": null,
                                "args": [
                                    "self",
                                    "method"
                                ]
                            },
                            {
                                "name": "message",
                                "docstring": "Get the response message.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "score",
                                "docstring": "Get the response score.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "temperature",
                                "docstring": "Get the temperature.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tokens",
                                "docstring": "Get the tokens.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "inference_time",
                                "docstring": "Get inference time.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "role",
                                "docstring": "Get the response role.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "model",
                                "docstring": "Get the agent model.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "date_created",
                                "docstring": "Get the response creation date.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "date_modified",
                                "docstring": "Get the response object modification date.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            }
        ],
        "test_cases": {
            "tests/test_prompt.py::test_message1": {
                "testid": "tests/test_prompt.py::test_message1",
                "result": "passed",
                "test_implementation": "def test_message1():\n    prompt = Prompt(message=\"Hello, how are you?\")\n    assert prompt.message == \"Hello, how are you?\""
            },
            "tests/test_prompt.py::test_message2": {
                "testid": "tests/test_prompt.py::test_message2",
                "result": "passed",
                "test_implementation": "def test_message2():\n    prompt = Prompt(message=\"Hello, how are you?\")\n    prompt.update_message(\"What's Up?\")\n    assert prompt.message == \"What's Up?\""
            },
            "tests/test_prompt.py::test_message3": {
                "testid": "tests/test_prompt.py::test_message3",
                "result": "passed",
                "test_implementation": "def test_message3():\n    prompt = Prompt(message=\"Hello, how are you?\")\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `message` must be a string.\"):\n        prompt.update_message(22)"
            },
            "tests/test_prompt.py::test_tokens1": {
                "testid": "tests/test_prompt.py::test_tokens1",
                "result": "passed",
                "test_implementation": "def test_tokens1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    assert prompt.tokens is None"
            },
            "tests/test_prompt.py::test_tokens2": {
                "testid": "tests/test_prompt.py::test_tokens2",
                "result": "passed",
                "test_implementation": "def test_tokens2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER, tokens=4)\n    assert prompt.tokens == 4"
            },
            "tests/test_prompt.py::test_tokens3": {
                "testid": "tests/test_prompt.py::test_tokens3",
                "result": "passed",
                "test_implementation": "def test_tokens3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER, tokens=4)\n    prompt.update_tokens(7)\n    assert prompt.tokens == 7"
            },
            "tests/test_prompt.py::test_tokens4": {
                "testid": "tests/test_prompt.py::test_tokens4",
                "result": "passed",
                "test_implementation": "def test_tokens4():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `tokens` must be a positive integer.\"):\n        prompt.update_tokens(\"4\")"
            },
            "tests/test_prompt.py::test_estimated_tokens1": {
                "testid": "tests/test_prompt.py::test_estimated_tokens1",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    assert prompt.estimate_tokens(TokensEstimator.UNIVERSAL) == 7"
            },
            "tests/test_prompt.py::test_estimated_tokens2": {
                "testid": "tests/test_prompt.py::test_estimated_tokens2",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    assert prompt.estimate_tokens(TokensEstimator.OPENAI_GPT_3_5) == 7"
            },
            "tests/test_prompt.py::test_estimated_tokens3": {
                "testid": "tests/test_prompt.py::test_estimated_tokens3",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    assert prompt.estimate_tokens(TokensEstimator.OPENAI_GPT_4) == 8"
            },
            "tests/test_prompt.py::test_role1": {
                "testid": "tests/test_prompt.py::test_role1",
                "result": "passed",
                "test_implementation": "def test_role1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    assert prompt.role == Role.USER"
            },
            "tests/test_prompt.py::test_role2": {
                "testid": "tests/test_prompt.py::test_role2",
                "result": "passed",
                "test_implementation": "def test_role2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    prompt.update_role(Role.SYSTEM)\n    assert prompt.role == Role.SYSTEM"
            },
            "tests/test_prompt.py::test_role3": {
                "testid": "tests/test_prompt.py::test_role3",
                "result": "passed",
                "test_implementation": "def test_role3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=None)\n    assert prompt.role == Role.USER"
            },
            "tests/test_prompt.py::test_role4": {
                "testid": "tests/test_prompt.py::test_role4",
                "result": "passed",
                "test_implementation": "def test_role4():\n    prompt = Prompt(message=\"Hello, how are you?\", role=None)\n    with pytest.raises(MemorValidationError, match=r\"Invalid role. It must be an instance of Role enum.\"):\n        prompt.update_role(2)"
            },
            "tests/test_prompt.py::test_responses1": {
                "testid": "tests/test_prompt.py::test_responses1",
                "result": "passed",
                "test_implementation": "def test_responses1():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\")\n    prompt = Prompt(message=message, responses=[response])\n    assert prompt.responses[0].message == \"I am fine.\""
            },
            "tests/test_prompt.py::test_responses2": {
                "testid": "tests/test_prompt.py::test_responses2",
                "result": "passed",
                "test_implementation": "def test_responses2():\n    message = \"Hello, how are you?\"\n    response0 = Response(message=\"I am fine.\")\n    response1 = Response(message=\"Good!\")\n    prompt = Prompt(message=message, responses=[response0, response1])\n    assert prompt.responses[0].message == \"I am fine.\" and prompt.responses[1].message == \"Good!\""
            },
            "tests/test_prompt.py::test_responses3": {
                "testid": "tests/test_prompt.py::test_responses3",
                "result": "passed",
                "test_implementation": "def test_responses3():\n    message = \"Hello, how are you?\"\n    response0 = Response(message=\"I am fine.\")\n    response1 = Response(message=\"Good!\")\n    prompt = Prompt(message=message)\n    prompt.update_responses([response0, response1])\n    assert prompt.responses[0].message == \"I am fine.\" and prompt.responses[1].message == \"Good!\""
            },
            "tests/test_prompt.py::test_responses4": {
                "testid": "tests/test_prompt.py::test_responses4",
                "result": "passed",
                "test_implementation": "def test_responses4():\n    message = \"Hello, how are you?\"\n    prompt = Prompt(message=message)\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `responses` must be a list of `Response`.\"):\n        prompt.update_responses({\"I am fine.\", \"Good!\"})"
            },
            "tests/test_prompt.py::test_responses5": {
                "testid": "tests/test_prompt.py::test_responses5",
                "result": "passed",
                "test_implementation": "def test_responses5():\n    message = \"Hello, how are you?\"\n    response0 = Response(message=\"I am fine.\")\n    prompt = Prompt(message=message)\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `responses` must be a list of `Response`.\"):\n        prompt.update_responses([response0, \"Good!\"])"
            },
            "tests/test_prompt.py::test_add_response1": {
                "testid": "tests/test_prompt.py::test_add_response1",
                "result": "passed",
                "test_implementation": "def test_add_response1():\n    message = \"Hello, how are you?\"\n    response0 = Response(message=\"I am fine.\")\n    prompt = Prompt(message=message, responses=[response0])\n    response1 = Response(message=\"Great!\")\n    prompt.add_response(response1)\n    assert prompt.responses[0] == response0 and prompt.responses[1] == response1"
            },
            "tests/test_prompt.py::test_add_response2": {
                "testid": "tests/test_prompt.py::test_add_response2",
                "result": "passed",
                "test_implementation": "def test_add_response2():\n    message = \"Hello, how are you?\"\n    response0 = Response(message=\"I am fine.\")\n    prompt = Prompt(message=message, responses=[response0])\n    response1 = Response(message=\"Great!\")\n    prompt.add_response(response1, index=0)\n    assert prompt.responses[0] == response1 and prompt.responses[1] == response0"
            },
            "tests/test_prompt.py::test_add_response3": {
                "testid": "tests/test_prompt.py::test_add_response3",
                "result": "passed",
                "test_implementation": "def test_add_response3():\n    message = \"Hello, how are you?\"\n    response0 = Response(message=\"I am fine.\")\n    prompt = Prompt(message=message, responses=[response0])\n    with pytest.raises(MemorValidationError, match=r\"Invalid response. It must be an instance of `Response`.\"):\n        prompt.add_response(1)"
            },
            "tests/test_prompt.py::test_remove_response": {
                "testid": "tests/test_prompt.py::test_remove_response",
                "result": "passed",
                "test_implementation": "def test_remove_response():\n    message = \"Hello, how are you?\"\n    response0 = Response(message=\"I am fine.\")\n    response1 = Response(message=\"Great!\")\n    prompt = Prompt(message=message, responses=[response0, response1])\n    prompt.remove_response(0)\n    assert response0 not in prompt.responses"
            },
            "tests/test_prompt.py::test_select_response": {
                "testid": "tests/test_prompt.py::test_select_response",
                "result": "passed",
                "test_implementation": "def test_select_response():\n    message = \"Hello, how are you?\"\n    response0 = Response(message=\"I am fine.\")\n    prompt = Prompt(message=message, responses=[response0])\n    response1 = Response(message=\"Great!\")\n    prompt.add_response(response1)\n    prompt.select_response(index=1)\n    assert prompt.selected_response == response1"
            },
            "tests/test_prompt.py::test_template1": {
                "testid": "tests/test_prompt.py::test_template1",
                "result": "passed",
                "test_implementation": "def test_template1():\n    message = \"Hello, how are you?\"\n    prompt = Prompt(message=message, template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD, init_check=False)\n    assert prompt.template == PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD.value"
            },
            "tests/test_prompt.py::test_template2": {
                "testid": "tests/test_prompt.py::test_template2",
                "result": "passed",
                "test_implementation": "def test_template2():\n    message = \"Hello, how are you?\"\n    prompt = Prompt(message=message, template=PresetPromptTemplate.BASIC.RESPONSE, init_check=False)\n    prompt.update_template(PresetPromptTemplate.INSTRUCTION1.PROMPT)\n    assert prompt.template.content == PresetPromptTemplate.INSTRUCTION1.PROMPT.value.content"
            },
            "tests/test_prompt.py::test_template3": {
                "testid": "tests/test_prompt.py::test_template3",
                "result": "passed",
                "test_implementation": "def test_template3():\n    message = \"Hello, how are you?\"\n    template = PromptTemplate(content=\"{message}-{response}\")\n    prompt = Prompt(message=message, template=template, init_check=False)\n    assert prompt.template.content == \"{message}-{response}\""
            },
            "tests/test_prompt.py::test_template4": {
                "testid": "tests/test_prompt.py::test_template4",
                "result": "passed",
                "test_implementation": "def test_template4():\n    message = \"Hello, how are you?\"\n    prompt = Prompt(message=message, template=None)\n    assert prompt.template == PresetPromptTemplate.DEFAULT.value"
            },
            "tests/test_prompt.py::test_template5": {
                "testid": "tests/test_prompt.py::test_template5",
                "result": "passed",
                "test_implementation": "def test_template5():\n    message = \"Hello, how are you?\"\n    prompt = Prompt(message=message, template=PresetPromptTemplate.BASIC.RESPONSE, init_check=False)\n    with pytest.raises(MemorValidationError, match=r\"Invalid template. It must be an instance of `PromptTemplate` or `PresetPromptTemplate`.\"):\n        prompt.update_template(\"{prompt_message}\")"
            },
            "tests/test_prompt.py::test_copy1": {
                "testid": "tests/test_prompt.py::test_copy1",
                "result": "passed",
                "test_implementation": "def test_copy1():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\")\n    prompt1 = Prompt(message=message, responses=[response], role=Role.USER,\n                     template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    prompt2 = copy.copy(prompt1)\n    assert id(prompt1) != id(prompt2)"
            },
            "tests/test_prompt.py::test_copy2": {
                "testid": "tests/test_prompt.py::test_copy2",
                "result": "passed",
                "test_implementation": "def test_copy2():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\")\n    prompt1 = Prompt(message=message, responses=[response], role=Role.USER,\n                     template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    prompt2 = prompt1.copy()\n    assert id(prompt1) != id(prompt2)"
            },
            "tests/test_prompt.py::test_str": {
                "testid": "tests/test_prompt.py::test_str",
                "result": "passed",
                "test_implementation": "def test_str():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\")\n    prompt = Prompt(message=message, responses=[response], role=Role.USER,\n                    template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    assert str(prompt) == prompt.render(render_format=RenderFormat.STRING)"
            },
            "tests/test_prompt.py::test_repr": {
                "testid": "tests/test_prompt.py::test_repr",
                "result": "passed",
                "test_implementation": "def test_repr():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\")\n    prompt = Prompt(message=message, responses=[response], role=Role.USER,\n                    template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    assert repr(prompt) == \"Prompt(message={message})\".format(message=prompt.message)"
            },
            "tests/test_prompt.py::test_json1": {
                "testid": "tests/test_prompt.py::test_json1",
                "result": "passed",
                "test_implementation": "def test_json1():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt1 = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    prompt1_json = prompt1.to_json()\n    prompt2 = Prompt()\n    prompt2.from_json(prompt1_json)\n    assert prompt1 == prompt2"
            },
            "tests/test_prompt.py::test_json2": {
                "testid": "tests/test_prompt.py::test_json2",
                "result": "passed",
                "test_implementation": "def test_json2():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt1 = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    prompt1_json = prompt1.to_json(save_template=False)\n    prompt2 = Prompt()\n    prompt2.from_json(prompt1_json)\n    assert prompt1 != prompt2 and prompt1.template == PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD.value and prompt2.template == PresetPromptTemplate.DEFAULT.value"
            },
            "tests/test_prompt.py::test_json3": {
                "testid": "tests/test_prompt.py::test_json3",
                "result": "passed",
                "test_implementation": "def test_json3():\n    prompt = Prompt()\n    with pytest.raises(MemorValidationError, match=r\"Invalid prompt structure. It should be a JSON object with proper fields.\"):\n        prompt.from_json(\"{}\")"
            },
            "tests/test_prompt.py::test_save1": {
                "testid": "tests/test_prompt.py::test_save1",
                "result": "passed",
                "test_implementation": "def test_save1():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    result = prompt.save(\"f:/\")\n    assert result[\"status\"] == False"
            },
            "tests/test_prompt.py::test_save2": {
                "testid": "tests/test_prompt.py::test_save2",
                "result": "passed",
                "test_implementation": "def test_save2():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt1 = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    result = prompt1.save(\"prompt_test1.json\")\n    prompt2 = Prompt(file_path=\"prompt_test1.json\")\n    assert result[\"status\"] and prompt1 == prompt2"
            },
            "tests/test_prompt.py::test_load1": {
                "testid": "tests/test_prompt.py::test_load1",
                "result": "passed",
                "test_implementation": "def test_load1():\n    with pytest.raises(MemorValidationError, match=r\"Invalid path. Path must be a string.\"):\n        _ = Prompt(file_path=22)"
            },
            "tests/test_prompt.py::test_load2": {
                "testid": "tests/test_prompt.py::test_load2",
                "result": "passed",
                "test_implementation": "def test_load2():\n    with pytest.raises(FileNotFoundError, match=r\"Path prompt_test10.json does not exist.\"):\n        _ = Prompt(file_path=\"prompt_test10.json\")"
            },
            "tests/test_prompt.py::test_save3": {
                "testid": "tests/test_prompt.py::test_save3",
                "result": "passed",
                "test_implementation": "def test_save3():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt1 = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    result = prompt1.save(\"prompt_test2.json\", save_template=False)\n    prompt2 = Prompt(file_path=\"prompt_test2.json\")\n    assert result[\"status\"] and prompt1 != prompt2 and prompt1.template == PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD.value and prompt2.template == PresetPromptTemplate.DEFAULT.value"
            },
            "tests/test_prompt.py::test_render1": {
                "testid": "tests/test_prompt.py::test_render1",
                "result": "passed",
                "test_implementation": "def test_render1():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT)\n    assert prompt.render() == \"Hello, how are you?\""
            },
            "tests/test_prompt.py::test_render2": {
                "testid": "tests/test_prompt.py::test_render2",
                "result": "passed",
                "test_implementation": "def test_render2():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT)\n    assert prompt.render(RenderFormat.OPENAI) == {\"role\": \"user\", \"content\": \"Hello, how are you?\"}"
            },
            "tests/test_prompt.py::test_render3": {
                "testid": "tests/test_prompt.py::test_render3",
                "result": "passed",
                "test_implementation": "def test_render3():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT)\n    assert prompt.render(RenderFormat.DICTIONARY)[\"content\"] == \"Hello, how are you?\""
            },
            "tests/test_prompt.py::test_render4": {
                "testid": "tests/test_prompt.py::test_render4",
                "result": "passed",
                "test_implementation": "def test_render4():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT)\n    assert (\"content\", \"Hello, how are you?\") in prompt.render(RenderFormat.ITEMS)"
            },
            "tests/test_prompt.py::test_render5": {
                "testid": "tests/test_prompt.py::test_render5",
                "result": "passed",
                "test_implementation": "def test_render5():\n    message = \"How are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    template = PromptTemplate(content=\"{instruction}, {prompt[message]}\", custom_map={\"instruction\": \"Hi\"})\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=template)\n    assert prompt.render(RenderFormat.OPENAI) == {\"role\": \"user\", \"content\": \"Hi, How are you?\"}"
            },
            "tests/test_prompt.py::test_render6": {
                "testid": "tests/test_prompt.py::test_render6",
                "result": "passed",
                "test_implementation": "def test_render6():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    template = PromptTemplate(content=\"{response[2][message]}\")\n    prompt = Prompt(\n        message=message,\n        responses=[response],\n        role=Role.USER,\n        template=template,\n        init_check=False)\n    with pytest.raises(MemorRenderError, match=r\"Prompt template and properties are incompatible.\"):\n        prompt.render()"
            },
            "tests/test_prompt.py::test_init_check": {
                "testid": "tests/test_prompt.py::test_init_check",
                "result": "passed",
                "test_implementation": "def test_init_check():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    template = PromptTemplate(content=\"{response[2][message]}\")\n    with pytest.raises(MemorRenderError, match=r\"Prompt template and properties are incompatible.\"):\n        _ = Prompt(message=message, responses=[response], role=Role.USER, template=template)"
            },
            "tests/test_prompt.py::test_check_render1": {
                "testid": "tests/test_prompt.py::test_check_render1",
                "result": "passed",
                "test_implementation": "def test_check_render1():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    template = PromptTemplate(content=\"{response[2][message]}\")\n    prompt = Prompt(\n        message=message,\n        responses=[response],\n        role=Role.USER,\n        template=template,\n        init_check=False)\n    assert not prompt.check_render()"
            },
            "tests/test_prompt.py::test_check_render2": {
                "testid": "tests/test_prompt.py::test_check_render2",
                "result": "passed",
                "test_implementation": "def test_check_render2():\n    message = \"How are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    template = PromptTemplate(content=\"{instruction}, {prompt[message]}\", custom_map={\"instruction\": \"Hi\"})\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=template)\n    assert prompt.check_render()"
            },
            "tests/test_prompt.py::test_equality1": {
                "testid": "tests/test_prompt.py::test_equality1",
                "result": "passed",
                "test_implementation": "def test_equality1():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt1 = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    prompt2 = prompt1.copy()\n    assert prompt1 == prompt2"
            },
            "tests/test_prompt.py::test_equality2": {
                "testid": "tests/test_prompt.py::test_equality2",
                "result": "passed",
                "test_implementation": "def test_equality2():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt1 = Prompt(message=message, responses=[response1], role=Role.USER,\n                     template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    prompt2 = Prompt(message=message, responses=[response2], role=Role.USER,\n                     template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    assert prompt1 != prompt2"
            },
            "tests/test_prompt.py::test_equality3": {
                "testid": "tests/test_prompt.py::test_equality3",
                "result": "passed",
                "test_implementation": "def test_equality3():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt1 = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    prompt2 = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    assert prompt1 == prompt2"
            },
            "tests/test_prompt.py::test_equality4": {
                "testid": "tests/test_prompt.py::test_equality4",
                "result": "passed",
                "test_implementation": "def test_equality4():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    assert prompt != 2"
            },
            "tests/test_prompt.py::test_length1": {
                "testid": "tests/test_prompt.py::test_length1",
                "result": "passed",
                "test_implementation": "def test_length1():\n    prompt = Prompt(message=\"Hello, how are you?\")\n    assert len(prompt) == 19"
            },
            "tests/test_prompt.py::test_length2": {
                "testid": "tests/test_prompt.py::test_length2",
                "result": "passed",
                "test_implementation": "def test_length2():\n    message = \"Hello, how are you?\"\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    template = PromptTemplate(content=\"{response[2][message]}\")\n    prompt = Prompt(\n        message=message,\n        responses=[response],\n        role=Role.USER,\n        template=template,\n        init_check=False)\n    assert len(prompt) == 0"
            },
            "tests/test_prompt.py::test_length3": {
                "testid": "tests/test_prompt.py::test_length3",
                "result": "passed",
                "test_implementation": "def test_length3():\n    prompt = Prompt()\n    assert len(prompt) == 0"
            },
            "tests/test_prompt.py::test_date_modified": {
                "testid": "tests/test_prompt.py::test_date_modified",
                "result": "passed",
                "test_implementation": "def test_date_modified():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    assert isinstance(prompt.date_modified, datetime.datetime)"
            },
            "tests/test_prompt.py::test_date_created": {
                "testid": "tests/test_prompt.py::test_date_created",
                "result": "passed",
                "test_implementation": "def test_date_created():\n    message = \"Hello, how are you?\"\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"Thanks!\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    prompt = Prompt(\n        message=message,\n        responses=[\n            response1,\n            response2],\n        role=Role.USER,\n        template=PresetPromptTemplate.BASIC.PROMPT_RESPONSE_STANDARD)\n    assert isinstance(prompt.date_created, datetime.datetime)"
            },
            "tests/test_prompt_template.py::test_title1": {
                "testid": "tests/test_prompt_template.py::test_title1",
                "result": "passed",
                "test_implementation": "def test_title1():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    assert template.title is None"
            },
            "tests/test_prompt_template.py::test_title2": {
                "testid": "tests/test_prompt_template.py::test_title2",
                "result": "passed",
                "test_implementation": "def test_title2():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    template.update_title(\"template1\")\n    assert template.title == \"template1\""
            },
            "tests/test_prompt_template.py::test_title3": {
                "testid": "tests/test_prompt_template.py::test_title3",
                "result": "passed",
                "test_implementation": "def test_title3():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"},\n        title=None)\n    assert template.title is None"
            },
            "tests/test_prompt_template.py::test_title4": {
                "testid": "tests/test_prompt_template.py::test_title4",
                "result": "passed",
                "test_implementation": "def test_title4():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"},\n        title=None)\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `title` must be a string.\"):\n        template.update_title(25)"
            },
            "tests/test_prompt_template.py::test_content1": {
                "testid": "tests/test_prompt_template.py::test_content1",
                "result": "passed",
                "test_implementation": "def test_content1():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    assert template.content == \"Act as a {language} developer and respond to this question:\\n{prompt_message}\""
            },
            "tests/test_prompt_template.py::test_content2": {
                "testid": "tests/test_prompt_template.py::test_content2",
                "result": "passed",
                "test_implementation": "def test_content2():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    template.update_content(content=\"Act as a {language} developer and respond to this query:\\n{prompt_message}\")\n    assert template.content == \"Act as a {language} developer and respond to this query:\\n{prompt_message}\""
            },
            "tests/test_prompt_template.py::test_content3": {
                "testid": "tests/test_prompt_template.py::test_content3",
                "result": "passed",
                "test_implementation": "def test_content3():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `content` must be a string.\"):\n        template.update_content(content=22)"
            },
            "tests/test_prompt_template.py::test_custom_map1": {
                "testid": "tests/test_prompt_template.py::test_custom_map1",
                "result": "passed",
                "test_implementation": "def test_custom_map1():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    assert template.custom_map == {\"language\": \"Python\"}"
            },
            "tests/test_prompt_template.py::test_custom_map2": {
                "testid": "tests/test_prompt_template.py::test_custom_map2",
                "result": "passed",
                "test_implementation": "def test_custom_map2():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    template.update_map({\"language\": \"C++\"})\n    assert template.custom_map == {\"language\": \"C++\"}"
            },
            "tests/test_prompt_template.py::test_custom_map3": {
                "testid": "tests/test_prompt_template.py::test_custom_map3",
                "result": "passed",
                "test_implementation": "def test_custom_map3():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    with pytest.raises(MemorValidationError, match=r\"Invalid custom map: it must be a dictionary with keys and values that can be converted to strings.\"):\n        template.update_map([\"C++\"])"
            },
            "tests/test_prompt_template.py::test_date_modified": {
                "testid": "tests/test_prompt_template.py::test_date_modified",
                "result": "passed",
                "test_implementation": "def test_date_modified():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    assert isinstance(template.date_modified, datetime.datetime)"
            },
            "tests/test_prompt_template.py::test_date_created": {
                "testid": "tests/test_prompt_template.py::test_date_created",
                "result": "passed",
                "test_implementation": "def test_date_created():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    assert isinstance(template.date_created, datetime.datetime)"
            },
            "tests/test_prompt_template.py::test_json1": {
                "testid": "tests/test_prompt_template.py::test_json1",
                "result": "passed",
                "test_implementation": "def test_json1():\n    template1 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    template1_json = template1.to_json()\n    template2 = PromptTemplate()\n    template2.from_json(template1_json)\n    assert template1 == template2"
            },
            "tests/test_prompt_template.py::test_json2": {
                "testid": "tests/test_prompt_template.py::test_json2",
                "result": "passed",
                "test_implementation": "def test_json2():\n    template = PromptTemplate()\n    with pytest.raises(MemorValidationError, match=r\"Invalid template structure. It should be a JSON object with proper fields.\"):\n        template.from_json(\"{}\")"
            },
            "tests/test_prompt_template.py::test_save1": {
                "testid": "tests/test_prompt_template.py::test_save1",
                "result": "passed",
                "test_implementation": "def test_save1():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    result = template.save(\"template_test1.json\")\n    with open(\"template_test1.json\", \"r\") as file:\n        saved_template = json.loads(file.read())\n    assert result[\"status\"] and template.to_json() == saved_template"
            },
            "tests/test_prompt_template.py::test_save2": {
                "testid": "tests/test_prompt_template.py::test_save2",
                "result": "passed",
                "test_implementation": "def test_save2():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    result = template.save(\"f:/\")\n    assert result[\"status\"] == False"
            },
            "tests/test_prompt_template.py::test_load1": {
                "testid": "tests/test_prompt_template.py::test_load1",
                "result": "passed",
                "test_implementation": "def test_load1():\n    template1 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    result = template1.save(\"template_test2.json\")\n    template2 = PromptTemplate(file_path=\"template_test2.json\")\n    assert result[\"status\"] and template1 == template2"
            },
            "tests/test_prompt_template.py::test_load2": {
                "testid": "tests/test_prompt_template.py::test_load2",
                "result": "passed",
                "test_implementation": "def test_load2():\n    with pytest.raises(MemorValidationError, match=r\"Invalid path. Path must be a string.\"):\n        _ = PromptTemplate(file_path=22)"
            },
            "tests/test_prompt_template.py::test_load3": {
                "testid": "tests/test_prompt_template.py::test_load3",
                "result": "passed",
                "test_implementation": "def test_load3():\n    with pytest.raises(FileNotFoundError, match=r\"Path template_test10.json does not exist.\"):\n        _ = PromptTemplate(file_path=\"template_test10.json\")"
            },
            "tests/test_prompt_template.py::test_copy1": {
                "testid": "tests/test_prompt_template.py::test_copy1",
                "result": "passed",
                "test_implementation": "def test_copy1():\n    template1 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    template2 = copy.copy(template1)\n    assert id(template1) != id(template2)"
            },
            "tests/test_prompt_template.py::test_copy2": {
                "testid": "tests/test_prompt_template.py::test_copy2",
                "result": "passed",
                "test_implementation": "def test_copy2():\n    template1 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    template2 = template1.copy()\n    assert id(template1) != id(template2)"
            },
            "tests/test_prompt_template.py::test_str": {
                "testid": "tests/test_prompt_template.py::test_str",
                "result": "passed",
                "test_implementation": "def test_str():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    assert str(template) == template.content"
            },
            "tests/test_prompt_template.py::test_repr": {
                "testid": "tests/test_prompt_template.py::test_repr",
                "result": "passed",
                "test_implementation": "def test_repr():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    assert repr(template) == \"PromptTemplate(content={content})\".format(content=template.content)"
            },
            "tests/test_prompt_template.py::test_equality1": {
                "testid": "tests/test_prompt_template.py::test_equality1",
                "result": "passed",
                "test_implementation": "def test_equality1():\n    template1 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"})\n    template2 = template1.copy()\n    assert template1 == template2"
            },
            "tests/test_prompt_template.py::test_equality2": {
                "testid": "tests/test_prompt_template.py::test_equality2",
                "result": "passed",
                "test_implementation": "def test_equality2():\n    template1 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"},\n        title=\"template1\")\n    template2 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"},\n        title=\"template2\")\n    assert template1 != template2"
            },
            "tests/test_prompt_template.py::test_equality3": {
                "testid": "tests/test_prompt_template.py::test_equality3",
                "result": "passed",
                "test_implementation": "def test_equality3():\n    template1 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"},\n        title=\"template1\")\n    template2 = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"},\n        title=\"template1\")\n    assert template1 == template2"
            },
            "tests/test_prompt_template.py::test_equality4": {
                "testid": "tests/test_prompt_template.py::test_equality4",
                "result": "passed",
                "test_implementation": "def test_equality4():\n    template = PromptTemplate(\n        content=\"Act as a {language} developer and respond to this question:\\n{prompt_message}\",\n        custom_map={\n            \"language\": \"Python\"},\n        title=\"template1\")\n    assert template != 2"
            },
            "tests/test_response.py::test_message1": {
                "testid": "tests/test_response.py::test_message1",
                "result": "passed",
                "test_implementation": "def test_message1():\n    response = Response(message=\"I am fine.\")\n    assert response.message == \"I am fine.\""
            },
            "tests/test_response.py::test_message2": {
                "testid": "tests/test_response.py::test_message2",
                "result": "passed",
                "test_implementation": "def test_message2():\n    response = Response(message=\"I am fine.\")\n    response.update_message(\"OK!\")\n    assert response.message == \"OK!\""
            },
            "tests/test_response.py::test_message3": {
                "testid": "tests/test_response.py::test_message3",
                "result": "passed",
                "test_implementation": "def test_message3():\n    response = Response(message=\"I am fine.\")\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `message` must be a string.\"):\n        response.update_message(22)"
            },
            "tests/test_response.py::test_tokens1": {
                "testid": "tests/test_response.py::test_tokens1",
                "result": "passed",
                "test_implementation": "def test_tokens1():\n    response = Response(message=\"I am fine.\")\n    assert response.tokens is None"
            },
            "tests/test_response.py::test_tokens2": {
                "testid": "tests/test_response.py::test_tokens2",
                "result": "passed",
                "test_implementation": "def test_tokens2():\n    response = Response(message=\"I am fine.\", tokens=4)\n    assert response.tokens == 4"
            },
            "tests/test_response.py::test_tokens3": {
                "testid": "tests/test_response.py::test_tokens3",
                "result": "passed",
                "test_implementation": "def test_tokens3():\n    response = Response(message=\"I am fine.\", tokens=4)\n    response.update_tokens(6)\n    assert response.tokens == 6"
            },
            "tests/test_response.py::test_estimated_tokens1": {
                "testid": "tests/test_response.py::test_estimated_tokens1",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens1():\n    response = Response(message=\"I am fine.\")\n    assert response.estimate_tokens(TokensEstimator.UNIVERSAL) == 5"
            },
            "tests/test_response.py::test_estimated_tokens2": {
                "testid": "tests/test_response.py::test_estimated_tokens2",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens2():\n    response = Response(message=\"I am fine.\")\n    assert response.estimate_tokens(TokensEstimator.OPENAI_GPT_3_5) == 4"
            },
            "tests/test_response.py::test_estimated_tokens3": {
                "testid": "tests/test_response.py::test_estimated_tokens3",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens3():\n    response = Response(message=\"I am fine.\")\n    assert response.estimate_tokens(TokensEstimator.OPENAI_GPT_4) == 4"
            },
            "tests/test_response.py::test_tokens4": {
                "testid": "tests/test_response.py::test_tokens4",
                "result": "passed",
                "test_implementation": "def test_tokens4():\n    response = Response(message=\"I am fine.\", tokens=4)\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `tokens` must be a positive integer.\"):\n        response.update_tokens(-2)"
            },
            "tests/test_response.py::test_inference_time1": {
                "testid": "tests/test_response.py::test_inference_time1",
                "result": "passed",
                "test_implementation": "def test_inference_time1():\n    response = Response(message=\"I am fine.\")\n    assert response.inference_time is None"
            },
            "tests/test_response.py::test_inference_time2": {
                "testid": "tests/test_response.py::test_inference_time2",
                "result": "passed",
                "test_implementation": "def test_inference_time2():\n    response = Response(message=\"I am fine.\", inference_time=8.2)\n    assert response.inference_time == 8.2"
            },
            "tests/test_response.py::test_inference_time3": {
                "testid": "tests/test_response.py::test_inference_time3",
                "result": "passed",
                "test_implementation": "def test_inference_time3():\n    response = Response(message=\"I am fine.\", inference_time=8.2)\n    response.update_inference_time(9.5)\n    assert response.inference_time == 9.5"
            },
            "tests/test_response.py::test_inference_time4": {
                "testid": "tests/test_response.py::test_inference_time4",
                "result": "passed",
                "test_implementation": "def test_inference_time4():\n    response = Response(message=\"I am fine.\", inference_time=8.2)\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `inference_time` must be a positive float.\"):\n        response.update_inference_time(-5)"
            },
            "tests/test_response.py::test_score1": {
                "testid": "tests/test_response.py::test_score1",
                "result": "passed",
                "test_implementation": "def test_score1():\n    response = Response(message=\"I am fine.\", score=0.9)\n    assert response.score == 0.9"
            },
            "tests/test_response.py::test_score2": {
                "testid": "tests/test_response.py::test_score2",
                "result": "passed",
                "test_implementation": "def test_score2():\n    response = Response(message=\"I am fine.\", score=0.9)\n    response.update_score(0.5)\n    assert response.score == 0.5"
            },
            "tests/test_response.py::test_score3": {
                "testid": "tests/test_response.py::test_score3",
                "result": "passed",
                "test_implementation": "def test_score3():\n    response = Response(message=\"I am fine.\", score=0.9)\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `score` must be a value between 0 and 1.\"):\n        response.update_score(-2)"
            },
            "tests/test_response.py::test_role1": {
                "testid": "tests/test_response.py::test_role1",
                "result": "passed",
                "test_implementation": "def test_role1():\n    response = Response(message=\"I am fine.\", role=Role.ASSISTANT)\n    assert response.role == Role.ASSISTANT"
            },
            "tests/test_response.py::test_role2": {
                "testid": "tests/test_response.py::test_role2",
                "result": "passed",
                "test_implementation": "def test_role2():\n    response = Response(message=\"I am fine.\", role=Role.ASSISTANT)\n    response.update_role(Role.USER)\n    assert response.role == Role.USER"
            },
            "tests/test_response.py::test_role3": {
                "testid": "tests/test_response.py::test_role3",
                "result": "passed",
                "test_implementation": "def test_role3():\n    response = Response(message=\"I am fine.\", role=None)\n    assert response.role == Role.ASSISTANT"
            },
            "tests/test_response.py::test_role4": {
                "testid": "tests/test_response.py::test_role4",
                "result": "passed",
                "test_implementation": "def test_role4():\n    response = Response(message=\"I am fine.\", role=Role.ASSISTANT)\n    with pytest.raises(MemorValidationError, match=r\"Invalid role. It must be an instance of Role enum.\"):\n        response.update_role(2)"
            },
            "tests/test_response.py::test_temperature1": {
                "testid": "tests/test_response.py::test_temperature1",
                "result": "passed",
                "test_implementation": "def test_temperature1():\n    response = Response(message=\"I am fine.\", temperature=0.2)\n    assert response.temperature == 0.2"
            },
            "tests/test_response.py::test_temperature2": {
                "testid": "tests/test_response.py::test_temperature2",
                "result": "passed",
                "test_implementation": "def test_temperature2():\n    response = Response(message=\"I am fine.\", temperature=0.2)\n    response.update_temperature(0.7)\n    assert response.temperature == 0.7"
            },
            "tests/test_response.py::test_temperature3": {
                "testid": "tests/test_response.py::test_temperature3",
                "result": "passed",
                "test_implementation": "def test_temperature3():\n    response = Response(message=\"I am fine.\", temperature=0.2)\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `temperature` must be a positive float.\"):\n        response.update_temperature(-22)"
            },
            "tests/test_response.py::test_model1": {
                "testid": "tests/test_response.py::test_model1",
                "result": "passed",
                "test_implementation": "def test_model1():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\")\n    assert response.model == \"GPT-4\""
            },
            "tests/test_response.py::test_model2": {
                "testid": "tests/test_response.py::test_model2",
                "result": "passed",
                "test_implementation": "def test_model2():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\")\n    response.update_model(\"GPT-4o\")\n    assert response.model == \"GPT-4o\""
            },
            "tests/test_response.py::test_model3": {
                "testid": "tests/test_response.py::test_model3",
                "result": "passed",
                "test_implementation": "def test_model3():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\")\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `model` must be a string.\"):\n        response.update_model(4)"
            },
            "tests/test_response.py::test_date1": {
                "testid": "tests/test_response.py::test_date1",
                "result": "passed",
                "test_implementation": "def test_date1():\n    date_time_utc = datetime.datetime.now(datetime.timezone.utc)\n    response = Response(message=\"I am fine.\", date=date_time_utc)\n    assert response.date_created == date_time_utc"
            },
            "tests/test_response.py::test_date2": {
                "testid": "tests/test_response.py::test_date2",
                "result": "passed",
                "test_implementation": "def test_date2():\n    response = Response(message=\"I am fine.\", date=None)\n    assert isinstance(response.date_created, datetime.datetime)"
            },
            "tests/test_response.py::test_date3": {
                "testid": "tests/test_response.py::test_date3",
                "result": "passed",
                "test_implementation": "def test_date3():\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `date` must be a datetime object that includes timezone information.\"):\n        _ = Response(message=\"I am fine.\", date=\"2/25/2025\")"
            },
            "tests/test_response.py::test_date4": {
                "testid": "tests/test_response.py::test_date4",
                "result": "passed",
                "test_implementation": "def test_date4():\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `date` must be a datetime object that includes timezone information.\"):\n        _ = Response(message=\"I am fine.\", date=datetime.datetime.now())"
            },
            "tests/test_response.py::test_json1": {
                "testid": "tests/test_response.py::test_json1",
                "result": "passed",
                "test_implementation": "def test_json1():\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response1_json = response1.to_json()\n    response2 = Response()\n    response2.from_json(response1_json)\n    assert response1 == response2"
            },
            "tests/test_response.py::test_json2": {
                "testid": "tests/test_response.py::test_json2",
                "result": "passed",
                "test_implementation": "def test_json2():\n    response = Response()\n    with pytest.raises(MemorValidationError, match=r\"Invalid response structure. It should be a JSON object with proper fields.\"):\n        response.from_json(\"{}\")"
            },
            "tests/test_response.py::test_save1": {
                "testid": "tests/test_response.py::test_save1",
                "result": "passed",
                "test_implementation": "def test_save1():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    result = response.save(\"response_test1.json\")\n    with open(\"response_test1.json\", \"r\") as file:\n        saved_response = json.loads(file.read())\n    assert result[\"status\"] and response.to_json() == saved_response"
            },
            "tests/test_response.py::test_save2": {
                "testid": "tests/test_response.py::test_save2",
                "result": "passed",
                "test_implementation": "def test_save2():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    result = response.save(\"f:/\")\n    assert result[\"status\"] == False"
            },
            "tests/test_response.py::test_load1": {
                "testid": "tests/test_response.py::test_load1",
                "result": "passed",
                "test_implementation": "def test_load1():\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    result = response1.save(\"response_test2.json\")\n    response2 = Response(file_path=\"response_test2.json\")\n    assert result[\"status\"] and response1 == response2"
            },
            "tests/test_response.py::test_load2": {
                "testid": "tests/test_response.py::test_load2",
                "result": "passed",
                "test_implementation": "def test_load2():\n    with pytest.raises(MemorValidationError, match=r\"Invalid path. Path must be a string.\"):\n        response = Response(file_path=2)"
            },
            "tests/test_response.py::test_load3": {
                "testid": "tests/test_response.py::test_load3",
                "result": "passed",
                "test_implementation": "def test_load3():\n    with pytest.raises(FileNotFoundError, match=r\"Path response_test10.json does not exist.\"):\n        response = Response(file_path=\"response_test10.json\")"
            },
            "tests/test_response.py::test_copy1": {
                "testid": "tests/test_response.py::test_copy1",
                "result": "passed",
                "test_implementation": "def test_copy1():\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = copy.copy(response1)\n    assert id(response1) != id(response2)"
            },
            "tests/test_response.py::test_copy2": {
                "testid": "tests/test_response.py::test_copy2",
                "result": "passed",
                "test_implementation": "def test_copy2():\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = response1.copy()\n    assert id(response1) != id(response2)"
            },
            "tests/test_response.py::test_str": {
                "testid": "tests/test_response.py::test_str",
                "result": "passed",
                "test_implementation": "def test_str():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    assert str(response) == response.message"
            },
            "tests/test_response.py::test_repr": {
                "testid": "tests/test_response.py::test_repr",
                "result": "passed",
                "test_implementation": "def test_repr():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    assert repr(response) == \"Response(message={message})\".format(message=response.message)"
            },
            "tests/test_response.py::test_render1": {
                "testid": "tests/test_response.py::test_render1",
                "result": "passed",
                "test_implementation": "def test_render1():\n    response = Response(message=\"I am fine.\")\n    assert response.render() == \"I am fine.\""
            },
            "tests/test_response.py::test_render2": {
                "testid": "tests/test_response.py::test_render2",
                "result": "passed",
                "test_implementation": "def test_render2():\n    response = Response(message=\"I am fine.\")\n    assert response.render(RenderFormat.OPENAI) == {\"role\": \"assistant\", \"content\": \"I am fine.\"}"
            },
            "tests/test_response.py::test_render3": {
                "testid": "tests/test_response.py::test_render3",
                "result": "passed",
                "test_implementation": "def test_render3():\n    response = Response(message=\"I am fine.\")\n    assert response.render(RenderFormat.DICTIONARY) == response.to_dict()"
            },
            "tests/test_response.py::test_render4": {
                "testid": "tests/test_response.py::test_render4",
                "result": "passed",
                "test_implementation": "def test_render4():\n    response = Response(message=\"I am fine.\")\n    assert response.render(RenderFormat.ITEMS) == response.to_dict().items()"
            },
            "tests/test_response.py::test_render5": {
                "testid": "tests/test_response.py::test_render5",
                "result": "passed",
                "test_implementation": "def test_render5():\n    response = Response(message=\"I am fine.\")\n    with pytest.raises(MemorValidationError, match=r\"Invalid render format. It must be an instance of RenderFormat enum.\"):\n        response.render(\"OPENAI\")"
            },
            "tests/test_response.py::test_equality1": {
                "testid": "tests/test_response.py::test_equality1",
                "result": "passed",
                "test_implementation": "def test_equality1():\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = response1.copy()\n    assert response1 == response2"
            },
            "tests/test_response.py::test_equality2": {
                "testid": "tests/test_response.py::test_equality2",
                "result": "passed",
                "test_implementation": "def test_equality2():\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.6)\n    assert response1 != response2"
            },
            "tests/test_response.py::test_equality3": {
                "testid": "tests/test_response.py::test_equality3",
                "result": "passed",
                "test_implementation": "def test_equality3():\n    response1 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    response2 = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    assert response1 == response2"
            },
            "tests/test_response.py::test_equality4": {
                "testid": "tests/test_response.py::test_equality4",
                "result": "passed",
                "test_implementation": "def test_equality4():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    assert response != 2"
            },
            "tests/test_response.py::test_length1": {
                "testid": "tests/test_response.py::test_length1",
                "result": "passed",
                "test_implementation": "def test_length1():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    assert len(response) == 10"
            },
            "tests/test_response.py::test_length2": {
                "testid": "tests/test_response.py::test_length2",
                "result": "passed",
                "test_implementation": "def test_length2():\n    response = Response()\n    assert len(response) == 0"
            },
            "tests/test_response.py::test_date_modified": {
                "testid": "tests/test_response.py::test_date_modified",
                "result": "passed",
                "test_implementation": "def test_date_modified():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    assert isinstance(response.date_modified, datetime.datetime)"
            },
            "tests/test_response.py::test_date_created": {
                "testid": "tests/test_response.py::test_date_created",
                "result": "passed",
                "test_implementation": "def test_date_created():\n    response = Response(message=\"I am fine.\", model=\"GPT-4\", temperature=0.5, role=Role.USER, score=0.8)\n    assert isinstance(response.date_created, datetime.datetime)"
            },
            "tests/test_session.py::test_title1": {
                "testid": "tests/test_session.py::test_title1",
                "result": "passed",
                "test_implementation": "def test_title1():\n    session = Session(title=\"session1\")\n    assert session.title == \"session1\""
            },
            "tests/test_session.py::test_title2": {
                "testid": "tests/test_session.py::test_title2",
                "result": "passed",
                "test_implementation": "def test_title2():\n    session = Session(title=\"session1\")\n    session.update_title(\"session2\")\n    assert session.title == \"session2\""
            },
            "tests/test_session.py::test_title3": {
                "testid": "tests/test_session.py::test_title3",
                "result": "passed",
                "test_implementation": "def test_title3():\n    session = Session(title=\"session1\")\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `title` must be a string.\"):\n        session.update_title(2)"
            },
            "tests/test_session.py::test_messages1": {
                "testid": "tests/test_session.py::test_messages1",
                "result": "passed",
                "test_implementation": "def test_messages1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    assert session.messages == [prompt, response]"
            },
            "tests/test_session.py::test_messages2": {
                "testid": "tests/test_session.py::test_messages2",
                "result": "passed",
                "test_implementation": "def test_messages2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.update_messages([prompt, response, prompt, response])\n    assert session.messages == [prompt, response, prompt, response]"
            },
            "tests/test_session.py::test_messages3": {
                "testid": "tests/test_session.py::test_messages3",
                "result": "passed",
                "test_implementation": "def test_messages3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    session = Session(messages=[prompt])\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `messages` must be a list of `Prompt` or `Response`.\"):\n        session.update_messages([prompt, \"I am fine.\"])"
            },
            "tests/test_session.py::test_messages4": {
                "testid": "tests/test_session.py::test_messages4",
                "result": "passed",
                "test_implementation": "def test_messages4():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    session = Session(messages=[prompt])\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `messages` must be a list of `Prompt` or `Response`.\"):\n        session.update_messages(\"I am fine.\")"
            },
            "tests/test_session.py::test_messages_status1": {
                "testid": "tests/test_session.py::test_messages_status1",
                "result": "passed",
                "test_implementation": "def test_messages_status1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    assert session.messages_status == [True, True]"
            },
            "tests/test_session.py::test_messages_status2": {
                "testid": "tests/test_session.py::test_messages_status2",
                "result": "passed",
                "test_implementation": "def test_messages_status2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.update_messages_status([False, True])\n    assert session.messages_status == [False, True]"
            },
            "tests/test_session.py::test_messages_status3": {
                "testid": "tests/test_session.py::test_messages_status3",
                "result": "passed",
                "test_implementation": "def test_messages_status3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `status` must be a list of booleans.\"):\n        session.update_messages_status([\"False\", True])"
            },
            "tests/test_session.py::test_messages_status4": {
                "testid": "tests/test_session.py::test_messages_status4",
                "result": "passed",
                "test_implementation": "def test_messages_status4():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    with pytest.raises(MemorValidationError, match=r\"Invalid message status length. It must be equal to the number of messages.\"):\n        session.update_messages_status([False, True, True])"
            },
            "tests/test_session.py::test_enable_message": {
                "testid": "tests/test_session.py::test_enable_message",
                "result": "passed",
                "test_implementation": "def test_enable_message():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.update_messages_status([False, False])\n    session.enable_message(0)\n    assert session.messages_status == [True, False]"
            },
            "tests/test_session.py::test_disable_message": {
                "testid": "tests/test_session.py::test_disable_message",
                "result": "passed",
                "test_implementation": "def test_disable_message():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.update_messages_status([True, True])\n    session.disable_message(0)\n    assert session.messages_status == [False, True]"
            },
            "tests/test_session.py::test_mask_message": {
                "testid": "tests/test_session.py::test_mask_message",
                "result": "passed",
                "test_implementation": "def test_mask_message():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.mask_message(0)\n    assert session.messages_status == [False, True]"
            },
            "tests/test_session.py::test_unmask_message": {
                "testid": "tests/test_session.py::test_unmask_message",
                "result": "passed",
                "test_implementation": "def test_unmask_message():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.update_messages_status([False, False])\n    session.unmask_message(0)\n    assert session.messages_status == [True, False]"
            },
            "tests/test_session.py::test_masks": {
                "testid": "tests/test_session.py::test_masks",
                "result": "passed",
                "test_implementation": "def test_masks():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.update_messages_status([False, True])\n    assert session.masks == [True, False]"
            },
            "tests/test_session.py::test_add_message1": {
                "testid": "tests/test_session.py::test_add_message1",
                "result": "passed",
                "test_implementation": "def test_add_message1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.add_message(Response(\"Good!\"))\n    assert session.messages[2] == Response(\"Good!\")"
            },
            "tests/test_session.py::test_add_message2": {
                "testid": "tests/test_session.py::test_add_message2",
                "result": "passed",
                "test_implementation": "def test_add_message2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.add_message(message=Response(\"Good!\"), status=False, index=0)\n    assert session.messages[0] == Response(\"Good!\") and session.messages_status[0] == False"
            },
            "tests/test_session.py::test_add_message3": {
                "testid": "tests/test_session.py::test_add_message3",
                "result": "passed",
                "test_implementation": "def test_add_message3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    with pytest.raises(MemorValidationError, match=r\"Invalid message. It must be an instance of `Prompt` or `Response`.\"):\n        session.add_message(message=\"Good!\", status=False, index=0)"
            },
            "tests/test_session.py::test_add_message4": {
                "testid": "tests/test_session.py::test_add_message4",
                "result": "passed",
                "test_implementation": "def test_add_message4():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    with pytest.raises(MemorValidationError, match=r\"Invalid value. `status` must be a boolean.\"):\n        session.add_message(message=prompt, status=\"False\", index=0)"
            },
            "tests/test_session.py::test_remove_message": {
                "testid": "tests/test_session.py::test_remove_message",
                "result": "passed",
                "test_implementation": "def test_remove_message():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    session.remove_message(1)\n    assert session.messages == [prompt] and session.messages_status == [True]"
            },
            "tests/test_session.py::test_clear_messages": {
                "testid": "tests/test_session.py::test_clear_messages",
                "result": "passed",
                "test_implementation": "def test_clear_messages():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response])\n    assert len(session) == 2\n    session.clear_messages()\n    assert len(session) == 0"
            },
            "tests/test_session.py::test_copy1": {
                "testid": "tests/test_session.py::test_copy1",
                "result": "passed",
                "test_implementation": "def test_copy1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response], title=\"session\")\n    session2 = copy.copy(session1)\n    assert id(session1) != id(session2)"
            },
            "tests/test_session.py::test_copy2": {
                "testid": "tests/test_session.py::test_copy2",
                "result": "passed",
                "test_implementation": "def test_copy2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response], title=\"session\")\n    session2 = session1.copy()\n    assert id(session1) != id(session2)"
            },
            "tests/test_session.py::test_str": {
                "testid": "tests/test_session.py::test_str",
                "result": "passed",
                "test_implementation": "def test_str():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert str(session) == session.render(render_format=RenderFormat.STRING)"
            },
            "tests/test_session.py::test_repr": {
                "testid": "tests/test_session.py::test_repr",
                "result": "passed",
                "test_implementation": "def test_repr():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert repr(session) == \"Session(title={title})\".format(title=session.title)"
            },
            "tests/test_session.py::test_json": {
                "testid": "tests/test_session.py::test_json",
                "result": "passed",
                "test_implementation": "def test_json():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response], title=\"session1\")\n    session1_json = session1.to_json()\n    session2 = Session()\n    session2.from_json(session1_json)\n    assert session1 == session2"
            },
            "tests/test_session.py::test_save1": {
                "testid": "tests/test_session.py::test_save1",
                "result": "passed",
                "test_implementation": "def test_save1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    result = session.save(\"f:/\")\n    assert result[\"status\"] == False"
            },
            "tests/test_session.py::test_save2": {
                "testid": "tests/test_session.py::test_save2",
                "result": "passed",
                "test_implementation": "def test_save2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response], title=\"session1\")\n    result = session1.save(\"session_test1.json\")\n    session2 = Session(file_path=\"session_test1.json\")\n    assert result[\"status\"] and session1 == session2"
            },
            "tests/test_session.py::test_load1": {
                "testid": "tests/test_session.py::test_load1",
                "result": "passed",
                "test_implementation": "def test_load1():\n    with pytest.raises(MemorValidationError, match=r\"Invalid path. Path must be a string.\"):\n        _ = Session(file_path=22)"
            },
            "tests/test_session.py::test_load2": {
                "testid": "tests/test_session.py::test_load2",
                "result": "passed",
                "test_implementation": "def test_load2():\n    with pytest.raises(FileNotFoundError, match=r\"Path session_test10.json does not exist.\"):\n        _ = Session(file_path=\"session_test10.json\")"
            },
            "tests/test_session.py::test_render1": {
                "testid": "tests/test_session.py::test_render1",
                "result": "passed",
                "test_implementation": "def test_render1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert session.render() == \"Hello, how are you?\\nI am fine.\\n\""
            },
            "tests/test_session.py::test_render2": {
                "testid": "tests/test_session.py::test_render2",
                "result": "passed",
                "test_implementation": "def test_render2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert session.render(RenderFormat.OPENAI) == [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}, {\n        \"role\": \"assistant\", \"content\": \"I am fine.\"}]"
            },
            "tests/test_session.py::test_render3": {
                "testid": "tests/test_session.py::test_render3",
                "result": "passed",
                "test_implementation": "def test_render3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert session.render(RenderFormat.DICTIONARY)[\"content\"] == \"Hello, how are you?\\nI am fine.\\n\""
            },
            "tests/test_session.py::test_render4": {
                "testid": "tests/test_session.py::test_render4",
                "result": "passed",
                "test_implementation": "def test_render4():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert (\"content\", \"Hello, how are you?\\nI am fine.\\n\") in session.render(RenderFormat.ITEMS)"
            },
            "tests/test_session.py::test_render5": {
                "testid": "tests/test_session.py::test_render5",
                "result": "passed",
                "test_implementation": "def test_render5():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    with pytest.raises(MemorValidationError, match=r\"Invalid render format. It must be an instance of RenderFormat enum.\"):\n        session.render(\"OPENAI\")"
            },
            "tests/test_session.py::test_check_render1": {
                "testid": "tests/test_session.py::test_check_render1",
                "result": "passed",
                "test_implementation": "def test_check_render1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert session.check_render()"
            },
            "tests/test_session.py::test_check_render2": {
                "testid": "tests/test_session.py::test_check_render2",
                "result": "passed",
                "test_implementation": "def test_check_render2():\n    template = PromptTemplate(content=\"{response[2][message]}\")\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER, template=template, init_check=False)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\", init_check=False)\n    assert not session.check_render()"
            },
            "tests/test_session.py::test_init_check": {
                "testid": "tests/test_session.py::test_init_check",
                "result": "passed",
                "test_implementation": "def test_init_check():\n    template = PromptTemplate(content=\"{response[2][message]}\")\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER, template=template, init_check=False)\n    response = Response(message=\"I am fine.\")\n    with pytest.raises(MemorRenderError, match=r\"Prompt template and properties are incompatible.\"):\n        _ = Session(messages=[prompt, response], title=\"session1\")"
            },
            "tests/test_session.py::test_equality1": {
                "testid": "tests/test_session.py::test_equality1",
                "result": "passed",
                "test_implementation": "def test_equality1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response], title=\"session1\")\n    session2 = session1.copy()\n    assert session1 == session2"
            },
            "tests/test_session.py::test_equality2": {
                "testid": "tests/test_session.py::test_equality2",
                "result": "passed",
                "test_implementation": "def test_equality2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response], title=\"session1\")\n    session2 = Session(messages=[prompt, response], title=\"session2\")\n    assert session1 != session2"
            },
            "tests/test_session.py::test_equality3": {
                "testid": "tests/test_session.py::test_equality3",
                "result": "passed",
                "test_implementation": "def test_equality3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response], title=\"session1\")\n    session2 = Session(messages=[prompt, response], title=\"session1\")\n    assert session1 == session2"
            },
            "tests/test_session.py::test_equality4": {
                "testid": "tests/test_session.py::test_equality4",
                "result": "passed",
                "test_implementation": "def test_equality4():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert session != 2"
            },
            "tests/test_session.py::test_date_modified": {
                "testid": "tests/test_session.py::test_date_modified",
                "result": "passed",
                "test_implementation": "def test_date_modified():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert isinstance(session.date_modified, datetime.datetime)"
            },
            "tests/test_session.py::test_date_created": {
                "testid": "tests/test_session.py::test_date_created",
                "result": "passed",
                "test_implementation": "def test_date_created():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert isinstance(session.date_created, datetime.datetime)"
            },
            "tests/test_session.py::test_length": {
                "testid": "tests/test_session.py::test_length",
                "result": "passed",
                "test_implementation": "def test_length():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session1\")\n    assert len(session) == len(session.messages) and len(session) == 2"
            },
            "tests/test_session.py::test_iter": {
                "testid": "tests/test_session.py::test_iter",
                "result": "passed",
                "test_implementation": "def test_iter():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    messages = []\n    for message in session:\n        messages.append(message)\n    assert session.messages == messages"
            },
            "tests/test_session.py::test_addition1": {
                "testid": "tests/test_session.py::test_addition1",
                "result": "passed",
                "test_implementation": "def test_addition1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    session2 = Session(messages=[prompt, prompt, response, response], title=\"session2\")\n    session3 = session1 + session2\n    assert session3.title is None and session3.messages == session1.messages + session2.messages"
            },
            "tests/test_session.py::test_addition2": {
                "testid": "tests/test_session.py::test_addition2",
                "result": "passed",
                "test_implementation": "def test_addition2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    session2 = Session(messages=[prompt, prompt, response, response], title=\"session2\")\n    session3 = session2 + session1\n    assert session3.title is None and session3.messages == session2.messages + session1.messages"
            },
            "tests/test_session.py::test_addition3": {
                "testid": "tests/test_session.py::test_addition3",
                "result": "passed",
                "test_implementation": "def test_addition3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    session2 = session1 + response\n    assert session2.title == \"session1\" and session2.messages == session1.messages + [response]"
            },
            "tests/test_session.py::test_addition4": {
                "testid": "tests/test_session.py::test_addition4",
                "result": "passed",
                "test_implementation": "def test_addition4():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    session2 = session1 + prompt\n    assert session2.title == \"session1\" and session2.messages == session1.messages + [prompt]"
            },
            "tests/test_session.py::test_addition5": {
                "testid": "tests/test_session.py::test_addition5",
                "result": "passed",
                "test_implementation": "def test_addition5():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    session2 = response + session1\n    assert session2.title == \"session1\" and session2.messages == [response] + session1.messages"
            },
            "tests/test_session.py::test_addition6": {
                "testid": "tests/test_session.py::test_addition6",
                "result": "passed",
                "test_implementation": "def test_addition6():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    session2 = prompt + session1\n    assert session2.title == \"session1\" and session2.messages == [prompt] + session1.messages"
            },
            "tests/test_session.py::test_addition7": {
                "testid": "tests/test_session.py::test_addition7",
                "result": "passed",
                "test_implementation": "def test_addition7():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    with pytest.raises(TypeError, match=re.escape(r\"Unsupported operand type(s) for +: `Session` and `int`\")):\n        _ = session1 + 2"
            },
            "tests/test_session.py::test_addition8": {
                "testid": "tests/test_session.py::test_addition8",
                "result": "passed",
                "test_implementation": "def test_addition8():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session1 = Session(messages=[prompt, response, prompt, response], title=\"session1\")\n    with pytest.raises(TypeError, match=re.escape(r\"Unsupported operand type(s) for +: `Session` and `int`\")):\n        _ = 2 + session1"
            },
            "tests/test_session.py::test_contains1": {
                "testid": "tests/test_session.py::test_contains1",
                "result": "passed",
                "test_implementation": "def test_contains1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session\")\n    assert prompt in session and response in session"
            },
            "tests/test_session.py::test_contains2": {
                "testid": "tests/test_session.py::test_contains2",
                "result": "passed",
                "test_implementation": "def test_contains2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response1 = Response(message=\"I am fine.\")\n    response2 = Response(message=\"Good!\")\n    session = Session(messages=[prompt, response1], title=\"session\")\n    assert response2 not in session"
            },
            "tests/test_session.py::test_contains3": {
                "testid": "tests/test_session.py::test_contains3",
                "result": "passed",
                "test_implementation": "def test_contains3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session\")\n    assert \"I am fine.\" not in session"
            },
            "tests/test_session.py::test_getitem1": {
                "testid": "tests/test_session.py::test_getitem1",
                "result": "passed",
                "test_implementation": "def test_getitem1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session\")\n    assert session[0] == session.messages[0] and session[1] == session.messages[1]"
            },
            "tests/test_session.py::test_getitem2": {
                "testid": "tests/test_session.py::test_getitem2",
                "result": "passed",
                "test_implementation": "def test_getitem2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response, response, response], title=\"session\")\n    assert session[:] == session.messages"
            },
            "tests/test_session.py::test_estimated_tokens1": {
                "testid": "tests/test_session.py::test_estimated_tokens1",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens1():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session\")\n    assert session.estimate_tokens(TokensEstimator.UNIVERSAL) == 12"
            },
            "tests/test_session.py::test_estimated_tokens2": {
                "testid": "tests/test_session.py::test_estimated_tokens2",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens2():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session\")\n    assert session.estimate_tokens(TokensEstimator.OPENAI_GPT_3_5) == 14"
            },
            "tests/test_session.py::test_estimated_tokens3": {
                "testid": "tests/test_session.py::test_estimated_tokens3",
                "result": "passed",
                "test_implementation": "def test_estimated_tokens3():\n    prompt = Prompt(message=\"Hello, how are you?\", role=Role.USER)\n    response = Response(message=\"I am fine.\")\n    session = Session(messages=[prompt, response], title=\"session\")\n    assert session.estimate_tokens(TokensEstimator.OPENAI_GPT_4) == 15"
            },
            "tests/test_token_estimators.py::test_universal_tokens_estimator_with_contractions": {
                "testid": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_contractions",
                "result": "passed",
                "test_implementation": "def test_universal_tokens_estimator_with_contractions():\n    message = \"I'm going to the park.\"\n    assert universal_tokens_estimator(message) == 7\n    message = \"They'll be here soon.\"\n    assert universal_tokens_estimator(message) == 7"
            },
            "tests/test_token_estimators.py::test_universal_tokens_estimator_with_code_snippets": {
                "testid": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_code_snippets",
                "result": "passed",
                "test_implementation": "def test_universal_tokens_estimator_with_code_snippets():\n    message = \"def foo(): return 42\"\n    assert universal_tokens_estimator(message) == 7\n    message = \"if x == 10:\"\n    assert universal_tokens_estimator(message) == 6"
            },
            "tests/test_token_estimators.py::test_universal_tokens_estimator_with_loops": {
                "testid": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_loops",
                "result": "passed",
                "test_implementation": "def test_universal_tokens_estimator_with_loops():\n    message = \"for i in range(10):\"\n    assert universal_tokens_estimator(message) == 8\n    message = \"while True:\"\n    assert universal_tokens_estimator(message) == 4"
            },
            "tests/test_token_estimators.py::test_universal_tokens_estimator_with_long_sentences": {
                "testid": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_long_sentences",
                "result": "passed",
                "test_implementation": "def test_universal_tokens_estimator_with_long_sentences():\n    message = \"Understanding natural language processing is fun!\"\n    assert universal_tokens_estimator(message) == 17\n    message = \"Tokenization involves splitting text into meaningful units.\"\n    assert universal_tokens_estimator(message) == 24"
            },
            "tests/test_token_estimators.py::test_universal_tokens_estimator_with_variable_names": {
                "testid": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_variable_names",
                "result": "passed",
                "test_implementation": "def test_universal_tokens_estimator_with_variable_names():\n    message = \"some_variable_name = 100\"\n    assert universal_tokens_estimator(message) == 5\n    message = \"another_long_var_name = 'test'\"\n    assert universal_tokens_estimator(message) == 6"
            },
            "tests/test_token_estimators.py::test_universal_tokens_estimator_with_function_definitions": {
                "testid": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_function_definitions",
                "result": "passed",
                "test_implementation": "def test_universal_tokens_estimator_with_function_definitions():\n    message = \"The function `def add(x, y): return x + y` adds two numbers.\"\n    assert universal_tokens_estimator(message) == 20\n    message = \"Use `for i in range(5):` to loop.\"\n    assert universal_tokens_estimator(message) == 14"
            },
            "tests/test_token_estimators.py::test_universal_tokens_estimator_with_numbers": {
                "testid": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_numbers",
                "result": "passed",
                "test_implementation": "def test_universal_tokens_estimator_with_numbers():\n    message = \"The year 2023 was great!\"\n    assert universal_tokens_estimator(message) == 6\n    message = \"42 is the answer to everything.\"\n    assert universal_tokens_estimator(message) == 11"
            },
            "tests/test_token_estimators.py::test_universal_tokens_estimator_with_print_statements": {
                "testid": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_print_statements",
                "result": "passed",
                "test_implementation": "def test_universal_tokens_estimator_with_print_statements():\n    message = \"print('Hello, world!')\"\n    assert universal_tokens_estimator(message) == 5\n    message = \"name = \\\"Alice\\\"\"\n    assert universal_tokens_estimator(message) == 3"
            },
            "tests/test_token_estimators.py::test_openai_tokens_estimator_with_function_definition": {
                "testid": "tests/test_token_estimators.py::test_openai_tokens_estimator_with_function_definition",
                "result": "passed",
                "test_implementation": "def test_openai_tokens_estimator_with_function_definition():\n    message = \"def add(a, b): return a + b\"\n    assert openai_tokens_estimator_gpt_3_5(message) == 11"
            },
            "tests/test_token_estimators.py::test_openai_tokens_estimator_with_url": {
                "testid": "tests/test_token_estimators.py::test_openai_tokens_estimator_with_url",
                "result": "passed",
                "test_implementation": "def test_openai_tokens_estimator_with_url():\n    message = \"Visit https://openai.com for more info.\"\n    assert openai_tokens_estimator_gpt_3_5(message) == 18"
            },
            "tests/test_token_estimators.py::test_openai_tokens_estimator_with_long_words": {
                "testid": "tests/test_token_estimators.py::test_openai_tokens_estimator_with_long_words",
                "result": "passed",
                "test_implementation": "def test_openai_tokens_estimator_with_long_words():\n    message = \"This is a verylongwordwithoutspaces and should be counted properly.\"\n    assert openai_tokens_estimator_gpt_3_5(message) == 25"
            },
            "tests/test_token_estimators.py::test_openai_tokens_estimator_with_newlines": {
                "testid": "tests/test_token_estimators.py::test_openai_tokens_estimator_with_newlines",
                "result": "passed",
                "test_implementation": "def test_openai_tokens_estimator_with_newlines():\n    message = \"Line1\\nLine2\\nLine3\\n\"\n    assert openai_tokens_estimator_gpt_3_5(message) == 7"
            },
            "tests/test_token_estimators.py::test_openai_tokens_estimator_with_gpt4_model": {
                "testid": "tests/test_token_estimators.py::test_openai_tokens_estimator_with_gpt4_model",
                "result": "passed",
                "test_implementation": "def test_openai_tokens_estimator_with_gpt4_model():\n    message = \"This is a test sentence that should be counted properly even with GPT-4. I am making it longer to test the model.\"\n    assert openai_tokens_estimator_gpt_4(message) == 45"
            }
        },
        "SRS_document": "**Software Requirements Specification: Memor Library**\n\n**Table of Contents**\n1.  Introduction\n    1.1 Purpose\n    1.2 Scope\n    1.3 Definitions, Acronyms, and Abbreviations\n    1.4 References\n    1.5 Overview\n2.  Overall Description\n    2.1 Product Perspective\n    2.2 Product Functions\n    2.3 User Characteristics\n    2.4 Operating Environment\n    2.5 Design and Implementation Constraints\n    2.6 Assumptions and Dependencies\n3.  Specific Requirements\n    3.1 Functional Requirements\n        3.1.1 Response Object Management (FR-RES)\n        3.1.2 Prompt Template Object Management (FR-TPL)\n        3.1.3 Prompt Object Management (FR-PRM)\n        3.1.4 Session Object Management (FR-SES)\n        3.1.5 Token Estimation (FR-TOK)\n        3.1.6 Enumerations (FR-ENUM)\n        3.1.7 Error Handling (FR-ERR)\n        3.1.8 Utility and Validation Functions (FR-UTIL)\n    3.2 Non-Functional Requirements\n    3.3 External Interface Requirements\n\n---\n\n**1. Introduction**\n\n**1.1 Purpose**\nThis Software Requirements Specification (SRS) document defines the functional and non-functional requirements for the \"Memor\" Python library. Its primary goal is to serve as the definitive guide for software developers who will be assessed on their ability to develop a complete, functional software project based solely on this SRS and a provided subset of public test cases. The developers' success will be measured by their implementation's ability to pass all original test cases, including private ones. Therefore, this document prioritizes clarity, comprehensiveness in functionality, and appropriate abstraction to allow for independent design choices.\n\n**1.2 Scope**\nThe Memor library is designed to manage conversational memory for interactions with Large Language Models (LLMs). It enables users to:\n*   Structure and store individual prompts and their corresponding LLM responses.\n*   Organize prompts and responses into conversational sessions.\n*   Utilize flexible templating for formatting prompts and responses.\n*   Render conversational history in formats suitable for various LLM APIs (e.g., OpenAI).\n*   Estimate token counts for text, which is crucial for LLM interactions.\n*   Persist and load conversation data.\n\nThis SRS covers all externally observable functionalities necessary to build a compatible version of the Memor library. It does not prescribe internal implementation details unless they are part of an explicit external contract or format.\n\n**1.3 Definitions, Acronyms, and Abbreviations**\n*   **SRS:** Software Requirements Specification\n*   **LLM:** Large Language Model\n*   **API:** Application Programming Interface\n*   **JSON:** JavaScript Object Notation\n*   **UTC:** Coordinated Universal Time\n*   **Role:** The originator of a message in a conversation (e.g., user, assistant, system).\n\n**1.4 References**\nThis document is based on the conceptual understanding derived from:\n*   The original README.md documentation of the Memor library.\n*   The original source code of the Memor library (version 0.5).\n*   The complete set of original test cases for the Memor library.\n\n**1.5 Overview**\nThis SRS is organized into three main sections:\n*   **Section 1 (Introduction):** Provides the purpose, scope, definitions, references, and overview of the SRS.\n*   **Section 2 (Overall Description):** Offers a high-level view of the product, its functionalities, user characteristics, operating environment, and constraints.\n*   **Section 3 (Specific Requirements):** Details all functional and non-functional requirements. Functional requirements are grouped by major system capabilities.\n\n---\n\n**2. Overall Description**\n\n**2.1 Product Perspective**\nMemor is a Python library intended to be used by other Python applications that interact with LLMs. It acts as a utility for managing and formatting conversational data.\n\n**2.2 Product Functions**\nThe Memor library provides the following key functions:\n*   **Response Management:** Creating, storing, and managing individual LLM responses with associated metadata (e.g., message, score, model, temperature).\n*   **Prompt Template Management:** Defining and using templates to format prompt and response data. This includes support for custom templates and a set of predefined templates.\n*   **Prompt Management:** Creating, storing, and managing user prompts, including their association with one or more responses and a specific prompt template.\n*   **Session Management:** Grouping sequences of prompts and responses into conversational sessions, managing their order and inclusion status for rendering.\n*   **Data Persistence:** Saving and loading Response, PromptTemplate, Prompt, and Session objects to/from JSON files.\n*   **Rendering:** Transforming Prompt and Session objects into various output formats, including plain strings and structures compatible with LLM APIs (e.g., OpenAI message format).\n*   **Token Estimation:** Providing utilities to estimate the number of tokens a given string would represent, using different estimation strategies.\n*   **Data Validation:** Ensuring the integrity of data through input validation for attributes and parameters.\n\n**2.3 User Characteristics**\nThe primary users of the Memor library are Python developers building applications that require:\n*   Maintaining contextual history for LLM interactions.\n*   Formatting conversational data for LLM APIs.\n*   Transferring conversational context between different LLMs or LLM sessions.\n*   Estimating token usage for LLM prompts.\n\nUsers are expected to have proficiency in Python programming.\n\n**2.4 Operating Environment**\nThe Memor library is designed to operate in a Python environment.\n\n**2.5 Design and Implementation Constraints**\n*   **C1:** The system shall be implemented as a Python library.\n*   **C2:** The system shall be compatible with Python versions 3.7 through 3.13 (inclusive).\n*   **C3:** Timestamps for creation and modification of objects shall be stored and handled in UTC.\n*   **C4:** Serialization and deserialization to/from files shall use the JSON format.\n\n**2.6 Assumptions and Dependencies**\n*   The system assumes it will be run in a standard Python environment where file system access for saving and loading data is available.\n*   The system depends on the availability of the `datetime` and `json` standard Python libraries.\n\n---\n\n**3. Specific Requirements**\n\n**3.1 Functional Requirements**\n\n**3.1.1 Response Object Management (FR-RES)**\n\n*   **FR-RES-1: Response Object Creation**\n    *   The system shall allow the creation of a Response object.\n    *   A Response object can be initialized:\n        *   a. With default values for all attributes.\n        *   b. With specified initial values for attributes such as message, score, role, temperature, tokens, inference time, model, and creation date.\n        *   c. By loading its state from a specified JSON file path.\n\n*   **FR-RES-2: Response Message Attribute**\n    *   The system shall allow a Response object to store a message as a string.\n    *   The message attribute shall be updatable.\n    *   If no message is provided during creation, it defaults to an empty string.\n\n*   **FR-RES-3: Response Score Attribute**\n    *   The system shall allow a Response object to optionally store a score as a float.\n    *   The score attribute shall be updatable.\n\n*   **FR-RES-4: Response Role Attribute**\n    *   The system shall allow a Response object to store a role, which must be a value from the `Role` enumeration (see FR-ENUM-1).\n    *   If no role is provided during creation, it shall default to `ASSISTANT`.\n    *   The role attribute shall be updatable.\n\n*   **FR-RES-5: Response Temperature Attribute**\n    *   The system shall allow a Response object to optionally store a temperature as a positive float.\n    *   The temperature attribute shall be updatable.\n\n*   **FR-RES-6: Response Tokens Attribute**\n    *   The system shall allow a Response object to optionally store a token count as a positive integer.\n    *   The tokens attribute shall be updatable.\n\n*   **FR-RES-7: Response Inference Time Attribute**\n    *   The system shall allow a Response object to optionally store an inference time as a positive float.\n    *   The inference time attribute shall be updatable.\n\n*   **FR-RES-8: Response Model Attribute**\n    *   The system shall allow a Response object to optionally store a model identifier as a string.\n    *   The model attribute shall be updatable.\n\n*   **FR-RES-9: Response Timestamps**\n    *   Each Response object shall automatically record its creation timestamp (UTC).\n    *   The creation timestamp can be set explicitly during initialization with a timezone-aware datetime object.\n    *   Each Response object shall automatically update a modification timestamp (UTC) whenever any of its attributes are updated.\n\n*   **FR-RES-10: Response Attribute Validation**\n    *   The system shall validate attributes of a Response object upon setting or updating them:\n        *   Message must be a string. \n        *   Tokens must be a positive integer. \n        *   Inference time must be a positive float. \n        *   Score must be a float between 0 and 1 (inclusive). \n        *   Role must be an instance of the `Role` enumeration. \n        *   Temperature must be a positive float. \n        *   Model must be a string. \n        *   Creation date (if provided) must be a timezone-aware datetime object. \n    *   Validation failures shall raise a `MemorValidationError`.\n\n*   **FR-RES-11: Response Serialization and Deserialization**\n    *   The system shall provide a mechanism to serialize a Response object to a JSON-compatible dictionary.\n    *   The system shall provide a mechanism to deserialize a Response object from a JSON-compatible dictionary or a JSON string.\n    *   Serialization shall include all attributes, type (\"Response\"), and the Memor library version. Timestamps shall be formatted as strings. Role shall be its string value.\n    *   Deserialization shall correctly reconstruct the Response object, including its attributes and timestamps.\n    *   Attempting to deserialize from an invalid structure shall raise a `MemorValidationError`.\n\n*   **FR-RES-12: Response Persistence**\n    *   The system shall allow saving a Response object to a specified file path in JSON format.\n    *   The system shall allow loading a Response object from a specified file path, assuming it contains valid JSON data for a Response.\n    *   Saving to an invalid path should report failure. Loading from a non-existent path should raise `FileNotFoundError`. Loading from an invalid path type should raise `MemorValidationError`.\n\n*   **FR-RES-13: Response Copying**\n    *   The system shall allow creating a shallow copy of a Response object. The copy shall be a new instance with identical attribute values but a different object ID.\n\n*   **FR-RES-14: Response String Representation**\n    *   The string representation of a Response object (e.g., via `str()`) shall be its message content.\n    *   The `repr()` representation shall provide a developer-friendly string indicating it's a Response object and its message.\n\n*   **FR-RES-15: Response Rendering**\n    *   The system shall allow a Response object to be rendered into different formats specified by the `RenderFormat` enumeration (see FR-ENUM-2).\n    *   Supported formats:\n        *   `STRING`: Returns the response message.\n        *   `OPENAI`: Returns a dictionary with \"role\" (string value of `Role`) and \"content\" (response message).\n        *   `DICTIONARY`: Returns the full dictionary representation of the response (as per `to_dict`).\n        *   `ITEMS`: Returns a list of key-value pairs from the dictionary representation.\n    *   If an invalid render format is provided, a `MemorValidationError` shall be raised.\n\n*   **FR-RES-16: Response Token Estimation**\n    *   The system shall allow estimating the number of tokens for the Response object's message content using a specified token estimation strategy (see FR-TOK-1).\n\n*   **FR-RES-17: Response Equality Comparison**\n    *   The system shall allow comparing two Response objects for equality.\n    *   Two Response objects are considered equal if all their primary attributes (message, score, role, temperature, model, tokens, inference_time) are equal. Timestamps are not part of this equality check.\n    *   Comparison with non-Response objects should result in inequality.\n\n*   **FR-RES-18: Response Length**\n    *   The length of a Response object (e.g., via `len()`) shall be the length of its rendered string representation (i.e., its message content).\n    *   An empty response message results in a length of 0.\n\n**3.1.2 Prompt Template Object Management (FR-TPL)**\n\n*   **FR-TPL-1: PromptTemplate Object Creation**\n    *   The system shall allow the creation of a PromptTemplate object.\n    *   A PromptTemplate object can be initialized:\n        *   a. With default values (None for content, title, custom_map).\n        *   b. With specified initial values for content (string), title (string), and custom_map (dictionary of string-convertible key-value pairs).\n        *   c. By loading its state from a specified JSON file path.\n\n*   **FR-TPL-2: PromptTemplate Content Attribute**\n    *   The system shall allow a PromptTemplate object to store a content string, which defines the template structure.\n    *   The content string uses Python-style formatting placeholders (e.g., `{placeholder_name}`).\n    *   The content attribute shall be updatable.\n\n*   **FR-TPL-3: PromptTemplate Title Attribute**\n    *   The system shall allow a PromptTemplate object to optionally store a title as a string.\n    *   The title attribute shall be updatable.\n\n*   **FR-TPL-4: PromptTemplate Custom Map Attribute**\n    *   The system shall allow a PromptTemplate object to optionally store a custom map.\n    *   A custom map is a dictionary where keys are placeholder names (strings) and values are their corresponding string-convertible values to be inserted during rendering.\n    *   The custom_map attribute shall be updatable.\n\n*   **FR-TPL-5: PromptTemplate Timestamps**\n    *   Each PromptTemplate object shall automatically record its creation timestamp (UTC).\n    *   Each PromptTemplate object shall automatically update a modification timestamp (UTC) whenever any of its attributes (title, content, custom_map) are updated.\n\n*   **FR-TPL-6: PromptTemplate Attribute Validation**\n    *   The system shall validate attributes of a PromptTemplate object upon setting or updating them:\n        *   Title must be a string. \n        *   Content must be a string. \n        *   Custom map must be a dictionary with string-convertible keys and values. \n    *   Validation failures shall raise a `MemorValidationError`.\n\n*   **FR-TPL-7: PromptTemplate Serialization and Deserialization**\n    *   The system shall provide a mechanism to serialize a PromptTemplate object to a JSON-compatible dictionary.\n    *   The system shall provide a mechanism to deserialize a PromptTemplate object from a JSON-compatible dictionary or a JSON string.\n    *   Serialization shall include title, content, custom_map, Memor library version, and formatted timestamps.\n    *   Deserialization shall correctly reconstruct the PromptTemplate object.\n    *   Attempting to deserialize from an invalid structure shall raise a `MemorValidationError`.\n\n*   **FR-TPL-8: PromptTemplate Persistence**\n    *   The system shall allow saving a PromptTemplate object to a specified file path in JSON format.\n    *   The system shall allow loading a PromptTemplate object from a specified file path.\n    *   Saving to an invalid path should report failure. Loading from a non-existent path should raise `FileNotFoundError`. Loading from an invalid path type should raise `MemorValidationError`.\n\n*   **FR-TPL-9: PromptTemplate Copying**\n    *   The system shall allow creating a shallow copy of a PromptTemplate object. The copy shall be a new instance with identical attribute values but a different object ID.\n\n*   **FR-TPL-10: PromptTemplate String Representation**\n    *   The string representation of a PromptTemplate object (e.g., via `str()`) shall be its content string.\n    *   The `repr()` representation shall provide a developer-friendly string indicating it's a PromptTemplate object and its content.\n\n*   **FR-TPL-11: PromptTemplate Equality Comparison**\n    *   The system shall allow comparing two PromptTemplate objects for equality.\n    *   Two PromptTemplate objects are considered equal if their content, title, and custom_map attributes are equal.\n    *   Comparison with non-PromptTemplate objects should result in inequality.\n\n*   **FR-TPL-12: Preset Prompt Templates**\n    *   The system shall provide access to a collection of preset `PromptTemplate` objects.\n    *   Preset templates are organized into groups: `BASIC`, `INSTRUCTION1`, `INSTRUCTION2`, `INSTRUCTION3`.\n    *   Each group contains templates for common rendering needs, such as:\n        *   `PROMPT`: Renders only the prompt message (optionally prefixed by an instruction).\n        *   `RESPONSE`: Renders only the selected response message (optionally prefixed by an instruction).\n        *   `RESPONSE0` to `RESPONSE3`: Renders specific responses from the prompt's list of responses by index (optionally prefixed by an instruction).\n        *   `PROMPT_WITH_LABEL`: Renders the prompt message prefixed with \"Prompt: \" (and optionally a main instruction).\n        *   `RESPONSE_WITH_LABEL`: Renders the selected response message prefixed with \"Response: \" (and optionally a main instruction).\n        *   `RESPONSE0_WITH_LABEL` to `RESPONSE3_WITH_LABEL`: Labeled rendering for specific responses by index.\n        *   `PROMPT_RESPONSE_STANDARD`: Renders labeled prompt and selected response, typically on separate lines (optionally prefixed by an instruction).\n        *   `PROMPT_RESPONSE_FULL`: Renders a detailed, multi-line representation of prompt and selected response attributes (optionally prefixed by an instruction).\n    *   The `INSTRUCTION1`, `INSTRUCTION2`, `INSTRUCTION3` groups use specific predefined instruction strings prepended to the rendered output (e.g., \"I'm providing you with a history...\"). The `BASIC` group uses an empty instruction string.\n    *   The system must define a default prompt template, which is `PresetPromptTemplate.BASIC.PROMPT`.\n\n**3.1.3 Prompt Object Management (FR-PRM)**\n\n*   **FR-PRM-1: Prompt Object Creation**\n    *   The system shall allow the creation of a Prompt object.\n    *   A Prompt object can be initialized:\n        *   a. With default values for its attributes.\n        *   b. With specified initial values for attributes such as message, responses (list of `Response` objects), role, tokens, and template.\n        *   c. By loading its state from a specified JSON file path.\n    *   Upon creation, an initial check for renderability shall be performed by default; if this check fails due to incompatible template and properties, a `MemorRenderError` shall be raised. This check can be optionally suppressed.\n\n*   **FR-PRM-2: Prompt Message Attribute**\n    *   The system shall allow a Prompt object to store a message as a string.\n    *   The message attribute shall be updatable.\n    *   If no message is provided, it defaults to an empty string.\n\n*   **FR-PRM-3: Prompt Responses Attribute**\n    *   The system shall allow a Prompt object to store an ordered list of associated `Response` objects.\n    *   The list of responses shall be updatable (replacing the entire list).\n    *   The system shall allow adding a new `Response` object to the list, either at the end or at a specified index.\n    *   The system shall allow removing a `Response` object from the list by its index.\n    *   Initially, the list of responses is empty if not provided.\n\n*   **FR-PRM-4: Prompt Selected Response**\n    *   The system shall allow selecting one `Response` object from the Prompt's list of responses as the \"selected response\" by its index.\n    *   If responses exist, the first response (index 0) is selected by default.\n    *   The selected response is used during rendering when the template refers to a single response (e.g., `{response[message]}`).\n\n*   **FR-PRM-5: Prompt Role Attribute**\n    *   The system shall allow a Prompt object to store a role, which must be a value from the `Role` enumeration (see FR-ENUM-1).\n    *   If no role is provided, it shall default to `USER`.\n    *   The role attribute shall be updatable.\n\n*   **FR-PRM-6: Prompt Tokens Attribute**\n    *   The system shall allow a Prompt object to optionally store a token count as a positive integer.\n    *   The tokens attribute shall be updatable.\n\n*   **FR-PRM-7: Prompt Template Attribute**\n    *   The system shall allow a Prompt object to be associated with a `PromptTemplate` object (either a custom one or a preset one).\n    *   If no template is provided, it shall default to `PresetPromptTemplate.DEFAULT` (see FR-TPL-12).\n    *   The template attribute shall be updatable.\n\n*   **FR-PRM-8: Prompt Timestamps**\n    *   Each Prompt object shall automatically record its creation timestamp (UTC).\n    *   Each Prompt object shall automatically update a modification timestamp (UTC) whenever any of its attributes or its list of responses is updated.\n\n*   **FR-PRM-9: Prompt Attribute Validation**\n    *   The system shall validate attributes and parameters for a Prompt object:\n        *   Message must be a string. \n        *   Tokens must be a positive integer. \n        *   Role must be an instance of `Role`. \n        *   Responses (when updating the list) must be a list of `Response` objects. \n        *   Response (when adding) must be an instance of `Response`. \n        *   Template must be an instance of `PromptTemplate` or a `PresetPromptTemplate` enum member. \n    *   Validation failures shall raise a `MemorValidationError`.\n\n*   **FR-PRM-10: Prompt Serialization and Deserialization**\n    *   The system shall provide a mechanism to serialize a Prompt object to a JSON-compatible dictionary.\n    *   The system shall provide a mechanism to deserialize a Prompt object from a JSON-compatible dictionary or a JSON string.\n    *   Serialization shall include message, list of serialized responses, selected response index, role (as string value), tokens, Memor library version, and formatted timestamps.\n    *   Serialization can optionally include the associated `PromptTemplate` (serialized). If not included, the prompt will use the default template upon deserialization.\n    *   Deserialization shall correctly reconstruct the Prompt object. If template data is not present in the JSON, the default template is used.\n    *   Attempting to deserialize from an invalid structure shall raise a `MemorValidationError`.\n\n*   **FR-PRM-11: Prompt Persistence**\n    *   The system shall allow saving a Prompt object to a specified file path in JSON format. This can optionally include its template.\n    *   The system shall allow loading a Prompt object from a specified file path.\n    *   Saving to an invalid path should report failure. Loading from a non-existent path should raise `FileNotFoundError`. Loading from an invalid path type should raise `MemorValidationError`.\n\n*   **FR-PRM-12: Prompt Copying**\n    *   The system shall allow creating a shallow copy of a Prompt object. The copy shall be a new instance with identical attribute values but a different object ID. Responses and template will be references to the original objects if they are mutable, or copies if they are immutable values.\n\n*   **FR-PRM-13: Prompt Rendering**\n    *   The system shall allow a Prompt object to be rendered into different formats using its associated `PromptTemplate` and attributes.\n    *   The rendering process substitutes placeholders in the template's content string. Placeholders can refer to:\n        *   Prompt attributes (e.g., `{prompt[message]}`, `{prompt[role]}`).\n        *   The selected response's attributes (e.g., `{response[message]}`, `{response[temperature]}`).\n        *   Attributes of specific responses in the prompt's list by index (e.g., `{responses[0][message]}`).\n        *   Values from the template's `custom_map` (e.g., `{instruction}`).\n    *   Supported output formats are specified by `RenderFormat` (see FR-ENUM-2):\n        *   `STRING`: Returns the fully rendered string.\n        *   `OPENAI`: Returns a dictionary with \"role\" (Prompt's role as string) and \"content\" (the rendered string).\n        *   `DICTIONARY`: Returns a dictionary containing the Prompt's attributes and the rendered content string under a \"content\" key.\n        *   `ITEMS`: Returns a list of key-value pairs from the `DICTIONARY` representation.\n    *   If rendering fails due to incompatible template/placeholders and prompt data (e.g., placeholder refers to a non-existent attribute or response index), a `MemorRenderError` shall be raised.\n    *   If an invalid render format is provided, a `MemorValidationError` shall be raised.\n\n*   **FR-PRM-14: Prompt Renderability Check**\n    *   The system shall provide a mechanism to check if a Prompt object can be successfully rendered with its current template and data without raising an error. This returns a boolean.\n\n*   **FR-PRM-15: Prompt String Representation**\n    *   The string representation of a Prompt object (e.g., via `str()`) shall be its rendered content in `STRING` format.\n    *   The `repr()` representation shall provide a developer-friendly string indicating it's a Prompt object and its message.\n\n*   **FR-PRM-16: Prompt Token Estimation**\n    *   The system shall allow estimating the number of tokens for the Prompt object's rendered string output using a specified token estimation strategy (see FR-TOK-1).\n\n*   **FR-PRM-17: Prompt Equality Comparison**\n    *   The system shall allow comparing two Prompt objects for equality.\n    *   Two Prompt objects are considered equal if their message, list of responses, role, template, and tokens attributes are all equal.\n    *   Comparison with non-Prompt objects should result in inequality.\n\n*   **FR-PRM-18: Prompt Length**\n    *   The length of a Prompt object (e.g., via `len()`) shall be the length of its rendered string representation.\n    *   If the prompt is not renderable (e.g., due to template errors), its length shall be 0.\n    *   An empty prompt message (with a renderable template) may result in a non-zero length if the template itself has content.\n\n**3.1.4 Session Object Management (FR-SES)**\n\n*   **FR-SES-1: Session Object Creation**\n    *   The system shall allow the creation of a Session object.\n    *   A Session object can be initialized:\n        *   a. With a default `None` title and an empty list of messages.\n        *   b. With a specified title (string) and an initial list of `Prompt` or `Response` objects.\n        *   c. By loading its state from a specified JSON file path.\n    *   Upon creation, an initial check for renderability of all contained messages shall be performed by default; if this check fails for any message, a `MemorRenderError` shall be raised. This check can be optionally suppressed.\n\n*   **FR-SES-2: Session Title Attribute**\n    *   The system shall allow a Session object to optionally store a title as a string.\n    *   The title attribute shall be updatable.\n\n*   **FR-SES-3: Session Messages Management**\n    *   A Session object shall maintain an ordered list of messages, where each message is an instance of `Prompt` or `Response`.\n    *   The system shall allow updating the entire list of messages. If a new list of messages is provided, their statuses default to enabled (True).\n    *   The system shall allow adding a new message (`Prompt` or `Response`) to the list, either at the end or at a specified index. When adding a message, its status (enabled/disabled) can also be specified (defaults to enabled).\n    *   The system shall allow removing a message from the list by its index. This also removes its corresponding status.\n    *   The system shall allow clearing all messages from the session.\n\n*   **FR-SES-4: Session Message Status Management**\n    *   For each message in the session, the system shall maintain a corresponding boolean status (enabled/disabled). Enabled messages are included in rendering; disabled messages are skipped.\n    *   The list of message statuses shall be updatable. The length of the status list must match the length of the messages list.\n    *   The system shall provide methods to:\n        *   Enable a message at a specific index.\n        *   Disable a message at a specific index.\n        *   Mask a message (equivalent to disabling it).\n        *   Unmask a message (equivalent to enabling it).\n    *   The system shall provide a way to get the list of \"masks\" (where True means masked/disabled).\n\n*   **FR-SES-5: Session Timestamps**\n    *   Each Session object shall automatically record its creation timestamp (UTC).\n    *   Each Session object shall automatically update a modification timestamp (UTC) whenever its title, messages, or message statuses are updated.\n\n*   **FR-SES-6: Session Attribute Validation**\n    *   The system shall validate attributes and parameters for a Session object:\n        *   Title must be a string. \n        *   Messages (when updating the list) must be a list of `Prompt` or `Response` objects. \n        *   Message (when adding) must be an instance of `Prompt` or `Response`. \n        *   Status for an added message must be a boolean. \n        *   Message status list must be a list of booleans. \n        *   Message status list length must match the number of messages. \n    *   Validation failures shall raise a `MemorValidationError`.\n\n*   **FR-SES-7: Session Serialization and Deserialization**\n    *   The system shall provide a mechanism to serialize a Session object to a JSON-compatible dictionary.\n    *   The system shall provide a mechanism to deserialize a Session object from a JSON-compatible dictionary or a JSON string.\n    *   Serialization shall include title, list of serialized messages (each with its type: \"Prompt\" or \"Response\"), list of message statuses, Memor library version, and formatted timestamps.\n    *   Deserialization shall correctly reconstruct the Session object, including its messages (instantiating them as `Prompt` or `Response` based on their \"type\" field).\n    *   Attempting to deserialize from an invalid structure (e.g., missing fields) will likely result in errors during attribute access or type casting, effectively leading to failure. Explicit top-level structure validation should result in `MemorValidationError`.\n\n*   **FR-SES-8: Session Persistence**\n    *   The system shall allow saving a Session object to a specified file path in JSON format.\n    *   The system shall allow loading a Session object from a specified file path.\n    *   Saving to an invalid path should report failure. Loading from a non-existent path should raise `FileNotFoundError`. Loading from an invalid path type should raise `MemorValidationError`.\n\n*   **FR-SES-9: Session Copying**\n    *   The system shall allow creating a shallow copy of a Session object. The copy shall be a new instance with identical attribute values but a different object ID. The list of messages will be a shallow copy.\n\n*   **FR-SES-10: Session Rendering**\n    *   The system shall allow a Session object to be rendered into different formats, considering only enabled messages.\n    *   Supported output formats by `RenderFormat` (see FR-ENUM-2):\n        *   `STRING`: Returns a concatenation of the string-rendered content of each enabled message, with each message typically followed by a newline.\n        *   `OPENAI`: Returns a list, where each element is the OpenAI-formatted rendering (dictionary with \"role\" and \"content\") of an enabled message. If a message itself is a Session, its OpenAI rendering (a list) is extended into the main list.\n        *   `DICTIONARY`: Returns a dictionary containing the Session's attributes and the concatenated string content (as per `STRING` format) under a \"content\" key.\n        *   `ITEMS`: Returns a list of key-value pairs from the `DICTIONARY` representation.\n    *   If rendering any of its enabled messages fails, the overall session rendering will also fail (raising `MemorRenderError`).\n    *   If an invalid render format is provided, a `MemorValidationError` shall be raised.\n\n*   **FR-SES-11: Session Renderability Check**\n    *   The system shall provide a mechanism to check if a Session object (all its enabled messages) can be successfully rendered without raising an error. This returns a boolean.\n\n*   **FR-SES-12: Session String Representation**\n    *   The string representation of a Session object (e.g., via `str()`) shall be its rendered content in `STRING` format.\n    *   The `repr()` representation shall provide a developer-friendly string indicating it's a Session object and its title (if any).\n\n*   **FR-SES-13: Session Token Estimation**\n    *   The system shall allow estimating the number of tokens for the Session's concatenated rendered string output (STRING format) using a specified token estimation strategy (see FR-TOK-1).\n\n*   **FR-SES-14: Session Equality Comparison**\n    *   The system shall allow comparing two Session objects for equality.\n    *   Two Session objects are considered equal if their title and list of messages are equal. Message statuses are implicitly part of message list equality if messages are compared deeply or if statuses are also compared. (Source code suggests messages_status are also compared via `to_dict` in practice, if objects are fully serialized then compared). For direct `__eq__`, it's title and messages list.\n    *   The original code's `__eq__` compares `_title` and `_messages`. The `_messages_status` is not directly compared in `__eq__`.\n    *   Based on `Session.__eq__`, two sessions are equal if their `_title` and `_messages` lists (comparing `Prompt`/`Response` objects within) are equal.\n    *   Comparison with non-Session objects should result in inequality.\n\n*   **FR-SES-15: Session Length**\n    *   The length of a Session object (e.g., via `len()`) shall be the total number of messages it contains (both enabled and disabled).\n\n*   **FR-SES-16: Session Iteration**\n    *   A Session object shall be iterable, yielding its contained `Prompt` or `Response` objects in order.\n\n*   **FR-SES-17: Session Concatenation**\n    *   The system shall support concatenating two Session objects using the `+` operator. This results in a new Session object containing all messages from the first operand followed by all messages from the second. The new Session's title shall be `None`. Message statuses are inherited with the messages.\n    *   The system shall support concatenating a Session object with a `Prompt` or `Response` object using the `+` operator (Session on left or right). This results in a new Session object with the `Prompt`/`Response` added to the original Session's messages. The original Session's title is preserved.\n    *   Attempting to concatenate with an unsupported type shall raise a `TypeError`.\n\n*   **FR-SES-18: Session Membership Testing**\n    *   The system shall support checking if a specific `Prompt` or `Response` object is contained within a Session's messages using the `in` operator.\n\n*   **FR-SES-19: Session Message Access by Index**\n    *   The system shall allow accessing individual messages within a Session by their integer index or a slice.\n\n**3.1.5 Token Estimation (FR-TOK)**\n\n*   **FR-TOK-1: Token Estimation Strategies**\n    *   The system shall provide multiple strategies for estimating the number of tokens in a given text string, accessible via the `TokensEstimator` enumeration (see FR-ENUM-3).\n    *   Supported strategies:\n        *   `UNIVERSAL`: A general-purpose estimator.\n        *   `OPENAI_GPT_3_5`: An estimator tuned for OpenAI GPT-3.5 models.\n        *   `OPENAI_GPT_4`: An estimator tuned for OpenAI GPT-4 models.\n    *   A default strategy (`UNIVERSAL`) shall be defined.\n\n*   **FR-TOK-2: Universal Token Estimator Behavior**\n    *   The `UNIVERSAL` token estimator shall process input strings to count tokens. It should exhibit distinct counting behavior for:\n        *   Text with contractions (e.g., \"I'm\").\n        *   Code snippets containing common programming symbols and structures (e.g., assignments, operators, control flow keywords).\n        *   Programming language keywords from a predefined set (Python, JavaScript, Java, C, C++, C#, Go, Rust, Swift, Kotlin, TypeScript, PHP, Ruby, SQL, Bash, MATLAB, R, Perl, Lua, Scala, Dart, Julia, Haskell, COBOL, Objective-C, F#, Lisp, Prolog, Ada, Delphi, Visual Basic, HTML, CSS).\n        *   Tokens with common prefixes (e.g., \"un\", \"re\") and suffixes (e.g., \"ing\", \"ed\").\n        *   Numerical digits.\n        *   String literals.\n        *   Identifiers with underscores or camelCase.\n    *   The specific counting logic aims to approximate tokenization by considering word-like units, symbols, and structural elements in code.\n\n*   **FR-TOK-3: OpenAI Token Estimator Behavior (GPT-3.5 and GPT-4)**\n    *   The `OPENAI_GPT_3_5` and `OPENAI_GPT_4` token estimators shall process input strings to count tokens using heuristics based on character counts, spacing, punctuation, presence of newlines, long words, URLs, and programming keywords.\n    *   The `OPENAI_GPT_4` estimator may apply a slight adjustment factor compared to the `OPENAI_GPT_3_5` estimator.\n    *   The minimum estimated token count shall be 1.\n\n**3.1.6 Enumerations (FR-ENUM)**\n\n*   **FR-ENUM-1: Role Enumeration**\n    *   The system shall define an enumeration `Role` for representing the originator of a message.\n    *   Members shall include: `SYSTEM`, `USER`, `ASSISTANT`.\n    *   `Role.USER` shall be designated as the default value for Prompts if not specified.\n    *   `Role.ASSISTANT` shall be designated as the default value for Responses if not specified.\n    *   Each member shall have an associated string value (e.g., \"system\", \"user\", \"assistant\").\n\n*   **FR-ENUM-2: RenderFormat Enumeration**\n    *   The system shall define an enumeration `RenderFormat` for specifying output formats.\n    *   Members shall include: `STRING`, `OPENAI`, `DICTIONARY`, `ITEMS`.\n    *   `RenderFormat.STRING` shall be designated as the default value if not specified.\n\n*   **FR-ENUM-3: TokensEstimator Enumeration**\n    *   The system shall define an enumeration `TokensEstimator` for selecting token estimation strategies.\n    *   Members shall include: `UNIVERSAL`, `OPENAI_GPT_3_5`, `OPENAI_GPT_4`.\n    *   Each member shall be associated with a corresponding token estimation function.\n    *   `TokensEstimator.UNIVERSAL` shall be designated as the default value if not specified.\n\n**3.1.7 Error Handling (FR-ERR)**\n\n*   **FR-ERR-1: Custom Validation Error**\n    *   The system shall define and use a custom exception class `MemorValidationError` (subclass of `ValueError`) for errors related to invalid input data, types, or values.\n\n*   **FR-ERR-2: Custom Render Error**\n    *   The system shall define and use a custom exception class `MemorRenderError` (subclass of `Exception`) for errors occurring during the rendering process, specifically when a prompt's template and its properties are incompatible.\n\n**3.1.8 Utility and Validation Functions (FR-UTIL)** (These are mostly internal helpers whose effects are specified in the requirements of the classes that use them, but their existence ensures consistent validation logic)\n\n*   **FR-UTIL-1: UTC Timestamp Generation**\n    *   The system shall provide a utility to get the current time as a timezone-aware `datetime` object in UTC.\n\n*   **FR-UTIL-2: Common Input Validations**\n    *   The system shall use internal validation functions to ensure:\n        *   Strings are actual strings \n        *   Booleans are actual booleans \n        *   Positive integers are integers >= 0 \n        *   Positive floats are floats >= 0 \n        *   Probabilities are floats between 0 and 1 inclusive \n        *   Lists contain elements of a specific type \n        *   Datetime objects are timezone-aware \n        *   File paths are strings and exist \n        *   Custom maps for templates are dictionaries with string-convertible keys/values \n    *   These validations, when failed, should result in a `MemorValidationError` or `FileNotFoundError` as appropriate.\n\n**3.2 Non-Functional Requirements**\nNo specific non-functional requirements (e.g., performance, security, reliability targets) are explicitly verifiable by the provided test suite beyond the functional correctness they ensure. Therefore, this section is intentionally sparse.\n\n*   **NFR-1: Version Information**\n    *   The library shall expose its version number (e.g., \"0.5\").\n    *   *(Note: While testable, this is borderline. Including for completeness as it's an observable aspect from `params.py` and `__init__.py`.)*\n\n**3.3 External Interface Requirements**\n\n*   **EIF-1: OpenAI Rendering Format**\n    *   When rendering for OpenAI (using `RenderFormat.OPENAI`), individual `Prompt` or `Response` objects shall produce a dictionary with two keys:\n        *   `\"role\"`: The string value of the object's role (e.g., \"user\", \"assistant\", \"system\").\n        *   `\"content\"`: The rendered string content of the object.\n    *   A `Session` rendered for OpenAI shall produce a list of such dictionaries, corresponding to its enabled messages.\n\n*   **EIF-2: JSON Persistence Format**\n    *   When `Response`, `PromptTemplate`, `Prompt`, and `Session` objects are serialized to JSON (e.g., for saving to a file or using `to_json()` methods), they must adhere to a consistent structure:\n        *   Each object's JSON representation includes a `\"type\"` field for `Prompt` and `Response` when part of a `Session`'s message list, indicating \"Prompt\" or \"Response\". (Note: Top-level `to_json` for Prompt/Response might not add this if not strictly needed for self-contained deserialization, but Session's `from_json` relies on it for messages).\n        *   Timestamps (`date_created`, `date_modified`) are serialized as strings in the format `\"%Y-%m-%d %H:%M:%S %z\"`.\n        *   `Role` enum values are serialized as their string values (e.g., \"user\").\n        *   `memor_version` attribute is included, storing the version of the Memor library that created the JSON.\n        *   All relevant attributes of the object are included. Nested objects (like `Response` in `Prompt`, `PromptTemplate` in `Prompt`, messages in `Session`) are also serialized to their JSON representations.\n\n---\n**End of SRS Document**",
        "structured_requirements": [
            {
                "requirement_id": "C1",
                "requirement_description": "The system shall be implemented as a Python library.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "C2",
                "requirement_description": "The system shall be compatible with Python versions 3.7 through 3.13 (inclusive).",
                "test_traceability": [
                    {
                        "id": "setup.py",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "setup.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "C3",
                "requirement_description": "Timestamps for creation and modification of objects shall be stored and handled in UTC.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_date2",
                        "description": "Implicit in various tests checking `date_created`/`date_modified` which use `get_time_utc`"
                    },
                    {
                        "id": "tests/test_response.py::test_date_modified",
                        "description": "Implicit in various tests checking `date_created`/`date_modified` which use `get_time_utc`"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/functions.py::get_time_utc",
                        "description": "Implicit in various tests checking `date_created`/`date_modified` which use `get_time_utc`"
                    },
                    {
                        "id": "usage in constructors of `Response`",
                        "description": "Implicit in various tests checking `date_created`/`date_modified` which use `get_time_utc`"
                    },
                    {
                        "id": "Prompt",
                        "description": "Implicit in various tests checking `date_created`/`date_modified` which use `get_time_utc`"
                    },
                    {
                        "id": "PromptTemplate",
                        "description": "Implicit in various tests checking `date_created`/`date_modified` which use `get_time_utc`"
                    },
                    {
                        "id": "Session",
                        "description": "Implicit in various tests checking `date_created`/`date_modified` which use `get_time_utc`"
                    }
                ]
            },
            {
                "requirement_id": "C4",
                "requirement_description": "Serialization and deserialization to/from files shall use the JSON format.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_save1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_save1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_save2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_save2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::save",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt_template.py::PromptTemplate::save",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::save",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::save",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-1",
                "requirement_description": "Response Object Creation\nThe system shall allow the creation of a Response object.\nA Response object can be initialized:\na. With default values for all attributes.\nb. With specified initial values for attributes such as message, score, role, temperature, tokens, inference time, model, and creation date.\nc. By loading its state from a specified JSON file path.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_message1",
                        "description": "partial"
                    },
                    {
                        "id": "tests/test_response.py::test_load1",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-2",
                "requirement_description": "Response Message Attribute\nThe system shall allow a Response object to store a message as a string.\nThe message attribute shall be updatable.\nIf no message is provided during creation, it defaults to an empty string.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_message1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_message2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::update_message",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::message",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-3",
                "requirement_description": "Response Score Attribute\nThe system shall allow a Response object to optionally store a score as a float.\nThe score attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_score1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_score2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::update_score",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::score",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-4",
                "requirement_description": "Response Role Attribute\nThe system shall allow a Response object to store a role, which must be a value from the `Role` enumeration (see FR-ENUM-1).\nIf no role is provided during creation, it shall default to `ASSISTANT`.\nThe role attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_role1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_role2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_role3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::update_role",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::role",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-5",
                "requirement_description": "Response Temperature Attribute\nThe system shall allow a Response object to optionally store a temperature as a positive float.\nThe temperature attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_temperature1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_temperature2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::update_temperature",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::temperature",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-6",
                "requirement_description": "Response Tokens Attribute\nThe system shall allow a Response object to optionally store a token count as a positive integer.\nThe tokens attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_tokens1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_tokens2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_tokens3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::update_tokens",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::tokens",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-7",
                "requirement_description": "Response Inference Time Attribute\nThe system shall allow a Response object to optionally store an inference time as a positive float.\nThe inference time attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_inference_time1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_inference_time2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_inference_time3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::update_inference_time",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::inference_time",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-8",
                "requirement_description": "Response Model Attribute\nThe system shall allow a Response object to optionally store a model identifier as a string.\nThe model attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_model1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_model2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::update_model",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::model",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-9",
                "requirement_description": "Response Timestamps\nEach Response object shall automatically record its creation timestamp (UTC).\nThe creation timestamp can be set explicitly during initialization with a timezone-aware datetime object.\nEach Response object shall automatically update a modification timestamp (UTC) whenever any of its attributes are updated.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_date1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_date2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_date_created",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_date_modified",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::update_* methods",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::date_created",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::date_modified",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-10",
                "requirement_description": "Response Attribute Validation\nThe system shall validate attributes of a Response object upon setting or updating them:\nMessage must be a string.\nTokens must be a positive integer.\nInference time must be a positive float.\nScore must be a float between 0 and 1 (inclusive).\nRole must be an instance of the `Role` enumeration.\nTemperature must be a positive float.\nModel must be a string.\nCreation date (if provided) must be a timezone-aware datetime object.\nValidation failures shall raise a `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_message3",
                        "description": "Message must be a string."
                    },
                    {
                        "id": "tests/test_response.py::test_tokens4",
                        "description": "Tokens must be a positive integer."
                    },
                    {
                        "id": "tests/test_response.py::test_inference_time4",
                        "description": "Inference time must be a positive float."
                    },
                    {
                        "id": "tests/test_response.py::test_score3",
                        "description": "Score must be a float between 0 and 1 (inclusive)."
                    },
                    {
                        "id": "tests/test_response.py::test_role4",
                        "description": "Role must be an instance of the `Role` enumeration."
                    },
                    {
                        "id": "tests/test_response.py::test_temperature3",
                        "description": "Temperature must be a positive float."
                    },
                    {
                        "id": "tests/test_response.py::test_model3",
                        "description": "Model must be a string."
                    },
                    {
                        "id": "tests/test_response.py::test_date3",
                        "description": "Creation date (if provided) must be a timezone-aware datetime object."
                    },
                    {
                        "id": "tests/test_response.py::test_date4",
                        "description": "Creation date (if provided) must be a timezone-aware datetime object."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::update_* methods",
                        "description": ""
                    },
                    {
                        "id": "memor/functions.py validation functions",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-11",
                "requirement_description": "Response Serialization and Deserialization\nThe system shall provide a mechanism to serialize a Response object to a JSON-compatible dictionary.\nThe system shall provide a mechanism to deserialize a Response object from a JSON-compatible dictionary or a JSON string.\nSerialization shall include all attributes, type (\"Response\"), and the Memor library version. Timestamps shall be formatted as strings. Role shall be its string value.\nDeserialization shall correctly reconstruct the Response object, including its attributes and timestamps.\nAttempting to deserialize from an invalid structure shall raise a `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_json1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_json2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::to_json",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::from_json",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::to_dict",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-12",
                "requirement_description": "Response Persistence\nThe system shall allow saving a Response object to a specified file path in JSON format.\nThe system shall allow loading a Response object from a specified file path, assuming it contains valid JSON data for a Response.\nSaving to an invalid path should report failure. Loading from a non-existent path should raise `FileNotFoundError`. Loading from an invalid path type should raise `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_save1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_save2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_load1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_load2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_load3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::save",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::load",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-13",
                "requirement_description": "Response Copying\nThe system shall allow creating a shallow copy of a Response object. The copy shall be a new instance with identical attribute values but a different object ID.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_copy1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_copy2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::copy",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::__copy__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-14",
                "requirement_description": "Response String Representation\nThe string representation of a Response object (e.g., via `str()`) shall be its message content.\nThe `repr()` representation shall provide a developer-friendly string indicating it's a Response object and its message.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_str",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_repr",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__str__",
                        "description": ""
                    },
                    {
                        "id": "memor/response.py::Response::__repr__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-15",
                "requirement_description": "Response Rendering\nThe system shall allow a Response object to be rendered into different formats specified by the `RenderFormat` enumeration (see FR-ENUM-2).\nSupported formats:\n`STRING`: Returns the response message.\n`OPENAI`: Returns a dictionary with \"role\" (string value of `Role`) and \"content\" (response message).\n`DICTIONARY`: Returns the full dictionary representation of the response (as per `to_dict`).\n`ITEMS`: Returns a list of key-value pairs from the dictionary representation.\nIf an invalid render format is provided, a `MemorValidationError` shall be raised.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_render1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_render2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_render3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_render4",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_render5",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::render",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-16",
                "requirement_description": "Response Token Estimation\nThe system shall allow estimating the number of tokens for the Response object's message content using a specified token estimation strategy (see FR-TOK-1).",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_estimated_tokens1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_estimated_tokens2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_estimated_tokens3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::estimate_tokens",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-17",
                "requirement_description": "Response Equality Comparison\nThe system shall allow comparing two Response objects for equality.\nTwo Response objects are considered equal if all their primary attributes (message, score, role, temperature, model, tokens, inference_time) are equal. Timestamps are not part of this equality check.\nComparison with non-Response objects should result in inequality.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_equality1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_equality2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_equality3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_equality4",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__eq__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RES-18",
                "requirement_description": "Response Length\nThe length of a Response object (e.g., via `len()`) shall be the length of its rendered string representation (i.e., its message content).\nAn empty response message results in a length of 0.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_length1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_response.py::test_length2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::__len__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-1",
                "requirement_description": "PromptTemplate Object Creation\nThe system shall allow the creation of a PromptTemplate object.\nA PromptTemplate object can be initialized:\na. With default values (None for content, title, custom_map).\nb. With specified initial values for content (string), title (string), and custom_map (dictionary of string-convertible key-value pairs).\nc. By loading its state from a specified JSON file path.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_title1",
                        "description": "implies creation"
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_load1",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-2",
                "requirement_description": "PromptTemplate Content Attribute\nThe system shall allow a PromptTemplate object to store a content string, which defines the template structure.\nThe content string uses Python-style formatting placeholders (e.g., `{placeholder_name}`).\nThe content attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_content1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_content2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::update_content",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::content",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-3",
                "requirement_description": "PromptTemplate Title Attribute\nThe system shall allow a PromptTemplate object to optionally store a title as a string.\nThe title attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_title1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_title2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_title3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::update_title",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::title",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-4",
                "requirement_description": "PromptTemplate Custom Map Attribute\nThe system shall allow a PromptTemplate object to optionally store a custom map.\nA custom map is a dictionary where keys are placeholder names (strings) and values are their corresponding string-convertible values to be inserted during rendering.\nThe custom_map attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_custom_map1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_custom_map2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::update_map",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::custom_map",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-5",
                "requirement_description": "PromptTemplate Timestamps\nEach PromptTemplate object shall automatically record its creation timestamp (UTC).\nEach PromptTemplate object shall automatically update a modification timestamp (UTC) whenever any of its attributes (title, content, custom_map) are updated.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_date_created",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_date_modified",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::update_* methods",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-6",
                "requirement_description": "PromptTemplate Attribute Validation\nThe system shall validate attributes of a PromptTemplate object upon setting or updating them:\nTitle must be a string.\nContent must be a string.\nCustom map must be a dictionary with string-convertible keys and values.\nValidation failures shall raise a `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_title4",
                        "description": "Title must be a string."
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_content3",
                        "description": "Content must be a string."
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_custom_map3",
                        "description": "Custom map must be a dictionary with string-convertible keys and values."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::update_* methods",
                        "description": ""
                    },
                    {
                        "id": "memor/functions.py validation functions",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-7",
                "requirement_description": "PromptTemplate Serialization and Deserialization\nThe system shall provide a mechanism to serialize a PromptTemplate object to a JSON-compatible dictionary.\nThe system shall provide a mechanism to deserialize a PromptTemplate object from a JSON-compatible dictionary or a JSON string.\nSerialization shall include title, content, custom_map, Memor library version, and formatted timestamps.\nDeserialization shall correctly reconstruct the PromptTemplate object.\nAttempting to deserialize from an invalid structure shall raise a `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_json1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_json2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::to_json",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::from_json",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::to_dict",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-8",
                "requirement_description": "PromptTemplate Persistence\nThe system shall allow saving a PromptTemplate object to a specified file path in JSON format.\nThe system shall allow loading a PromptTemplate object from a specified file path.\nSaving to an invalid path should report failure. Loading from a non-existent path should raise `FileNotFoundError`. Loading from an invalid path type should raise `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_save1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_save2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_load1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_load2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_load3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::save",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::load",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-9",
                "requirement_description": "PromptTemplate Copying\nThe system shall allow creating a shallow copy of a PromptTemplate object. The copy shall be a new instance with identical attribute values but a different object ID.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_copy1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_copy2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::copy",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::__copy__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-10",
                "requirement_description": "PromptTemplate String Representation\nThe string representation of a PromptTemplate object (e.g., via `str()`) shall be its content string.\nThe `repr()` representation shall provide a developer-friendly string indicating it's a PromptTemplate object and its content.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_str",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_repr",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::__str__",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::PromptTemplate::__repr__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-11",
                "requirement_description": "PromptTemplate Equality Comparison\nThe system shall allow comparing two PromptTemplate objects for equality.\nTwo PromptTemplate objects are considered equal if their content, title, and custom_map attributes are equal.\nComparison with non-PromptTemplate objects should result in inequality.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt_template.py::test_equality1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_equality2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_equality3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_equality4",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PromptTemplate::__eq__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TPL-12",
                "requirement_description": "Preset Prompt Templates\nThe system shall provide access to a collection of preset `PromptTemplate` objects.\nPreset templates are organized into groups: `BASIC`, `INSTRUCTION1`, `INSTRUCTION2`, `INSTRUCTION3`.\nEach group contains templates for common rendering needs, such as:\n`PROMPT`: Renders only the prompt message (optionally prefixed by an instruction).\n`RESPONSE`: Renders only the selected response message (optionally prefixed by an instruction).\n`RESPONSE0` to `RESPONSE3`: Renders specific responses from the prompt's list of responses by index (optionally prefixed by an instruction).\n`PROMPT_WITH_LABEL`: Renders the prompt message prefixed with \"Prompt: \" (and optionally a main instruction).\n`RESPONSE_WITH_LABEL`: Renders the selected response message prefixed with \"Response: \" (and optionally a main instruction).\n`RESPONSE0_WITH_LABEL` to `RESPONSE3_WITH_LABEL`: Labeled rendering for specific responses by index.\n`PROMPT_RESPONSE_STANDARD`: Renders labeled prompt and selected response, typically on separate lines (optionally prefixed by an instruction).\n`PROMPT_RESPONSE_FULL`: Renders a detailed, multi-line representation of prompt and selected response attributes (optionally prefixed by an instruction).\nThe `INSTRUCTION1`, `INSTRUCTION2`, `INSTRUCTION3` groups use specific predefined instruction strings prepended to the rendered output (e.g., \"I'm providing you with a history...\"). The `BASIC` group uses an empty instruction string.\nThe system must define a default prompt template, which is `PresetPromptTemplate.BASIC.PROMPT`.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_template1",
                        "description": "uses preset"
                    },
                    {
                        "id": "README.md examples",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py",
                        "description": "definitions of presets"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/template.py::PresetPromptTemplate",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::_BasicPresetPromptTemplate",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::_Instruction1PresetPromptTemplate",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::_Instruction2PresetPromptTemplate",
                        "description": ""
                    },
                    {
                        "id": "memor/template.py::_Instruction3PresetPromptTemplate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-1",
                "requirement_description": "Prompt Object Creation\nThe system shall allow the creation of a Prompt object.\nA Prompt object can be initialized:\na. With default values for its attributes.\nb. With specified initial values for attributes such as message, responses (list of `Response` objects), role, tokens, and template.\nc. By loading its state from a specified JSON file path.\nUpon creation, an initial check for renderability shall be performed by default; if this check fails due to incompatible template and properties, a `MemorRenderError` shall be raised. This check can be optionally suppressed.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_message1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_responses1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_load1",
                        "description": "implicit creation"
                    },
                    {
                        "id": "tests/test_prompt.py::test_init_check",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-2",
                "requirement_description": "Prompt Message Attribute\nThe system shall allow a Prompt object to store a message as a string.\nThe message attribute shall be updatable.\nIf no message is provided, it defaults to an empty string.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_message1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_message2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::update_message",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::message",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-3",
                "requirement_description": "Prompt Responses Attribute\nThe system shall allow a Prompt object to store an ordered list of associated `Response` objects.\nThe list of responses shall be updatable (replacing the entire list).\nThe system shall allow adding a new `Response` object to the list, either at the end or at a specified index.\nThe system shall allow removing a `Response` object from the list by its index.\nInitially, the list of responses is empty if not provided.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_responses1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_responses2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_responses3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_add_response1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_add_response2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_remove_response",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::update_responses",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::add_response",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::remove_response",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::responses",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-4",
                "requirement_description": "Prompt Selected Response\nThe system shall allow selecting one `Response` object from the Prompt's list of responses as the \"selected response\" by its index.\nIf responses exist, the first response (index 0) is selected by default.\nThe selected response is used during rendering when the template refers to a single response (e.g., `{response[message]}`).",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_select_response",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::select_response",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::selected_response",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-5",
                "requirement_description": "Prompt Role Attribute\nThe system shall allow a Prompt object to store a role, which must be a value from the `Role` enumeration (see FR-ENUM-1).\nIf no role is provided, it shall default to `USER`.\nThe role attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_role1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_role2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_role3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::update_role",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::role",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-6",
                "requirement_description": "Prompt Tokens Attribute\nThe system shall allow a Prompt object to optionally store a token count as a positive integer.\nThe tokens attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_tokens1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_tokens2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_tokens3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::update_tokens",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::tokens",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-7",
                "requirement_description": "Prompt Template Attribute\nThe system shall allow a Prompt object to be associated with a `PromptTemplate` object (either a custom one or a preset one).\nIf no template is provided, it shall default to `PresetPromptTemplate.DEFAULT` (see FR-TPL-12).\nThe template attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_template1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_template2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_template3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_template4",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::update_template",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::template",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-8",
                "requirement_description": "Prompt Timestamps\nEach Prompt object shall automatically record its creation timestamp (UTC).\nEach Prompt object shall automatically update a modification timestamp (UTC) whenever any of its attributes or its list of responses is updated.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_date_created",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_date_modified",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::update_* methods",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::add_response",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::remove_response",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::select_response",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-9",
                "requirement_description": "Prompt Attribute Validation\nThe system shall validate attributes and parameters for a Prompt object:\nMessage must be a string.\nTokens must be a positive integer.\nRole must be an instance of `Role`.\nResponses (when updating the list) must be a list of `Response` objects.\nResponse (when adding) must be an instance of `Response`.\nTemplate must be an instance of `PromptTemplate` or a `PresetPromptTemplate` enum member.\nValidation failures shall raise a `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_message3",
                        "description": "Message must be a string."
                    },
                    {
                        "id": "tests/test_prompt.py::test_tokens4",
                        "description": "Tokens must be a positive integer."
                    },
                    {
                        "id": "tests/test_prompt.py::test_role4",
                        "description": "Role must be an instance of `Role`."
                    },
                    {
                        "id": "tests/test_prompt.py::test_responses4",
                        "description": "Responses (when updating the list) must be a list of `Response` objects."
                    },
                    {
                        "id": "tests/test_prompt.py::test_responses5",
                        "description": "Responses (when updating the list) must be a list of `Response` objects."
                    },
                    {
                        "id": "tests/test_prompt.py::test_add_response3",
                        "description": "Response (when adding) must be an instance of `Response`."
                    },
                    {
                        "id": "tests/test_prompt.py::test_template5",
                        "description": "Template must be an instance of `PromptTemplate` or a `PresetPromptTemplate` enum member."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::update_* methods",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::add_response",
                        "description": ""
                    },
                    {
                        "id": "memor/functions.py validation functions",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-10",
                "requirement_description": "Prompt Serialization and Deserialization\nThe system shall provide a mechanism to serialize a Prompt object to a JSON-compatible dictionary.\nThe system shall provide a mechanism to deserialize a Prompt object from a JSON-compatible dictionary or a JSON string.\nSerialization shall include message, list of serialized responses, selected response index, role (as string value), tokens, Memor library version, and formatted timestamps.\nSerialization can optionally include the associated `PromptTemplate` (serialized). If not included, the prompt will use the default template upon deserialization.\nDeserialization shall correctly reconstruct the Prompt object. If template data is not present in the JSON, the default template is used.\nAttempting to deserialize from an invalid structure shall raise a `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_json1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_json2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_json3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::to_json",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::from_json",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::to_dict",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-11",
                "requirement_description": "Prompt Persistence\nThe system shall allow saving a Prompt object to a specified file path in JSON format. This can optionally include its template.\nThe system shall allow loading a Prompt object from a specified file path.\nSaving to an invalid path should report failure. Loading from a non-existent path should raise `FileNotFoundError`. Loading from an invalid path type should raise `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_save1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_save2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_save3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_load1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_load2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::save",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::load",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-12",
                "requirement_description": "Prompt Copying\nThe system shall allow creating a shallow copy of a Prompt object. The copy shall be a new instance with identical attribute values but a different object ID. Responses and template will be references to the original objects if they are mutable, or copies if they are immutable values.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_copy1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_copy2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::copy",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::__copy__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-13",
                "requirement_description": "Prompt Rendering\nThe system shall allow a Prompt object to be rendered into different formats using its associated `PromptTemplate` and attributes.\nThe rendering process substitutes placeholders in the template's content string. Placeholders can refer to:\nPrompt attributes (e.g., `{prompt[message]}`, `{prompt[role]}`).\nThe selected response's attributes (e.g., `{response[message]}`, `{response[temperature]}`).\nAttributes of specific responses in the prompt's list by index (e.g., `{responses[0][message]}`).\nValues from the template's `custom_map` (e.g., `{instruction}`).\nSupported output formats are specified by `RenderFormat` (see FR-ENUM-2):\n`STRING`: Returns the fully rendered string.\n`OPENAI`: Returns a dictionary with \"role\" (Prompt's role as string) and \"content\" (the rendered string).\n`DICTIONARY`: Returns a dictionary containing the Prompt's attributes and the rendered content string under a \"content\" key.\n`ITEMS`: Returns a list of key-value pairs from the `DICTIONARY` representation.\nIf rendering fails due to incompatible template/placeholders and prompt data (e.g., placeholder refers to a non-existent attribute or response index), a `MemorRenderError` shall be raised.\nIf an invalid render format is provided, a `MemorValidationError` shall be raised.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_render1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_render2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_render3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_render4",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_render5",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_render6",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::render",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-14",
                "requirement_description": "Prompt Renderability Check\nThe system shall provide a mechanism to check if a Prompt object can be successfully rendered with its current template and data without raising an error. This returns a boolean.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_check_render1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_check_render2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::check_render",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-15",
                "requirement_description": "Prompt String Representation\nThe string representation of a Prompt object (e.g., via `str()`) shall be its rendered content in `STRING` format.\nThe `repr()` representation shall provide a developer-friendly string indicating it's a Prompt object and its message.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_str",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_repr",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::__str__",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::__repr__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-16",
                "requirement_description": "Prompt Token Estimation\nThe system shall allow estimating the number of tokens for the Prompt object's rendered string output using a specified token estimation strategy (see FR-TOK-1).",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_estimated_tokens1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_estimated_tokens2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_estimated_tokens3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::estimate_tokens",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-17",
                "requirement_description": "Prompt Equality Comparison\nThe system shall allow comparing two Prompt objects for equality.\nTwo Prompt objects are considered equal if their message, list of responses, role, template, and tokens attributes are all equal.\nComparison with non-Prompt objects should result in inequality.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_equality1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_equality2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_equality3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_equality4",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::__eq__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRM-18",
                "requirement_description": "Prompt Length\nThe length of a Prompt object (e.g., via `len()`) shall be the length of its rendered string representation.\nIf the prompt is not renderable (e.g., due to template errors), its length shall be 0.\nAn empty prompt message (with a renderable template) may result in a non-zero length if the template itself has content.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_length1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_length2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_length3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/prompt.py::Prompt::__len__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-1",
                "requirement_description": "Session Object Creation\nThe system shall allow the creation of a Session object.\nA Session object can be initialized:\na. With a default `None` title and an empty list of messages.\nb. With a specified title (string) and an initial list of `Prompt` or `Response` objects.\nc. By loading its state from a specified JSON file path.\nUpon creation, an initial check for renderability of all contained messages shall be performed by default; if this check fails for any message, a `MemorRenderError` shall be raised. This check can be optionally suppressed.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_title1",
                        "description": "implies creation"
                    },
                    {
                        "id": "tests/test_session.py::test_messages1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_load1",
                        "description": "implicit creation"
                    },
                    {
                        "id": "tests/test_session.py::test_init_check",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-2",
                "requirement_description": "Session Title Attribute\nThe system shall allow a Session object to optionally store a title as a string.\nThe title attribute shall be updatable.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_title1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_title2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::update_title",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::title",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-3",
                "requirement_description": "Session Messages Management\nA Session object shall maintain an ordered list of messages, where each message is an instance of `Prompt` or `Response`.\nThe system shall allow updating the entire list of messages. If a new list of messages is provided, their statuses default to enabled (True).\nThe system shall allow adding a new message (`Prompt` or `Response`) to the list, either at the end or at a specified index. When adding a message, its status (enabled/disabled) can also be specified (defaults to enabled).\nThe system shall allow removing a message from the list by its index. This also removes its corresponding status.\nThe system shall allow clearing all messages from the session.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_messages1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_messages2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_add_message1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_add_message2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_remove_message",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_clear_messages",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::update_messages",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::add_message",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::remove_message",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::clear_messages",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::messages",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-4",
                "requirement_description": "Session Message Status Management\nFor each message in the session, the system shall maintain a corresponding boolean status (enabled/disabled). Enabled messages are included in rendering; disabled messages are skipped.\nThe list of message statuses shall be updatable. The length of the status list must match the length of the messages list.\nThe system shall provide methods to:\nEnable a message at a specific index.\nDisable a message at a specific index.\nMask a message (equivalent to disabling it).\nUnmask a message (equivalent to enabling it).\nThe system shall provide a way to get the list of \"masks\" (where True means masked/disabled).",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_messages_status1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_messages_status2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_enable_message",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_disable_message",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_mask_message",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_unmask_message",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_masks",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::update_messages_status",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::enable_message",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::disable_message",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::mask_message",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::unmask_message",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::messages_status",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::masks",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-5",
                "requirement_description": "Session Timestamps\nEach Session object shall automatically record its creation timestamp (UTC).\nEach Session object shall automatically update a modification timestamp (UTC) whenever its title, messages, or message statuses are updated.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_date_created",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_date_modified",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__init__",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::update_* methods",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::add_message",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::remove_message",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::clear_messages",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-6",
                "requirement_description": "Session Attribute Validation\nThe system shall validate attributes and parameters for a Session object:\nTitle must be a string.\nMessages (when updating the list) must be a list of `Prompt` or `Response` objects.\nMessage (when adding) must be an instance of `Prompt` or `Response`.\nStatus for an added message must be a boolean.\nMessage status list must be a list of booleans.\nMessage status list length must match the number of messages.\nValidation failures shall raise a `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_title3",
                        "description": "Title must be a string."
                    },
                    {
                        "id": "tests/test_session.py::test_messages3",
                        "description": "Messages (when updating the list) must be a list of `Prompt` or `Response` objects."
                    },
                    {
                        "id": "tests/test_session.py::test_messages4",
                        "description": "Messages (when updating the list) must be a list of `Prompt` or `Response` objects."
                    },
                    {
                        "id": "tests/test_session.py::test_add_message3",
                        "description": "Message (when adding) must be an instance of `Prompt` or `Response`."
                    },
                    {
                        "id": "tests/test_session.py::test_add_message4",
                        "description": "Status for an added message must be a boolean."
                    },
                    {
                        "id": "tests/test_session.py::test_messages_status3",
                        "description": "Message status list must be a list of booleans."
                    },
                    {
                        "id": "tests/test_session.py::test_messages_status4",
                        "description": "Message status list length must match the number of messages."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::update_* methods",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::add_message",
                        "description": ""
                    },
                    {
                        "id": "memor/functions.py validation functions",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-7",
                "requirement_description": "Session Serialization and Deserialization\nThe system shall provide a mechanism to serialize a Session object to a JSON-compatible dictionary.\nThe system shall provide a mechanism to deserialize a Session object from a JSON-compatible dictionary or a JSON string.\nSerialization shall include title, list of serialized messages (each with its type: \"Prompt\" or \"Response\"), list of message statuses, Memor library version, and formatted timestamps.\nDeserialization shall correctly reconstruct the Session object, including its messages (instantiating them as `Prompt` or `Response` based on their \"type\" field).\nAttempting to deserialize from an invalid structure (e.g., missing fields) will likely result in errors during attribute access or type casting, effectively leading to failure. Explicit top-level structure validation should result in `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_json",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::to_json",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::from_json",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::to_dict",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-8",
                "requirement_description": "Session Persistence\nThe system shall allow saving a Session object to a specified file path in JSON format.\nThe system shall allow loading a Session object from a specified file path.\nSaving to an invalid path should report failure. Loading from a non-existent path should raise `FileNotFoundError`. Loading from an invalid path type should raise `MemorValidationError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_save1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_save2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_load1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_load2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::save",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::load",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-9",
                "requirement_description": "Session Copying\nThe system shall allow creating a shallow copy of a Session object. The copy shall be a new instance with identical attribute values but a different object ID. The list of messages will be a shallow copy.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_copy1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_copy2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::copy",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::__copy__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-10",
                "requirement_description": "Session Rendering\nThe system shall allow a Session object to be rendered into different formats, considering only enabled messages.\nSupported output formats by `RenderFormat` (see FR-ENUM-2):\n`STRING`: Returns a concatenation of the string-rendered content of each enabled message, with each message typically followed by a newline.\n`OPENAI`: Returns a list, where each element is the OpenAI-formatted rendering (dictionary with \"role\" and \"content\") of an enabled message. If a message itself is a Session, its OpenAI rendering (a list) is extended into the main list.\n`DICTIONARY`: Returns a dictionary containing the Session's attributes and the concatenated string content (as per `STRING` format) under a \"content\" key.\n`ITEMS`: Returns a list of key-value pairs from the `DICTIONARY` representation.\nIf rendering any of its enabled messages fails, the overall session rendering will also fail (raising `MemorRenderError`).\nIf an invalid render format is provided, a `MemorValidationError` shall be raised.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_render1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_render2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_render3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_render4",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_render5",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::render",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-11",
                "requirement_description": "Session Renderability Check\nThe system shall provide a mechanism to check if a Session object (all its enabled messages) can be successfully rendered without raising an error. This returns a boolean.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_check_render1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_check_render2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::check_render",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-12",
                "requirement_description": "Session String Representation\nThe string representation of a Session object (e.g., via `str()`) shall be its rendered content in `STRING` format.\nThe `repr()` representation shall provide a developer-friendly string indicating it's a Session object and its title (if any).",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_str",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_repr",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__str__",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::__repr__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-13",
                "requirement_description": "Session Token Estimation\nThe system shall allow estimating the number of tokens for the Session's concatenated rendered string output (STRING format) using a specified token estimation strategy (see FR-TOK-1).",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_estimated_tokens1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_estimated_tokens2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_estimated_tokens3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::estimate_tokens",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-14",
                "requirement_description": "Session Equality Comparison\nThe system shall allow comparing two Session objects for equality.\nTwo Session objects are considered equal if their title and list of messages are equal. Message statuses are implicitly part of message list equality if messages are compared deeply or if statuses are also compared. (Source code suggests messages_status are also compared via `to_dict` in practice, if objects are fully serialized then compared). For direct `__eq__`, it's title and messages list.\nThe original code's `__eq__` compares `_title` and `_messages`. The `_messages_status` is not directly compared in `__eq__`.\nBased on `Session.__eq__`, two sessions are equal if their `_title` and `_messages` lists (comparing `Prompt`/`Response` objects within) are equal.\nComparison with non-Session objects should result in inequality.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_equality1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_equality2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_equality3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_equality4",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__eq__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-15",
                "requirement_description": "Session Length\nThe length of a Session object (e.g., via `len()`) shall be the total number of messages it contains (both enabled and disabled).",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_length",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__len__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-16",
                "requirement_description": "Session Iteration\nA Session object shall be iterable, yielding its contained `Prompt` or `Response` objects in order.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_iter",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__iter__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-17",
                "requirement_description": "Session Concatenation\nThe system shall support concatenating two Session objects using the `+` operator. This results in a new Session object containing all messages from the first operand followed by all messages from the second. The new Session's title shall be `None`. Message statuses are inherited with the messages.\nThe system shall support concatenating a Session object with a `Prompt` or `Response` object using the `+` operator (Session on left or right). This results in a new Session object with the `Prompt`/`Response` added to the original Session's messages. The original Session's title is preserved.\nAttempting to concatenate with an unsupported type shall raise a `TypeError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_addition1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_addition8",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__add__",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::__radd__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-18",
                "requirement_description": "Session Membership Testing\nThe system shall support checking if a specific `Prompt` or `Response` object is contained within a Session's messages using the `in` operator.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_contains1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_contains2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_contains3",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__contains__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SES-19",
                "requirement_description": "Session Message Access by Index\nThe system shall allow accessing individual messages within a Session by their integer index or a slice.",
                "test_traceability": [
                    {
                        "id": "tests/test_session.py::test_getitem1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_getitem2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/session.py::Session::__getitem__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TOK-1",
                "requirement_description": "Token Estimation Strategies\nThe system shall provide multiple strategies for estimating the number of tokens in a given text string, accessible via the `TokensEstimator` enumeration (see FR-ENUM-3).\nSupported strategies:\n`UNIVERSAL`: A general-purpose estimator.\n`OPENAI_GPT_3_5`: An estimator tuned for OpenAI GPT-3.5 models.\n`OPENAI_GPT_4`: An estimator tuned for OpenAI GPT-4 models.\nA default strategy (`UNIVERSAL`) shall be defined.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_estimated_tokens1",
                        "description": "demonstrates usage of enum"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/tokens_estimator.py::TokensEstimator",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TOK-2",
                "requirement_description": "Universal Token Estimator Behavior\nThe `UNIVERSAL` token estimator shall process input strings to count tokens. It should exhibit distinct counting behavior for:\nText with contractions (e.g., \"I'm\").\nCode snippets containing common programming symbols and structures (e.g., assignments, operators, control flow keywords).\nProgramming language keywords from a predefined set (Python, JavaScript, Java, C, C++, C#, Go, Rust, Swift, Kotlin, TypeScript, PHP, Ruby, SQL, Bash, MATLAB, R, Perl, Lua, Scala, Dart, Julia, Haskell, COBOL, Objective-C, F#, Lisp, Prolog, Ada, Delphi, Visual Basic, HTML, CSS).\nTokens with common prefixes (e.g., \"un\", \"re\") and suffixes (e.g., \"ing\", \"ed\").\nNumerical digits.\nString literals.\nIdentifiers with underscores or camelCase.\nThe specific counting logic aims to approximate tokenization by considering word-like units, symbols, and structural elements in code.",
                "test_traceability": [
                    {
                        "id": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_contractions",
                        "description": ""
                    },
                    {
                        "id": "::test_universal_tokens_estimator_with_code_snippets",
                        "description": ""
                    },
                    {
                        "id": "::test_universal_tokens_estimator_with_loops",
                        "description": ""
                    },
                    {
                        "id": "::test_universal_tokens_estimator_with_long_sentences",
                        "description": ""
                    },
                    {
                        "id": "::test_universal_tokens_estimator_with_variable_names",
                        "description": ""
                    },
                    {
                        "id": "::test_universal_tokens_estimator_with_function_definitions",
                        "description": ""
                    },
                    {
                        "id": "::test_universal_tokens_estimator_with_numbers",
                        "description": ""
                    },
                    {
                        "id": "::test_universal_tokens_estimator_with_print_statements",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/tokens_estimator.py::universal_tokens_estimator",
                        "description": ""
                    },
                    {
                        "id": "memor/tokens_estimator.py::_is_code_snippet",
                        "description": ""
                    },
                    {
                        "id": "memor/tokens_estimator.py::_preprocess_message",
                        "description": ""
                    },
                    {
                        "id": "memor/tokens_estimator.py::_tokenize_message",
                        "description": ""
                    },
                    {
                        "id": "memor/tokens_estimator.py::_count_code_tokens",
                        "description": ""
                    },
                    {
                        "id": "memor/tokens_estimator.py::_count_text_tokens",
                        "description": ""
                    },
                    {
                        "id": "memor/keywords.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TOK-3",
                "requirement_description": "OpenAI Token Estimator Behavior (GPT-3.5 and GPT-4)\nThe `OPENAI_GPT_3_5` and `OPENAI_GPT_4` token estimators shall process input strings to count tokens using heuristics based on character counts, spacing, punctuation, presence of newlines, long words, URLs, and programming keywords.\nThe `OPENAI_GPT_4` estimator may apply a slight adjustment factor compared to the `OPENAI_GPT_3_5` estimator.\nThe minimum estimated token count shall be 1.",
                "test_traceability": [
                    {
                        "id": "tests/test_token_estimators.py::test_openai_tokens_estimator_with_function_definition",
                        "description": ""
                    },
                    {
                        "id": "::test_openai_tokens_estimator_with_url",
                        "description": ""
                    },
                    {
                        "id": "::test_openai_tokens_estimator_with_long_words",
                        "description": ""
                    },
                    {
                        "id": "::test_openai_tokens_estimator_with_newlines",
                        "description": ""
                    },
                    {
                        "id": "::test_openai_tokens_estimator_with_gpt4_model",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/tokens_estimator.py::openai_tokens_estimator_gpt_3_5",
                        "description": ""
                    },
                    {
                        "id": "memor/tokens_estimator.py::openai_tokens_estimator_gpt_4",
                        "description": ""
                    },
                    {
                        "id": "memor/tokens_estimator.py::_openai_tokens_estimator",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ENUM-1",
                "requirement_description": "Role Enumeration\nThe system shall define an enumeration `Role` for representing the originator of a message.\nMembers shall include: `SYSTEM`, `USER`, `ASSISTANT`.\n`Role.USER` shall be designated as the default value for Prompts if not specified.\n`Role.ASSISTANT` shall be designated as the default value for Responses if not specified.\nEach member shall have an associated string value (e.g., \"system\", \"user\", \"assistant\").",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_role3",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_role3",
                        "description": ""
                    },
                    {
                        "id": "memor/params.py::Role",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/params.py::Role",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ENUM-2",
                "requirement_description": "RenderFormat Enumeration\nThe system shall define an enumeration `RenderFormat` for specifying output formats.\nMembers shall include: `STRING`, `OPENAI`, `DICTIONARY`, `ITEMS`.\n`RenderFormat.STRING` shall be designated as the default value if not specified.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_render1",
                        "description": "implicit default"
                    },
                    {
                        "id": "memor/params.py::RenderFormat",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/params.py::RenderFormat",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ENUM-3",
                "requirement_description": "TokensEstimator Enumeration\nThe system shall define an enumeration `TokensEstimator` for selecting token estimation strategies.\nMembers shall include: `UNIVERSAL`, `OPENAI_GPT_3_5`, `OPENAI_GPT_4`.\nEach member shall be associated with a corresponding token estimation function.\n`TokensEstimator.UNIVERSAL` shall be designated as the default value if not specified.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_estimated_tokens1",
                        "description": "implicit default"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/tokens_estimator.py::TokensEstimator",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-1",
                "requirement_description": "Custom Validation Error\nThe system shall define and use a custom exception class `MemorValidationError` (subclass of `ValueError`) for errors related to invalid input data, types, or values.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_message3",
                        "description": "Numerous tests catching `MemorValidationError`"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/errors.py::MemorValidationError",
                        "description": ""
                    },
                    {
                        "id": "usage throughout validation functions and methods",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-2",
                "requirement_description": "Custom Render Error\nThe system shall define and use a custom exception class `MemorRenderError` (subclass of `Exception`) for errors occurring during the rendering process, specifically when a prompt's template and its properties are incompatible.",
                "test_traceability": [
                    {
                        "id": "tests/test_prompt.py::test_render6",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_init_check",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/errors.py::MemorRenderError",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::render",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UTIL-1",
                "requirement_description": "UTC Timestamp Generation\nThe system shall provide a utility to get the current time as a timezone-aware `datetime` object in UTC.",
                "test_traceability": [
                    {
                        "id": "Implicitly tested by all `date_created`/`date_modified` tests.",
                        "description": "Implicitly tested by all `date_created`/`date_modified` tests."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/functions.py::get_time_utc",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UTIL-2",
                "requirement_description": "Common Input Validations\nThe system shall use internal validation functions to ensure:\nStrings are actual strings\nBooleans are actual booleans\nPositive integers are integers >= 0\nPositive floats are floats >= 0\nProbabilities are floats between 0 and 1 inclusive\nLists contain elements of a specific type\nDatetime objects are timezone-aware\nFile paths are strings and exist\nCustom maps for templates are dictionaries with string-convertible keys/values\nThese validations, when failed, should result in a `MemorValidationError` or `FileNotFoundError` as appropriate.",
                "test_traceability": [
                    {
                        "id": "Effects tested in entity-specific validation tests",
                        "description": "Effects tested in entity-specific validation tests (e.g., FR-RES-10)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/functions.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "NFR-1",
                "requirement_description": "Version Information\nThe library shall expose its version number (e.g., \"0.5\").\n*(Note: While testable, this is borderline. Including for completeness as it's an observable aspect from `params.py` and `__init__.py`.)*",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "memor/params.py::MEMOR_VERSION",
                        "description": ""
                    },
                    {
                        "id": "memor/__init__.py::__version__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "EIF-1",
                "requirement_description": "OpenAI Rendering Format\nWhen rendering for OpenAI (using `RenderFormat.OPENAI`), individual `Prompt` or `Response` objects shall produce a dictionary with two keys:\n`\"role\"`: The string value of the object's role (e.g., \"user\", \"assistant\", \"system\").\n`\"content\"`: The rendered string content of the object.\nA `Session` rendered for OpenAI shall produce a list of such dictionaries, corresponding to its enabled messages.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_render2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_render2",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_render2",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::render",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::render",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::render",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "EIF-2",
                "requirement_description": "JSON Persistence Format\nWhen `Response`, `PromptTemplate`, `Prompt`, and `Session` objects are serialized to JSON (e.g., for saving to a file or using `to_json()` methods), they must adhere to a consistent structure:\nEach object's JSON representation includes a `\"type\"` field for `Prompt` and `Response` when part of a `Session`'s message list, indicating \"Prompt\" or \"Response\". (Note: Top-level `to_json` for Prompt/Response might not add this if not strictly needed for self-contained deserialization, but Session's `from_json` relies on it for messages).\nTimestamps (`date_created`, `date_modified`) are serialized as strings in the format `\"%Y-%m-%d %H:%M:%S %z\"`.\n`Role` enum values are serialized as their string values (e.g., \"user\").\n`memor_version` attribute is included, storing the version of the Memor library that created the JSON.\nAll relevant attributes of the object are included. Nested objects (like `Response` in `Prompt`, `PromptTemplate` in `Prompt`, messages in `Session`) are also serialized to their JSON representations.",
                "test_traceability": [
                    {
                        "id": "tests/test_response.py::test_json1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt_template.py::test_json1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_prompt.py::test_json1",
                        "description": ""
                    },
                    {
                        "id": "tests/test_session.py::test_json",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "memor/response.py::Response::to_json",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt_template.py::PromptTemplate::to_json",
                        "description": ""
                    },
                    {
                        "id": "memor/prompt.py::Prompt::to_json",
                        "description": ""
                    },
                    {
                        "id": "memor/session.py::Session::to_json",
                        "description": ""
                    }
                ]
            }
        ],
        "commit_sha": "eeea8dd50d8dd59edf2eb6cc13057a4761820574",
        "full_code_skeleton": "--- File: memor/keywords.py ---\n```python\nCOMMON_PREFIXES = {\"un\", \"re\", \"in\", \"dis\", \"pre\", \"mis\", \"non\", \"over\", \"under\", \"sub\", \"trans\"}\n\nCOMMON_SUFFIXES = {\"ing\", \"ed\", \"ly\", \"es\", \"s\", \"ment\", \"able\", \"ness\", \"tion\", \"ive\", \"ous\"}\n\nPYTHON_KEYWORDS = {\"if\", \"else\", \"elif\", \"while\", \"for\", \"def\", \"return\", \"import\", \"from\", \"class\",\n                   \"try\", \"except\", \"finally\", \"with\", \"as\", \"break\", \"continue\", \"pass\", \"lambda\",\n                   \"True\", \"False\", \"None\", \"and\", \"or\", \"not\", \"in\", \"is\", \"global\", \"nonlocal\"}\n\nJAVASCRIPT_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                       \"continue\", \"function\", \"return\", \"var\", \"let\", \"const\", \"class\", \"extends\",\n                       \"super\", \"import\", \"export\", \"try\", \"catch\", \"finally\", \"throw\", \"new\",\n                       \"delete\", \"typeof\", \"instanceof\", \"in\", \"void\", \"yield\", \"this\", \"async\",\n                       \"await\", \"static\", \"get\", \"set\", \"true\", \"false\", \"null\", \"undefined\"}\n\nJAVA_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                 \"continue\", \"return\", \"void\", \"int\", \"float\", \"double\", \"char\", \"long\", \"short\",\n                 \"boolean\", \"byte\", \"class\", \"interface\", \"extends\", \"implements\", \"new\", \"import\",\n                 \"package\", \"public\", \"private\", \"protected\", \"static\", \"final\", \"abstract\",\n                 \"try\", \"catch\", \"finally\", \"throw\", \"throws\", \"synchronized\", \"volatile\", \"transient\",\n                 \"native\", \"strictfp\", \"assert\", \"instanceof\", \"super\", \"this\", \"true\", \"false\", \"null\"}\n\n\nC_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\", \"continue\",\n              \"return\", \"void\", \"char\", \"int\", \"float\", \"double\", \"short\", \"long\", \"signed\",\n              \"unsigned\", \"struct\", \"union\", \"typedef\", \"enum\", \"const\", \"volatile\", \"extern\",\n              \"register\", \"static\", \"auto\", \"sizeof\", \"goto\"}\n\n\nCPP_KEYWORDS = C_KEYWORDS | {\"new\", \"delete\", \"class\",\n                \"public\", \"private\", \"protected\", \"namespace\", \"using\", \"template\", \"friend\",\n                \"virtual\", \"inline\", \"operator\", \"explicit\", \"this\", \"true\", \"false\", \"nullptr\"}\n\n\nCSHARP_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                   \"continue\", \"return\", \"void\", \"int\", \"float\", \"double\", \"char\", \"long\", \"short\",\n                   \"bool\", \"byte\", \"class\", \"interface\", \"struct\", \"new\", \"namespace\", \"using\",\n                   \"public\", \"private\", \"protected\", \"static\", \"readonly\", \"const\", \"try\", \"catch\",\n                   \"finally\", \"throw\", \"async\", \"await\", \"true\", \"false\", \"null\"}\n\n\nGO_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"break\", \"continue\", \"return\",\n               \"func\", \"var\", \"const\", \"type\", \"struct\", \"interface\", \"map\", \"chan\", \"package\",\n               \"import\", \"defer\", \"go\", \"select\", \"range\", \"fallthrough\", \"goto\"}\n\n\nRUST_KEYWORDS = {\"if\", \"else\", \"match\", \"loop\", \"for\", \"while\", \"break\", \"continue\", \"return\",\n                 \"fn\", \"let\", \"const\", \"static\", \"struct\", \"enum\", \"trait\", \"impl\", \"mod\",\n                 \"use\", \"crate\", \"super\", \"self\", \"as\", \"type\", \"where\", \"pub\", \"unsafe\",\n                 \"dyn\", \"move\", \"async\", \"await\", \"true\", \"false\"}\n\n\nSWIFT_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"repeat\", \"break\",\n                  \"continue\", \"return\", \"func\", \"var\", \"let\", \"class\", \"struct\", \"enum\", \"protocol\",\n                  \"import\", \"defer\", \"as\", \"is\", \"try\", \"catch\", \"throw\", \"throws\", \"inout\",\n                  \"guard\", \"self\", \"super\", \"true\", \"false\", \"nil\"}\n\n\nKOTLIN_KEYWORDS = {\"if\", \"else\", \"when\", \"for\", \"while\", \"do\", \"break\", \"continue\", \"return\",\n                   \"fun\", \"val\", \"var\", \"class\", \"object\", \"interface\", \"enum\", \"sealed\",\n                   \"import\", \"package\", \"as\", \"is\", \"in\", \"try\", \"catch\", \"finally\", \"throw\",\n                   \"super\", \"this\", \"by\", \"constructor\", \"init\", \"companion\", \"override\",\n                   \"abstract\", \"final\", \"open\", \"private\", \"protected\", \"public\", \"internal\",\n                   \"inline\", \"suspend\", \"operator\", \"true\", \"false\", \"null\"}\n\nTYPESCRIPT_KEYWORDS = JAVASCRIPT_KEYWORDS | {\"interface\", \"type\", \"namespace\", \"declare\"}\n\n\nPHP_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                \"continue\", \"return\", \"function\", \"class\", \"public\", \"private\", \"protected\",\n                \"extends\", \"implements\", \"namespace\", \"use\", \"new\", \"static\", \"global\",\n                \"const\", \"var\", \"echo\", \"print\", \"try\", \"catch\", \"finally\", \"throw\", \"true\", \"false\", \"null\"}\n\nRUBY_KEYWORDS = {\"if\", \"else\", \"elsif\", \"unless\", \"case\", \"when\", \"for\", \"while\", \"do\", \"break\",\n                 \"continue\", \"return\", \"def\", \"class\", \"module\", \"end\", \"begin\", \"rescue\", \"ensure\",\n                 \"yield\", \"super\", \"self\", \"alias\", \"true\", \"false\", \"nil\"}\n\nSQL_KEYWORDS = {\"SELECT\", \"INSERT\", \"UPDATE\", \"DELETE\", \"FROM\", \"WHERE\", \"JOIN\", \"INNER\", \"LEFT\",\n                \"RIGHT\", \"FULL\", \"ON\", \"GROUP BY\", \"HAVING\", \"ORDER BY\", \"LIMIT\", \"OFFSET\", \"AS\",\n                \"AND\", \"OR\", \"NOT\", \"NULL\", \"TRUE\", \"FALSE\"}\n\nBASH_KEYWORDS = {\"if\", \"else\", \"fi\", \"then\", \"elif\", \"case\", \"esac\", \"for\", \"while\", \"do\", \"done\",\n                 \"break\", \"continue\", \"return\", \"function\", \"export\", \"readonly\", \"local\", \"declare\",\n                 \"eval\", \"trap\", \"exec\", \"true\", \"false\"}\n\nMATLAB_KEYWORDS = {\"if\", \"else\", \"elseif\", \"end\", \"for\", \"while\", \"break\", \"continue\", \"return\",\n                   \"function\", \"global\", \"persistent\", \"switch\", \"case\", \"otherwise\", \"try\", \"catch\",\n                   \"true\", \"false\"}\n\nR_KEYWORDS = {\"if\", \"else\", \"repeat\", \"while\", \"for\", \"break\", \"next\", \"return\", \"function\",\n              \"TRUE\", \"FALSE\", \"NULL\", \"Inf\", \"NaN\", \"NA\"}\n\n\nPERL_KEYWORDS = {\"if\", \"else\", \"elsif\", \"unless\", \"while\", \"for\", \"foreach\", \"do\", \"last\", \"next\",\n                 \"redo\", \"goto\", \"return\", \"sub\", \"package\", \"use\", \"require\", \"my\", \"local\", \"our\",\n                 \"state\", \"BEGIN\", \"END\", \"true\", \"false\"}\n\nLUA_KEYWORDS = {\"if\", \"else\", \"elseif\", \"then\", \"for\", \"while\", \"repeat\", \"until\", \"break\", \"return\",\n                \"function\", \"end\", \"local\", \"do\", \"true\", \"false\", \"nil\"}\n\nSCALA_KEYWORDS = {\"if\", \"else\", \"match\", \"case\", \"for\", \"while\", \"do\", \"yield\", \"return\",\n                  \"def\", \"val\", \"var\", \"lazy\", \"class\", \"object\", \"trait\", \"extends\",\n                  \"with\", \"import\", \"package\", \"new\", \"this\", \"super\", \"implicit\",\n                  \"override\", \"abstract\", \"final\", \"sealed\", \"private\", \"protected\",\n                  \"public\", \"try\", \"catch\", \"finally\", \"throw\", \"true\", \"false\", \"null\"}\n\nDART_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                 \"continue\", \"return\", \"var\", \"final\", \"const\", \"dynamic\", \"void\",\n                 \"int\", \"double\", \"bool\", \"String\", \"class\", \"interface\", \"extends\",\n                 \"implements\", \"mixin\", \"import\", \"library\", \"part\", \"typedef\",\n                 \"this\", \"super\", \"as\", \"is\", \"new\", \"try\", \"catch\", \"finally\", \"throw\",\n                 \"async\", \"await\", \"true\", \"false\", \"null\"}\n\nJULIA_KEYWORDS = {\"if\", \"else\", \"elseif\", \"for\", \"while\", \"break\", \"continue\", \"return\",\n                  \"function\", \"macro\", \"module\", \"import\", \"using\", \"export\", \"struct\",\n                  \"mutable\", \"const\", \"begin\", \"end\", \"do\", \"try\", \"catch\", \"finally\",\n                  \"true\", \"false\", \"nothing\"}\n\nHASKELL_KEYWORDS = {\"if\", \"then\", \"else\", \"case\", \"of\", \"let\", \"in\", \"where\", \"do\", \"module\",\n                    \"import\", \"class\", \"instance\", \"data\", \"type\", \"newtype\", \"deriving\",\n                    \"default\", \"foreign\", \"safe\", \"unsafe\", \"qualified\", \"true\", \"false\"}\n\nCOBOL_KEYWORDS = {\"ACCEPT\", \"ADD\", \"CALL\", \"CANCEL\", \"CLOSE\", \"COMPUTE\", \"CONTINUE\", \"DELETE\",\n                  \"DISPLAY\", \"DIVIDE\", \"EVALUATE\", \"EXIT\", \"GOBACK\", \"GO\", \"IF\", \"INITIALIZE\",\n                  \"INSPECT\", \"MERGE\", \"MOVE\", \"MULTIPLY\", \"OPEN\", \"PERFORM\", \"READ\", \"RETURN\",\n                  \"REWRITE\", \"SEARCH\", \"SET\", \"SORT\", \"START\", \"STOP\", \"STRING\", \"SUBTRACT\",\n                  \"UNSTRING\", \"WRITE\", \"END-IF\", \"END-PERFORM\"}\n\nOBJECTIVEC_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                       \"continue\", \"return\", \"void\", \"int\", \"float\", \"double\", \"char\", \"long\", \"short\",\n                       \"signed\", \"unsigned\", \"class\", \"interface\", \"protocol\", \"implementation\",\n                       \"try\", \"catch\", \"finally\", \"throw\", \"import\", \"self\", \"super\", \"atomic\",\n                       \"nonatomic\", \"strong\", \"weak\", \"retain\", \"copy\", \"assign\", \"true\", \"false\", \"nil\"}\n\nFSHARP_KEYWORDS = {\"if\", \"then\", \"else\", \"match\", \"with\", \"for\", \"while\", \"do\", \"done\", \"let\",\n                   \"rec\", \"in\", \"try\", \"finally\", \"raise\", \"exception\", \"function\", \"return\",\n                   \"type\", \"mutable\", \"namespace\", \"module\", \"open\", \"abstract\", \"override\",\n                   \"inherit\", \"base\", \"new\", \"true\", \"false\", \"null\"}\n\nLISP_KEYWORDS = {\"defun\", \"setq\", \"let\", \"lambda\", \"if\", \"cond\", \"loop\", \"dolist\", \"dotimes\",\n                 \"progn\", \"return\", \"function\", \"defmacro\", \"quote\", \"eval\", \"apply\", \"car\",\n                 \"cdr\", \"cons\", \"list\", \"mapcar\", \"format\", \"read\", \"print\", \"load\", \"t\", \"nil\"}\n\nPROLOG_KEYWORDS = {\"if\", \"else\", \"end\", \"fail\", \"true\", \"false\", \"not\", \"repeat\", \"is\",\n                   \"assert\", \"retract\", \"call\", \"findall\", \"bagof\", \"setof\", \"atom\",\n                   \"integer\", \"float\", \"char_code\", \"compound\", \"number\", \"var\"}\n\nADA_KEYWORDS = {\"if\", \"then\", \"else\", \"elsif\", \"case\", \"when\", \"for\", \"while\", \"loop\", \"exit\",\n                \"return\", \"procedure\", \"function\", \"package\", \"use\", \"is\", \"begin\", \"end\",\n                \"record\", \"type\", \"constant\", \"exception\", \"raise\", \"declare\", \"private\",\n                \"null\", \"true\", \"false\"}\n\nDELPHI_KEYWORDS = {\"if\", \"then\", \"else\", \"case\", \"of\", \"for\", \"while\", \"repeat\", \"until\", \"break\",\n                   \"continue\", \"begin\", \"end\", \"procedure\", \"function\", \"var\", \"const\", \"type\",\n                   \"class\", \"record\", \"interface\", \"implementation\", \"unit\", \"uses\", \"inherited\",\n                   \"try\", \"except\", \"finally\", \"raise\", \"private\", \"public\", \"protected\", \"published\",\n                   \"true\", \"false\", \"nil\"}\n\nVB_KEYWORDS = {\"If\", \"Then\", \"Else\", \"ElseIf\", \"End\", \"For\", \"Each\", \"While\", \"Do\", \"Loop\",\n               \"Select\", \"Case\", \"Try\", \"Catch\", \"Finally\", \"Throw\", \"Return\", \"Function\",\n               \"Sub\", \"Class\", \"Module\", \"Namespace\", \"Imports\", \"Inherits\", \"Implements\",\n               \"Public\", \"Private\", \"Protected\", \"Friend\", \"Shared\", \"Static\", \"Dim\", \"Const\",\n               \"New\", \"Me\", \"MyBase\", \"MyClass\", \"Not\", \"And\", \"Or\", \"True\", \"False\", \"Nothing\"}\n\nHTML_KEYWORDS = {\"html\", \"head\", \"title\", \"meta\", \"link\", \"style\", \"script\", \"body\", \"div\", \"span\",\n                 \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"a\", \"img\", \"ul\", \"ol\", \"li\", \"table\",\n                 \"tr\", \"td\", \"th\", \"thead\", \"tbody\", \"tfoot\", \"form\", \"input\", \"button\", \"label\",\n                 \"select\", \"option\", \"textarea\", \"fieldset\", \"legend\", \"iframe\", \"nav\", \"section\",\n                 \"article\", \"aside\", \"header\", \"footer\", \"main\", \"blockquote\", \"cite\", \"code\",\n                 \"pre\", \"em\", \"strong\", \"b\", \"i\", \"u\", \"small\", \"br\", \"hr\"}\n\nCSS_KEYWORDS = {\"color\", \"background\", \"border\", \"margin\", \"padding\", \"width\", \"height\", \"font-size\",\n                \"font-family\", \"text-align\", \"display\", \"position\", \"top\", \"bottom\", \"left\", \"right\",\n                \"z-index\", \"visibility\", \"opacity\", \"overflow\", \"cursor\", \"flex\", \"grid\", \"align-items\",\n                \"justify-content\", \"box-shadow\", \"text-shadow\", \"animation\", \"transition\", \"transform\",\n                \"clip-path\", \"content\", \"filter\", \"outline\", \"max-width\", \"min-width\", \"max-height\",\n                \"min-height\", \"letter-spacing\", \"line-height\", \"white-space\", \"word-break\"}\n\n\nPROGRAMMING_LANGUAGES = {\n    \"Python\": PYTHON_KEYWORDS,\n    \"JavaScript\": JAVASCRIPT_KEYWORDS,\n    \"Java\": JAVA_KEYWORDS,\n    \"C\": C_KEYWORDS,\n    \"C++\": CPP_KEYWORDS,\n    \"C#\": CSHARP_KEYWORDS,\n    \"Go\": GO_KEYWORDS,\n    \"Rust\": RUST_KEYWORDS,\n    \"Swift\": SWIFT_KEYWORDS,\n    \"Kotlin\": KOTLIN_KEYWORDS,\n    \"TypeScript\": TYPESCRIPT_KEYWORDS,\n    \"PHP\": PHP_KEYWORDS,\n    \"Ruby\": RUBY_KEYWORDS,\n    \"SQL\": SQL_KEYWORDS,\n    \"Bash\": BASH_KEYWORDS,\n    \"MATLAB\": MATLAB_KEYWORDS,\n    \"R\": R_KEYWORDS,\n    \"Perl\": PERL_KEYWORDS,\n    \"Lua\": LUA_KEYWORDS,\n    \"Scala\": SCALA_KEYWORDS,\n    \"Dart\": DART_KEYWORDS,\n    \"Julia\": JULIA_KEYWORDS,\n    \"Haskell\": HASKELL_KEYWORDS,\n    \"COBOL\": COBOL_KEYWORDS,\n    \"Objective-C\": OBJECTIVEC_KEYWORDS,\n    \"F#\": FSHARP_KEYWORDS,\n    \"Lisp\": LISP_KEYWORDS,\n    \"Prolog\": PROLOG_KEYWORDS,\n    \"Ada\": ADA_KEYWORDS,\n    \"Delphi\": DELPHI_KEYWORDS,\n    \"Visual Basic\": VB_KEYWORDS,\n    \"HTML\": HTML_KEYWORDS,\n    \"CSS\": CSS_KEYWORDS}\n```\n\n--- File: memor/prompt.py ---\n```python\nclass Prompt:\n    \"\"\"\n    Prompt class.\n\n    >>> from memor import Prompt, Role, Response\n    >>> responses = [Response(message=\"I am fine.\"), Response(message=\"I am not fine.\"), Response(message=\"I am okay.\")]\n    >>> prompt = Prompt(message=\"Hello, how are you?\", responses=responses)\n    >>> prompt.message\n    'Hello, how are you?'\n    >>> prompt.responses[1].message\n    'I am not fine.'\n    \"\"\"\n\n    def __init__(\n            self,\n            message: str = \"\",\n            responses: List[Response] = [],\n            role: Role = Role.DEFAULT,\n            tokens: int = None,\n            template: Union[PresetPromptTemplate, PromptTemplate] = PresetPromptTemplate.DEFAULT,\n            file_path: str = None,\n            init_check: bool = True) -> None:\n        \"\"\"\n        Prompt object initiator.\n\n        :param message: prompt message\n        :param responses: prompt responses\n        :param role: prompt role\n        :param tokens: tokens\n        :param template: prompt template\n        :param file_path: prompt file path\n        :param init_check: initial check flag\n        \"\"\"\n        pass\n\n    def __eq__(self, other_prompt: \"Prompt\") -> bool:\n        \"\"\"\n        Check prompts equality.\n\n        :param other_prompt: another prompt\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Prompt.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Prompt.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Prompt object.\"\"\"\n        pass\n\n    def __copy__(self) -> \"Prompt\":\n        \"\"\"\n        Return a copy of the Prompt object.\n\n        :return: a copy of Prompt object\n        \"\"\"\n        pass\n\n    def copy(self) -> \"Prompt\":\n        \"\"\"\n        Return a copy of the Prompt object.\n\n        :return: a copy of Prompt object\n        \"\"\"\n        pass\n\n    def add_response(self, response: Response, index: int = None) -> None:\n        \"\"\"\n        Add a response to the prompt object.\n\n        :param response: response\n        :param index: index\n        \"\"\"\n        pass\n\n    def remove_response(self, index: int) -> None:\n        \"\"\"\n        Remove a response from the prompt object.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def select_response(self, index: int) -> None:\n        \"\"\"\n        Select a response as selected response.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def update_responses(self, responses: List[Response]) -> None:\n        \"\"\"\n        Update the prompt responses.\n\n        :param responses: responses\n        \"\"\"\n        pass\n\n    def update_message(self, message: str) -> None:\n        \"\"\"\n        Update the prompt message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def update_role(self, role: Role) -> None:\n        \"\"\"\n        Update the prompt role.\n\n        :param role: role\n        \"\"\"\n        pass\n\n    def update_tokens(self, tokens: int) -> None:\n        \"\"\"\n        Update the tokens.\n\n        :param tokens: tokens\n        \"\"\"\n        pass\n\n    def update_template(self, template: PromptTemplate) -> None:\n        \"\"\"\n        Update the prompt template.\n\n        :param template: template\n        \"\"\"\n        pass\n\n    def save(self, file_path: str, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: prompt file path\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: prompt file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Convert the prompt to a JSON object.\n\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    def to_dict(self, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Convert the prompt to a dictionary.\n\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    @property\n    def message(self) -> str:\n        \"\"\"Get the prompt message.\"\"\"\n        pass\n\n    @property\n    def responses(self) -> List[Response]:\n        \"\"\"Get the prompt responses.\"\"\"\n        pass\n\n    @property\n    def role(self) -> Role:\n        \"\"\"Get the prompt role.\"\"\"\n        pass\n\n    @property\n    def tokens(self) -> int:\n        \"\"\"Get the prompt tokens.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the prompt creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the prompt object modification date.\"\"\"\n        pass\n\n    @property\n    def template(self) -> PromptTemplate:\n        \"\"\"Get the prompt template.\"\"\"\n        pass\n\n    @property\n    def selected_response(self) -> Response:\n        \"\"\"Get the prompt selected response.\"\"\"\n        pass\n\n    def render(self, render_format: RenderFormat = RenderFormat.DEFAULT) -> Union[str,\n                                                                                  Dict[str, Any],\n                                                                                  List[Tuple[str, Any]]]:\n        \"\"\"\n        Render method.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def check_render(self) -> bool:\n        \"\"\"Check render.\"\"\"\n        pass\n\n    def estimate_tokens(self, method: TokensEstimator = TokensEstimator.DEFAULT) -> int:\n        \"\"\"\n        Estimate the number of tokens in the prompt message.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n```\n\n--- File: memor/params.py ---\n```python\nMEMOR_VERSION = \"0.5\"\n\nDATE_TIME_FORMAT = \"%Y-%m-%d %H:%M:%S %z\"\n\nINVALID_PATH_MESSAGE = \"Invalid path. Path must be a string.\"\nPATH_DOES_NOT_EXIST_MESSAGE = \"Path {0} does not exist.\"\nINVALID_STR_VALUE_MESSAGE = \"Invalid value. `{0}` must be a string.\"\nINVALID_BOOL_VALUE_MESSAGE = \"Invalid value. `{0}` must be a boolean.\"\nINVALID_POSFLOAT_VALUE_MESSAGE = \"Invalid value. `{0}` must be a positive float.\"\nINVALID_POSINT_VALUE_MESSAGE = \"Invalid value. `{0}` must be a positive integer.\"\nINVALID_PROB_VALUE_MESSAGE = \"Invalid value. `{0}` must be a value between 0 and 1.\"\nINVALID_LIST_OF_X_MESSAGE = \"Invalid value. `{0}` must be a list of {1}.\"\nINVALID_DATETIME_MESSAGE = \"Invalid value. `{0}` must be a datetime object that includes timezone information.\"\nINVALID_TEMPLATE_MESSAGE = \"Invalid template. It must be an instance of `PromptTemplate` or `PresetPromptTemplate`.\"\nINVALID_RESPONSE_MESSAGE = \"Invalid response. It must be an instance of `Response`.\"\nINVALID_MESSAGE = \"Invalid message. It must be an instance of `Prompt` or `Response`.\"\nINVALID_MESSAGE_STATUS_LEN_MESSAGE = \"Invalid message status length. It must be equal to the number of messages.\"\nINVALID_CUSTOM_MAP_MESSAGE = \"Invalid custom map: it must be a dictionary with keys and values that can be converted to strings.\"\nINVALID_ROLE_MESSAGE = \"Invalid role. It must be an instance of Role enum.\"\nINVALID_TEMPLATE_STRUCTURE_MESSAGE = \"Invalid template structure. It should be a JSON object with proper fields.\"\nINVALID_PROMPT_STRUCTURE_MESSAGE = \"Invalid prompt structure. It should be a JSON object with proper fields.\"\nINVALID_RESPONSE_STRUCTURE_MESSAGE = \"Invalid response structure. It should be a JSON object with proper fields.\"\nINVALID_RENDER_FORMAT_MESSAGE = \"Invalid render format. It must be an instance of RenderFormat enum.\"\nPROMPT_RENDER_ERROR_MESSAGE = \"Prompt template and properties are incompatible.\"\nUNSUPPORTED_OPERAND_ERROR_MESSAGE = \"Unsupported operand type(s) for {0}: `{1}` and `{2}`\"\nDATA_SAVE_SUCCESS_MESSAGE = \"Everything seems good.\"\n\n\nclass Role(Enum):\n    \"\"\"Role enum.\"\"\"\n\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    DEFAULT = USER\n\n\nclass RenderFormat(Enum):\n    \"\"\"Render format.\"\"\"\n\n    STRING = \"STRING\"\n    OPENAI = \"OPENAI\"\n    DICTIONARY = \"DICTIONARY\"\n    ITEMS = \"ITEMS\"\n    DEFAULT = STRING\n```\n\n--- File: memor/__init__.py ---\n```python\n__version__ = MEMOR_VERSION\n```\n\n--- File: memor/template.py ---\n```python\nclass PromptTemplate:\n    r\"\"\"\n    Prompt template.\n\n    >>> template = PromptTemplate(content=\"Take a deep breath\\n{prompt_message}!\", title=\"Greeting\")\n    >>> template.title\n    'Greeting'\n    \"\"\"\n\n    def __init__(\n            self,\n            content: str = None,\n            file_path: str = None,\n            title: str = None,\n            custom_map: Dict[str, str] = None) -> None:\n        \"\"\"\n        Prompt template object initiator.\n\n        :param content: template content\n        :param file_path: template file path\n        :param title: template title\n        :param custom_map: custom map\n        \"\"\"\n        pass\n\n    def __eq__(self, other_template: \"PromptTemplate\") -> bool:\n        \"\"\"\n        Check templates equality.\n\n        :param other_template: another template\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of PromptTemplate.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of PromptTemplate.\"\"\"\n        pass\n\n    def __copy__(self) -> \"PromptTemplate\":\n        \"\"\"Return a copy of the PromptTemplate object.\"\"\"\n        pass\n\n    def copy(self) -> \"PromptTemplate\":\n        \"\"\"Return a copy of the PromptTemplate object.\"\"\"\n        pass\n\n    def update_title(self, title: str) -> None:\n        \"\"\"\n        Update title.\n\n        :param title: title\n        \"\"\"\n        pass\n\n    def update_content(self, content: str) -> None:\n        \"\"\"\n        Update content.\n\n        :param content: content\n        \"\"\"\n        pass\n\n    def update_map(self, custom_map: Dict[str, str]) -> None:\n        \"\"\"\n        Update custom map.\n\n        :param custom_map: custom map\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: template file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: template file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert PromptTemplate to json.\"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert PromptTemplate to dict.\"\"\"\n        pass\n\n    @property\n    def content(self) -> str:\n        \"\"\"Get the PromptTemplate content.\"\"\"\n        pass\n\n    @property\n    def title(self) -> str:\n        \"\"\"Get the PromptTemplate title.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the PromptTemplate creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the PromptTemplate modification date.\"\"\"\n        pass\n\n    @property\n    def custom_map(self) -> Dict[str, str]:\n        \"\"\"Get the PromptTemplate custom map.\"\"\"\n        pass\n\n\nPROMPT_INSTRUCTION1 = \"I'm providing you with a history of a previous conversation. Please consider this context when responding to my new question.\\n\"\nPROMPT_INSTRUCTION2 = \"Here is the context from a prior conversation. Please learn from this information and use it to provide a thoughtful and context-aware response to my next questions.\\n\"\nPROMPT_INSTRUCTION3 = \"I am sharing a record of a previous discussion. Use this information to provide a consistent and relevant answer to my next query.\\n\"\n\nBASIC_PROMPT_CONTENT = \"{instruction}{prompt[message]}\"\nBASIC_RESPONSE_CONTENT = \"{instruction}{response[message]}\"\nBASIC_RESPONSE0_CONTENT = \"{instruction}{responses[0][message]}\"\nBASIC_RESPONSE1_CONTENT = \"{instruction}{responses[1][message]}\"\nBASIC_RESPONSE2_CONTENT = \"{instruction}{responses[2][message]}\"\nBASIC_RESPONSE3_CONTENT = \"{instruction}{responses[3][message]}\"\nBASIC_PROMPT_CONTENT_LABEL = \"{instruction}Prompt: {prompt[message]}\"\nBASIC_RESPONSE_CONTENT_LABEL = \"{instruction}Response: {response[message]}\"\nBASIC_RESPONSE0_CONTENT_LABEL = \"{instruction}Response: {responses[0][message]}\"\nBASIC_RESPONSE1_CONTENT_LABEL = \"{instruction}Response: {responses[1][message]}\"\nBASIC_RESPONSE2_CONTENT_LABEL = \"{instruction}Response: {responses[2][message]}\"\nBASIC_RESPONSE3_CONTENT_LABEL = \"{instruction}Response: {responses[3][message]}\"\nBASIC_PROMPT_RESPONSE_STANDARD_CONTENT = \"{instruction}Prompt: {prompt[message]}\\nResponse: {response[message]}\"\nBASIC_PROMPT_RESPONSE_FULL_CONTENT = \"\"\"{instruction}\nPrompt:\n    Message: {prompt[message]}\n    Role: {prompt[role]}\n    Tokens: {prompt[tokens]}\n    Date: {prompt[date]}\nResponse:\n    Message: {response[message]}\n    Role: {response[role]}\n    Temperature: {response[temperature]}\n    Model: {response[model]}\n    Score: {response[score]}\n    Tokens: {response[tokens]}\n    Inference Time: {response[inference_time]}\n    Date: {response[date]}\"\"\"\n\n\nclass _BasicPresetPromptTemplate(Enum):\n    \"\"\"Preset basic-prompt templates.\"\"\"\n\n    PROMPT\n    RESPONSE\n    RESPONSE0\n    RESPONSE1\n    RESPONSE2\n    RESPONSE3\n    PROMPT_WITH_LABEL\n    RESPONSE_WITH_LABEL\n    RESPONSE0_WITH_LABEL\n    RESPONSE1_WITH_LABEL\n    RESPONSE2_WITH_LABEL\n    RESPONSE3_WITH_LABEL\n    PROMPT_RESPONSE_STANDARD\n    PROMPT_RESPONSE_FULL\n\n\nclass _Instruction1PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction1-prompt templates.\"\"\"\n\n    PROMPT\n    RESPONSE\n    RESPONSE0\n    RESPONSE1\n    RESPONSE2\n    RESPONSE3\n    PROMPT_WITH_LABEL\n    RESPONSE_WITH_LABEL\n    RESPONSE0_WITH_LABEL\n    RESPONSE1_WITH_LABEL\n    RESPONSE2_WITH_LABEL\n    RESPONSE3_WITH_LABEL\n    PROMPT_RESPONSE_STANDARD\n    PROMPT_RESPONSE_FULL\n\n\nclass _Instruction2PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction2-prompt templates.\"\"\"\n\n    PROMPT\n    RESPONSE\n    RESPONSE0\n    RESPONSE1\n    RESPONSE2\n    RESPONSE3\n    PROMPT_WITH_LABEL\n    RESPONSE_WITH_LABEL\n    RESPONSE0_WITH_LABEL\n    RESPONSE1_WITH_LABEL\n    RESPONSE2_WITH_LABEL\n    RESPONSE3_WITH_LABEL\n    PROMPT_RESPONSE_STANDARD\n    PROMPT_RESPONSE_FULL\n\n\nclass _Instruction3PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction3-prompt templates.\"\"\"\n\n    PROMPT\n    RESPONSE\n    RESPONSE0\n    RESPONSE1\n    RESPONSE2\n    RESPONSE3\n    PROMPT_WITH_LABEL\n    RESPONSE_WITH_LABEL\n    RESPONSE0_WITH_LABEL\n    RESPONSE1_WITH_LABEL\n    RESPONSE2_WITH_LABEL\n    RESPONSE3_WITH_LABEL\n    PROMPT_RESPONSE_STANDARD\n    PROMPT_RESPONSE_FULL\n\n\nclass PresetPromptTemplate:\n    \"\"\"Preset prompt templates.\"\"\"\n\n    BASIC = _BasicPresetPromptTemplate\n    INSTRUCTION1 = _Instruction1PresetPromptTemplate\n    INSTRUCTION2 = _Instruction2PresetPromptTemplate\n    INSTRUCTION3 = _Instruction3PresetPromptTemplate\n    DEFAULT\n```\n\n--- File: memor/errors.py ---\n```python\nclass MemorValidationError(ValueError):\n    \"\"\"Base class for validation errors in Memor.\"\"\"\n\n    pass\n\n\nclass MemorRenderError(Exception):\n    \"\"\"Base class for render error in Memor.\"\"\"\n\n    pass\n```\n\n--- File: memor/functions.py ---\n```python\ndef get_time_utc() -> datetime.datetime:\n    \"\"\"\n    Get time in UTC format.\n\n    :return: UTC format time as a datetime object\n    \"\"\"\n    pass\n\n\ndef _validate_string(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate string.\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_bool(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate boolean.\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _can_convert_to_string(value: Any) -> bool:\n    \"\"\"\n    Check if value can be converted to string.\n\n    :param value: value\n    \"\"\"\n    pass\n\n\ndef _validate_pos_int(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate positive integer.\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_pos_float(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate positive float.\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_probability(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate probability (a float between 0 and 1).\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_list_of(value: Any, parameter_name: str, type_: Type, type_name: str) -> bool:\n    \"\"\"\n    Validate list of values.\n\n    :param value: value\n    :param parameter_name: parameter name\n    :param type_: type\n    :param type_name: type name\n    \"\"\"\n    pass\n\n\ndef _validate_date_time(date_time: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate date time.\n\n    :param date_time: date time\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_path(path: Any) -> bool:\n    \"\"\"\n    Validate path property.\n\n    :param path: path\n    \"\"\"\n    pass\n\n\ndef _validate_custom_map(custom_map: Any) -> bool:\n    \"\"\"\n    Validate custom map a dictionary with keys and values that can be converted to strings.\n\n    :param custom_map: custom map\n    \"\"\"\n    pass\n```\n\n--- File: memor/session.py ---\n```python\nclass Session:\n    \"\"\"Session class.\"\"\"\n\n    def __init__(\n            self,\n            title: str = None,\n            messages: List[Union[Prompt, Response]] = [],\n            file_path: str = None,\n            init_check: bool = True) -> None:\n        \"\"\"\n        Session object initiator.\n\n        :param title: title\n        :param messages: messages\n        :param file_path: file path\n        :param init_check: initial check flag\n        \"\"\"\n        pass\n\n    def __eq__(self, other_session: \"Session\") -> bool:\n        \"\"\"\n        Check sessions equality.\n\n        :param other_session: other session\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Session.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Session.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Session object.\"\"\"\n        pass\n\n    def __iter__(self) -> Generator[Union[Prompt, Response], None, None]:\n        \"\"\"Iterate through the Session object.\"\"\"\n        pass\n\n    def __add__(self, other_object: Union[\"Session\", Response, Prompt]) -> \"Session\":\n        \"\"\"\n        Addition method.\n\n        :param other_object: other object\n        \"\"\"\n        pass\n\n    def __radd__(self, other_object: Union[\"Session\", Response, Prompt]) -> \"Session\":\n        \"\"\"\n        Reverse addition method.\n\n        :param other_object: other object\n        \"\"\"\n        pass\n\n    def __contains__(self, message: Union[Prompt, Response]) -> bool:\n        \"\"\"\n        Check if the Session contains the given message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def __getitem__(self, index: int) -> Union[Prompt, Response]:\n        \"\"\"\n        Return the Session message(s).\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def __copy__(self) -> \"Session\":\n        \"\"\"Return a copy of the Session object.\"\"\"\n        pass\n\n    def copy(self) -> \"Session\":\n        \"\"\"Return a copy of the Session object.\"\"\"\n        pass\n\n    def add_message(self,\n                    message: Union[Prompt, Response],\n                    status: bool = True,\n                    index: int = None) -> None:\n        \"\"\"\n        Add a message to the session object.\n\n        :param message: message\n        :param status: status\n        :param index: index\n        \"\"\"\n        pass\n\n    def remove_message(self, index: int) -> None:\n        \"\"\"\n        Remove a message from the session object.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def clear_messages(self) -> None:\n        \"\"\"Remove all messages.\"\"\"\n        pass\n\n    def enable_message(self, index: int) -> None:\n        \"\"\"\n        Enable a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def disable_message(self, index: int) -> None:\n        \"\"\"\n        Disable a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def mask_message(self, index: int) -> None:\n        \"\"\"\n        Mask a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def unmask_message(self, index: int) -> None:\n        \"\"\"\n        Unmask a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def update_title(self, title: str) -> None:\n        \"\"\"\n        Update the session title.\n\n        :param title: title\n        \"\"\"\n        pass\n\n    def update_messages(self,\n                        messages: List[Union[Prompt, Response]],\n                        status: List[bool] = None) -> None:\n        \"\"\"\n        Update the session messages.\n\n        :param messages: messages\n        :param status: status\n        \"\"\"\n        pass\n\n    def update_messages_status(self, status: List[bool]) -> None:\n        \"\"\"\n        Update the session messages status.\n\n        :param status: status\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: session file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: session file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert the session to a JSON object.\"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Convert the session to a dictionary.\n\n        :return: dict\n        \"\"\"\n        pass\n\n    def render(self, render_format: RenderFormat = RenderFormat.DEFAULT) -> Union[str,\n                                                                                  Dict[str, Any],\n                                                                                  List[Tuple[str, Any]]]:\n        \"\"\"\n        Render method.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def check_render(self) -> bool:\n        \"\"\"Check render.\"\"\"\n        pass\n\n    def estimate_tokens(self, method: TokensEstimator = TokensEstimator.DEFAULT) -> int:\n        \"\"\"\n        Estimate the number of tokens in the session.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the session creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the session object modification date.\"\"\"\n        pass\n\n    @property\n    def title(self) -> str:\n        \"\"\"Get the session title.\"\"\"\n        pass\n\n    @property\n    def messages(self) -> List[Union[Prompt, Response]]:\n        \"\"\"Get the session messages.\"\"\"\n        pass\n\n    @property\n    def messages_status(self) -> List[bool]:\n        \"\"\"Get the session messages status.\"\"\"\n        pass\n\n    @property\n    def masks(self) -> List[bool]:\n        \"\"\"Get the session masks.\"\"\"\n        pass\n```\n\n--- File: memor/tokens_estimator.py ---\n```python\ndef _is_code_snippet(message: str) -> bool:\n    \"\"\"\n    Check if the message is a code snippet based on common coding symbols.\n\n    :param message: The input message to check.\n    :return: Boolean indicating if the message is a code snippet.\n    \"\"\"\n    pass\n\n\ndef _preprocess_message(message: str, is_code: bool) -> str:\n    \"\"\"\n    Preprocess message by replacing contractions in non-code text.\n\n    :param message: The input message to preprocess.\n    :param is_code: Boolean indicating if the message is a code.\n    :return: Preprocessed message.\n    \"\"\"\n    pass\n\n\ndef _tokenize_message(message: str) -> List[str]:\n    \"\"\"\n    Tokenize the message based on words, symbols, and numbers.\n\n    :param message: The input message to tokenize.\n    :return: List of tokens.\n    \"\"\"\n    pass\n\n\ndef _count_code_tokens(token: str, common_keywords: Set[str]) -> int:\n    \"\"\"\n    Count tokens in code snippets considering different token types.\n\n    :param token: The token to count.\n    :param common_keywords: Set of common keywords in programming languages.\n    :return: Count of tokens.\n    \"\"\"\n    pass\n\n\ndef _count_text_tokens(token: str, prefixes: Set[str], suffixes: Set[str]) -> int:\n    \"\"\"\n    Count tokens in text based on prefixes, suffixes, and subwords.\n\n    :param token: The token to count.\n    :param prefixes: Set of common prefixes.\n    :param suffixes: Set of common suffixes.\n    :return: Token count.\n    \"\"\"\n    pass\n\n\ndef universal_tokens_estimator(message: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text or code snippet.\n\n    :param message: The input text or code snippet to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\n\n\ndef _openai_tokens_estimator(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's models.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\n\n\ndef openai_tokens_estimator_gpt_3_5(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's GPT-3.5 Turbo model.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\n\n\ndef openai_tokens_estimator_gpt_4(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's GPT-4 model.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\n\n\nclass TokensEstimator(Enum):\n    \"\"\"Token estimator enum.\"\"\"\n\n    UNIVERSAL = universal_tokens_estimator\n    OPENAI_GPT_3_5 = openai_tokens_estimator_gpt_3_5\n    OPENAI_GPT_4 = openai_tokens_estimator_gpt_4\n    DEFAULT = UNIVERSAL\n```\n\n--- File: memor/response.py ---\n```python\nclass Response:\n    \"\"\"\n    Response class.\n\n    >>> from memor import Response, Role\n    >>> response = Response(message=\"Hello!\", score=0.9, role=Role.ASSISTANT, temperature=0.5, model=\"gpt-3.5\")\n    >>> response.message\n    'Hello!'\n    \"\"\"\n\n    def __init__(\n            self,\n            message: str = \"\",\n            score: float = None,\n            role: Role = Role.ASSISTANT,\n            temperature: float = None,\n            tokens: int = None,\n            inference_time: float = None,\n            model: str = None,\n            date: datetime.datetime = get_time_utc(),\n            file_path: str = None) -> None:\n        \"\"\"\n        Response object initiator.\n\n        :param message: response message\n        :param score: response score\n        :param role: response role\n        :param temperature: temperature\n        :param tokens: tokens\n        :param inference_time: inference time\n        :param model: agent model\n        :param date: response date\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def __eq__(self, other_response: \"Response\") -> bool:\n        \"\"\"\n        Check responses equality.\n\n        :param other_response: another response\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Response.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Response.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Response object.\"\"\"\n        pass\n\n    def __copy__(self) -> \"Response\":\n        \"\"\"Return a copy of the Response object.\"\"\"\n        pass\n\n    def copy(self) -> \"Response\":\n        \"\"\"Return a copy of the Response object.\"\"\"\n        pass\n\n    def update_message(self, message: str) -> None:\n        \"\"\"\n        Update the response message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def update_score(self, score: float) -> None:\n        \"\"\"\n        Update the response score.\n\n        :param score: score\n        \"\"\"\n        pass\n\n    def update_role(self, role: Role) -> None:\n        \"\"\"\n        Update the response role.\n\n        :param role: role\n        \"\"\"\n        pass\n\n    def update_temperature(self, temperature: float) -> None:\n        \"\"\"\n        Update the temperature.\n\n        :param temperature: temperature\n        \"\"\"\n        pass\n\n    def update_tokens(self, tokens: int) -> None:\n        \"\"\"\n        Update the tokens.\n\n        :param tokens: tokens\n        \"\"\"\n        pass\n\n    def update_inference_time(self, inference_time: float) -> None:\n        \"\"\"\n        Update inference time.\n\n        :param inference_time: inference time\n        \"\"\"\n        pass\n\n    def update_model(self, model: str) -> None:\n        \"\"\"\n        Update the agent model.\n\n        :param model: model\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert the response to a JSON object.\"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert the response to a dictionary.\"\"\"\n        pass\n\n    def render(self,\n               render_format: RenderFormat = RenderFormat.DEFAULT) -> Union[str,\n                                                                            Dict[str, Any],\n                                                                            List[Tuple[str, Any]]]:\n        \"\"\"\n        Render the response.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def estimate_tokens(self, method: TokensEstimator = TokensEstimator.DEFAULT) -> int:\n        \"\"\"\n        Estimate the number of tokens in the response message.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n\n    @property\n    def message(self) -> str:\n        \"\"\"Get the response message.\"\"\"\n        pass\n\n    @property\n    def score(self) -> float:\n        \"\"\"Get the response score.\"\"\"\n        pass\n\n    @property\n    def temperature(self) -> float:\n        \"\"\"Get the temperature.\"\"\"\n        pass\n\n    @property\n    def tokens(self) -> int:\n        \"\"\"Get the tokens.\"\"\"\n        pass\n\n    @property\n    def inference_time(self) -> float:\n        \"\"\"Get inference time.\"\"\"\n        pass\n\n    @property\n    def role(self) -> Role:\n        \"\"\"Get the response role.\"\"\"\n        pass\n\n    @property\n    def model(self) -> str:\n        \"\"\"Get the agent model.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the response creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the response object modification date.\"\"\"\n        pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "memor/keywords.py",
                "code": "COMMON_PREFIXES = {\"un\", \"re\", \"in\", \"dis\", \"pre\", \"mis\", \"non\", \"over\", \"under\", \"sub\", \"trans\"}\n\nCOMMON_SUFFIXES = {\"ing\", \"ed\", \"ly\", \"es\", \"s\", \"ment\", \"able\", \"ness\", \"tion\", \"ive\", \"ous\"}\n\nPYTHON_KEYWORDS = {\"if\", \"else\", \"elif\", \"while\", \"for\", \"def\", \"return\", \"import\", \"from\", \"class\",\n                   \"try\", \"except\", \"finally\", \"with\", \"as\", \"break\", \"continue\", \"pass\", \"lambda\",\n                   \"True\", \"False\", \"None\", \"and\", \"or\", \"not\", \"in\", \"is\", \"global\", \"nonlocal\"}\n\nJAVASCRIPT_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                       \"continue\", \"function\", \"return\", \"var\", \"let\", \"const\", \"class\", \"extends\",\n                       \"super\", \"import\", \"export\", \"try\", \"catch\", \"finally\", \"throw\", \"new\",\n                       \"delete\", \"typeof\", \"instanceof\", \"in\", \"void\", \"yield\", \"this\", \"async\",\n                       \"await\", \"static\", \"get\", \"set\", \"true\", \"false\", \"null\", \"undefined\"}\n\nJAVA_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                 \"continue\", \"return\", \"void\", \"int\", \"float\", \"double\", \"char\", \"long\", \"short\",\n                 \"boolean\", \"byte\", \"class\", \"interface\", \"extends\", \"implements\", \"new\", \"import\",\n                 \"package\", \"public\", \"private\", \"protected\", \"static\", \"final\", \"abstract\",\n                 \"try\", \"catch\", \"finally\", \"throw\", \"throws\", \"synchronized\", \"volatile\", \"transient\",\n                 \"native\", \"strictfp\", \"assert\", \"instanceof\", \"super\", \"this\", \"true\", \"false\", \"null\"}\n\n\nC_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\", \"continue\",\n              \"return\", \"void\", \"char\", \"int\", \"float\", \"double\", \"short\", \"long\", \"signed\",\n              \"unsigned\", \"struct\", \"union\", \"typedef\", \"enum\", \"const\", \"volatile\", \"extern\",\n              \"register\", \"static\", \"auto\", \"sizeof\", \"goto\"}\n\n\nCPP_KEYWORDS = C_KEYWORDS | {\"new\", \"delete\", \"class\",\n                \"public\", \"private\", \"protected\", \"namespace\", \"using\", \"template\", \"friend\",\n                \"virtual\", \"inline\", \"operator\", \"explicit\", \"this\", \"true\", \"false\", \"nullptr\"}\n\n\nCSHARP_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                   \"continue\", \"return\", \"void\", \"int\", \"float\", \"double\", \"char\", \"long\", \"short\",\n                   \"bool\", \"byte\", \"class\", \"interface\", \"struct\", \"new\", \"namespace\", \"using\",\n                   \"public\", \"private\", \"protected\", \"static\", \"readonly\", \"const\", \"try\", \"catch\",\n                   \"finally\", \"throw\", \"async\", \"await\", \"true\", \"false\", \"null\"}\n\n\nGO_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"break\", \"continue\", \"return\",\n               \"func\", \"var\", \"const\", \"type\", \"struct\", \"interface\", \"map\", \"chan\", \"package\",\n               \"import\", \"defer\", \"go\", \"select\", \"range\", \"fallthrough\", \"goto\"}\n\n\nRUST_KEYWORDS = {\"if\", \"else\", \"match\", \"loop\", \"for\", \"while\", \"break\", \"continue\", \"return\",\n                 \"fn\", \"let\", \"const\", \"static\", \"struct\", \"enum\", \"trait\", \"impl\", \"mod\",\n                 \"use\", \"crate\", \"super\", \"self\", \"as\", \"type\", \"where\", \"pub\", \"unsafe\",\n                 \"dyn\", \"move\", \"async\", \"await\", \"true\", \"false\"}\n\n\nSWIFT_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"repeat\", \"break\",\n                  \"continue\", \"return\", \"func\", \"var\", \"let\", \"class\", \"struct\", \"enum\", \"protocol\",\n                  \"import\", \"defer\", \"as\", \"is\", \"try\", \"catch\", \"throw\", \"throws\", \"inout\",\n                  \"guard\", \"self\", \"super\", \"true\", \"false\", \"nil\"}\n\n\nKOTLIN_KEYWORDS = {\"if\", \"else\", \"when\", \"for\", \"while\", \"do\", \"break\", \"continue\", \"return\",\n                   \"fun\", \"val\", \"var\", \"class\", \"object\", \"interface\", \"enum\", \"sealed\",\n                   \"import\", \"package\", \"as\", \"is\", \"in\", \"try\", \"catch\", \"finally\", \"throw\",\n                   \"super\", \"this\", \"by\", \"constructor\", \"init\", \"companion\", \"override\",\n                   \"abstract\", \"final\", \"open\", \"private\", \"protected\", \"public\", \"internal\",\n                   \"inline\", \"suspend\", \"operator\", \"true\", \"false\", \"null\"}\n\nTYPESCRIPT_KEYWORDS = JAVASCRIPT_KEYWORDS | {\"interface\", \"type\", \"namespace\", \"declare\"}\n\n\nPHP_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                \"continue\", \"return\", \"function\", \"class\", \"public\", \"private\", \"protected\",\n                \"extends\", \"implements\", \"namespace\", \"use\", \"new\", \"static\", \"global\",\n                \"const\", \"var\", \"echo\", \"print\", \"try\", \"catch\", \"finally\", \"throw\", \"true\", \"false\", \"null\"}\n\nRUBY_KEYWORDS = {\"if\", \"else\", \"elsif\", \"unless\", \"case\", \"when\", \"for\", \"while\", \"do\", \"break\",\n                 \"continue\", \"return\", \"def\", \"class\", \"module\", \"end\", \"begin\", \"rescue\", \"ensure\",\n                 \"yield\", \"super\", \"self\", \"alias\", \"true\", \"false\", \"nil\"}\n\nSQL_KEYWORDS = {\"SELECT\", \"INSERT\", \"UPDATE\", \"DELETE\", \"FROM\", \"WHERE\", \"JOIN\", \"INNER\", \"LEFT\",\n                \"RIGHT\", \"FULL\", \"ON\", \"GROUP BY\", \"HAVING\", \"ORDER BY\", \"LIMIT\", \"OFFSET\", \"AS\",\n                \"AND\", \"OR\", \"NOT\", \"NULL\", \"TRUE\", \"FALSE\"}\n\nBASH_KEYWORDS = {\"if\", \"else\", \"fi\", \"then\", \"elif\", \"case\", \"esac\", \"for\", \"while\", \"do\", \"done\",\n                 \"break\", \"continue\", \"return\", \"function\", \"export\", \"readonly\", \"local\", \"declare\",\n                 \"eval\", \"trap\", \"exec\", \"true\", \"false\"}\n\nMATLAB_KEYWORDS = {\"if\", \"else\", \"elseif\", \"end\", \"for\", \"while\", \"break\", \"continue\", \"return\",\n                   \"function\", \"global\", \"persistent\", \"switch\", \"case\", \"otherwise\", \"try\", \"catch\",\n                   \"true\", \"false\"}\n\nR_KEYWORDS = {\"if\", \"else\", \"repeat\", \"while\", \"for\", \"break\", \"next\", \"return\", \"function\",\n              \"TRUE\", \"FALSE\", \"NULL\", \"Inf\", \"NaN\", \"NA\"}\n\n\nPERL_KEYWORDS = {\"if\", \"else\", \"elsif\", \"unless\", \"while\", \"for\", \"foreach\", \"do\", \"last\", \"next\",\n                 \"redo\", \"goto\", \"return\", \"sub\", \"package\", \"use\", \"require\", \"my\", \"local\", \"our\",\n                 \"state\", \"BEGIN\", \"END\", \"true\", \"false\"}\n\nLUA_KEYWORDS = {\"if\", \"else\", \"elseif\", \"then\", \"for\", \"while\", \"repeat\", \"until\", \"break\", \"return\",\n                \"function\", \"end\", \"local\", \"do\", \"true\", \"false\", \"nil\"}\n\nSCALA_KEYWORDS = {\"if\", \"else\", \"match\", \"case\", \"for\", \"while\", \"do\", \"yield\", \"return\",\n                  \"def\", \"val\", \"var\", \"lazy\", \"class\", \"object\", \"trait\", \"extends\",\n                  \"with\", \"import\", \"package\", \"new\", \"this\", \"super\", \"implicit\",\n                  \"override\", \"abstract\", \"final\", \"sealed\", \"private\", \"protected\",\n                  \"public\", \"try\", \"catch\", \"finally\", \"throw\", \"true\", \"false\", \"null\"}\n\nDART_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                 \"continue\", \"return\", \"var\", \"final\", \"const\", \"dynamic\", \"void\",\n                 \"int\", \"double\", \"bool\", \"String\", \"class\", \"interface\", \"extends\",\n                 \"implements\", \"mixin\", \"import\", \"library\", \"part\", \"typedef\",\n                 \"this\", \"super\", \"as\", \"is\", \"new\", \"try\", \"catch\", \"finally\", \"throw\",\n                 \"async\", \"await\", \"true\", \"false\", \"null\"}\n\nJULIA_KEYWORDS = {\"if\", \"else\", \"elseif\", \"for\", \"while\", \"break\", \"continue\", \"return\",\n                  \"function\", \"macro\", \"module\", \"import\", \"using\", \"export\", \"struct\",\n                  \"mutable\", \"const\", \"begin\", \"end\", \"do\", \"try\", \"catch\", \"finally\",\n                  \"true\", \"false\", \"nothing\"}\n\nHASKELL_KEYWORDS = {\"if\", \"then\", \"else\", \"case\", \"of\", \"let\", \"in\", \"where\", \"do\", \"module\",\n                    \"import\", \"class\", \"instance\", \"data\", \"type\", \"newtype\", \"deriving\",\n                    \"default\", \"foreign\", \"safe\", \"unsafe\", \"qualified\", \"true\", \"false\"}\n\nCOBOL_KEYWORDS = {\"ACCEPT\", \"ADD\", \"CALL\", \"CANCEL\", \"CLOSE\", \"COMPUTE\", \"CONTINUE\", \"DELETE\",\n                  \"DISPLAY\", \"DIVIDE\", \"EVALUATE\", \"EXIT\", \"GOBACK\", \"GO\", \"IF\", \"INITIALIZE\",\n                  \"INSPECT\", \"MERGE\", \"MOVE\", \"MULTIPLY\", \"OPEN\", \"PERFORM\", \"READ\", \"RETURN\",\n                  \"REWRITE\", \"SEARCH\", \"SET\", \"SORT\", \"START\", \"STOP\", \"STRING\", \"SUBTRACT\",\n                  \"UNSTRING\", \"WRITE\", \"END-IF\", \"END-PERFORM\"}\n\nOBJECTIVEC_KEYWORDS = {\"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \"do\", \"break\",\n                       \"continue\", \"return\", \"void\", \"int\", \"float\", \"double\", \"char\", \"long\", \"short\",\n                       \"signed\", \"unsigned\", \"class\", \"interface\", \"protocol\", \"implementation\",\n                       \"try\", \"catch\", \"finally\", \"throw\", \"import\", \"self\", \"super\", \"atomic\",\n                       \"nonatomic\", \"strong\", \"weak\", \"retain\", \"copy\", \"assign\", \"true\", \"false\", \"nil\"}\n\nFSHARP_KEYWORDS = {\"if\", \"then\", \"else\", \"match\", \"with\", \"for\", \"while\", \"do\", \"done\", \"let\",\n                   \"rec\", \"in\", \"try\", \"finally\", \"raise\", \"exception\", \"function\", \"return\",\n                   \"type\", \"mutable\", \"namespace\", \"module\", \"open\", \"abstract\", \"override\",\n                   \"inherit\", \"base\", \"new\", \"true\", \"false\", \"null\"}\n\nLISP_KEYWORDS = {\"defun\", \"setq\", \"let\", \"lambda\", \"if\", \"cond\", \"loop\", \"dolist\", \"dotimes\",\n                 \"progn\", \"return\", \"function\", \"defmacro\", \"quote\", \"eval\", \"apply\", \"car\",\n                 \"cdr\", \"cons\", \"list\", \"mapcar\", \"format\", \"read\", \"print\", \"load\", \"t\", \"nil\"}\n\nPROLOG_KEYWORDS = {\"if\", \"else\", \"end\", \"fail\", \"true\", \"false\", \"not\", \"repeat\", \"is\",\n                   \"assert\", \"retract\", \"call\", \"findall\", \"bagof\", \"setof\", \"atom\",\n                   \"integer\", \"float\", \"char_code\", \"compound\", \"number\", \"var\"}\n\nADA_KEYWORDS = {\"if\", \"then\", \"else\", \"elsif\", \"case\", \"when\", \"for\", \"while\", \"loop\", \"exit\",\n                \"return\", \"procedure\", \"function\", \"package\", \"use\", \"is\", \"begin\", \"end\",\n                \"record\", \"type\", \"constant\", \"exception\", \"raise\", \"declare\", \"private\",\n                \"null\", \"true\", \"false\"}\n\nDELPHI_KEYWORDS = {\"if\", \"then\", \"else\", \"case\", \"of\", \"for\", \"while\", \"repeat\", \"until\", \"break\",\n                   \"continue\", \"begin\", \"end\", \"procedure\", \"function\", \"var\", \"const\", \"type\",\n                   \"class\", \"record\", \"interface\", \"implementation\", \"unit\", \"uses\", \"inherited\",\n                   \"try\", \"except\", \"finally\", \"raise\", \"private\", \"public\", \"protected\", \"published\",\n                   \"true\", \"false\", \"nil\"}\n\nVB_KEYWORDS = {\"If\", \"Then\", \"Else\", \"ElseIf\", \"End\", \"For\", \"Each\", \"While\", \"Do\", \"Loop\",\n               \"Select\", \"Case\", \"Try\", \"Catch\", \"Finally\", \"Throw\", \"Return\", \"Function\",\n               \"Sub\", \"Class\", \"Module\", \"Namespace\", \"Imports\", \"Inherits\", \"Implements\",\n               \"Public\", \"Private\", \"Protected\", \"Friend\", \"Shared\", \"Static\", \"Dim\", \"Const\",\n               \"New\", \"Me\", \"MyBase\", \"MyClass\", \"Not\", \"And\", \"Or\", \"True\", \"False\", \"Nothing\"}\n\nHTML_KEYWORDS = {\"html\", \"head\", \"title\", \"meta\", \"link\", \"style\", \"script\", \"body\", \"div\", \"span\",\n                 \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"a\", \"img\", \"ul\", \"ol\", \"li\", \"table\",\n                 \"tr\", \"td\", \"th\", \"thead\", \"tbody\", \"tfoot\", \"form\", \"input\", \"button\", \"label\",\n                 \"select\", \"option\", \"textarea\", \"fieldset\", \"legend\", \"iframe\", \"nav\", \"section\",\n                 \"article\", \"aside\", \"header\", \"footer\", \"main\", \"blockquote\", \"cite\", \"code\",\n                 \"pre\", \"em\", \"strong\", \"b\", \"i\", \"u\", \"small\", \"br\", \"hr\"}\n\nCSS_KEYWORDS = {\"color\", \"background\", \"border\", \"margin\", \"padding\", \"width\", \"height\", \"font-size\",\n                \"font-family\", \"text-align\", \"display\", \"position\", \"top\", \"bottom\", \"left\", \"right\",\n                \"z-index\", \"visibility\", \"opacity\", \"overflow\", \"cursor\", \"flex\", \"grid\", \"align-items\",\n                \"justify-content\", \"box-shadow\", \"text-shadow\", \"animation\", \"transition\", \"transform\",\n                \"clip-path\", \"content\", \"filter\", \"outline\", \"max-width\", \"min-width\", \"max-height\",\n                \"min-height\", \"letter-spacing\", \"line-height\", \"white-space\", \"word-break\"}\n\n\nPROGRAMMING_LANGUAGES = {\n    \"Python\": PYTHON_KEYWORDS,\n    \"JavaScript\": JAVASCRIPT_KEYWORDS,\n    \"Java\": JAVA_KEYWORDS,\n    \"C\": C_KEYWORDS,\n    \"C++\": CPP_KEYWORDS,\n    \"C#\": CSHARP_KEYWORDS,\n    \"Go\": GO_KEYWORDS,\n    \"Rust\": RUST_KEYWORDS,\n    \"Swift\": SWIFT_KEYWORDS,\n    \"Kotlin\": KOTLIN_KEYWORDS,\n    \"TypeScript\": TYPESCRIPT_KEYWORDS,\n    \"PHP\": PHP_KEYWORDS,\n    \"Ruby\": RUBY_KEYWORDS,\n    \"SQL\": SQL_KEYWORDS,\n    \"Bash\": BASH_KEYWORDS,\n    \"MATLAB\": MATLAB_KEYWORDS,\n    \"R\": R_KEYWORDS,\n    \"Perl\": PERL_KEYWORDS,\n    \"Lua\": LUA_KEYWORDS,\n    \"Scala\": SCALA_KEYWORDS,\n    \"Dart\": DART_KEYWORDS,\n    \"Julia\": JULIA_KEYWORDS,\n    \"Haskell\": HASKELL_KEYWORDS,\n    \"COBOL\": COBOL_KEYWORDS,\n    \"Objective-C\": OBJECTIVEC_KEYWORDS,\n    \"F#\": FSHARP_KEYWORDS,\n    \"Lisp\": LISP_KEYWORDS,\n    \"Prolog\": PROLOG_KEYWORDS,\n    \"Ada\": ADA_KEYWORDS,\n    \"Delphi\": DELPHI_KEYWORDS,\n    \"Visual Basic\": VB_KEYWORDS,\n    \"HTML\": HTML_KEYWORDS,\n    \"CSS\": CSS_KEYWORDS}\n"
            },
            {
                "file_path": "memor/prompt.py",
                "code": "class Prompt:\n    \"\"\"\n    Prompt class.\n\n    >>> from memor import Prompt, Role, Response\n    >>> responses = [Response(message=\"I am fine.\"), Response(message=\"I am not fine.\"), Response(message=\"I am okay.\")]\n    >>> prompt = Prompt(message=\"Hello, how are you?\", responses=responses)\n    >>> prompt.message\n    'Hello, how are you?'\n    >>> prompt.responses[1].message\n    'I am not fine.'\n    \"\"\"\n\n    def __init__(\n            self,\n            message: str = \"\",\n            responses: List[Response] = [],\n            role: Role = Role.DEFAULT,\n            tokens: int = None,\n            template: Union[PresetPromptTemplate, PromptTemplate] = PresetPromptTemplate.DEFAULT,\n            file_path: str = None,\n            init_check: bool = True) -> None:\n        \"\"\"\n        Prompt object initiator.\n\n        :param message: prompt message\n        :param responses: prompt responses\n        :param role: prompt role\n        :param tokens: tokens\n        :param template: prompt template\n        :param file_path: prompt file path\n        :param init_check: initial check flag\n        \"\"\"\n        pass\n\n    def __eq__(self, other_prompt: \"Prompt\") -> bool:\n        \"\"\"\n        Check prompts equality.\n\n        :param other_prompt: another prompt\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Prompt.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Prompt.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Prompt object.\"\"\"\n        pass\n\n    def __copy__(self) -> \"Prompt\":\n        \"\"\"\n        Return a copy of the Prompt object.\n\n        :return: a copy of Prompt object\n        \"\"\"\n        pass\n\n    def copy(self) -> \"Prompt\":\n        \"\"\"\n        Return a copy of the Prompt object.\n\n        :return: a copy of Prompt object\n        \"\"\"\n        pass\n\n    def add_response(self, response: Response, index: int = None) -> None:\n        \"\"\"\n        Add a response to the prompt object.\n\n        :param response: response\n        :param index: index\n        \"\"\"\n        pass\n\n    def remove_response(self, index: int) -> None:\n        \"\"\"\n        Remove a response from the prompt object.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def select_response(self, index: int) -> None:\n        \"\"\"\n        Select a response as selected response.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def update_responses(self, responses: List[Response]) -> None:\n        \"\"\"\n        Update the prompt responses.\n\n        :param responses: responses\n        \"\"\"\n        pass\n\n    def update_message(self, message: str) -> None:\n        \"\"\"\n        Update the prompt message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def update_role(self, role: Role) -> None:\n        \"\"\"\n        Update the prompt role.\n\n        :param role: role\n        \"\"\"\n        pass\n\n    def update_tokens(self, tokens: int) -> None:\n        \"\"\"\n        Update the tokens.\n\n        :param tokens: tokens\n        \"\"\"\n        pass\n\n    def update_template(self, template: PromptTemplate) -> None:\n        \"\"\"\n        Update the prompt template.\n\n        :param template: template\n        \"\"\"\n        pass\n\n    def save(self, file_path: str, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: prompt file path\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: prompt file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Convert the prompt to a JSON object.\n\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    def to_dict(self, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Convert the prompt to a dictionary.\n\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    @property\n    def message(self) -> str:\n        \"\"\"Get the prompt message.\"\"\"\n        pass\n\n    @property\n    def responses(self) -> List[Response]:\n        \"\"\"Get the prompt responses.\"\"\"\n        pass\n\n    @property\n    def role(self) -> Role:\n        \"\"\"Get the prompt role.\"\"\"\n        pass\n\n    @property\n    def tokens(self) -> int:\n        \"\"\"Get the prompt tokens.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the prompt creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the prompt object modification date.\"\"\"\n        pass\n\n    @property\n    def template(self) -> PromptTemplate:\n        \"\"\"Get the prompt template.\"\"\"\n        pass\n\n    @property\n    def selected_response(self) -> Response:\n        \"\"\"Get the prompt selected response.\"\"\"\n        pass\n\n    def render(self, render_format: RenderFormat = RenderFormat.DEFAULT) -> Union[str,\n                                                                                  Dict[str, Any],\n                                                                                  List[Tuple[str, Any]]]:\n        \"\"\"\n        Render method.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def check_render(self) -> bool:\n        \"\"\"Check render.\"\"\"\n        pass\n\n    def estimate_tokens(self, method: TokensEstimator = TokensEstimator.DEFAULT) -> int:\n        \"\"\"\n        Estimate the number of tokens in the prompt message.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "memor/params.py",
                "code": "MEMOR_VERSION = \"0.5\"\n\nDATE_TIME_FORMAT = \"%Y-%m-%d %H:%M:%S %z\"\n\nINVALID_PATH_MESSAGE = \"Invalid path. Path must be a string.\"\nPATH_DOES_NOT_EXIST_MESSAGE = \"Path {0} does not exist.\"\nINVALID_STR_VALUE_MESSAGE = \"Invalid value. `{0}` must be a string.\"\nINVALID_BOOL_VALUE_MESSAGE = \"Invalid value. `{0}` must be a boolean.\"\nINVALID_POSFLOAT_VALUE_MESSAGE = \"Invalid value. `{0}` must be a positive float.\"\nINVALID_POSINT_VALUE_MESSAGE = \"Invalid value. `{0}` must be a positive integer.\"\nINVALID_PROB_VALUE_MESSAGE = \"Invalid value. `{0}` must be a value between 0 and 1.\"\nINVALID_LIST_OF_X_MESSAGE = \"Invalid value. `{0}` must be a list of {1}.\"\nINVALID_DATETIME_MESSAGE = \"Invalid value. `{0}` must be a datetime object that includes timezone information.\"\nINVALID_TEMPLATE_MESSAGE = \"Invalid template. It must be an instance of `PromptTemplate` or `PresetPromptTemplate`.\"\nINVALID_RESPONSE_MESSAGE = \"Invalid response. It must be an instance of `Response`.\"\nINVALID_MESSAGE = \"Invalid message. It must be an instance of `Prompt` or `Response`.\"\nINVALID_MESSAGE_STATUS_LEN_MESSAGE = \"Invalid message status length. It must be equal to the number of messages.\"\nINVALID_CUSTOM_MAP_MESSAGE = \"Invalid custom map: it must be a dictionary with keys and values that can be converted to strings.\"\nINVALID_ROLE_MESSAGE = \"Invalid role. It must be an instance of Role enum.\"\nINVALID_TEMPLATE_STRUCTURE_MESSAGE = \"Invalid template structure. It should be a JSON object with proper fields.\"\nINVALID_PROMPT_STRUCTURE_MESSAGE = \"Invalid prompt structure. It should be a JSON object with proper fields.\"\nINVALID_RESPONSE_STRUCTURE_MESSAGE = \"Invalid response structure. It should be a JSON object with proper fields.\"\nINVALID_RENDER_FORMAT_MESSAGE = \"Invalid render format. It must be an instance of RenderFormat enum.\"\nPROMPT_RENDER_ERROR_MESSAGE = \"Prompt template and properties are incompatible.\"\nUNSUPPORTED_OPERAND_ERROR_MESSAGE = \"Unsupported operand type(s) for {0}: `{1}` and `{2}`\"\nDATA_SAVE_SUCCESS_MESSAGE = \"Everything seems good.\"\n\n\nclass Role(Enum):\n    \"\"\"Role enum.\"\"\"\n\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    DEFAULT = USER\n\n\nclass RenderFormat(Enum):\n    \"\"\"Render format.\"\"\"\n\n    STRING = \"STRING\"\n    OPENAI = \"OPENAI\"\n    DICTIONARY = \"DICTIONARY\"\n    ITEMS = \"ITEMS\"\n    DEFAULT = STRING\n"
            },
            {
                "file_path": "memor/__init__.py",
                "code": "__version__ = MEMOR_VERSION\n"
            },
            {
                "file_path": "memor/template.py",
                "code": "class PromptTemplate:\n    r\"\"\"\n    Prompt template.\n\n    >>> template = PromptTemplate(content=\"Take a deep breath\\n{prompt_message}!\", title=\"Greeting\")\n    >>> template.title\n    'Greeting'\n    \"\"\"\n\n    def __init__(\n            self,\n            content: str = None,\n            file_path: str = None,\n            title: str = None,\n            custom_map: Dict[str, str] = None) -> None:\n        \"\"\"\n        Prompt template object initiator.\n\n        :param content: template content\n        :param file_path: template file path\n        :param title: template title\n        :param custom_map: custom map\n        \"\"\"\n        pass\n\n    def __eq__(self, other_template: \"PromptTemplate\") -> bool:\n        \"\"\"\n        Check templates equality.\n\n        :param other_template: another template\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of PromptTemplate.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of PromptTemplate.\"\"\"\n        pass\n\n    def __copy__(self) -> \"PromptTemplate\":\n        \"\"\"Return a copy of the PromptTemplate object.\"\"\"\n        pass\n\n    def copy(self) -> \"PromptTemplate\":\n        \"\"\"Return a copy of the PromptTemplate object.\"\"\"\n        pass\n\n    def update_title(self, title: str) -> None:\n        \"\"\"\n        Update title.\n\n        :param title: title\n        \"\"\"\n        pass\n\n    def update_content(self, content: str) -> None:\n        \"\"\"\n        Update content.\n\n        :param content: content\n        \"\"\"\n        pass\n\n    def update_map(self, custom_map: Dict[str, str]) -> None:\n        \"\"\"\n        Update custom map.\n\n        :param custom_map: custom map\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: template file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: template file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert PromptTemplate to json.\"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert PromptTemplate to dict.\"\"\"\n        pass\n\n    @property\n    def content(self) -> str:\n        \"\"\"Get the PromptTemplate content.\"\"\"\n        pass\n\n    @property\n    def title(self) -> str:\n        \"\"\"Get the PromptTemplate title.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the PromptTemplate creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the PromptTemplate modification date.\"\"\"\n        pass\n\n    @property\n    def custom_map(self) -> Dict[str, str]:\n        \"\"\"Get the PromptTemplate custom map.\"\"\"\n        pass\n\n\nPROMPT_INSTRUCTION1 = \"I'm providing you with a history of a previous conversation. Please consider this context when responding to my new question.\\n\"\nPROMPT_INSTRUCTION2 = \"Here is the context from a prior conversation. Please learn from this information and use it to provide a thoughtful and context-aware response to my next questions.\\n\"\nPROMPT_INSTRUCTION3 = \"I am sharing a record of a previous discussion. Use this information to provide a consistent and relevant answer to my next query.\\n\"\n\nBASIC_PROMPT_CONTENT = \"{instruction}{prompt[message]}\"\nBASIC_RESPONSE_CONTENT = \"{instruction}{response[message]}\"\nBASIC_RESPONSE0_CONTENT = \"{instruction}{responses[0][message]}\"\nBASIC_RESPONSE1_CONTENT = \"{instruction}{responses[1][message]}\"\nBASIC_RESPONSE2_CONTENT = \"{instruction}{responses[2][message]}\"\nBASIC_RESPONSE3_CONTENT = \"{instruction}{responses[3][message]}\"\nBASIC_PROMPT_CONTENT_LABEL = \"{instruction}Prompt: {prompt[message]}\"\nBASIC_RESPONSE_CONTENT_LABEL = \"{instruction}Response: {response[message]}\"\nBASIC_RESPONSE0_CONTENT_LABEL = \"{instruction}Response: {responses[0][message]}\"\nBASIC_RESPONSE1_CONTENT_LABEL = \"{instruction}Response: {responses[1][message]}\"\nBASIC_RESPONSE2_CONTENT_LABEL = \"{instruction}Response: {responses[2][message]}\"\nBASIC_RESPONSE3_CONTENT_LABEL = \"{instruction}Response: {responses[3][message]}\"\nBASIC_PROMPT_RESPONSE_STANDARD_CONTENT = \"{instruction}Prompt: {prompt[message]}\\nResponse: {response[message]}\"\nBASIC_PROMPT_RESPONSE_FULL_CONTENT = \"\"\"{instruction}\nPrompt:\n    Message: {prompt[message]}\n    Role: {prompt[role]}\n    Tokens: {prompt[tokens]}\n    Date: {prompt[date]}\nResponse:\n    Message: {response[message]}\n    Role: {response[role]}\n    Temperature: {response[temperature]}\n    Model: {response[model]}\n    Score: {response[score]}\n    Tokens: {response[tokens]}\n    Inference Time: {response[inference_time]}\n    Date: {response[date]}\"\"\"\n\n\nclass _BasicPresetPromptTemplate(Enum):\n    \"\"\"Preset basic-prompt templates.\"\"\"\n\n    PROMPT\n    RESPONSE\n    RESPONSE0\n    RESPONSE1\n    RESPONSE2\n    RESPONSE3\n    PROMPT_WITH_LABEL\n    RESPONSE_WITH_LABEL\n    RESPONSE0_WITH_LABEL\n    RESPONSE1_WITH_LABEL\n    RESPONSE2_WITH_LABEL\n    RESPONSE3_WITH_LABEL\n    PROMPT_RESPONSE_STANDARD\n    PROMPT_RESPONSE_FULL\n\n\nclass _Instruction1PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction1-prompt templates.\"\"\"\n\n    PROMPT\n    RESPONSE\n    RESPONSE0\n    RESPONSE1\n    RESPONSE2\n    RESPONSE3\n    PROMPT_WITH_LABEL\n    RESPONSE_WITH_LABEL\n    RESPONSE0_WITH_LABEL\n    RESPONSE1_WITH_LABEL\n    RESPONSE2_WITH_LABEL\n    RESPONSE3_WITH_LABEL\n    PROMPT_RESPONSE_STANDARD\n    PROMPT_RESPONSE_FULL\n\n\nclass _Instruction2PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction2-prompt templates.\"\"\"\n\n    PROMPT\n    RESPONSE\n    RESPONSE0\n    RESPONSE1\n    RESPONSE2\n    RESPONSE3\n    PROMPT_WITH_LABEL\n    RESPONSE_WITH_LABEL\n    RESPONSE0_WITH_LABEL\n    RESPONSE1_WITH_LABEL\n    RESPONSE2_WITH_LABEL\n    RESPONSE3_WITH_LABEL\n    PROMPT_RESPONSE_STANDARD\n    PROMPT_RESPONSE_FULL\n\n\nclass _Instruction3PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction3-prompt templates.\"\"\"\n\n    PROMPT\n    RESPONSE\n    RESPONSE0\n    RESPONSE1\n    RESPONSE2\n    RESPONSE3\n    PROMPT_WITH_LABEL\n    RESPONSE_WITH_LABEL\n    RESPONSE0_WITH_LABEL\n    RESPONSE1_WITH_LABEL\n    RESPONSE2_WITH_LABEL\n    RESPONSE3_WITH_LABEL\n    PROMPT_RESPONSE_STANDARD\n    PROMPT_RESPONSE_FULL\n\n\nclass PresetPromptTemplate:\n    \"\"\"Preset prompt templates.\"\"\"\n\n    BASIC = _BasicPresetPromptTemplate\n    INSTRUCTION1 = _Instruction1PresetPromptTemplate\n    INSTRUCTION2 = _Instruction2PresetPromptTemplate\n    INSTRUCTION3 = _Instruction3PresetPromptTemplate\n    DEFAULT\n"
            },
            {
                "file_path": "memor/errors.py",
                "code": "class MemorValidationError(ValueError):\n    \"\"\"Base class for validation errors in Memor.\"\"\"\n\n    pass\n\n\nclass MemorRenderError(Exception):\n    \"\"\"Base class for render error in Memor.\"\"\"\n\n    pass\n"
            },
            {
                "file_path": "memor/functions.py",
                "code": "def get_time_utc() -> datetime.datetime:\n    \"\"\"\n    Get time in UTC format.\n\n    :return: UTC format time as a datetime object\n    \"\"\"\n    pass\n\n\ndef _validate_string(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate string.\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_bool(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate boolean.\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _can_convert_to_string(value: Any) -> bool:\n    \"\"\"\n    Check if value can be converted to string.\n\n    :param value: value\n    \"\"\"\n    pass\n\n\ndef _validate_pos_int(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate positive integer.\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_pos_float(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate positive float.\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_probability(value: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate probability (a float between 0 and 1).\n\n    :param value: value\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_list_of(value: Any, parameter_name: str, type_: Type, type_name: str) -> bool:\n    \"\"\"\n    Validate list of values.\n\n    :param value: value\n    :param parameter_name: parameter name\n    :param type_: type\n    :param type_name: type name\n    \"\"\"\n    pass\n\n\ndef _validate_date_time(date_time: Any, parameter_name: str) -> bool:\n    \"\"\"\n    Validate date time.\n\n    :param date_time: date time\n    :param parameter_name: parameter name\n    \"\"\"\n    pass\n\n\ndef _validate_path(path: Any) -> bool:\n    \"\"\"\n    Validate path property.\n\n    :param path: path\n    \"\"\"\n    pass\n\n\ndef _validate_custom_map(custom_map: Any) -> bool:\n    \"\"\"\n    Validate custom map a dictionary with keys and values that can be converted to strings.\n\n    :param custom_map: custom map\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "memor/session.py",
                "code": "class Session:\n    \"\"\"Session class.\"\"\"\n\n    def __init__(\n            self,\n            title: str = None,\n            messages: List[Union[Prompt, Response]] = [],\n            file_path: str = None,\n            init_check: bool = True) -> None:\n        \"\"\"\n        Session object initiator.\n\n        :param title: title\n        :param messages: messages\n        :param file_path: file path\n        :param init_check: initial check flag\n        \"\"\"\n        pass\n\n    def __eq__(self, other_session: \"Session\") -> bool:\n        \"\"\"\n        Check sessions equality.\n\n        :param other_session: other session\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Session.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Session.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Session object.\"\"\"\n        pass\n\n    def __iter__(self) -> Generator[Union[Prompt, Response], None, None]:\n        \"\"\"Iterate through the Session object.\"\"\"\n        pass\n\n    def __add__(self, other_object: Union[\"Session\", Response, Prompt]) -> \"Session\":\n        \"\"\"\n        Addition method.\n\n        :param other_object: other object\n        \"\"\"\n        pass\n\n    def __radd__(self, other_object: Union[\"Session\", Response, Prompt]) -> \"Session\":\n        \"\"\"\n        Reverse addition method.\n\n        :param other_object: other object\n        \"\"\"\n        pass\n\n    def __contains__(self, message: Union[Prompt, Response]) -> bool:\n        \"\"\"\n        Check if the Session contains the given message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def __getitem__(self, index: int) -> Union[Prompt, Response]:\n        \"\"\"\n        Return the Session message(s).\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def __copy__(self) -> \"Session\":\n        \"\"\"Return a copy of the Session object.\"\"\"\n        pass\n\n    def copy(self) -> \"Session\":\n        \"\"\"Return a copy of the Session object.\"\"\"\n        pass\n\n    def add_message(self,\n                    message: Union[Prompt, Response],\n                    status: bool = True,\n                    index: int = None) -> None:\n        \"\"\"\n        Add a message to the session object.\n\n        :param message: message\n        :param status: status\n        :param index: index\n        \"\"\"\n        pass\n\n    def remove_message(self, index: int) -> None:\n        \"\"\"\n        Remove a message from the session object.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def clear_messages(self) -> None:\n        \"\"\"Remove all messages.\"\"\"\n        pass\n\n    def enable_message(self, index: int) -> None:\n        \"\"\"\n        Enable a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def disable_message(self, index: int) -> None:\n        \"\"\"\n        Disable a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def mask_message(self, index: int) -> None:\n        \"\"\"\n        Mask a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def unmask_message(self, index: int) -> None:\n        \"\"\"\n        Unmask a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def update_title(self, title: str) -> None:\n        \"\"\"\n        Update the session title.\n\n        :param title: title\n        \"\"\"\n        pass\n\n    def update_messages(self,\n                        messages: List[Union[Prompt, Response]],\n                        status: List[bool] = None) -> None:\n        \"\"\"\n        Update the session messages.\n\n        :param messages: messages\n        :param status: status\n        \"\"\"\n        pass\n\n    def update_messages_status(self, status: List[bool]) -> None:\n        \"\"\"\n        Update the session messages status.\n\n        :param status: status\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: session file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: session file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert the session to a JSON object.\"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Convert the session to a dictionary.\n\n        :return: dict\n        \"\"\"\n        pass\n\n    def render(self, render_format: RenderFormat = RenderFormat.DEFAULT) -> Union[str,\n                                                                                  Dict[str, Any],\n                                                                                  List[Tuple[str, Any]]]:\n        \"\"\"\n        Render method.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def check_render(self) -> bool:\n        \"\"\"Check render.\"\"\"\n        pass\n\n    def estimate_tokens(self, method: TokensEstimator = TokensEstimator.DEFAULT) -> int:\n        \"\"\"\n        Estimate the number of tokens in the session.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the session creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the session object modification date.\"\"\"\n        pass\n\n    @property\n    def title(self) -> str:\n        \"\"\"Get the session title.\"\"\"\n        pass\n\n    @property\n    def messages(self) -> List[Union[Prompt, Response]]:\n        \"\"\"Get the session messages.\"\"\"\n        pass\n\n    @property\n    def messages_status(self) -> List[bool]:\n        \"\"\"Get the session messages status.\"\"\"\n        pass\n\n    @property\n    def masks(self) -> List[bool]:\n        \"\"\"Get the session masks.\"\"\"\n        pass\n"
            },
            {
                "file_path": "memor/tokens_estimator.py",
                "code": "def _is_code_snippet(message: str) -> bool:\n    \"\"\"\n    Check if the message is a code snippet based on common coding symbols.\n\n    :param message: The input message to check.\n    :return: Boolean indicating if the message is a code snippet.\n    \"\"\"\n    pass\n\n\ndef _preprocess_message(message: str, is_code: bool) -> str:\n    \"\"\"\n    Preprocess message by replacing contractions in non-code text.\n\n    :param message: The input message to preprocess.\n    :param is_code: Boolean indicating if the message is a code.\n    :return: Preprocessed message.\n    \"\"\"\n    pass\n\n\ndef _tokenize_message(message: str) -> List[str]:\n    \"\"\"\n    Tokenize the message based on words, symbols, and numbers.\n\n    :param message: The input message to tokenize.\n    :return: List of tokens.\n    \"\"\"\n    pass\n\n\ndef _count_code_tokens(token: str, common_keywords: Set[str]) -> int:\n    \"\"\"\n    Count tokens in code snippets considering different token types.\n\n    :param token: The token to count.\n    :param common_keywords: Set of common keywords in programming languages.\n    :return: Count of tokens.\n    \"\"\"\n    pass\n\n\ndef _count_text_tokens(token: str, prefixes: Set[str], suffixes: Set[str]) -> int:\n    \"\"\"\n    Count tokens in text based on prefixes, suffixes, and subwords.\n\n    :param token: The token to count.\n    :param prefixes: Set of common prefixes.\n    :param suffixes: Set of common suffixes.\n    :return: Token count.\n    \"\"\"\n    pass\n\n\ndef universal_tokens_estimator(message: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text or code snippet.\n\n    :param message: The input text or code snippet to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\n\n\ndef _openai_tokens_estimator(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's models.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\n\n\ndef openai_tokens_estimator_gpt_3_5(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's GPT-3.5 Turbo model.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\n\n\ndef openai_tokens_estimator_gpt_4(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's GPT-4 model.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\n\n\nclass TokensEstimator(Enum):\n    \"\"\"Token estimator enum.\"\"\"\n\n    UNIVERSAL = universal_tokens_estimator\n    OPENAI_GPT_3_5 = openai_tokens_estimator_gpt_3_5\n    OPENAI_GPT_4 = openai_tokens_estimator_gpt_4\n    DEFAULT = UNIVERSAL\n"
            },
            {
                "file_path": "memor/response.py",
                "code": "class Response:\n    \"\"\"\n    Response class.\n\n    >>> from memor import Response, Role\n    >>> response = Response(message=\"Hello!\", score=0.9, role=Role.ASSISTANT, temperature=0.5, model=\"gpt-3.5\")\n    >>> response.message\n    'Hello!'\n    \"\"\"\n\n    def __init__(\n            self,\n            message: str = \"\",\n            score: float = None,\n            role: Role = Role.ASSISTANT,\n            temperature: float = None,\n            tokens: int = None,\n            inference_time: float = None,\n            model: str = None,\n            date: datetime.datetime = get_time_utc(),\n            file_path: str = None) -> None:\n        \"\"\"\n        Response object initiator.\n\n        :param message: response message\n        :param score: response score\n        :param role: response role\n        :param temperature: temperature\n        :param tokens: tokens\n        :param inference_time: inference time\n        :param model: agent model\n        :param date: response date\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def __eq__(self, other_response: \"Response\") -> bool:\n        \"\"\"\n        Check responses equality.\n\n        :param other_response: another response\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Response.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Response.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Response object.\"\"\"\n        pass\n\n    def __copy__(self) -> \"Response\":\n        \"\"\"Return a copy of the Response object.\"\"\"\n        pass\n\n    def copy(self) -> \"Response\":\n        \"\"\"Return a copy of the Response object.\"\"\"\n        pass\n\n    def update_message(self, message: str) -> None:\n        \"\"\"\n        Update the response message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def update_score(self, score: float) -> None:\n        \"\"\"\n        Update the response score.\n\n        :param score: score\n        \"\"\"\n        pass\n\n    def update_role(self, role: Role) -> None:\n        \"\"\"\n        Update the response role.\n\n        :param role: role\n        \"\"\"\n        pass\n\n    def update_temperature(self, temperature: float) -> None:\n        \"\"\"\n        Update the temperature.\n\n        :param temperature: temperature\n        \"\"\"\n        pass\n\n    def update_tokens(self, tokens: int) -> None:\n        \"\"\"\n        Update the tokens.\n\n        :param tokens: tokens\n        \"\"\"\n        pass\n\n    def update_inference_time(self, inference_time: float) -> None:\n        \"\"\"\n        Update inference time.\n\n        :param inference_time: inference time\n        \"\"\"\n        pass\n\n    def update_model(self, model: str) -> None:\n        \"\"\"\n        Update the agent model.\n\n        :param model: model\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert the response to a JSON object.\"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert the response to a dictionary.\"\"\"\n        pass\n\n    def render(self,\n               render_format: RenderFormat = RenderFormat.DEFAULT) -> Union[str,\n                                                                            Dict[str, Any],\n                                                                            List[Tuple[str, Any]]]:\n        \"\"\"\n        Render the response.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def estimate_tokens(self, method: TokensEstimator = TokensEstimator.DEFAULT) -> int:\n        \"\"\"\n        Estimate the number of tokens in the response message.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n\n    @property\n    def message(self) -> str:\n        \"\"\"Get the response message.\"\"\"\n        pass\n\n    @property\n    def score(self) -> float:\n        \"\"\"Get the response score.\"\"\"\n        pass\n\n    @property\n    def temperature(self) -> float:\n        \"\"\"Get the temperature.\"\"\"\n        pass\n\n    @property\n    def tokens(self) -> int:\n        \"\"\"Get the tokens.\"\"\"\n        pass\n\n    @property\n    def inference_time(self) -> float:\n        \"\"\"Get inference time.\"\"\"\n        pass\n\n    @property\n    def role(self) -> Role:\n        \"\"\"Get the response role.\"\"\"\n        pass\n\n    @property\n    def model(self) -> str:\n        \"\"\"Get the agent model.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the response creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the response object modification date.\"\"\"\n        pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: memor/errors.py ---\n```python\nclass MemorValidationError(ValueError):\n    \"\"\"Base class for validation errors in Memor.\"\"\"\n\n    pass\nclass MemorRenderError(Exception):\n    \"\"\"Base class for render error in Memor.\"\"\"\n\n    pass\n```\n--- File: memor/params.py ---\n```python\nfrom enum import Enum\n\nclass Role(Enum):\n    \"\"\"Role enum.\"\"\"\n\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    DEFAULT = USER\nclass RenderFormat(Enum):\n    \"\"\"Render format.\"\"\"\n\n    STRING = \"STRING\"\n    OPENAI = \"OPENAI\"\n    DICTIONARY = \"DICTIONARY\"\n    ITEMS = \"ITEMS\"\n    DEFAULT = STRING\n```\n--- File: memor/response.py ---\n```python\nfrom typing import List, Dict, Union, Tuple, Any\nimport datetime\n# get_time_utc would be undefined here. The user of the skeleton needs to handle this.\n# For the purpose of the skeleton, we keep the original signature.\n# A possible way for the user to handle this is to define a dummy get_time_utc or replace it.\n# However, the problem states \"extract ... methods with their full signatures\".\n# If get_time_utc is not part of the skeleton (as it's not directly called by tests),\n# then this default value will cause an issue. The spec is a bit contradictory here.\n# I will assume the literal signature is required, and the user must make it runnable.\n# A practical skeleton might replace get_time_utc() with None or a placeholder.\n# For strict adherence, I'll keep it as is.\n# from .params import Role, RenderFormat # Not allowed\n# from .tokens_estimator import TokensEstimator # Not allowed\n# from .errors import MemorValidationError # Not allowed\n# from .functions import get_time_utc # Not allowed\n\nclass Response:\n    \"\"\"\n    Response class.\n\n    >>> from memor import Response, Role\n    >>> response = Response(message=\"Hello!\", score=0.9, role=Role.ASSISTANT, temperature=0.5, model=\"gpt-3.5\")\n    >>> response.message\n    'Hello!'\n    \"\"\"\n\n    def __init__(\n            self,\n            message: str = \"\",\n            score: float = None,\n            role: 'Role' = Role.ASSISTANT, # type: ignore\n            temperature: float = None,\n            tokens: int = None,\n            inference_time: float = None,\n            model: str = None,\n            date: datetime.datetime = None, # Replaced get_time_utc() for parsability\n            file_path: str = None) -> None:\n        \"\"\"\n        Response object initiator.\n\n        :param message: response message\n        :param score: response score\n        :param role: response role\n        :param temperature: temperature\n        :param tokens: tokens\n        :param inference_time: inference time\n        :param model: agent model\n        :param date: response date\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def __eq__(self, other_response: \"Response\") -> bool:\n        \"\"\"\n        Check responses equality.\n\n        :param other_response: another response\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Response.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Response.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Response object.\"\"\"\n        pass\n\n    def __copy__(self) -> \"Response\":\n        \"\"\"Return a copy of the Response object.\"\"\"\n        pass\n\n    def copy(self) -> \"Response\":\n        \"\"\"Return a copy of the Response object.\"\"\"\n        pass\n\n    def update_message(self, message: str) -> None:\n        \"\"\"\n        Update the response message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def update_score(self, score: float) -> None:\n        \"\"\"\n        Update the response score.\n\n        :param score: score\n        \"\"\"\n        pass\n\n    def update_role(self, role: 'Role') -> None: # type: ignore\n        \"\"\"\n        Update the response role.\n\n        :param role: role\n        \"\"\"\n        pass\n\n    def update_temperature(self, temperature: float) -> None:\n        \"\"\"\n        Update the temperature.\n\n        :param temperature: temperature\n        \"\"\"\n        pass\n\n    def update_tokens(self, tokens: int) -> None:\n        \"\"\"\n        Update the tokens.\n\n        :param tokens: tokens\n        \"\"\"\n        pass\n\n    def update_inference_time(self, inference_time: float) -> None:\n        \"\"\"\n        Update inference time.\n\n        :param inference_time: inference time\n        \"\"\"\n        pass\n\n    def update_model(self, model: str) -> None:\n        \"\"\"\n        Update the agent model.\n\n        :param model: model\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert the response to a JSON object.\"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert the response to a dictionary.\"\"\"\n        pass\n\n    def render(self,\n               render_format: 'RenderFormat' = RenderFormat.DEFAULT) -> Union[str, # type: ignore\n                                                                            Dict[str, Any],\n                                                                            List[Tuple[str, Any]]]:\n        \"\"\"\n        Render the response.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def estimate_tokens(self, method: 'TokensEstimator' = None) -> int: # type: ignore # TokensEstimator.DEFAULT\n        \"\"\"\n        Estimate the number of tokens in the response message.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n\n    @property\n    def message(self) -> str:\n        \"\"\"Get the response message.\"\"\"\n        pass\n\n    @property\n    def score(self) -> float:\n        \"\"\"Get the response score.\"\"\"\n        pass\n\n    @property\n    def temperature(self) -> float:\n        \"\"\"Get the temperature.\"\"\"\n        pass\n\n    @property\n    def tokens(self) -> int:\n        \"\"\"Get the tokens.\"\"\"\n        pass\n\n    @property\n    def inference_time(self) -> float:\n        \"\"\"Get inference time.\"\"\"\n        pass\n\n    @property\n    def role(self) -> 'Role': # type: ignore\n        \"\"\"Get the response role.\"\"\"\n        pass\n\n    @property\n    def model(self) -> str:\n        \"\"\"Get the agent model.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the response creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the response object modification date.\"\"\"\n        pass\n```\n--- File: memor/tokens_estimator.py ---\n```python\nfrom enum import Enum\nfrom typing import Set, List\n\ndef universal_tokens_estimator(message: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text or code snippet.\n\n    :param message: The input text or code snippet to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\ndef openai_tokens_estimator_gpt_3_5(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's GPT-3.5 Turbo model.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\ndef openai_tokens_estimator_gpt_4(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's GPT-4 model.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\nclass TokensEstimator(Enum):\n    \"\"\"Token estimator enum.\"\"\"\n\n    UNIVERSAL = universal_tokens_estimator\n    OPENAI_GPT_3_5 = openai_tokens_estimator_gpt_3_5\n    OPENAI_GPT_4 = openai_tokens_estimator_gpt_4\n    DEFAULT = UNIVERSAL\n```\n--- File: memor/template.py ---\n```python\nfrom typing import Dict, Any, Union\nimport json\nimport datetime\nfrom enum import Enum\n# from .params import DATE_TIME_FORMAT # Not allowed\n# from .params import DATA_SAVE_SUCCESS_MESSAGE # Not allowed\n# from .params import INVALID_TEMPLATE_STRUCTURE_MESSAGE # Not allowed\n# from .params import MEMOR_VERSION # Not allowed\n# from .errors import MemorValidationError # Not allowed\n# from .functions import get_time_utc # Not allowed\n# from .functions import _validate_path, _validate_custom_map # Not allowed\n# from .functions import _validate_string # Not allowed\n\nclass PromptTemplate:\n    r\"\"\"\n    Prompt template.\n\n    >>> template = PromptTemplate(content=\"Take a deep breath\\n{prompt_message}!\", title=\"Greeting\")\n    >>> template.title\n    'Greeting'\n    \"\"\"\n\n    def __init__(\n            self,\n            content: str = None,\n            file_path: str = None,\n            title: str = None,\n            custom_map: Dict[str, str] = None) -> None:\n        \"\"\"\n        Prompt template object initiator.\n\n        :param content: template content\n        :param file_path: template file path\n        :param title: template title\n        :param custom_map: custom map\n        \"\"\"\n        pass\n\n    def __eq__(self, other_template: \"PromptTemplate\") -> bool:\n        \"\"\"\n        Check templates equality.\n\n        :param other_template: another template\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of PromptTemplate.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of PromptTemplate.\"\"\"\n        pass\n\n    def __copy__(self) -> \"PromptTemplate\":\n        \"\"\"Return a copy of the PromptTemplate object.\"\"\"\n        pass\n\n    def copy(self) -> \"PromptTemplate\":\n        \"\"\"Return a copy of the PromptTemplate object.\"\"\"\n        pass\n\n    def update_title(self, title: str) -> None:\n        \"\"\"\n        Update title.\n\n        :param title: title\n        \"\"\"\n        pass\n\n    def update_content(self, content: str) -> None:\n        \"\"\"\n        Update content.\n\n        :param content: content\n        \"\"\"\n        pass\n\n    def update_map(self, custom_map: Dict[str, str]) -> None:\n        \"\"\"\n        Update custom map.\n\n        :param custom_map: custom map\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: template file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: template file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert PromptTemplate to json.\"\"\"\n        pass\n\n    # to_dict is not directly called by tests, so it's excluded.\n    # def to_dict(self) -> Dict[str, Any]:\n    #     \"\"\"Convert PromptTemplate to dict.\"\"\"\n    #     pass\n\n    @property\n    def content(self) -> str:\n        \"\"\"Get the PromptTemplate content.\"\"\"\n        pass\n\n    @property\n    def title(self) -> str:\n        \"\"\"Get the PromptTemplate title.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the PromptTemplate creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the PromptTemplate modification date.\"\"\"\n        pass\n\n    @property\n    def custom_map(self) -> Dict[str, str]:\n        \"\"\"Get the PromptTemplate custom map.\"\"\"\n        pass\n\n# Inlined constants for PresetPromptTemplate definitions\n_PROMPT_INSTRUCTION1 = \"I'm providing you with a history of a previous conversation. Please consider this context when responding to my new question.\\n\"\n\n_BASIC_PROMPT_CONTENT = \"{instruction}{prompt[message]}\"\n_BASIC_RESPONSE_CONTENT = \"{instruction}{response[message]}\"\n_BASIC_PROMPT_RESPONSE_STANDARD_CONTENT = \"{instruction}Prompt: {prompt[message]}\\nResponse: {response[message]}\"\n\n\nclass _BasicPresetPromptTemplate(Enum):\n    \"\"\"Preset basic-prompt templates.\"\"\"\n\n    PROMPT = PromptTemplate(content=_BASIC_PROMPT_CONTENT, title=\"Basic/Prompt\", custom_map={\"instruction\": \"\"})\n    RESPONSE = PromptTemplate(\n        content=_BASIC_RESPONSE_CONTENT,\n        title=\"Basic/Response\",\n        custom_map={\n            \"instruction\": \"\"})\n    PROMPT_RESPONSE_STANDARD = PromptTemplate(\n        content=_BASIC_PROMPT_RESPONSE_STANDARD_CONTENT,\n        title=\"Basic/Prompt-Response Standard\",\n        custom_map={\n            \"instruction\": \"\"})\n\nclass _Instruction1PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction1-prompt templates.\"\"\"\n\n    PROMPT = PromptTemplate(\n        content=_BASIC_PROMPT_CONTENT, # Uses inlined or locally defined constant\n        title=\"Instruction1/Prompt\",\n        custom_map={\n            \"instruction\": _PROMPT_INSTRUCTION1}) # Uses inlined or locally defined constant\n\nclass _Instruction2PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction2-prompt templates.\"\"\"\n    pass # No members directly used by tests\n\nclass _Instruction3PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction3-prompt templates.\"\"\"\n    pass # No members directly used by tests\n\n\nclass PresetPromptTemplate:\n    \"\"\"Preset prompt templates.\"\"\"\n\n    BASIC = _BasicPresetPromptTemplate\n    INSTRUCTION1 = _Instruction1PresetPromptTemplate\n    INSTRUCTION2 = _Instruction2PresetPromptTemplate\n    INSTRUCTION3 = _Instruction3PresetPromptTemplate\n    DEFAULT = BASIC.PROMPT\n```\n--- File: memor/prompt.py ---\n```python\nfrom typing import List, Dict, Union, Tuple, Any\nimport datetime\n# from .params import MEMOR_VERSION # Not allowed\n# from .params import DATE_TIME_FORMAT # Not allowed\n# from .params import RenderFormat, DATA_SAVE_SUCCESS_MESSAGE # Not allowed\n# from .params import Role # Not allowed\n# from .tokens_estimator import TokensEstimator # Not allowed\n# from .params import INVALID_PROMPT_STRUCTURE_MESSAGE, INVALID_TEMPLATE_MESSAGE # Not allowed\n# from .params import INVALID_ROLE_MESSAGE, INVALID_RESPONSE_MESSAGE # Not allowed\n# from .params import PROMPT_RENDER_ERROR_MESSAGE # Not allowed\n# from .params import INVALID_RENDER_FORMAT_MESSAGE # Not allowed\n# from .errors import MemorValidationError, MemorRenderError # Not allowed\n# from .functions import get_time_utc # Not allowed\n# from .functions import _validate_string, _validate_pos_int, _validate_list_of # Not allowed\n# from .functions import _validate_path # Not allowed\n# from .template import PromptTemplate, PresetPromptTemplate # Not allowed\n# from .response import Response # Not allowed\n\nclass Prompt:\n    \"\"\"\n    Prompt class.\n\n    >>> from memor import Prompt, Role, Response\n    >>> responses = [Response(message=\"I am fine.\"), Response(message=\"I am not fine.\"), Response(message=\"I am okay.\")]\n    >>> prompt = Prompt(message=\"Hello, how are you?\", responses=responses)\n    >>> prompt.message\n    'Hello, how are you?'\n    >>> prompt.responses[1].message\n    'I am not fine.'\n    \"\"\"\n\n    def __init__(\n            self,\n            message: str = \"\",\n            responses: List['Response'] = [], # type: ignore\n            role: 'Role' = None, # Role.DEFAULT, # type: ignore\n            tokens: int = None,\n            template: Union['PresetPromptTemplate', 'PromptTemplate'] = None, # PresetPromptTemplate.DEFAULT, # type: ignore\n            file_path: str = None,\n            init_check: bool = True) -> None:\n        \"\"\"\n        Prompt object initiator.\n\n        :param message: prompt message\n        :param responses: prompt responses\n        :param role: prompt role\n        :param tokens: tokens\n        :param template: prompt template\n        :param file_path: prompt file path\n        :param init_check: initial check flag\n        \"\"\"\n        pass\n\n    def __eq__(self, other_prompt: \"Prompt\") -> bool:\n        \"\"\"\n        Check prompts equality.\n\n        :param other_prompt: another prompt\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Prompt.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Prompt.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Prompt object.\"\"\"\n        pass\n\n    def __copy__(self) -> \"Prompt\":\n        \"\"\"\n        Return a copy of the Prompt object.\n\n        :return: a copy of Prompt object\n        \"\"\"\n        pass\n\n    def copy(self) -> \"Prompt\":\n        \"\"\"\n        Return a copy of the Prompt object.\n\n        :return: a copy of Prompt object\n        \"\"\"\n        pass\n\n    def add_response(self, response: 'Response', index: int = None) -> None: # type: ignore\n        \"\"\"\n        Add a response to the prompt object.\n\n        :param response: response\n        :param index: index\n        \"\"\"\n        pass\n\n    def remove_response(self, index: int) -> None:\n        \"\"\"\n        Remove a response from the prompt object.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def select_response(self, index: int) -> None:\n        \"\"\"\n        Select a response as selected response.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def update_responses(self, responses: List['Response']) -> None: # type: ignore\n        \"\"\"\n        Update the prompt responses.\n\n        :param responses: responses\n        \"\"\"\n        pass\n\n    def update_message(self, message: str) -> None:\n        \"\"\"\n        Update the prompt message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def update_role(self, role: 'Role') -> None: # type: ignore\n        \"\"\"\n        Update the prompt role.\n\n        :param role: role\n        \"\"\"\n        pass\n\n    def update_tokens(self, tokens: int) -> None:\n        \"\"\"\n        Update the tokens.\n\n        :param tokens: tokens\n        \"\"\"\n        pass\n\n    def update_template(self, template: 'PromptTemplate') -> None: # type: ignore\n        \"\"\"\n        Update the prompt template.\n\n        :param template: template\n        \"\"\"\n        pass\n\n    def save(self, file_path: str, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: prompt file path\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: prompt file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Convert the prompt to a JSON object.\n\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    # to_dict is not directly called by tests\n    # def to_dict(self, save_template: bool = True) -> Dict[str, Any]:\n    #     \"\"\"\n    #     Convert the prompt to a dictionary.\n    #\n    #     :param save_template: save template flag\n    #     \"\"\"\n    #     pass\n\n    @property\n    def message(self) -> str:\n        \"\"\"Get the prompt message.\"\"\"\n        pass\n\n    @property\n    def responses(self) -> List['Response']: # type: ignore\n        \"\"\"Get the prompt responses.\"\"\"\n        pass\n\n    @property\n    def role(self) -> 'Role': # type: ignore\n        \"\"\"Get the prompt role.\"\"\"\n        pass\n\n    @property\n    def tokens(self) -> int:\n        \"\"\"Get the prompt tokens.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the prompt creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the prompt object modification date.\"\"\"\n        pass\n\n    @property\n    def template(self) -> 'PromptTemplate': # type: ignore\n        \"\"\"Get the prompt template.\"\"\"\n        pass\n\n    @property\n    def selected_response(self) -> 'Response': # type: ignore\n        \"\"\"Get the prompt selected response.\"\"\"\n        pass\n\n    def render(self, render_format: 'RenderFormat' = None) -> Union[str, # type: ignore # RenderFormat.DEFAULT\n                                                                                  Dict[str, Any],\n                                                                                  List[Tuple[str, Any]]]:\n        \"\"\"\n        Render method.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def check_render(self) -> bool:\n        \"\"\"Check render.\"\"\"\n        pass\n\n    def estimate_tokens(self, method: 'TokensEstimator' = None) -> int: # type: ignore # TokensEstimator.DEFAULT\n        \"\"\"\n        Estimate the number of tokens in the prompt message.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n```\n--- File: memor/session.py ---\n```python\nfrom typing import List, Dict, Tuple, Any, Union, Generator\nimport datetime\n# from .params import MEMOR_VERSION # Not allowed\n# from .params import DATE_TIME_FORMAT # Not allowed\n# from .params import DATA_SAVE_SUCCESS_MESSAGE # Not allowed\n# from .params import INVALID_MESSAGE # Not allowed\n# from .params import INVALID_MESSAGE_STATUS_LEN_MESSAGE # Not allowed\n# from .params import INVALID_RENDER_FORMAT_MESSAGE # Not allowed\n# from .params import UNSUPPORTED_OPERAND_ERROR_MESSAGE # Not allowed\n# from .params import RenderFormat # Not allowed\n# from .tokens_estimator import TokensEstimator # Not allowed\n# from .prompt import Prompt # Not allowed\n# from .response import Response # Not allowed\n# from .errors import MemorValidationError # Not allowed\n# from .functions import get_time_utc # Not allowed\n# from .functions import _validate_bool, _validate_path # Not allowed\n# from .functions import _validate_list_of, _validate_string # Not allowed\n\n\nclass Session:\n    \"\"\"Session class.\"\"\"\n\n    def __init__(\n            self,\n            title: str = None,\n            messages: List[Union['Prompt', 'Response']] = [], # type: ignore\n            file_path: str = None,\n            init_check: bool = True) -> None:\n        \"\"\"\n        Session object initiator.\n\n        :param title: title\n        :param messages: messages\n        :param file_path: file path\n        :param init_check: initial check flag\n        \"\"\"\n        pass\n\n    def __eq__(self, other_session: \"Session\") -> bool:\n        \"\"\"\n        Check sessions equality.\n\n        :param other_session: other session\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Session.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Session.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Session object.\"\"\"\n        pass\n\n    def __iter__(self) -> Generator[Union['Prompt', 'Response'], None, None]: # type: ignore\n        \"\"\"Iterate through the Session object.\"\"\"\n        pass\n\n    def __add__(self, other_object: Union[\"Session\", 'Response', 'Prompt']) -> \"Session\": # type: ignore\n        \"\"\"\n        Addition method.\n\n        :param other_object: other object\n        \"\"\"\n        pass\n\n    def __radd__(self, other_object: Union['Response', 'Prompt']) -> \"Session\": # type: ignore\n        \"\"\"\n        Reverse addition method.\n\n        :param other_object: other object\n        \"\"\"\n        pass\n\n    def __contains__(self, message: Union['Prompt', 'Response']) -> bool: # type: ignore\n        \"\"\"\n        Check if the Session contains the given message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def __getitem__(self, index: Union[int, slice]) -> Union['Prompt', 'Response', List[Union['Prompt', 'Response']]]: # type: ignore\n        \"\"\"\n        Return the Session message(s).\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def __copy__(self) -> \"Session\":\n        \"\"\"Return a copy of the Session object.\"\"\"\n        pass\n\n    def copy(self) -> \"Session\":\n        \"\"\"Return a copy of the Session object.\"\"\"\n        pass\n\n    def add_message(self,\n                    message: Union['Prompt', 'Response'], # type: ignore\n                    status: bool = True,\n                    index: int = None) -> None:\n        \"\"\"\n        Add a message to the session object.\n\n        :param message: message\n        :param status: status\n        :param index: index\n        \"\"\"\n        pass\n\n    def remove_message(self, index: int) -> None:\n        \"\"\"\n        Remove a message from the session object.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def clear_messages(self) -> None:\n        \"\"\"Remove all messages.\"\"\"\n        pass\n\n    def enable_message(self, index: int) -> None:\n        \"\"\"\n        Enable a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def disable_message(self, index: int) -> None:\n        \"\"\"\n        Disable a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def mask_message(self, index: int) -> None:\n        \"\"\"\n        Mask a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def unmask_message(self, index: int) -> None:\n        \"\"\"\n        Unmask a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def update_title(self, title: str) -> None:\n        \"\"\"\n        Update the session title.\n\n        :param title: title\n        \"\"\"\n        pass\n\n    def update_messages(self,\n                        messages: List[Union['Prompt', 'Response']], # type: ignore\n                        status: List[bool] = None) -> None:\n        \"\"\"\n        Update the session messages.\n\n        :param messages: messages\n        :param status: status\n        \"\"\"\n        pass\n\n    def update_messages_status(self, status: List[bool]) -> None:\n        \"\"\"\n        Update the session messages status.\n\n        :param status: status\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: session file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: session file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert the session to a JSON object.\"\"\"\n        pass\n\n    # to_dict is not directly called by tests\n    # def to_dict(self) -> Dict[str, Any]:\n    #     \"\"\"\n    #     Convert the session to a dictionary.\n    #\n    #     :return: dict\n    #     \"\"\"\n    #     pass\n\n    def render(self, render_format: 'RenderFormat' = None) -> Union[str, # type: ignore # RenderFormat.DEFAULT\n                                                                                  List[Dict[str, str]], # For OPENAI\n                                                                                  Dict[str, Any], # For DICTIONARY\n                                                                                  List[Tuple[str, Any]]]: # For ITEMS\n        \"\"\"\n        Render method.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def check_render(self) -> bool:\n        \"\"\"Check render.\"\"\"\n        pass\n\n    def estimate_tokens(self, method: 'TokensEstimator' = None) -> int: # type: ignore # TokensEstimator.DEFAULT\n        \"\"\"\n        Estimate the number of tokens in the session.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the session creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the session object modification date.\"\"\"\n        pass\n\n    @property\n    def title(self) -> str:\n        \"\"\"Get the session title.\"\"\"\n        pass\n\n    @property\n    def messages(self) -> List[Union['Prompt', 'Response']]: # type: ignore\n        \"\"\"Get the session messages.\"\"\"\n        pass\n\n    @property\n    def messages_status(self) -> List[bool]:\n        \"\"\"Get the session messages status.\"\"\"\n        pass\n\n    @property\n    def masks(self) -> List[bool]:\n        \"\"\"Get the session masks.\"\"\"\n        pass\n\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "memor/errors.py",
                "code": "class MemorValidationError(ValueError):\n    \"\"\"Base class for validation errors in Memor.\"\"\"\n\n    pass\nclass MemorRenderError(Exception):\n    \"\"\"Base class for render error in Memor.\"\"\"\n\n    pass\n"
            },
            {
                "file_path": "memor/params.py",
                "code": "from enum import Enum\n\nclass Role(Enum):\n    \"\"\"Role enum.\"\"\"\n\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    DEFAULT = USER\nclass RenderFormat(Enum):\n    \"\"\"Render format.\"\"\"\n\n    STRING = \"STRING\"\n    OPENAI = \"OPENAI\"\n    DICTIONARY = \"DICTIONARY\"\n    ITEMS = \"ITEMS\"\n    DEFAULT = STRING\n"
            },
            {
                "file_path": "memor/response.py",
                "code": "from typing import List, Dict, Union, Tuple, Any\nimport datetime\n# get_time_utc would be undefined here. The user of the skeleton needs to handle this.\n# For the purpose of the skeleton, we keep the original signature.\n# A possible way for the user to handle this is to define a dummy get_time_utc or replace it.\n# However, the problem states \"extract ... methods with their full signatures\".\n# If get_time_utc is not part of the skeleton (as it's not directly called by tests),\n# then this default value will cause an issue. The spec is a bit contradictory here.\n# I will assume the literal signature is required, and the user must make it runnable.\n# A practical skeleton might replace get_time_utc() with None or a placeholder.\n# For strict adherence, I'll keep it as is.\n# from .params import Role, RenderFormat # Not allowed\n# from .tokens_estimator import TokensEstimator # Not allowed\n# from .errors import MemorValidationError # Not allowed\n# from .functions import get_time_utc # Not allowed\n\nclass Response:\n    \"\"\"\n    Response class.\n\n    >>> from memor import Response, Role\n    >>> response = Response(message=\"Hello!\", score=0.9, role=Role.ASSISTANT, temperature=0.5, model=\"gpt-3.5\")\n    >>> response.message\n    'Hello!'\n    \"\"\"\n\n    def __init__(\n            self,\n            message: str = \"\",\n            score: float = None,\n            role: 'Role' = Role.ASSISTANT, # type: ignore\n            temperature: float = None,\n            tokens: int = None,\n            inference_time: float = None,\n            model: str = None,\n            date: datetime.datetime = None, # Replaced get_time_utc() for parsability\n            file_path: str = None) -> None:\n        \"\"\"\n        Response object initiator.\n\n        :param message: response message\n        :param score: response score\n        :param role: response role\n        :param temperature: temperature\n        :param tokens: tokens\n        :param inference_time: inference time\n        :param model: agent model\n        :param date: response date\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def __eq__(self, other_response: \"Response\") -> bool:\n        \"\"\"\n        Check responses equality.\n\n        :param other_response: another response\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Response.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Response.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Response object.\"\"\"\n        pass\n\n    def __copy__(self) -> \"Response\":\n        \"\"\"Return a copy of the Response object.\"\"\"\n        pass\n\n    def copy(self) -> \"Response\":\n        \"\"\"Return a copy of the Response object.\"\"\"\n        pass\n\n    def update_message(self, message: str) -> None:\n        \"\"\"\n        Update the response message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def update_score(self, score: float) -> None:\n        \"\"\"\n        Update the response score.\n\n        :param score: score\n        \"\"\"\n        pass\n\n    def update_role(self, role: 'Role') -> None: # type: ignore\n        \"\"\"\n        Update the response role.\n\n        :param role: role\n        \"\"\"\n        pass\n\n    def update_temperature(self, temperature: float) -> None:\n        \"\"\"\n        Update the temperature.\n\n        :param temperature: temperature\n        \"\"\"\n        pass\n\n    def update_tokens(self, tokens: int) -> None:\n        \"\"\"\n        Update the tokens.\n\n        :param tokens: tokens\n        \"\"\"\n        pass\n\n    def update_inference_time(self, inference_time: float) -> None:\n        \"\"\"\n        Update inference time.\n\n        :param inference_time: inference time\n        \"\"\"\n        pass\n\n    def update_model(self, model: str) -> None:\n        \"\"\"\n        Update the agent model.\n\n        :param model: model\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: response file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert the response to a JSON object.\"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert the response to a dictionary.\"\"\"\n        pass\n\n    def render(self,\n               render_format: 'RenderFormat' = RenderFormat.DEFAULT) -> Union[str, # type: ignore\n                                                                            Dict[str, Any],\n                                                                            List[Tuple[str, Any]]]:\n        \"\"\"\n        Render the response.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def estimate_tokens(self, method: 'TokensEstimator' = None) -> int: # type: ignore # TokensEstimator.DEFAULT\n        \"\"\"\n        Estimate the number of tokens in the response message.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n\n    @property\n    def message(self) -> str:\n        \"\"\"Get the response message.\"\"\"\n        pass\n\n    @property\n    def score(self) -> float:\n        \"\"\"Get the response score.\"\"\"\n        pass\n\n    @property\n    def temperature(self) -> float:\n        \"\"\"Get the temperature.\"\"\"\n        pass\n\n    @property\n    def tokens(self) -> int:\n        \"\"\"Get the tokens.\"\"\"\n        pass\n\n    @property\n    def inference_time(self) -> float:\n        \"\"\"Get inference time.\"\"\"\n        pass\n\n    @property\n    def role(self) -> 'Role': # type: ignore\n        \"\"\"Get the response role.\"\"\"\n        pass\n\n    @property\n    def model(self) -> str:\n        \"\"\"Get the agent model.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the response creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the response object modification date.\"\"\"\n        pass\n"
            },
            {
                "file_path": "memor/tokens_estimator.py",
                "code": "from enum import Enum\nfrom typing import Set, List\n\ndef universal_tokens_estimator(message: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text or code snippet.\n\n    :param message: The input text or code snippet to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\ndef openai_tokens_estimator_gpt_3_5(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's GPT-3.5 Turbo model.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\ndef openai_tokens_estimator_gpt_4(text: str) -> int:\n    \"\"\"\n    Estimate the number of tokens in a given text for OpenAI's GPT-4 model.\n\n    :param text: The input text to estimate tokens for.\n    :return: Estimated number of tokens.\n    \"\"\"\n    pass\nclass TokensEstimator(Enum):\n    \"\"\"Token estimator enum.\"\"\"\n\n    UNIVERSAL = universal_tokens_estimator\n    OPENAI_GPT_3_5 = openai_tokens_estimator_gpt_3_5\n    OPENAI_GPT_4 = openai_tokens_estimator_gpt_4\n    DEFAULT = UNIVERSAL\n"
            },
            {
                "file_path": "memor/template.py",
                "code": "from typing import Dict, Any, Union\nimport json\nimport datetime\nfrom enum import Enum\n# from .params import DATE_TIME_FORMAT # Not allowed\n# from .params import DATA_SAVE_SUCCESS_MESSAGE # Not allowed\n# from .params import INVALID_TEMPLATE_STRUCTURE_MESSAGE # Not allowed\n# from .params import MEMOR_VERSION # Not allowed\n# from .errors import MemorValidationError # Not allowed\n# from .functions import get_time_utc # Not allowed\n# from .functions import _validate_path, _validate_custom_map # Not allowed\n# from .functions import _validate_string # Not allowed\n\nclass PromptTemplate:\n    r\"\"\"\n    Prompt template.\n\n    >>> template = PromptTemplate(content=\"Take a deep breath\\n{prompt_message}!\", title=\"Greeting\")\n    >>> template.title\n    'Greeting'\n    \"\"\"\n\n    def __init__(\n            self,\n            content: str = None,\n            file_path: str = None,\n            title: str = None,\n            custom_map: Dict[str, str] = None) -> None:\n        \"\"\"\n        Prompt template object initiator.\n\n        :param content: template content\n        :param file_path: template file path\n        :param title: template title\n        :param custom_map: custom map\n        \"\"\"\n        pass\n\n    def __eq__(self, other_template: \"PromptTemplate\") -> bool:\n        \"\"\"\n        Check templates equality.\n\n        :param other_template: another template\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of PromptTemplate.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of PromptTemplate.\"\"\"\n        pass\n\n    def __copy__(self) -> \"PromptTemplate\":\n        \"\"\"Return a copy of the PromptTemplate object.\"\"\"\n        pass\n\n    def copy(self) -> \"PromptTemplate\":\n        \"\"\"Return a copy of the PromptTemplate object.\"\"\"\n        pass\n\n    def update_title(self, title: str) -> None:\n        \"\"\"\n        Update title.\n\n        :param title: title\n        \"\"\"\n        pass\n\n    def update_content(self, content: str) -> None:\n        \"\"\"\n        Update content.\n\n        :param content: content\n        \"\"\"\n        pass\n\n    def update_map(self, custom_map: Dict[str, str]) -> None:\n        \"\"\"\n        Update custom map.\n\n        :param custom_map: custom map\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: template file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: template file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert PromptTemplate to json.\"\"\"\n        pass\n\n    # to_dict is not directly called by tests, so it's excluded.\n    # def to_dict(self) -> Dict[str, Any]:\n    #     \"\"\"Convert PromptTemplate to dict.\"\"\"\n    #     pass\n\n    @property\n    def content(self) -> str:\n        \"\"\"Get the PromptTemplate content.\"\"\"\n        pass\n\n    @property\n    def title(self) -> str:\n        \"\"\"Get the PromptTemplate title.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the PromptTemplate creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the PromptTemplate modification date.\"\"\"\n        pass\n\n    @property\n    def custom_map(self) -> Dict[str, str]:\n        \"\"\"Get the PromptTemplate custom map.\"\"\"\n        pass\n\n# Inlined constants for PresetPromptTemplate definitions\n_PROMPT_INSTRUCTION1 = \"I'm providing you with a history of a previous conversation. Please consider this context when responding to my new question.\\n\"\n\n_BASIC_PROMPT_CONTENT = \"{instruction}{prompt[message]}\"\n_BASIC_RESPONSE_CONTENT = \"{instruction}{response[message]}\"\n_BASIC_PROMPT_RESPONSE_STANDARD_CONTENT = \"{instruction}Prompt: {prompt[message]}\\nResponse: {response[message]}\"\n\n\nclass _BasicPresetPromptTemplate(Enum):\n    \"\"\"Preset basic-prompt templates.\"\"\"\n\n    PROMPT = PromptTemplate(content=_BASIC_PROMPT_CONTENT, title=\"Basic/Prompt\", custom_map={\"instruction\": \"\"})\n    RESPONSE = PromptTemplate(\n        content=_BASIC_RESPONSE_CONTENT,\n        title=\"Basic/Response\",\n        custom_map={\n            \"instruction\": \"\"})\n    PROMPT_RESPONSE_STANDARD = PromptTemplate(\n        content=_BASIC_PROMPT_RESPONSE_STANDARD_CONTENT,\n        title=\"Basic/Prompt-Response Standard\",\n        custom_map={\n            \"instruction\": \"\"})\n\nclass _Instruction1PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction1-prompt templates.\"\"\"\n\n    PROMPT = PromptTemplate(\n        content=_BASIC_PROMPT_CONTENT, # Uses inlined or locally defined constant\n        title=\"Instruction1/Prompt\",\n        custom_map={\n            \"instruction\": _PROMPT_INSTRUCTION1}) # Uses inlined or locally defined constant\n\nclass _Instruction2PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction2-prompt templates.\"\"\"\n    pass # No members directly used by tests\n\nclass _Instruction3PresetPromptTemplate(Enum):\n    \"\"\"Preset instruction3-prompt templates.\"\"\"\n    pass # No members directly used by tests\n\n\nclass PresetPromptTemplate:\n    \"\"\"Preset prompt templates.\"\"\"\n\n    BASIC = _BasicPresetPromptTemplate\n    INSTRUCTION1 = _Instruction1PresetPromptTemplate\n    INSTRUCTION2 = _Instruction2PresetPromptTemplate\n    INSTRUCTION3 = _Instruction3PresetPromptTemplate\n    DEFAULT = BASIC.PROMPT\n"
            },
            {
                "file_path": "memor/prompt.py",
                "code": "from typing import List, Dict, Union, Tuple, Any\nimport datetime\n# from .params import MEMOR_VERSION # Not allowed\n# from .params import DATE_TIME_FORMAT # Not allowed\n# from .params import RenderFormat, DATA_SAVE_SUCCESS_MESSAGE # Not allowed\n# from .params import Role # Not allowed\n# from .tokens_estimator import TokensEstimator # Not allowed\n# from .params import INVALID_PROMPT_STRUCTURE_MESSAGE, INVALID_TEMPLATE_MESSAGE # Not allowed\n# from .params import INVALID_ROLE_MESSAGE, INVALID_RESPONSE_MESSAGE # Not allowed\n# from .params import PROMPT_RENDER_ERROR_MESSAGE # Not allowed\n# from .params import INVALID_RENDER_FORMAT_MESSAGE # Not allowed\n# from .errors import MemorValidationError, MemorRenderError # Not allowed\n# from .functions import get_time_utc # Not allowed\n# from .functions import _validate_string, _validate_pos_int, _validate_list_of # Not allowed\n# from .functions import _validate_path # Not allowed\n# from .template import PromptTemplate, PresetPromptTemplate # Not allowed\n# from .response import Response # Not allowed\n\nclass Prompt:\n    \"\"\"\n    Prompt class.\n\n    >>> from memor import Prompt, Role, Response\n    >>> responses = [Response(message=\"I am fine.\"), Response(message=\"I am not fine.\"), Response(message=\"I am okay.\")]\n    >>> prompt = Prompt(message=\"Hello, how are you?\", responses=responses)\n    >>> prompt.message\n    'Hello, how are you?'\n    >>> prompt.responses[1].message\n    'I am not fine.'\n    \"\"\"\n\n    def __init__(\n            self,\n            message: str = \"\",\n            responses: List['Response'] = [], # type: ignore\n            role: 'Role' = None, # Role.DEFAULT, # type: ignore\n            tokens: int = None,\n            template: Union['PresetPromptTemplate', 'PromptTemplate'] = None, # PresetPromptTemplate.DEFAULT, # type: ignore\n            file_path: str = None,\n            init_check: bool = True) -> None:\n        \"\"\"\n        Prompt object initiator.\n\n        :param message: prompt message\n        :param responses: prompt responses\n        :param role: prompt role\n        :param tokens: tokens\n        :param template: prompt template\n        :param file_path: prompt file path\n        :param init_check: initial check flag\n        \"\"\"\n        pass\n\n    def __eq__(self, other_prompt: \"Prompt\") -> bool:\n        \"\"\"\n        Check prompts equality.\n\n        :param other_prompt: another prompt\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Prompt.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Prompt.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Prompt object.\"\"\"\n        pass\n\n    def __copy__(self) -> \"Prompt\":\n        \"\"\"\n        Return a copy of the Prompt object.\n\n        :return: a copy of Prompt object\n        \"\"\"\n        pass\n\n    def copy(self) -> \"Prompt\":\n        \"\"\"\n        Return a copy of the Prompt object.\n\n        :return: a copy of Prompt object\n        \"\"\"\n        pass\n\n    def add_response(self, response: 'Response', index: int = None) -> None: # type: ignore\n        \"\"\"\n        Add a response to the prompt object.\n\n        :param response: response\n        :param index: index\n        \"\"\"\n        pass\n\n    def remove_response(self, index: int) -> None:\n        \"\"\"\n        Remove a response from the prompt object.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def select_response(self, index: int) -> None:\n        \"\"\"\n        Select a response as selected response.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def update_responses(self, responses: List['Response']) -> None: # type: ignore\n        \"\"\"\n        Update the prompt responses.\n\n        :param responses: responses\n        \"\"\"\n        pass\n\n    def update_message(self, message: str) -> None:\n        \"\"\"\n        Update the prompt message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def update_role(self, role: 'Role') -> None: # type: ignore\n        \"\"\"\n        Update the prompt role.\n\n        :param role: role\n        \"\"\"\n        pass\n\n    def update_tokens(self, tokens: int) -> None:\n        \"\"\"\n        Update the tokens.\n\n        :param tokens: tokens\n        \"\"\"\n        pass\n\n    def update_template(self, template: 'PromptTemplate') -> None: # type: ignore\n        \"\"\"\n        Update the prompt template.\n\n        :param template: template\n        \"\"\"\n        pass\n\n    def save(self, file_path: str, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: prompt file path\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: prompt file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self, save_template: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Convert the prompt to a JSON object.\n\n        :param save_template: save template flag\n        \"\"\"\n        pass\n\n    # to_dict is not directly called by tests\n    # def to_dict(self, save_template: bool = True) -> Dict[str, Any]:\n    #     \"\"\"\n    #     Convert the prompt to a dictionary.\n    #\n    #     :param save_template: save template flag\n    #     \"\"\"\n    #     pass\n\n    @property\n    def message(self) -> str:\n        \"\"\"Get the prompt message.\"\"\"\n        pass\n\n    @property\n    def responses(self) -> List['Response']: # type: ignore\n        \"\"\"Get the prompt responses.\"\"\"\n        pass\n\n    @property\n    def role(self) -> 'Role': # type: ignore\n        \"\"\"Get the prompt role.\"\"\"\n        pass\n\n    @property\n    def tokens(self) -> int:\n        \"\"\"Get the prompt tokens.\"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the prompt creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the prompt object modification date.\"\"\"\n        pass\n\n    @property\n    def template(self) -> 'PromptTemplate': # type: ignore\n        \"\"\"Get the prompt template.\"\"\"\n        pass\n\n    @property\n    def selected_response(self) -> 'Response': # type: ignore\n        \"\"\"Get the prompt selected response.\"\"\"\n        pass\n\n    def render(self, render_format: 'RenderFormat' = None) -> Union[str, # type: ignore # RenderFormat.DEFAULT\n                                                                                  Dict[str, Any],\n                                                                                  List[Tuple[str, Any]]]:\n        \"\"\"\n        Render method.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def check_render(self) -> bool:\n        \"\"\"Check render.\"\"\"\n        pass\n\n    def estimate_tokens(self, method: 'TokensEstimator' = None) -> int: # type: ignore # TokensEstimator.DEFAULT\n        \"\"\"\n        Estimate the number of tokens in the prompt message.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "memor/session.py",
                "code": "from typing import List, Dict, Tuple, Any, Union, Generator\nimport datetime\n# from .params import MEMOR_VERSION # Not allowed\n# from .params import DATE_TIME_FORMAT # Not allowed\n# from .params import DATA_SAVE_SUCCESS_MESSAGE # Not allowed\n# from .params import INVALID_MESSAGE # Not allowed\n# from .params import INVALID_MESSAGE_STATUS_LEN_MESSAGE # Not allowed\n# from .params import INVALID_RENDER_FORMAT_MESSAGE # Not allowed\n# from .params import UNSUPPORTED_OPERAND_ERROR_MESSAGE # Not allowed\n# from .params import RenderFormat # Not allowed\n# from .tokens_estimator import TokensEstimator # Not allowed\n# from .prompt import Prompt # Not allowed\n# from .response import Response # Not allowed\n# from .errors import MemorValidationError # Not allowed\n# from .functions import get_time_utc # Not allowed\n# from .functions import _validate_bool, _validate_path # Not allowed\n# from .functions import _validate_list_of, _validate_string # Not allowed\n\n\nclass Session:\n    \"\"\"Session class.\"\"\"\n\n    def __init__(\n            self,\n            title: str = None,\n            messages: List[Union['Prompt', 'Response']] = [], # type: ignore\n            file_path: str = None,\n            init_check: bool = True) -> None:\n        \"\"\"\n        Session object initiator.\n\n        :param title: title\n        :param messages: messages\n        :param file_path: file path\n        :param init_check: initial check flag\n        \"\"\"\n        pass\n\n    def __eq__(self, other_session: \"Session\") -> bool:\n        \"\"\"\n        Check sessions equality.\n\n        :param other_session: other session\n        \"\"\"\n        pass\n\n    def __str__(self) -> str:\n        \"\"\"Return string representation of Session.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return string representation of Session.\"\"\"\n        pass\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the Session object.\"\"\"\n        pass\n\n    def __iter__(self) -> Generator[Union['Prompt', 'Response'], None, None]: # type: ignore\n        \"\"\"Iterate through the Session object.\"\"\"\n        pass\n\n    def __add__(self, other_object: Union[\"Session\", 'Response', 'Prompt']) -> \"Session\": # type: ignore\n        \"\"\"\n        Addition method.\n\n        :param other_object: other object\n        \"\"\"\n        pass\n\n    def __radd__(self, other_object: Union['Response', 'Prompt']) -> \"Session\": # type: ignore\n        \"\"\"\n        Reverse addition method.\n\n        :param other_object: other object\n        \"\"\"\n        pass\n\n    def __contains__(self, message: Union['Prompt', 'Response']) -> bool: # type: ignore\n        \"\"\"\n        Check if the Session contains the given message.\n\n        :param message: message\n        \"\"\"\n        pass\n\n    def __getitem__(self, index: Union[int, slice]) -> Union['Prompt', 'Response', List[Union['Prompt', 'Response']]]: # type: ignore\n        \"\"\"\n        Return the Session message(s).\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def __copy__(self) -> \"Session\":\n        \"\"\"Return a copy of the Session object.\"\"\"\n        pass\n\n    def copy(self) -> \"Session\":\n        \"\"\"Return a copy of the Session object.\"\"\"\n        pass\n\n    def add_message(self,\n                    message: Union['Prompt', 'Response'], # type: ignore\n                    status: bool = True,\n                    index: int = None) -> None:\n        \"\"\"\n        Add a message to the session object.\n\n        :param message: message\n        :param status: status\n        :param index: index\n        \"\"\"\n        pass\n\n    def remove_message(self, index: int) -> None:\n        \"\"\"\n        Remove a message from the session object.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def clear_messages(self) -> None:\n        \"\"\"Remove all messages.\"\"\"\n        pass\n\n    def enable_message(self, index: int) -> None:\n        \"\"\"\n        Enable a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def disable_message(self, index: int) -> None:\n        \"\"\"\n        Disable a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def mask_message(self, index: int) -> None:\n        \"\"\"\n        Mask a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def unmask_message(self, index: int) -> None:\n        \"\"\"\n        Unmask a message.\n\n        :param index: index\n        \"\"\"\n        pass\n\n    def update_title(self, title: str) -> None:\n        \"\"\"\n        Update the session title.\n\n        :param title: title\n        \"\"\"\n        pass\n\n    def update_messages(self,\n                        messages: List[Union['Prompt', 'Response']], # type: ignore\n                        status: List[bool] = None) -> None:\n        \"\"\"\n        Update the session messages.\n\n        :param messages: messages\n        :param status: status\n        \"\"\"\n        pass\n\n    def update_messages_status(self, status: List[bool]) -> None:\n        \"\"\"\n        Update the session messages status.\n\n        :param status: status\n        \"\"\"\n        pass\n\n    def save(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Save method.\n\n        :param file_path: session file path\n        \"\"\"\n        pass\n\n    def load(self, file_path: str) -> None:\n        \"\"\"\n        Load method.\n\n        :param file_path: session file path\n        \"\"\"\n        pass\n\n    def from_json(self, json_object: Union[str, Dict[str, Any]]) -> None:\n        \"\"\"\n        Load attributes from the JSON object.\n\n        :param json_object: JSON object\n        \"\"\"\n        pass\n\n    def to_json(self) -> Dict[str, Any]:\n        \"\"\"Convert the session to a JSON object.\"\"\"\n        pass\n\n    # to_dict is not directly called by tests\n    # def to_dict(self) -> Dict[str, Any]:\n    #     \"\"\"\n    #     Convert the session to a dictionary.\n    #\n    #     :return: dict\n    #     \"\"\"\n    #     pass\n\n    def render(self, render_format: 'RenderFormat' = None) -> Union[str, # type: ignore # RenderFormat.DEFAULT\n                                                                                  List[Dict[str, str]], # For OPENAI\n                                                                                  Dict[str, Any], # For DICTIONARY\n                                                                                  List[Tuple[str, Any]]]: # For ITEMS\n        \"\"\"\n        Render method.\n\n        :param render_format: render format\n        \"\"\"\n        pass\n\n    def check_render(self) -> bool:\n        \"\"\"Check render.\"\"\"\n        pass\n\n    def estimate_tokens(self, method: 'TokensEstimator' = None) -> int: # type: ignore # TokensEstimator.DEFAULT\n        \"\"\"\n        Estimate the number of tokens in the session.\n\n        :param method: token estimator method\n        \"\"\"\n        pass\n\n    @property\n    def date_created(self) -> datetime.datetime:\n        \"\"\"Get the session creation date.\"\"\"\n        pass\n\n    @property\n    def date_modified(self) -> datetime.datetime:\n        \"\"\"Get the session object modification date.\"\"\"\n        pass\n\n    @property\n    def title(self) -> str:\n        \"\"\"Get the session title.\"\"\"\n        pass\n\n    @property\n    def messages(self) -> List[Union['Prompt', 'Response']]: # type: ignore\n        \"\"\"Get the session messages.\"\"\"\n        pass\n\n    @property\n    def messages_status(self) -> List[bool]:\n        \"\"\"Get the session messages status.\"\"\"\n        pass\n\n    @property\n    def masks(self) -> List[bool]:\n        \"\"\"Get the session masks.\"\"\"\n        pass\n\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "tests/test_token_estimators.py::test_universal_tokens_estimator_with_contractions",
                "covers": [
                    "memor.tokens_estimator.universal_tokens_estimator - happy path with text and contractions"
                ]
            },
            {
                "test_id": "tests/test_token_estimators.py::test_openai_tokens_estimator_with_function_definition",
                "covers": [
                    "memor.tokens_estimator.openai_tokens_estimator_gpt_3_5 - happy path with code-like text"
                ]
            },
            {
                "test_id": "tests/test_token_estimators.py::test_openai_tokens_estimator_with_gpt4_model",
                "covers": [
                    "memor.tokens_estimator.openai_tokens_estimator_gpt_4 - happy path for GPT-4 model"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_message1",
                "covers": [
                    "memor.Response.__init__ - basic instantiation and default property access (message, role, tokens, etc.)"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_json1",
                "covers": [
                    "memor.Response.__init__ - instantiation with multiple parameters",
                    "memor.Response.to_json - serialization",
                    "memor.Response.from_json - deserialization",
                    "memor.Response.__eq__ - equality check"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_message2",
                "covers": [
                    "memor.Response.update_message - happy path"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_score2",
                "covers": [
                    "memor.Response.update_score - happy path"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_role2",
                "covers": [
                    "memor.Response.update_role - happy path using memor.Role"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_temperature2",
                "covers": [
                    "memor.Response.update_temperature - happy path"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_tokens3",
                "covers": [
                    "memor.Response.update_tokens - happy path"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_inference_time3",
                "covers": [
                    "memor.Response.update_inference_time - happy path"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_model2",
                "covers": [
                    "memor.Response.update_model - happy path"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_save1",
                "covers": [
                    "memor.Response.save - happy path file save"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_load1",
                "covers": [
                    "memor.Response.load - happy path file load via __init__(file_path=...)"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_render1",
                "covers": [
                    "memor.Response.render - default format (STRING)"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_render2",
                "covers": [
                    "memor.Response.render - RenderFormat.OPENAI usage"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_render3",
                "covers": [
                    "memor.Response.render - RenderFormat.DICTIONARY usage (covers Response.to_dict)"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_render4",
                "covers": [
                    "memor.Response.render - RenderFormat.ITEMS usage"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_estimated_tokens1",
                "covers": [
                    "memor.Response.estimate_tokens - happy path using memor.TokensEstimator.UNIVERSAL"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_copy2",
                "covers": [
                    "memor.Response.copy - direct method call"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_copy1",
                "covers": [
                    "memor.Response.__copy__ - usage with copy.copy for shallow copy behavior"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_str",
                "covers": [
                    "memor.Response.__str__ - string representation"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_repr",
                "covers": [
                    "memor.Response.__repr__ - object representation"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_length1",
                "covers": [
                    "memor.Response.__len__ - length based on rendered string"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_date1",
                "covers": [
                    "memor.Response.__init__ - instantiation with custom date parameter"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_content1",
                "covers": [
                    "memor.PromptTemplate.__init__ - basic instantiation with content and default property access (title, custom_map)"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_json1",
                "covers": [
                    "memor.PromptTemplate.__init__ - instantiation with content and custom_map",
                    "memor.PromptTemplate.to_json - serialization",
                    "memor.PromptTemplate.from_json - deserialization",
                    "memor.PromptTemplate.__eq__ - equality check",
                    "memor.PromptTemplate.to_dict - implicit via to_json"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_title2",
                "covers": [
                    "memor.PromptTemplate.update_title - happy path"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_content2",
                "covers": [
                    "memor.PromptTemplate.update_content - happy path"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_custom_map2",
                "covers": [
                    "memor.PromptTemplate.update_map - happy path"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_save1",
                "covers": [
                    "memor.PromptTemplate.save - happy path file save"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_load1",
                "covers": [
                    "memor.PromptTemplate.load - happy path file load via __init__(file_path=...)"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_copy2",
                "covers": [
                    "memor.PromptTemplate.copy - direct method call"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_copy1",
                "covers": [
                    "memor.PromptTemplate.__copy__ - usage with copy.copy for shallow copy behavior"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_str",
                "covers": [
                    "memor.PromptTemplate.__str__ - string representation"
                ]
            },
            {
                "test_id": "tests/test_prompt_template.py::test_repr",
                "covers": [
                    "memor.PromptTemplate.__repr__ - object representation"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_message1",
                "covers": [
                    "memor.Prompt.__init__ - basic instantiation and default property access (message, role, template, etc.)"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_json1",
                "covers": [
                    "memor.Prompt.__init__ - instantiation with responses, role, and PresetPromptTemplate",
                    "memor.Prompt.to_json - serialization",
                    "memor.Prompt.from_json - deserialization",
                    "memor.Prompt.__eq__ - equality check"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_message2",
                "covers": [
                    "memor.Prompt.update_message - happy path"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_tokens3",
                "covers": [
                    "memor.Prompt.update_tokens - happy path"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_role2",
                "covers": [
                    "memor.Prompt.update_role - happy path using memor.Role"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_responses3",
                "covers": [
                    "memor.Prompt.update_responses - happy path"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_add_response1",
                "covers": [
                    "memor.Prompt.add_response - happy path"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_remove_response",
                "covers": [
                    "memor.Prompt.remove_response - happy path"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_select_response",
                "covers": [
                    "memor.Prompt.select_response - happy path and selected_response property"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_template2",
                "covers": [
                    "memor.Prompt.update_template - with memor.PresetPromptTemplate"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_template3",
                "covers": [
                    "memor.Prompt.__init__ - with custom memor.PromptTemplate argument"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_save2",
                "covers": [
                    "memor.Prompt.save - happy path file save",
                    "memor.Prompt.load - happy path file load via __init__(file_path=...)"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_render1",
                "covers": [
                    "memor.Prompt.render - default format (STRING)"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_render2",
                "covers": [
                    "memor.Prompt.render - RenderFormat.OPENAI usage"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_render3",
                "covers": [
                    "memor.Prompt.render - RenderFormat.DICTIONARY usage (covers Prompt.to_dict)"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_render4",
                "covers": [
                    "memor.Prompt.render - RenderFormat.ITEMS usage"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_render5",
                "covers": [
                    "memor.Prompt.render - with custom template content and map values"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_check_render2",
                "covers": [
                    "memor.Prompt.check_render - happy path (True result)"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_estimated_tokens1",
                "covers": [
                    "memor.Prompt.estimate_tokens - happy path using memor.TokensEstimator.UNIVERSAL"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_copy2",
                "covers": [
                    "memor.Prompt.copy - direct method call"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_copy1",
                "covers": [
                    "memor.Prompt.__copy__ - usage with copy.copy for shallow copy behavior"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_str",
                "covers": [
                    "memor.Prompt.__str__ - string representation"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_repr",
                "covers": [
                    "memor.Prompt.__repr__ - object representation"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_length1",
                "covers": [
                    "memor.Prompt.__len__ - length based on rendered string"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_title1",
                "covers": [
                    "memor.Session.__init__ - basic instantiation with title and default property access"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_messages1",
                "covers": [
                    "memor.Session.__init__ - instantiation with messages and messages_status property default"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_json",
                "covers": [
                    "memor.Session.to_json - serialization",
                    "memor.Session.from_json - deserialization",
                    "memor.Session.__eq__ - equality check"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_title2",
                "covers": [
                    "memor.Session.update_title - happy path"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_messages2",
                "covers": [
                    "memor.Session.update_messages - happy path"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_messages_status2",
                "covers": [
                    "memor.Session.update_messages_status - happy path"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_add_message1",
                "covers": [
                    "memor.Session.add_message - happy path"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_remove_message",
                "covers": [
                    "memor.Session.remove_message - happy path"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_clear_messages",
                "covers": [
                    "memor.Session.clear_messages - happy path"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_enable_message",
                "covers": [
                    "memor.Session.enable_message - happy path"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_disable_message",
                "covers": [
                    "memor.Session.disable_message - happy path"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_mask_message",
                "covers": [
                    "memor.Session.mask_message - happy path (calls disable_message)"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_unmask_message",
                "covers": [
                    "memor.Session.unmask_message - happy path (calls enable_message)"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_masks",
                "covers": [
                    "memor.Session.masks - property read"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_save2",
                "covers": [
                    "memor.Session.save - happy path file save",
                    "memor.Session.load - happy path file load via __init__(file_path=...)"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_render1",
                "covers": [
                    "memor.Session.render - default format (STRING)"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_render2",
                "covers": [
                    "memor.Session.render - RenderFormat.OPENAI usage"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_render3",
                "covers": [
                    "memor.Session.render - RenderFormat.DICTIONARY usage (covers Session.to_dict)"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_render4",
                "covers": [
                    "memor.Session.render - RenderFormat.ITEMS usage"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_check_render1",
                "covers": [
                    "memor.Session.check_render - happy path (True result)"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_estimated_tokens1",
                "covers": [
                    "memor.Session.estimate_tokens - happy path using memor.TokensEstimator.UNIVERSAL"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_copy2",
                "covers": [
                    "memor.Session.copy - direct method call"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_copy1",
                "covers": [
                    "memor.Session.__copy__ - usage with copy.copy for shallow copy behavior"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_str",
                "covers": [
                    "memor.Session.__str__ - string representation"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_repr",
                "covers": [
                    "memor.Session.__repr__ - object representation"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_length",
                "covers": [
                    "memor.Session.__len__ - number of messages"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_iter",
                "covers": [
                    "memor.Session.__iter__ - iteration over messages"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_addition1",
                "covers": [
                    "memor.Session.__add__ - Session + Session operation"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_addition3",
                "covers": [
                    "memor.Session.__add__ - Session + Response operation"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_addition4",
                "covers": [
                    "memor.Session.__add__ - Session + Prompt operation"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_addition5",
                "covers": [
                    "memor.Session.__radd__ - Response + Session operation"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_addition6",
                "covers": [
                    "memor.Session.__radd__ - Prompt + Session operation"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_contains1",
                "covers": [
                    "memor.Session.__contains__ - item in session check"
                ]
            },
            {
                "test_id": "tests/test_session.py::test_getitem1",
                "covers": [
                    "memor.Session.__getitem__ - access message by index"
                ]
            },
            {
                "test_id": "tests/test_prompt.py::test_render6",
                "covers": [
                    "memor.MemorRenderError - raising on incompatible prompt template/properties"
                ]
            },
            {
                "test_id": "tests/test_response.py::test_message3",
                "covers": [
                    "memor.MemorValidationError - raising on invalid input for Response.update_message"
                ]
            }
        ]
    },
    {
        "idx": 63973,
        "repo_name": "denisalevi_bib4llm",
        "url": "https://github.com/denisalevi/bib4llm",
        "description": "Convert your PDF library into LLM-readable format for AI-assisted research.",
        "stars": 24,
        "forks": 3,
        "language": "python",
        "size": 12183,
        "created_at": "2024-12-13T15:39:52+00:00",
        "updated_at": "2025-04-24T07:37:23+00:00",
        "pypi_info": {
            "name": "bib4llm",
            "version": "0.2.2",
            "url": "https://files.pythonhosted.org/packages/f3/04/4f69928a8ca81b39f73338153398b76b5b20c33fabcbbe4e17681d2a75a9/bib4llm-0.2.2.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 12,
            "comment_ratio": 0.2781709308655416,
            "pyfile_content_length": 148193,
            "pyfile_code_lines": 3674,
            "test_file_exist": true,
            "test_file_content_length": 74366,
            "pytest_framework": true,
            "test_case_num": 33,
            "metadata_path": [
                "pyproject.toml"
            ],
            "readme_content_length": 5096,
            "llm_reason": "The project 'bib4llm' is a strong candidate for an AI 'Build from Scratch' benchmark. \nPositive aspects:\n*   **Clear & Well-Defined Functionality:** The project converts PDF libraries (from directories or BibTeX files) into Markdown and PNGs. This is a specific and understandable task.\n*   **Self-Contained for AI Rebuild:** Core operations involve local file processing. It does not require internet access or external APIs for its primary function. Dependencies like `PyMuPDF4LLM` (for PDF parsing), `bibtexparser` (assumed for BibTeX), and `watchdog` (assumed for file watching) are expected to be pip-installable and operate locally. The use of SQLite for state tracking is also self-contained (local file).\n*   **Testable & Verifiable Output:** The project produces files (Markdown, PNGs) in a defined directory structure. The presence of extensive test files in the original project (`test_conversion.py`, `test_cli.py`, `test_pdf_processing.py`) is a significant advantage, as these can be adapted for verifying the AI's output.\n*   **No GUI:** It is a command-line interface (CLI) tool, which is suitable.\n*   **Appropriate Complexity (Medium):** The project involves several components: CLI argument parsing, BibTeX file parsing, PDF processing orchestration (using a library like `PyMuPDF4LLM`), file and directory management, multiprocessing for parallel processing, file system watching for automatic updates, and SQLite database interaction for tracking processed files. This combination presents a meaningful, non-trivial challenge without being excessively complex, assuming the core PDF parsing is delegated to a library.\n*   **Well-Understood Problem Domain:** File conversion and CLI utility development are common programming tasks.\n*   **Modular Structure:** The codebase is organized into logical components (CLI, processing, watcher), which is good for a replication task.\n\nNegative aspects or concerns (mostly minor or addressable by benchmark specification):\n*   **Core Dependency on C Library:** The primary PDF processing capability relies on `PyMuPDF4LLM`, which in turn uses `PyMuPDF`, a Python binding for the C library MuPDF. While `pip install PyMuPDF` usually handles this seamlessly via pre-compiled wheels, it's a step beyond pure Python dependencies. This is generally acceptable if `pip install` works reliably.\n*   **Benchmark Specification Clarity:** It must be explicitly stated that the AI is expected to *use* an existing library like `PyMuPDF4LLM` for the PDF-to-Markdown conversion, rather than re-implementing PDF parsing from scratch (which would make the task 'Very Hard / Unsuitable'). The benchmark would focus on the AI building the surrounding application logic and integration.\n*   **Complexity of Multiprocessing/Watching:** Implementing robust multiprocessing and file watching can be tricky (e.g., issues with progress bars and logging, as hinted in the project's 'Future work'). This, however, also makes for a good, realistic programming challenge.\n\nOverall, if the PDF parsing aspect is handled by allowing the use of `PyMuPDF4LLM`, the project offers a well-scoped, testable, and appropriately complex task for an AI to rebuild from scratch.",
            "llm_project_type": "CLI document processing and conversion utility",
            "llm_rating": 80,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "denisalevi_bib4llm",
            "finish_test": true,
            "test_case_result": {
                "tests/test_basic.py::TestBibliographyProcessor::test_get_log_file": "passed",
                "tests/test_basic.py::TestBibliographyProcessor::test_get_output_dir": "passed",
                "tests/test_cli.py::TestCLI::test_clean_dry_run": "passed",
                "tests/test_cli.py::TestCLI::test_clean_help": "passed",
                "tests/test_cli.py::TestCLI::test_clean_removes_output_dir": "passed",
                "tests/test_cli.py::TestCLI::test_convert_creates_output_dir": "passed",
                "tests/test_cli.py::TestCLI::test_convert_dry_run": "passed",
                "tests/test_cli.py::TestCLI::test_convert_help": "passed",
                "tests/test_cli.py::TestCLI::test_convert_with_processes": "passed",
                "tests/test_cli.py::TestCLI::test_help": "passed",
                "tests/test_cli.py::TestCLI::test_processes_option": "passed",
                "tests/test_cli.py::TestCLI::test_watch_help": "passed",
                "tests/test_conversion.py::TestConversion::test_conversion_compare_to_example": "passed",
                "tests/test_conversion.py::TestConversion::test_conversion_output_structure": "passed",
                "tests/test_conversion.py::TestConversion::test_conversion_using_cli": "passed",
                "tests/test_conversion.py::TestConversion::test_convert_pdf_directory": "passed",
                "tests/test_conversion.py::TestConversion::test_convert_pdf_directory_with_subdirs": "passed",
                "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion": "passed",
                "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion_using_cli": "passed",
                "tests/test_conversion.py::TestConversion::test_pdf_subdirectory_conversion": "passed",
                "tests/test_example_conversion.py::TestExampleConversion::test_example_conversion": "passed",
                "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing": "passed",
                "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing_with_custom_options": "passed",
                "tests/test_process_bibliography.py::TestProcessingResult::test_init": "passed",
                "tests/test_process_bibliography.py::TestBibliographyProcessor::test_context_manager": "passed",
                "tests/test_process_bibliography.py::TestBibliographyProcessor::test_get_log_file": "passed",
                "tests/test_process_bibliography.py::TestBibliographyProcessor::test_get_output_dir": "passed",
                "tests/test_process_bibliography.py::TestBibliographyProcessor::test_init": "passed",
                "tests/test_process_bibliography.py::TestBibliographyProcessor::test_init_with_dry_run": "passed",
                "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths": "passed",
                "tests/test_process_bibliography.py::TestBibliographyProcessor::test_process_with_relative_paths": "passed",
                "tests/test_process_bibliography.py::TestBibliographyProcessor::test_standalone_parse_file_field": "passed",
                "tests/test_watcher.py::TestWatcher::test_directory_watcher": "passed"
            },
            "success_count": 33,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "unknown_count": 0,
            "total_count": 33,
            "success_rate": 1.0,
            "coverage_report": {
                "covered_lines": 335,
                "num_statements": 911,
                "percent_covered": 34.77911646586345,
                "percent_covered_display": "35",
                "missing_lines": 576,
                "excluded_lines": 0,
                "num_branches": 334,
                "num_partial_branches": 42,
                "covered_branches": 98,
                "missing_branches": 236
            },
            "coverage_result": {}
        },
        "codelines_count": 3674,
        "codefiles_count": 12,
        "code_length": 148193,
        "test_files_count": 7,
        "test_code_length": 74366,
        "class_diagram": "@startuml\nclass TestWatcher {\n    setUp(): void\n    tearDown(): void\n    test_directory_watcher(): void\n}\nclass TestConversion {\n    setUp(): void\n    tearDown(): void\n    test_conversion_output_structure(): void\n    test_conversion_compare_to_example(): void\n    test_conversion_using_cli(): void\n    test_pdf_directory_conversion(): void\n    test_pdf_directory_conversion_using_cli(): void\n    test_pdf_subdirectory_conversion(): void\n    test_convert_pdf_directory(): void\n    test_convert_pdf_directory_with_subdirs(): void\n}\nclass TestBibliographyProcessor {\n    setUp(): void\n    tearDown(): void\n    test_get_output_dir(): void\n    test_get_log_file(): void\n    test_init(): void\n    test_init_with_dry_run(): void\n    test_context_manager(): void\n    test_parse_file_field_with_paths(): void\n    test_standalone_parse_file_field(): void\n    test_process_with_relative_paths(): void\n}\nclass TestCLI {\n    setUp(): void\n    tearDown(): void\n    test_help(): void\n    test_convert_help(): void\n    test_watch_help(): void\n    test_clean_help(): void\n    test_convert_dry_run(): void\n    test_convert_creates_output_dir(): void\n    test_clean_removes_output_dir(): void\n    test_clean_dry_run(): void\n    test_convert_with_processes(): void\n    test_processes_option(): void\n}\nclass TestPDFProcessing {\n    setUp(): void\n    tearDown(): void\n    test_pdf_processing(): void\n    test_pdf_processing_with_custom_options(): void\n}\nclass TestExampleConversion {\n    setUp(): void\n    tearDown(): void\n    test_example_conversion(): void\n}\nclass TestProcessingResult {\n    test_init(): void\n}\nclass ProcessingResult {\n    citation_key: str\n    file_hashes: Dict[str, str]\n    dir_hash: str\n    success: bool\n    mupdf_warning_count: int\n}\nclass BibliographyProcessor {\n    get_output_dir(input_file): Path\n    get_log_file(input_file): Path\n    is_pdf_file(file_path): bool\n    __init__(input_path, dry_run, quiet): void\n    __enter__(): void\n    __exit__(exc_type, exc_val, exc_tb): void\n    _compute_file_hash(filepath): str\n    _compute_dir_hash(directory): str\n    _parse_file_field(file_field): tuple[list[Path], int]\n    process_pdf(pdf_path, force, citation_key): ProcessingResult\n    process_all(force, num_processes): void\n}\nclass DirectoryProcessor {\n    __init__(directory_path, dry_run, quiet): void\n    find_files(recursive): Tuple[List[Path], List[Path]]\n    process_directory(recursive, force, num_processes): Dict\n    _process_pdf_wrapper(args): void\n    _process_pdf(pdf_path, output_dir, force): ProcessingResult\n    _compute_file_hash(filepath): str\n    _compute_dir_hash(directory): str\n}\nclass BibTexHandler {\n    __init__(bib_file, num_processes): void\n    on_modified(event): void\n    _process(): void\n}\nclass PDFHandler {\n    __init__(pdf_file, num_processes): void\n    on_modified(event): void\n    _process(): void\n}\nclass DirectoryHandler {\n    __init__(directory_path, recursive, num_processes): void\n    on_created(event): void\n    on_modified(event): void\n    _process_file(file_path): void\n    _process_directory(): void\n}\nTestProcessingResult --> ProcessingResult\nTestBibliographyProcessor --> BibliographyProcessor\nTestConversion --> DirectoryProcessor\nBibliographyProcessor --> ProcessingResult\nDirectoryHandler --> DirectoryProcessor\nTestPDFProcessing --> DirectoryProcessor\nDirectoryProcessor --> ProcessingResult\nBibliographyProcessor ..> ProcessingResult\nTestBibliographyProcessor ..> BibliographyProcessor\nTestProcessingResult ..> ProcessingResult\nDirectoryProcessor ..> ProcessingResult\nTestPDFProcessing ..> DirectoryProcessor\nTestConversion ..> DirectoryProcessor\nDirectoryHandler ..> DirectoryProcessor\n@enduml",
        "structure": [
            {
                "file": "tests/test_watcher.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestWatcher",
                        "docstring": "Test the watcher functionality.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": "Set up the test environment.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tearDown",
                                "docstring": "Clean up after the test.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_directory_watcher",
                                "docstring": "Test that the directory watcher script can be created and run.\n\nThis test:\n1. Creates a script that simulates the watcher functionality\n2. Runs the script in a subprocess\n3. Checks that the script completes successfully",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/test_conversion.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestConversion",
                        "docstring": "Test the conversion functionality.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": "Set up the test environment.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tearDown",
                                "docstring": "Clean up after the test.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_conversion_output_structure",
                                "docstring": "Test that the conversion creates the expected output structure.\n\nThis test:\n1. Copies bibtex_library.bib to a temporary directory\n2. Runs the conversion\n3. Checks that the output directory structure matches expectations",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_conversion_compare_to_example",
                                "docstring": "Test that the conversion produces output similar to the example.\n\nThis test:\n1. Copies bibtex_library.bib to a temporary directory\n2. Runs the conversion\n3. Compares the output with bibtex_library-bib4llm",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_conversion_using_cli",
                                "docstring": "Test that the conversion works using the CLI.\n\nThis test:\n1. Copies bibtex_library.bib to a temporary directory\n2. Runs the conversion using the CLI\n3. Checks that the output directory structure matches expectations",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_pdf_directory_conversion",
                                "docstring": "Test that the conversion works for a PDF directory.\n\nThis test:\n1. Copies the pdf_dir to a temporary directory\n2. Runs the conversion on the PDF directory\n3. Checks that the output directory structure matches expectations",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_pdf_directory_conversion_using_cli",
                                "docstring": "Test that the PDF directory conversion works using the CLI.\n\nThis test:\n1. Copies the pdf_dir to a temporary directory\n2. Runs the conversion using the CLI\n3. Checks that the output directory structure matches expectations",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_pdf_subdirectory_conversion",
                                "docstring": "Test that the conversion works for subdirectories in a PDF directory.\n\nThis test:\n1. Copies the pdf_dir with its subfolder to a temporary directory\n2. Runs the conversion on the PDF directory\n3. Checks that the output directory structure includes processed files from the subfolder",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_convert_pdf_directory",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_convert_pdf_directory_with_subdirs",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_basic.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestBibliographyProcessor",
                        "docstring": "Test the BibliographyProcessor class.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": "Set up the test environment.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tearDown",
                                "docstring": "Clean up after the test.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_get_output_dir",
                                "docstring": "Test the get_output_dir method.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_get_log_file",
                                "docstring": "Test the get_log_file method.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_cli.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestCLI",
                        "docstring": "Test the CLI functionality.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": "Set up the test environment.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tearDown",
                                "docstring": "Clean up after the test.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_help",
                                "docstring": "Test the help command.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_convert_help",
                                "docstring": "Test the convert help command.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_watch_help",
                                "docstring": "Test the watch help command.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_clean_help",
                                "docstring": "Test the clean help command.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_convert_dry_run",
                                "docstring": "Test the convert command with dry run.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_convert_creates_output_dir",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_clean_removes_output_dir",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_clean_dry_run",
                                "docstring": "Test the clean command with dry run.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_convert_with_processes",
                                "docstring": "Test the convert command with the --processes flag.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_processes_option",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_pdf_processing.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestPDFProcessing",
                        "docstring": "Test the PDF processing functionality.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": "Set up the test environment.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tearDown",
                                "docstring": "Clean up after the test.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_pdf_processing",
                                "docstring": "Test that a single PDF file can be processed correctly.\n\nThis test:\n1. Copies a sample PDF to a temporary directory\n2. Processes the PDF using DirectoryProcessor\n3. Checks that the output files are created correctly",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_pdf_processing_with_custom_options",
                                "docstring": "Test PDF processing with force option.\n\nThis test:\n1. Copies a sample PDF to a temporary directory\n2. Processes the PDF with force=True to ensure reprocessing\n3. Checks that the output is generated correctly",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_example_conversion.py",
                "functions": [
                    {
                        "name": "compare_directories",
                        "docstring": "Recursively compare directories and collect differences.\n\nArgs:\n    dcmp: A dircmp object\n    differences: Dictionary to collect differences\n    \nReturns:\n    Dictionary with differences",
                        "comments": null,
                        "args": [
                            "dcmp",
                            "differences"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "TestExampleConversion",
                        "docstring": "Test the conversion of bibtex_library.bib and compare with bibtex_library-bib4llm.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": "Set up the test environment.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tearDown",
                                "docstring": "Clean up after the test.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_example_conversion",
                                "docstring": "Test that converting bibtex_library.bib produces output similar to bibtex_library-bib4llm.\n\nThis test:\n1. Copies bibtex_library.bib to a temporary directory\n2. Runs the conversion\n3. Compares the output structure with bibtex_library-bib4llm",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_process_bibliography.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestProcessingResult",
                        "docstring": "Test the ProcessingResult class.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_init",
                                "docstring": "Test initialization of ProcessingResult.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestBibliographyProcessor",
                        "docstring": "Test the BibliographyProcessor class.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": "Set up the test environment.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "tearDown",
                                "docstring": "Clean up after the test.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_get_output_dir",
                                "docstring": "Test the get_output_dir method.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_get_log_file",
                                "docstring": "Test the get_log_file method.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_init",
                                "docstring": "Test initialization of BibliographyProcessor.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_init_with_dry_run",
                                "docstring": "Test initialization of BibliographyProcessor with dry_run=True.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_context_manager",
                                "docstring": "Test the context manager functionality.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_parse_file_field_with_paths",
                                "docstring": "Test the _parse_file_field method with relative and absolute paths.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_standalone_parse_file_field",
                                "docstring": "Test the standalone parse_file_field function with relative and absolute paths.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_process_with_relative_paths",
                                "docstring": "Test processing a BibTeX file with relative paths from a different directory.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "bib4llm/process_bibliography.py",
                "functions": [
                    {
                        "name": "convert_to_extended_path",
                        "docstring": "Convert a path to Windows extended-length format if needed.\n\nArgs:\n    path: Path to convert\n    \nReturns:\n    Path: Converted path with extended-length format if on Windows and needed",
                        "comments": null,
                        "args": [
                            "path"
                        ]
                    },
                    {
                        "name": "validate_windows_path",
                        "docstring": "Validate a path for Windows compatibility.\n\nArgs:\n    path: Path to validate\n    \nReturns:\n    Optional[str]: Error message if path is invalid, None if valid",
                        "comments": null,
                        "args": [
                            "path"
                        ]
                    },
                    {
                        "name": "standalone_process_entry",
                        "docstring": "Process a single bibliography entry in a separate process.\n\nArgs:\n    args: Tuple of (entry, output_dir, bib_file_path=None)\n        entry: Dictionary containing the bibliography entry data\n        output_dir: Path to the output directory\n        bib_file_path: Optional Path to the BibTeX file (to resolve relative paths)\n        \nReturns:\n    ProcessingResult: Object containing processing results and status",
                        "comments": null,
                        "args": [
                            "args"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "ProcessingResult",
                        "docstring": "Result of processing a bibliography entry.\n\nAttributes:\n    citation_key: The citation key from the bibliography entry\n    file_hashes: Dictionary mapping file paths to their SHA-256 hashes\n    dir_hash: Hash of the directory contents (excluding linked file contents)\n    success: Whether the processing was successful\n    mupdf_warning_count: Number of MuPDF warnings encountered during processing",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "BibliographyProcessor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "get_output_dir",
                                "docstring": "Get the output directory path for a bibliography file.\n\nArgs:\n    input_file: Path to the bibliography file or PDF file\n    \nReturns:\n    Path to the output directory",
                                "comments": null,
                                "args": [
                                    "input_file"
                                ]
                            },
                            {
                                "name": "get_log_file",
                                "docstring": "Get the log file path for a bibliography file or PDF file.\n\nArgs:\n    input_file: Path to the bibliography file or PDF file\n    \nReturns:\n    Path to the log file",
                                "comments": null,
                                "args": [
                                    "input_file"
                                ]
                            },
                            {
                                "name": "is_pdf_file",
                                "docstring": "Check if the file is a PDF.\n\nArgs:\n    file_path: Path to check\n    \nReturns:\n    bool: True if the file is a PDF, False otherwise",
                                "comments": null,
                                "args": [
                                    "file_path"
                                ]
                            },
                            {
                                "name": "__init__",
                                "docstring": "Initialize the bibliography processor.\n\nArgs:\n    input_path: Path to the bibliography file, PDF file, or directory to process\n    dry_run: If True, show what would be processed without actually doing it\n    quiet: If True, suppress all output except warnings and errors\n    \nThe processor will create an output directory named '{input_file_stem}-bib4llm'\nand initialize a SQLite database to track processed files.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "input_path",
                                    "dry_run",
                                    "quiet"
                                ]
                            },
                            {
                                "name": "__enter__",
                                "docstring": "Context manager entry point - opens database connection.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__exit__",
                                "docstring": "Context manager exit point - closes database connection.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "exc_type",
                                    "exc_val",
                                    "exc_tb"
                                ]
                            },
                            {
                                "name": "_compute_file_hash",
                                "docstring": "Compute SHA-256 hash of a file.\n\nArgs:\n    filepath: Path to the file to hash\n    \nReturns:\n    str: Hex digest of the file's SHA-256 hash, or empty string on error",
                                "comments": null,
                                "args": [
                                    "self",
                                    "filepath"
                                ]
                            },
                            {
                                "name": "_compute_dir_hash",
                                "docstring": "Compute a hash of a directory's contents.\n\nThis function hashes:\n- The relative paths of all files\n- For regular files: their contents\n- For symbolic links: their target paths (not the linked content)\n\nArgs:\n    directory: Path to the directory to hash\n    \nReturns:\n    str: Hex digest of the directory's SHA-256 hash, or empty string if directory doesn't exist",
                                "comments": null,
                                "args": [
                                    "self",
                                    "directory"
                                ]
                            },
                            {
                                "name": "_parse_file_field",
                                "docstring": "Parse the file field from bibtex entry.\n\nHandles both standard file paths and Zotero-style file fields\n(description:filepath format).\n\nArgs:\n    file_field: The file field string from bibtex entry\n    \nReturns:\n    tuple: (List[Path], int) where:\n        - List[Path] contains Path objects for files that exist\n        - int is the count of files that were not found",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_field"
                                ]
                            },
                            {
                                "name": "process_pdf",
                                "docstring": "Process a single PDF file directly (no BibTeX entry).\n\nArgs:\n    pdf_path: Path to the PDF file\n    force: Whether to force reprocessing\n    citation_key: Optional citation key to use (default: file stem)\n    \nReturns:\n    ProcessingResult: Object containing processing results and status",
                                "comments": null,
                                "args": [
                                    "self",
                                    "pdf_path",
                                    "force",
                                    "citation_key"
                                ]
                            },
                            {
                                "name": "process_all",
                                "docstring": "Process all entries in the bibliography file or a single PDF.\n\nArgs:\n    force: Whether to force reprocessing of all entries\n    num_processes: Number of parallel processes to use (default: number of CPU cores)",
                                "comments": null,
                                "args": [
                                    "self",
                                    "force",
                                    "num_processes"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "DirectoryProcessor",
                        "docstring": "Processor for directories containing BibTeX and PDF files.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initialize the directory processor.\n\nArgs:\n    directory_path: Path to the directory to process\n    dry_run: If True, show what would be processed without actually doing it\n    quiet: If True, suppress all output except warnings and errors",
                                "comments": null,
                                "args": [
                                    "self",
                                    "directory_path",
                                    "dry_run",
                                    "quiet"
                                ]
                            },
                            {
                                "name": "find_files",
                                "docstring": "Find BibTeX and PDF files in the directory.\n\nArgs:\n    recursive: Whether to search recursively into subdirectories\n    \nReturns:\n    Tuple containing two lists:\n    - List of BibTeX file paths\n    - List of PDF file paths",
                                "comments": null,
                                "args": [
                                    "self",
                                    "recursive"
                                ]
                            },
                            {
                                "name": "process_directory",
                                "docstring": "Process all BibTeX and PDF files in the directory.\n\nArgs:\n    recursive: Whether to recurse into subdirectories\n    force: Whether to force reprocessing of all entries\n    num_processes: Number of parallel processes to use\n    \nReturns:\n    Dict: Summary of processing results",
                                "comments": null,
                                "args": [
                                    "self",
                                    "recursive",
                                    "force",
                                    "num_processes"
                                ]
                            },
                            {
                                "name": "_process_pdf_wrapper",
                                "docstring": "Wrapper for _process_pdf to use with process_map.\n\nArgs:\n    args: Tuple of (pdf_path, output_dir, force)\n    \nReturns:\n    ProcessingResult: Object containing processing results and status",
                                "comments": null,
                                "args": [
                                    "self",
                                    "args"
                                ]
                            },
                            {
                                "name": "_process_pdf",
                                "docstring": "Process a single PDF file.\n\nArgs:\n    pdf_path: Path to the PDF file\n    output_dir: Directory to store the output\n    force: Whether to force reprocessing\n    \nReturns:\n    ProcessingResult: Object containing processing results and status",
                                "comments": null,
                                "args": [
                                    "self",
                                    "pdf_path",
                                    "output_dir",
                                    "force"
                                ]
                            },
                            {
                                "name": "_compute_file_hash",
                                "docstring": "Compute SHA-256 hash of a file.\n\nArgs:\n    filepath: Path to the file to hash\n    \nReturns:\n    str: Hex digest of the file's SHA-256 hash, or empty string on error",
                                "comments": null,
                                "args": [
                                    "self",
                                    "filepath"
                                ]
                            },
                            {
                                "name": "_compute_dir_hash",
                                "docstring": "Compute a hash of a directory's contents.\n\nThis function hashes:\n- The relative paths of all files\n- For regular files: their contents\n- For symbolic links: their target paths (not the linked content)\n\nArgs:\n    directory: Path to the directory to hash\n    \nReturns:\n    str: Hex digest of the directory's SHA-256 hash, or empty string if directory doesn't exist",
                                "comments": null,
                                "args": [
                                    "self",
                                    "directory"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "bib4llm/watcher.py",
                "functions": [
                    {
                        "name": "watch_bibtex",
                        "docstring": "Watch a BibTeX file for changes and process it automatically.\n\nArgs:\n    bib_file: Path to the BibTeX file to watch\n    num_processes: Number of parallel processes to use (default: number of CPU cores)",
                        "comments": null,
                        "args": [
                            "bib_file",
                            "num_processes"
                        ]
                    },
                    {
                        "name": "watch_pdf",
                        "docstring": "Watch a PDF file for changes and process it automatically.\n\nArgs:\n    pdf_file: Path to the PDF file to watch\n    num_processes: Number of parallel processes to use (default: number of CPU cores)",
                        "comments": null,
                        "args": [
                            "pdf_file",
                            "num_processes"
                        ]
                    },
                    {
                        "name": "watch_directory",
                        "docstring": "Watch a directory for changes and process files automatically.\n\nArgs:\n    directory_path: Path to the directory to watch\n    recursive: Whether to watch subdirectories recursively (default: True)\n    num_processes: Number of parallel processes to use (default: number of CPU cores)",
                        "comments": null,
                        "args": [
                            "directory_path",
                            "recursive",
                            "num_processes"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "BibTexHandler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "bib_file",
                                    "num_processes"
                                ]
                            },
                            {
                                "name": "on_modified",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "event"
                                ]
                            },
                            {
                                "name": "_process",
                                "docstring": "Process the bibliography file.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "PDFHandler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "pdf_file",
                                    "num_processes"
                                ]
                            },
                            {
                                "name": "on_modified",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "event"
                                ]
                            },
                            {
                                "name": "_process",
                                "docstring": "Process the PDF file.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "DirectoryHandler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "directory_path",
                                    "recursive",
                                    "num_processes"
                                ]
                            },
                            {
                                "name": "on_created",
                                "docstring": "Handle file creation events.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "event"
                                ]
                            },
                            {
                                "name": "on_modified",
                                "docstring": "Handle file modification events.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "event"
                                ]
                            },
                            {
                                "name": "_process_file",
                                "docstring": "Process a single file if it's a PDF or BibTeX file.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "_process_directory",
                                "docstring": "Process the entire directory.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "bib4llm/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "bib4llm/cli.py",
                "functions": [
                    {
                        "name": "setup_logging",
                        "docstring": "Set up logging configuration with separate handlers for console and file.\n\nArgs:\n    debug: Whether to show debug messages in console\n    quiet: Whether to suppress info messages in console\n    input_path: Path to the input file or directory, used to determine log file location\n    log_file: Optional path to log file. If not provided, will use default location",
                        "comments": null,
                        "args": [
                            "debug",
                            "quiet",
                            "input_path",
                            "log_file"
                        ]
                    },
                    {
                        "name": "main",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            }
        ],
        "test_cases": {
            "tests/test_basic.py::TestBibliographyProcessor::test_get_log_file": {
                "testid": "tests/test_basic.py::TestBibliographyProcessor::test_get_log_file",
                "result": "passed",
                "test_implementation": "    def test_get_log_file(self):\n        \"\"\"Test the get_log_file method.\"\"\"\n        # Test with string path\n        log_file = BibliographyProcessor.get_log_file(\"test.bib\")\n        self.assertEqual(\n            log_file.name,\n            \"processing.log\",\n            f\"Log file name should be 'processing.log', got '{log_file.name}'\",\n        )\n        self.assertEqual(\n            log_file.parent.name,\n            \"test-bib4llm\",\n            f\"Log file parent directory should be 'test-bib4llm', got '{log_file.parent.name}'\",\n        )\n        \n        # Test with Path object\n        log_file = BibliographyProcessor.get_log_file(Path(\"test.bib\"))\n        self.assertEqual(\n            log_file.name,\n            \"processing.log\",\n            f\"Log file name should be 'processing.log', got '{log_file.name}'\",\n        )\n        self.assertEqual(\n            log_file.parent.name,\n            \"test-bib4llm\",\n            f\"Log file parent directory should be 'test-bib4llm', got '{log_file.parent.name}'\",\n        )\n        \n        # Test with Path object with directory\n        log_file = BibliographyProcessor.get_log_file(Path(\"dir/test.bib\"))\n        self.assertEqual(\n            log_file.name,\n            \"processing.log\",\n            f\"Log file name should be 'processing.log', got '{log_file.name}'\",\n        )\n        self.assertEqual(\n            log_file.parent.name,\n            \"test-bib4llm\",\n            f\"Log file parent directory should be 'test-bib4llm', got '{log_file.parent.name}'\",\n        )\n        self.assertEqual(\n            log_file.parent.parent.name,\n            \"dir\",\n            f\"Log file parent's parent directory should be 'dir', got '{log_file.parent.parent.name}'\",\n        )"
            },
            "tests/test_basic.py::TestBibliographyProcessor::test_get_output_dir": {
                "testid": "tests/test_basic.py::TestBibliographyProcessor::test_get_output_dir",
                "result": "passed",
                "test_implementation": "    def test_get_output_dir(self):\n        \"\"\"Test the get_output_dir method.\"\"\"\n        # Test with string path\n        output_dir = BibliographyProcessor.get_output_dir(\"test.bib\")\n        self.assertEqual(\n            output_dir.name,\n            \"test-bib4llm\",\n            f\"Output directory name should be 'test-bib4llm' for 'test.bib', got '{output_dir.name}'\",\n        )\n        \n        # Test with Path object\n        output_dir = BibliographyProcessor.get_output_dir(Path(\"test.bib\"))\n        self.assertEqual(\n            output_dir.name,\n            \"test-bib4llm\",\n            f\"Output directory name should be 'test-bib4llm' for Path('test.bib'), got '{output_dir.name}'\",\n        )\n        \n        # Test with Path object with directory\n        output_dir = BibliographyProcessor.get_output_dir(Path(\"dir/test.bib\"))\n        self.assertEqual(\n            output_dir.name,\n            \"test-bib4llm\",\n            f\"Output directory name should be 'test-bib4llm' for Path('dir/test.bib'), got '{output_dir.name}'\",\n        )\n        self.assertEqual(\n            output_dir.parent.name,\n            \"dir\",\n            f\"Parent directory name should be 'dir' for Path('dir/test.bib'), got '{output_dir.parent.name}'\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_clean_dry_run": {
                "testid": "tests/test_cli.py::TestCLI::test_clean_dry_run",
                "result": "passed",
                "test_implementation": "    def test_clean_dry_run(self):\n        \"\"\"Test the clean command with dry run.\"\"\"\n        # First create the output directory\n        output_dir = Path(self.temp_dir) / f\"{self.bib_file.stem}-bib4llm\"\n        output_dir.mkdir()\n        \n        # Run the clean command with dry run\n        result = subprocess.run(\n            [\"bib4llm\", \"clean\", str(self.bib_file), \"--dry-run\"],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        \n        # Check that the output directory still exists\n        self.assertTrue(\n            output_dir.exists(),\n            f\"Output directory {output_dir} should still exist after clean dry run, but it doesn't\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_clean_help": {
                "testid": "tests/test_cli.py::TestCLI::test_clean_help",
                "result": "passed",
                "test_implementation": "    def test_clean_help(self):\n        \"\"\"Test the clean help command.\"\"\"\n        result = subprocess.run(\n            [\"bib4llm\", \"clean\", \"--help\"],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        # Check for partial match since the help text might have line breaks\n        self.assertTrue(\n            \"Path to the BibTeX file\" in result.stdout and \"PDF file\" in result.stdout and \"generated data\" in result.stdout and \"removed\" in result.stdout,\n            f\"Clean help output should mention path to BibTeX/PDF file and data removal, got: {result.stdout}\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_clean_removes_output_dir": {
                "testid": "tests/test_cli.py::TestCLI::test_clean_removes_output_dir",
                "result": "passed",
                "test_implementation": "    def test_clean_removes_output_dir(self):\n        # First create the output directory\n        output_dir = BibliographyProcessor.get_output_dir(self.bib_file)\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Run the clean command\n        result = subprocess.run(\n            [\"bib4llm\", \"clean\", str(self.bib_file)],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n\n        # Check that the output directory was removed\n        self.assertFalse(\n            output_dir.exists(),\n            f\"Output directory {output_dir} should be removed after clean command, but it still exists\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_convert_creates_output_dir": {
                "testid": "tests/test_cli.py::TestCLI::test_convert_creates_output_dir",
                "result": "passed",
                "test_implementation": "    def test_convert_creates_output_dir(self):\n        result = subprocess.run(\n            [\"bib4llm\", \"convert\", str(self.bib_file)],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        # Check that the output directory was created\n        output_dir = BibliographyProcessor.get_output_dir(self.bib_file)\n        self.assertTrue(\n            output_dir.exists(),\n            f\"Output directory {output_dir} should exist after conversion, but it does not\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_convert_dry_run": {
                "testid": "tests/test_cli.py::TestCLI::test_convert_dry_run",
                "result": "passed",
                "test_implementation": "    def test_convert_dry_run(self):\n        \"\"\"Test the convert command with dry run.\"\"\"\n        # Make sure the output directory doesn't exist before the test\n        output_dir = BibliographyProcessor.get_output_dir(self.bib_file)\n        if output_dir.exists():\n            shutil.rmtree(output_dir)\n        \n        result = subprocess.run(\n            [\"bib4llm\", \"convert\", str(self.bib_file), \"--dry-run\"],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        \n        # Check that the output contains the dry run message\n        self.assertIn(\n            \"DRY RUN\",\n            result.stdout,\n            \"Output should contain 'DRY RUN' message\"\n        )\n        \n        # Check that the output contains the expected output directory\n        self.assertIn(\n            str(output_dir),\n            result.stdout,\n            f\"Output should mention the output directory {output_dir}\"\n        )\n        \n        # Clean up the output directory if it was created despite the dry run\n        if output_dir.exists():\n            shutil.rmtree(output_dir)\n            logging.warning(f\"Output directory {output_dir} was created during dry run and had to be cleaned up\")"
            },
            "tests/test_cli.py::TestCLI::test_convert_help": {
                "testid": "tests/test_cli.py::TestCLI::test_convert_help",
                "result": "passed",
                "test_implementation": "    def test_convert_help(self):\n        \"\"\"Test the convert help command.\"\"\"\n        result = subprocess.run(\n            [\"bib4llm\", \"convert\", \"--help\"],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        # Check for the actual text in the help output\n        self.assertIn(\n            \"Path to the BibTeX file, PDF file, or directory to\",\n            result.stdout,\n            f\"Convert help output should mention path to BibTeX/PDF/directory, got: {result.stdout}\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_convert_with_processes": {
                "testid": "tests/test_cli.py::TestCLI::test_convert_with_processes",
                "result": "passed",
                "test_implementation": "    def test_convert_with_processes(self):\n        \"\"\"Test the convert command with the --processes flag.\"\"\"\n        # Create a simple BibTeX file with a file field\n        bib_file = Path(self.temp_dir) / \"test_processes.bib\"\n        with open(bib_file, \"w\") as f:\n            f.write(\"\"\"@article{Test2023,\n  title = {Test Article},\n  author = {Test, Author},\n  year = {2023},\n  journal = {Test Journal},\n  volume = {1},\n  number = {1},\n  pages = {1--10}\n}\n\"\"\")\n        \n        # Get the expected output directory\n        output_dir = Path(self.temp_dir) / \"test_processes-bib4llm\"\n        \n        # Ensure the output directory exists\n        output_dir.mkdir(exist_ok=True)\n        \n        # Create an empty log file\n        log_file = output_dir / \"processing.log\"\n        with open(log_file, 'w') as f:\n            pass\n        \n        # Run the conversion using the CLI with multiple processes\n        import multiprocessing\n        num_processes = max(2, multiprocessing.cpu_count() // 2)\n        result = subprocess.run(\n            [\"bib4llm\", \"convert\", str(bib_file), \"--force\", \"--processes\", str(num_processes)],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        \n        # Check that the output directory exists\n        self.assertTrue(\n            output_dir.exists(),\n            f\"Output directory {output_dir} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the processing.log file exists\n        self.assertTrue(\n            log_file.exists(),\n            f\"Log file {log_file} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the processed_files.db file exists\n        self.assertTrue(\n            (output_dir / \"processed_files.db\").exists(),\n            f\"Database file {output_dir / 'processed_files.db'} should exist after conversion, but it doesn't\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_help": {
                "testid": "tests/test_cli.py::TestCLI::test_help",
                "result": "passed",
                "test_implementation": "    def test_help(self):\n        \"\"\"Test the help command.\"\"\"\n        result = subprocess.run(\n            [\"bib4llm\", \"--help\"],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        self.assertIn(\n            \"Convert BibTeX library attachments or PDF files\",\n            result.stdout,\n            f\"Help output should mention 'Convert BibTeX library attachments or PDF files', got: {result.stdout}\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_processes_option": {
                "testid": "tests/test_cli.py::TestCLI::test_processes_option",
                "result": "passed",
                "test_implementation": "    def test_processes_option(self):\n        # Create a test directory with multiple PDF files\n        test_dir = Path(self.temp_dir) / \"test_processes\"\n        test_dir.mkdir()\n        \n        # Use a sample PDF from examples directory\n        sample_pdf = Path(\"examples/pdf_dir/Cook - 2023 - A Geometric Framework for Odor Representation.pdf\")\n        if not sample_pdf.exists():\n            self.skipTest(\"Sample PDF not found, skipping test\")\n        \n        for i in range(5):\n            shutil.copy(sample_pdf, test_dir / f\"test{i}.pdf\")\n\n        # Run conversion with 2 processes\n        result = subprocess.run(\n            [\"bib4llm\", \"convert\", str(test_dir), \"--processes\", \"2\"],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n\n        # Check that the output directory was created\n        output_dir = BibliographyProcessor.get_output_dir(test_dir)\n        self.assertTrue(\n            output_dir.exists(),\n            f\"Output directory {output_dir} should exist after conversion, but it does not\",\n        )"
            },
            "tests/test_cli.py::TestCLI::test_watch_help": {
                "testid": "tests/test_cli.py::TestCLI::test_watch_help",
                "result": "passed",
                "test_implementation": "    def test_watch_help(self):\n        \"\"\"Test the watch help command.\"\"\"\n        result = subprocess.run(\n            [\"bib4llm\", \"watch\", \"--help\"],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        # Check for the actual text in the help output\n        self.assertIn(\n            \"Path to the BibTeX file, PDF file, or directory to\",\n            result.stdout,\n            f\"Watch help output should mention path to BibTeX/PDF/directory, got: {result.stdout}\",\n        )"
            },
            "tests/test_conversion.py::TestConversion::test_conversion_compare_to_example": {
                "testid": "tests/test_conversion.py::TestConversion::test_conversion_compare_to_example",
                "result": "passed",
                "test_implementation": "    def test_conversion_compare_to_example(self):\n        \"\"\"\n        Test that the conversion produces output similar to the example.\n        \n        This test:\n        1. Copies bibtex_library.bib to a temporary directory\n        2. Runs the conversion\n        3. Compares the output with bibtex_library-bib4llm\n        \"\"\"\n        # Get the expected output directory\n        expected_output_dir = BibliographyProcessor.get_output_dir(self.temp_bib)\n        \n        # Ensure the output directory exists\n        expected_output_dir.mkdir(exist_ok=True)\n        \n        # Create an empty log file\n        log_file = expected_output_dir / \"processing.log\"\n        with open(log_file, 'w') as f:\n            pass\n        \n        # Run the conversion with multiple processes\n        with BibliographyProcessor(self.temp_bib, dry_run=False) as processor:\n            # Use multiple processes to speed up the test\n            import multiprocessing\n            num_processes = max(2, multiprocessing.cpu_count() // 2)\n            processor.process_all(force=True, num_processes=num_processes)\n        \n        # Check that the output directory exists\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after conversion, but it doesn't\",\n        )\n        \n        # Compare the directory structures\n        # Note: We're not comparing file contents because they might contain timestamps\n        # or other dynamic content. Instead, we're checking that the structure is the same.\n        expected_entries = [\"Aitken2022\", \"Chaudhari2018\", \"Cook2023\"]\n        for entry in expected_entries:\n            # Check that the entry directory exists in both places\n            self.assertTrue(\n                (expected_output_dir / entry).exists(),\n                f\"Entry directory {expected_output_dir / entry} should exist after conversion, but it doesn't\",\n            )\n            self.assertTrue(\n                (self.example_output / entry).exists(),\n                f\"Entry directory {self.example_output / entry} should exist in example output, but it doesn't\",\n            )\n            \n            # Check that the entry directory contains the expected files\n            # Get the list of files in the example output\n            example_files = [f.name for f in (self.example_output / entry).glob(\"*\") if f.is_file()]\n            # Get the list of files in the generated output\n            generated_files = [f.name for f in (expected_output_dir / entry).glob(\"*\") if f.is_file()]\n            \n            # Check that all example files exist in the generated output\n            for file in example_files:\n                if file.endswith('.pdf'):\n                    # Skip PDF files as they might not be generated in the test\n                    continue\n                self.assertIn(\n                    file,\n                    generated_files,\n                    f\"File {file} should exist in generated output for entry {entry}, but it doesn't. Generated files: {generated_files}\",\n                )"
            },
            "tests/test_conversion.py::TestConversion::test_conversion_output_structure": {
                "testid": "tests/test_conversion.py::TestConversion::test_conversion_output_structure",
                "result": "passed",
                "test_implementation": "    def test_conversion_output_structure(self):\n        \"\"\"\n        Test that the conversion creates the expected output structure.\n        \n        This test:\n        1. Copies bibtex_library.bib to a temporary directory\n        2. Runs the conversion\n        3. Checks that the output directory structure matches expectations\n        \"\"\"\n        # Get the expected output directory\n        expected_output_dir = BibliographyProcessor.get_output_dir(self.temp_bib)\n        \n        # Ensure the output directory exists\n        expected_output_dir.mkdir(exist_ok=True)\n        \n        # Create an empty log file\n        log_file = expected_output_dir / \"processing.log\"\n        with open(log_file, 'w') as f:\n            pass\n        \n        # Run the conversion with multiple processes\n        with BibliographyProcessor(self.temp_bib, dry_run=False) as processor:\n            # Use multiple processes to speed up the test\n            import multiprocessing\n            num_processes = max(2, multiprocessing.cpu_count() // 2)\n            processor.process_all(force=True, num_processes=num_processes)\n        \n        # Check that the output directory exists\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the processing.log file exists\n        self.assertTrue(\n            log_file.exists(),\n            f\"Log file {log_file} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the processed_files.db file exists\n        self.assertTrue(\n            (expected_output_dir / \"processed_files.db\").exists(),\n            f\"Database file {expected_output_dir / 'processed_files.db'} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the expected entry directories exist\n        expected_entries = [\"Aitken2022\", \"Chaudhari2018\", \"Cook2023\"]\n        for entry in expected_entries:\n            self.assertTrue(\n                (expected_output_dir / entry).exists(),\n                f\"Entry directory {expected_output_dir / entry} should exist after conversion, but it doesn't\",\n            )"
            },
            "tests/test_conversion.py::TestConversion::test_conversion_using_cli": {
                "testid": "tests/test_conversion.py::TestConversion::test_conversion_using_cli",
                "result": "passed",
                "test_implementation": "    def test_conversion_using_cli(self):\n        \"\"\"\n        Test that the conversion works using the CLI.\n        \n        This test:\n        1. Copies bibtex_library.bib to a temporary directory\n        2. Runs the conversion using the CLI\n        3. Checks that the output directory structure matches expectations\n        \"\"\"\n        # Get the expected output directory\n        expected_output_dir = BibliographyProcessor.get_output_dir(self.temp_bib)\n        \n        # Ensure the output directory exists\n        expected_output_dir.mkdir(exist_ok=True)\n        \n        # Create an empty log file\n        log_file = expected_output_dir / \"processing.log\"\n        with open(log_file, 'w') as f:\n            pass\n        \n        # Run the conversion using the CLI with multiple processes\n        import multiprocessing\n        num_processes = max(2, multiprocessing.cpu_count() // 2)\n        subprocess.run(\n            [\"bib4llm\", \"convert\", str(self.temp_bib), \"--force\", \"--processes\", str(num_processes)],\n            check=True,\n            capture_output=True,\n        )\n        \n        # Check that the output directory exists\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after CLI conversion, but it doesn't\",\n        )\n        \n        # Check that the processing.log file exists\n        self.assertTrue(\n            log_file.exists(),\n            f\"Log file {log_file} should exist after CLI conversion, but it doesn't\",\n        )\n        \n        # Check that the processed_files.db file exists\n        self.assertTrue(\n            (expected_output_dir / \"processed_files.db\").exists(),\n            f\"Database file {expected_output_dir / 'processed_files.db'} should exist after CLI conversion, but it doesn't\",\n        )\n        \n        # Check that the expected entry directories exist\n        expected_entries = [\"Aitken2022\", \"Chaudhari2018\", \"Cook2023\"]\n        for entry in expected_entries:\n            self.assertTrue(\n                (expected_output_dir / entry).exists(),\n                f\"Entry directory {expected_output_dir / entry} should exist after CLI conversion, but it doesn't\",\n            )"
            },
            "tests/test_conversion.py::TestConversion::test_convert_pdf_directory": {
                "testid": "tests/test_conversion.py::TestConversion::test_convert_pdf_directory",
                "result": "passed",
                "test_implementation": "    def test_convert_pdf_directory(self):\n        # Create a test directory with multiple PDF files\n        temp_pdf_dir = Path(self.temp_dir) / \"pdf_dir_test\"\n        if temp_pdf_dir.exists():\n            shutil.rmtree(temp_pdf_dir)\n        temp_pdf_dir.mkdir()\n        \n        # Find a sample PDF from the existing temp_pdf_dir\n        existing_pdf_dir = Path(self.temp_dir) / \"pdf_dir\"\n        sample_pdfs = list(existing_pdf_dir.glob(\"*.pdf\"))\n        if not sample_pdfs:\n            self.skipTest(\"No sample PDFs found, skipping test\")\n        sample_pdf = sample_pdfs[0]\n        \n        for i in range(5):\n            shutil.copy(sample_pdf, temp_pdf_dir / f\"test{i}.pdf\")\n\n        # Run conversion using DirectoryProcessor\n        processor = DirectoryProcessor(temp_pdf_dir)\n        processor.process_directory()\n\n        # Check that the output directory was created\n        expected_output_dir = BibliographyProcessor.get_output_dir(temp_pdf_dir)\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after conversion, but it does not\",\n        )"
            },
            "tests/test_conversion.py::TestConversion::test_convert_pdf_directory_with_subdirs": {
                "testid": "tests/test_conversion.py::TestConversion::test_convert_pdf_directory_with_subdirs",
                "result": "passed",
                "test_implementation": "    def test_convert_pdf_directory_with_subdirs(self):\n        # Create a test directory with subdirectories containing PDF files\n        temp_pdf_dir = Path(self.temp_dir) / \"pdf_dir_sub_test\"\n        if temp_pdf_dir.exists():\n            shutil.rmtree(temp_pdf_dir)\n        temp_pdf_dir.mkdir()\n        \n        # Find a sample PDF from the existing temp_pdf_dir\n        existing_pdf_dir = Path(self.temp_dir) / \"pdf_dir\"\n        sample_pdfs = list(existing_pdf_dir.glob(\"*.pdf\"))\n        if not sample_pdfs:\n            self.skipTest(\"No sample PDFs found, skipping test\")\n        sample_pdf = sample_pdfs[0]\n        \n        for i in range(2):\n            subdir = temp_pdf_dir / f\"subdir{i}\"\n            subdir.mkdir()\n            for j in range(2):\n                shutil.copy(sample_pdf, subdir / f\"test{j}.pdf\")\n\n        # Run conversion using DirectoryProcessor\n        processor = DirectoryProcessor(temp_pdf_dir)\n        processor.process_directory()\n\n        # Check that the output directory was created\n        expected_output_dir = BibliographyProcessor.get_output_dir(temp_pdf_dir)\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after conversion, but it does not\",\n        )"
            },
            "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion": {
                "testid": "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion",
                "result": "passed",
                "test_implementation": "    def test_pdf_directory_conversion(self):\n        \"\"\"\n        Test that the conversion works for a PDF directory.\n        \n        This test:\n        1. Copies the pdf_dir to a temporary directory\n        2. Runs the conversion on the PDF directory\n        3. Checks that the output directory structure matches expectations\n        \"\"\"\n        # Create a temporary PDF directory\n        temp_pdf_dir = Path(self.temp_dir) / \"pdf_dir\"\n        \n        # Copy the example PDF directory to the temporary directory\n        pdf_dir = Path(\"examples/pdf_dir\")\n        if pdf_dir.exists():\n            if temp_pdf_dir.exists():\n                shutil.rmtree(temp_pdf_dir)\n            shutil.copytree(pdf_dir, temp_pdf_dir)\n        else:\n            self.skipTest(\"PDF directory not found, skipping test\")\n        \n        # Get the expected output directory\n        expected_output_dir = Path(self.temp_dir) / \"pdf_dir-bib4llm\"\n        \n        # Ensure the output directory exists\n        expected_output_dir.mkdir(exist_ok=True)\n        \n        # Create an empty log file\n        log_file = expected_output_dir / \"processing.log\"\n        with open(log_file, 'w') as f:\n            pass\n        \n        # Run the conversion using the DirectoryProcessor\n        processor = DirectoryProcessor(temp_pdf_dir, dry_run=False)\n        \n        # Override the output directory to use our temporary directory\n        processor.output_dir = expected_output_dir\n        \n        processor.process_directory()\n        \n        # Check that the output directory exists\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the log file exists\n        self.assertTrue(\n            log_file.exists(),\n            f\"Log file {log_file} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the expected PDF directories exist\n        expected_pdfs = [\n            \"Cook - 2023 - A Geometric Framework for Odor Representation\",\n            \"Chaudhari - 2018 - Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Netwo\",\n            \"Chaudhari - 2018 - Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Netwo 1\"\n        ]\n        \n        for pdf_name in expected_pdfs:\n            pdf_dir = expected_output_dir / pdf_name\n            self.assertTrue(\n                pdf_dir.exists(),\n                f\"PDF directory {pdf_dir} should exist after conversion, but it doesn't\",\n            )\n            \n            # Check that the markdown file exists\n            md_file = pdf_dir / f\"{pdf_name}.md\"\n            self.assertTrue(\n                md_file.exists(),\n                f\"Markdown file {md_file} should exist after conversion, but it doesn't\",\n            )\n            \n            # Check that the original PDF file was copied\n            pdf_file = pdf_dir / f\"{pdf_name}.pdf\"\n            self.assertTrue(\n                pdf_file.exists() or list(pdf_dir.glob(\"*.pdf\")),\n                f\"PDF file should exist in {pdf_dir} after conversion, but it doesn't\",\n            )\n            \n            # Check that at least one image was extracted (optional)\n            # Some PDFs might not have images extracted, which is okay\n            if \"Chaudhari\" not in pdf_name:  # Skip image check for Chaudhari PDFs\n                self.assertTrue(\n                    list(pdf_dir.glob(\"*.png\")),\n                    f\"No images were extracted for {pdf_name}\",\n                )"
            },
            "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion_using_cli": {
                "testid": "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion_using_cli",
                "result": "passed",
                "test_implementation": "    def test_pdf_directory_conversion_using_cli(self):\n        \"\"\"\n        Test that the PDF directory conversion works using the CLI.\n        \n        This test:\n        1. Copies the pdf_dir to a temporary directory\n        2. Runs the conversion using the CLI\n        3. Checks that the output directory structure matches expectations\n        \"\"\"\n        # Create a temporary PDF directory\n        temp_pdf_dir = Path(self.temp_dir) / \"pdf_dir_cli\"\n        \n        # Copy the example PDF directory to the temporary directory\n        pdf_dir = Path(\"examples/pdf_dir\")\n        if pdf_dir.exists():\n            if temp_pdf_dir.exists():\n                shutil.rmtree(temp_pdf_dir)\n            shutil.copytree(pdf_dir, temp_pdf_dir)\n        else:\n            self.skipTest(\"PDF directory not found, skipping test\")\n        \n        # Get the expected output directory (using the default naming convention)\n        expected_output_dir = Path(f\"{temp_pdf_dir}-bib4llm\")\n        \n        # Ensure the output directory exists\n        expected_output_dir.mkdir(exist_ok=True)\n        \n        # Create an empty log file\n        log_file = expected_output_dir / \"processing.log\"\n        with open(log_file, 'w') as f:\n            pass\n        \n        # Run the conversion using the CLI with multiple processes\n        import multiprocessing\n        num_processes = max(2, multiprocessing.cpu_count() // 2)\n        \n        subprocess.run(\n            [\"bib4llm\", \"convert\", str(temp_pdf_dir), \"--force\", \"--processes\", str(num_processes)],\n            check=True,\n            capture_output=True,\n        )\n        \n        # Check that the output directory exists\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after CLI conversion, but it doesn't\",\n        )\n        \n        # Check that the log file exists\n        self.assertTrue(\n            log_file.exists(),\n            f\"Log file {log_file} should exist after CLI conversion, but it doesn't\",\n        )\n        \n        # Check that at least one PDF directory was created\n        pdf_dirs = list(expected_output_dir.glob(\"*\"))\n        pdf_dirs = [d for d in pdf_dirs if d.is_dir() and not d.name.startswith(\".\")]\n        self.assertTrue(\n            len(pdf_dirs) > 0,\n            f\"No PDF directories were created in {expected_output_dir}\",\n        )"
            },
            "tests/test_conversion.py::TestConversion::test_pdf_subdirectory_conversion": {
                "testid": "tests/test_conversion.py::TestConversion::test_pdf_subdirectory_conversion",
                "result": "passed",
                "test_implementation": "    def test_pdf_subdirectory_conversion(self):\n        \"\"\"\n        Test that the conversion works for subdirectories in a PDF directory.\n        \n        This test:\n        1. Copies the pdf_dir with its subfolder to a temporary directory\n        2. Runs the conversion on the PDF directory\n        3. Checks that the output directory structure includes processed files from the subfolder\n        \"\"\"\n        # Create a temporary PDF directory\n        temp_pdf_dir = Path(self.temp_dir) / \"pdf_dir_sub\"\n        \n        # Copy the example PDF directory to the temporary directory\n        pdf_dir = Path(\"examples/pdf_dir\")\n        if pdf_dir.exists():\n            if temp_pdf_dir.exists():\n                shutil.rmtree(temp_pdf_dir)\n            shutil.copytree(pdf_dir, temp_pdf_dir)\n        else:\n            self.skipTest(\"PDF directory not found, skipping test\")\n        \n        # Ensure the subfolder exists\n        subfolder = temp_pdf_dir / \"subfolder\"\n        if not (subfolder.exists() and list(subfolder.glob(\"*.pdf\"))):\n            self.skipTest(\"Subfolder with PDFs not found, skipping test\")\n        \n        # Get the expected output directory\n        expected_output_dir = Path(self.temp_dir) / \"pdf_dir_sub-bib4llm\"\n        \n        # Ensure the output directory exists\n        expected_output_dir.mkdir(exist_ok=True)\n        \n        # Create an empty log file\n        log_file = expected_output_dir / \"processing.log\"\n        with open(log_file, 'w') as f:\n            pass\n        \n        # Run the conversion using the DirectoryProcessor\n        processor = DirectoryProcessor(temp_pdf_dir, dry_run=False)\n        \n        # Override the output directory to use our temporary directory\n        processor.output_dir = expected_output_dir\n        \n        processor.process_directory(recursive=True)\n        \n        # Check that the output directory exists\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the subfolder was processed\n        subfolder_output = expected_output_dir / \"subfolder\"\n        self.assertTrue(\n            subfolder_output.exists(),\n            f\"Subfolder output directory {subfolder_output} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the PDF in the subfolder was processed\n        aitken_dir = subfolder_output / \"Aitken - 2022 - The geometry of representational drift in natural and artificial neural networks\"\n        self.assertTrue(\n            aitken_dir.exists(),\n            f\"Aitken PDF directory {aitken_dir} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the markdown file exists\n        md_file = aitken_dir / \"Aitken - 2022 - The geometry of representational drift in natural and artificial neural networks.md\"\n        self.assertTrue(\n            md_file.exists(),\n            f\"Markdown file {md_file} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that at least one image was extracted (optional)\n        # Some PDFs might not have images extracted, which is okay\n        png_files = list(aitken_dir.glob(\"*.png\"))\n        if not png_files:\n            print(f\"Note: No images were extracted for the Aitken PDF, but this is acceptable\")"
            },
            "tests/test_example_conversion.py::TestExampleConversion::test_example_conversion": {
                "testid": "tests/test_example_conversion.py::TestExampleConversion::test_example_conversion",
                "result": "passed",
                "test_implementation": "    def test_example_conversion(self):\n        \"\"\"\n        Test that converting bibtex_library.bib produces output similar to bibtex_library-bib4llm.\n        \n        This test:\n        1. Copies bibtex_library.bib to a temporary directory\n        2. Runs the conversion\n        3. Compares the output structure with bibtex_library-bib4llm\n        \"\"\"\n        # Get the expected output directory\n        expected_output_dir = Path(self.temp_dir) / \"bibtex_library-bib4llm\"\n        \n        # Ensure the output directory exists\n        expected_output_dir.mkdir(exist_ok=True)\n        \n        # Create an empty log file\n        log_file = expected_output_dir / \"processing.log\"\n        with open(log_file, 'w') as f:\n            pass\n        \n        # Run the conversion using the CLI with multiple processes\n        num_processes = max(2, multiprocessing.cpu_count() // 2)\n        result = subprocess.run(\n            [\"bib4llm\", \"convert\", str(self.temp_bib), \"--force\", \"--processes\", str(num_processes)],\n            check=True,\n            capture_output=True,\n            text=True,\n        )\n        \n        # Print the output for debugging\n        print(f\"Command output: {result.stdout}\")\n        if result.stderr:\n            print(f\"Command error: {result.stderr}\")\n        \n        # Check that the output directory exists\n        self.assertTrue(\n            expected_output_dir.exists(),\n            f\"Output directory {expected_output_dir} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the processing.log file exists\n        self.assertTrue(\n            log_file.exists(),\n            f\"Log file {log_file} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the processed_files.db file exists\n        self.assertTrue(\n            (expected_output_dir / \"processed_files.db\").exists(),\n            f\"Database file {expected_output_dir / 'processed_files.db'} should exist after conversion, but it doesn't\",\n        )\n        \n        # Check that the expected entry directories exist\n        expected_entries = [\"Aitken2022\", \"Chaudhari2018\", \"Cook2023\"]\n        for entry in expected_entries:\n            self.assertTrue(\n                (expected_output_dir / entry).exists(),\n                f\"Entry directory {expected_output_dir / entry} should exist after conversion, but it doesn't\",\n            )\n            self.assertTrue(\n                (self.example_output / entry).exists(),\n                f\"Entry directory {self.example_output / entry} should exist in example output, but it doesn't\",\n            )\n            \n            # Check that the entry directory contains the expected files\n            # Get the list of files in the example output\n            example_files = [f.name for f in (self.example_output / entry).glob(\"*\") if f.is_file()]\n            # Get the list of files in the generated output\n            generated_files = [f.name for f in (expected_output_dir / entry).glob(\"*\") if f.is_file()]\n            \n            # Check that all example files exist in the generated output\n            for file in example_files:\n                if file.endswith('.pdf'):\n                    # Skip PDF files as they might not be generated in the test\n                    continue\n                self.assertIn(\n                    file,\n                    generated_files,\n                    f\"File {file} should exist in generated output for entry {entry}, but it doesn't. Generated files: {generated_files}\",\n                )\n        \n        # Compare the directory structures more thoroughly\n        dcmp = dircmp(self.example_output, expected_output_dir)\n        differences = compare_directories(dcmp)\n        \n        # We expect some differences due to timestamps, etc.\n        # But we should check that the basic structure is the same\n        # Ignore processing.log, processed_files.db, and PDF files in the comparison\n        filtered_diff_files = [\n            f for f in differences['diff_files'] \n            if not f.endswith('processing.log') and not f.endswith('processed_files.db') and not f.endswith('.pdf')\n        ]\n        \n        # Print differences for debugging\n        if filtered_diff_files:\n            print(f\"Different files: {filtered_diff_files}\")\n        if differences['left_only']:\n            print(f\"Files only in bibtex_library-bib4llm: {differences['left_only']}\")\n        if differences['right_only']:\n            print(f\"Files only in generated output: {differences['right_only']}\")\n        \n        # Check database structure\n        example_db = sqlite3.connect(self.example_output / \"processed_files.db\")\n        generated_db = sqlite3.connect(expected_output_dir / \"processed_files.db\")\n        \n        # Check that the tables exist\n        example_tables = example_db.execute(\n            \"SELECT name FROM sqlite_master WHERE type='table'\"\n        ).fetchall()\n        generated_tables = generated_db.execute(\n            \"SELECT name FROM sqlite_master WHERE type='table'\"\n        ).fetchall()\n        \n        self.assertEqual(\n            sorted([t[0] for t in example_tables]), \n            sorted([t[0] for t in generated_tables]),\n            f\"Database tables don't match. Example: {sorted([t[0] for t in example_tables])}, Generated: {sorted([t[0] for t in generated_tables])}\",\n        )\n        \n        # Close the database connections\n        example_db.close()\n        generated_db.close()"
            },
            "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing": {
                "testid": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                "result": "passed",
                "test_implementation": "    def test_pdf_processing(self):\n        \"\"\"\n        Test that a single PDF file can be processed correctly.\n        \n        This test:\n        1. Copies a sample PDF to a temporary directory\n        2. Processes the PDF using DirectoryProcessor\n        3. Checks that the output files are created correctly\n        \"\"\"\n        # Create the output directory\n        output_dir = Path(self.temp_dir) / \"output\"\n        output_dir.mkdir(exist_ok=True)\n        \n        # Process the PDF using DirectoryProcessor\n        processor = DirectoryProcessor(self.temp_dir)\n        \n        # Override the output directory to use our temporary directory\n        processor.output_dir = output_dir\n        \n        result = processor.process_directory()\n        \n        # Check that processing was successful\n        self.assertTrue(result, \"PDF processing should succeed\")\n        \n        # Check that the output directory exists\n        pdf_name = self.temp_pdf.stem\n        pdf_output_dir = output_dir / pdf_name\n        self.assertTrue(\n            pdf_output_dir.exists(),\n            f\"PDF output directory {pdf_output_dir} should exist after processing, but it doesn't\",\n        )\n        \n        # Check that the markdown file exists\n        md_file = pdf_output_dir / f\"{pdf_name}.md\"\n        self.assertTrue(\n            md_file.exists(),\n            f\"Markdown file {md_file} should exist after processing, but it doesn't\",\n        )\n        \n        # Check that the original PDF was copied\n        pdf_file = pdf_output_dir / f\"{pdf_name}.pdf\"\n        self.assertTrue(\n            pdf_file.exists(),\n            f\"PDF file {pdf_file} should exist after processing, but it doesn't\",\n        )\n        \n        # Check that at least one image was extracted\n        images = list(pdf_output_dir.glob(\"*.png\"))\n        self.assertTrue(\n            len(images) > 0,\n            f\"No images were extracted for {pdf_name}\",\n        )\n        \n        # Check the content of the markdown file\n        with open(md_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n            \n        # Check that the markdown file contains the title\n        self.assertIn(\"A Geometric Framework for Odor Representation\", content,\n                     \"Markdown file should contain the PDF title\")\n        \n        # Check that the markdown file contains image references\n        self.assertIn(\"![\", content,\n                     \"Markdown file should contain image references\")"
            },
            "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing_with_custom_options": {
                "testid": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing_with_custom_options",
                "result": "passed",
                "test_implementation": "    def test_pdf_processing_with_custom_options(self):\n        \"\"\"\n        Test PDF processing with force option.\n        \n        This test:\n        1. Copies a sample PDF to a temporary directory\n        2. Processes the PDF with force=True to ensure reprocessing\n        3. Checks that the output is generated correctly\n        \"\"\"\n        # Create the output directory\n        output_dir = Path(self.temp_dir) / \"output_force\"\n        output_dir.mkdir(exist_ok=True)\n        \n        # Process the PDF using DirectoryProcessor with force=True\n        processor = DirectoryProcessor(self.temp_dir, dry_run=False)\n        \n        # Override the output directory to use our temporary directory\n        processor.output_dir = output_dir\n        \n        result = processor.process_directory(force=True)\n        \n        # Check that processing was successful\n        self.assertTrue(result is not None, \"PDF processing with force option should succeed\")\n        \n        # Check that the output directory exists\n        pdf_name = self.temp_pdf.stem\n        pdf_output_dir = output_dir / pdf_name\n        self.assertTrue(\n            pdf_output_dir.exists(),\n            f\"PDF output directory {pdf_output_dir} should exist after processing, but it doesn't\",\n        )\n        \n        # Process again to test force option\n        processor = DirectoryProcessor(self.temp_dir, dry_run=False)\n        result = processor.process_directory(force=True)\n        \n        # Check that processing was successful again\n        self.assertTrue(result is not None, \"PDF processing with force option should succeed on second run\")"
            },
            "tests/test_process_bibliography.py::TestProcessingResult::test_init": {
                "testid": "tests/test_process_bibliography.py::TestProcessingResult::test_init",
                "result": "passed",
                "test_implementation": "    def test_init(self):\n        \"\"\"Test initialization of BibliographyProcessor.\"\"\"\n        processor = BibliographyProcessor(self.bib_file)\n        self.assertEqual(\n            processor.input_path,\n            self.bib_file,\n            f\"BibliographyProcessor.input_path should match the input file, got '{processor.input_path}'\",\n        )\n        self.assertEqual(\n            processor.output_dir.name,\n            \"test-bib4llm\",\n            f\"BibliographyProcessor.output_dir name should be 'test-bib4llm', got '{processor.output_dir.name}'\",\n        )\n        self.assertFalse(\n            processor.dry_run,\n            f\"BibliographyProcessor.dry_run should be False by default, got {processor.dry_run}\",\n        )\n        \n        # Check that the database file exists in the output directory\n        db_file = processor.output_dir / \"processed_files.db\"\n        self.assertTrue(\n            db_file.exists(),\n            f\"Database file {db_file} should exist, but it doesn't\",\n        )\n        \n        # Clean up\n        if hasattr(processor, 'db_conn'):\n            processor.db_conn.close()"
            },
            "tests/test_process_bibliography.py::TestBibliographyProcessor::test_context_manager": {
                "testid": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_context_manager",
                "result": "passed",
                "test_implementation": "    def test_context_manager(self):\n        \"\"\"Test the context manager functionality.\"\"\"\n        with BibliographyProcessor(self.bib_file) as processor:\n            self.assertEqual(\n                processor.input_path,\n                self.bib_file,\n                f\"BibliographyProcessor.input_path should match the input file, got '{processor.input_path}'\",\n            )\n            self.assertEqual(\n                processor.output_dir.name,\n                \"test-bib4llm\",\n                f\"BibliographyProcessor.output_dir name should be 'test-bib4llm', got '{processor.output_dir.name}'\",\n            )\n            self.assertFalse(\n                processor.dry_run,\n                f\"BibliographyProcessor.dry_run should be False by default, got {processor.dry_run}\",\n            )\n            \n            # Check that the output directory was created\n            self.assertTrue(\n                processor.output_dir.exists(),\n                f\"Output directory {processor.output_dir} should exist, but it doesn't\",\n            )\n            \n            # Check that the database was created\n            db_file = processor.output_dir / \"processed_files.db\"\n            self.assertTrue(\n                db_file.exists(),\n                f\"Database file {db_file} should exist, but it doesn't\",\n            )\n            \n            # Check that the database has the expected tables\n            if hasattr(processor, 'db_conn'):\n                cursor = processor.db_conn.cursor()\n                tables = cursor.execute(\n                    \"SELECT name FROM sqlite_master WHERE type='table'\"\n                ).fetchall()\n                table_names = [t[0] for t in tables]\n                self.assertIn(\n                    \"processed_items\",\n                    table_names,\n                    f\"Database should contain a 'processed_items' table, got {table_names}\",\n                )"
            },
            "tests/test_process_bibliography.py::TestBibliographyProcessor::test_get_log_file": {
                "testid": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_get_log_file",
                "result": "passed",
                "test_implementation": "    def test_get_log_file(self):\n        \"\"\"Test the get_log_file method.\"\"\"\n        # Test with string path\n        log_file = BibliographyProcessor.get_log_file(\"test.bib\")\n        self.assertEqual(\n            log_file.name,\n            \"processing.log\",\n            f\"Log file name should be 'processing.log', got '{log_file.name}'\",\n        )\n        self.assertEqual(\n            log_file.parent.name,\n            \"test-bib4llm\",\n            f\"Log file parent directory should be 'test-bib4llm', got '{log_file.parent.name}'\",\n        )\n        \n        # Test with Path object\n        log_file = BibliographyProcessor.get_log_file(Path(\"test.bib\"))\n        self.assertEqual(\n            log_file.name,\n            \"processing.log\",\n            f\"Log file name should be 'processing.log', got '{log_file.name}'\",\n        )\n        self.assertEqual(\n            log_file.parent.name,\n            \"test-bib4llm\",\n            f\"Log file parent directory should be 'test-bib4llm', got '{log_file.parent.name}'\",\n        )\n        \n        # Test with Path object with directory\n        log_file = BibliographyProcessor.get_log_file(Path(\"dir/test.bib\"))\n        self.assertEqual(\n            log_file.name,\n            \"processing.log\",\n            f\"Log file name should be 'processing.log', got '{log_file.name}'\",\n        )\n        self.assertEqual(\n            log_file.parent.name,\n            \"test-bib4llm\",\n            f\"Log file parent directory should be 'test-bib4llm', got '{log_file.parent.name}'\",\n        )\n        self.assertEqual(\n            log_file.parent.parent.name,\n            \"dir\",\n            f\"Log file parent's parent directory should be 'dir', got '{log_file.parent.parent.name}'\",\n        )"
            },
            "tests/test_process_bibliography.py::TestBibliographyProcessor::test_get_output_dir": {
                "testid": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_get_output_dir",
                "result": "passed",
                "test_implementation": "    def test_get_output_dir(self):\n        \"\"\"Test the get_output_dir method.\"\"\"\n        # Test with string path\n        output_dir = BibliographyProcessor.get_output_dir(\"test.bib\")\n        self.assertEqual(\n            output_dir.name,\n            \"test-bib4llm\",\n            f\"Output directory name should be 'test-bib4llm' for 'test.bib', got '{output_dir.name}'\",\n        )\n        \n        # Test with Path object\n        output_dir = BibliographyProcessor.get_output_dir(Path(\"test.bib\"))\n        self.assertEqual(\n            output_dir.name,\n            \"test-bib4llm\",\n            f\"Output directory name should be 'test-bib4llm' for Path('test.bib'), got '{output_dir.name}'\",\n        )\n        \n        # Test with Path object with directory\n        output_dir = BibliographyProcessor.get_output_dir(Path(\"dir/test.bib\"))\n        self.assertEqual(\n            output_dir.name,\n            \"test-bib4llm\",\n            f\"Output directory name should be 'test-bib4llm' for Path('dir/test.bib'), got '{output_dir.name}'\",\n        )\n        self.assertEqual(\n            output_dir.parent.name,\n            \"dir\",\n            f\"Parent directory name should be 'dir' for Path('dir/test.bib'), got '{output_dir.parent.name}'\",\n        )"
            },
            "tests/test_process_bibliography.py::TestBibliographyProcessor::test_init": {
                "testid": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_init",
                "result": "passed",
                "test_implementation": "    def test_init(self):\n        \"\"\"Test initialization of BibliographyProcessor.\"\"\"\n        processor = BibliographyProcessor(self.bib_file)\n        self.assertEqual(\n            processor.input_path,\n            self.bib_file,\n            f\"BibliographyProcessor.input_path should match the input file, got '{processor.input_path}'\",\n        )\n        self.assertEqual(\n            processor.output_dir.name,\n            \"test-bib4llm\",\n            f\"BibliographyProcessor.output_dir name should be 'test-bib4llm', got '{processor.output_dir.name}'\",\n        )\n        self.assertFalse(\n            processor.dry_run,\n            f\"BibliographyProcessor.dry_run should be False by default, got {processor.dry_run}\",\n        )\n        \n        # Check that the database file exists in the output directory\n        db_file = processor.output_dir / \"processed_files.db\"\n        self.assertTrue(\n            db_file.exists(),\n            f\"Database file {db_file} should exist, but it doesn't\",\n        )\n        \n        # Clean up\n        if hasattr(processor, 'db_conn'):\n            processor.db_conn.close()"
            },
            "tests/test_process_bibliography.py::TestBibliographyProcessor::test_init_with_dry_run": {
                "testid": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_init_with_dry_run",
                "result": "passed",
                "test_implementation": "    def test_init_with_dry_run(self):\n        \"\"\"Test initialization of BibliographyProcessor with dry_run=True.\"\"\"\n        processor = BibliographyProcessor(self.bib_file, dry_run=True)\n        self.assertEqual(\n            processor.input_path,\n            self.bib_file,\n            f\"BibliographyProcessor.input_path should match the input file, got '{processor.input_path}'\",\n        )\n        self.assertEqual(\n            processor.output_dir.name,\n            \"test-bib4llm\",\n            f\"BibliographyProcessor.output_dir name should be 'test-bib4llm', got '{processor.output_dir.name}'\",\n        )\n        self.assertTrue(\n            processor.dry_run,\n            f\"BibliographyProcessor.dry_run should be True when set, got {processor.dry_run}\",\n        )\n        \n        # Clean up\n        if hasattr(processor, 'db_conn'):\n            processor.db_conn.close()"
            },
            "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths": {
                "testid": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths",
                "result": "passed",
                "test_implementation": "    def test_parse_file_field_with_paths(self):\n        \"\"\"Test the _parse_file_field method with relative and absolute paths.\"\"\"\n        # Create a test file structure:\n        # temp_dir/\n        #  ├── bib_subdir/\n        #  │   └── test.bib (references both relative and absolute paths)\n        #  ├── pdf_subdir/\n        #  │   └── relative_doc.pdf (referenced by relative path)\n        #  └── absolute_doc.pdf (referenced by absolute path)\n        \n        # Create subdirectories\n        bib_subdir = Path(self.temp_dir) / \"bib_subdir\"\n        pdf_subdir = Path(self.temp_dir) / \"pdf_subdir\"\n        bib_subdir.mkdir(exist_ok=True)\n        pdf_subdir.mkdir(exist_ok=True)\n        \n        # Create test PDF files (just empty files for testing)\n        relative_pdf = pdf_subdir / \"relative_doc.pdf\"\n        absolute_pdf = Path(self.temp_dir) / \"absolute_doc.pdf\"\n        relative_pdf.touch()\n        absolute_pdf.touch()\n        \n        # Create BibTeX file with both relative and absolute paths\n        bib_file_with_paths = bib_subdir / \"test_paths.bib\"\n        with open(bib_file_with_paths, \"w\") as f:\n            f.write(f\"\"\"@article{{RelativePathTest,\n  title = {{Test with Relative Path}},\n  author = {{Test, Author}},\n  year = {{2023}},\n  journal = {{Test Journal}},\n  file = {{../pdf_subdir/relative_doc.pdf}}\n}}\n\n@article{{AbsolutePathTest,\n  title = {{Test with Absolute Path}},\n  author = {{Test, Author}},\n  year = {{2023}},\n  journal = {{Test Journal}},\n  file = {{{absolute_pdf}}}\n}}\n\"\"\")\n        \n        # Initialize the processor with the BibTeX file\n        with BibliographyProcessor(bib_file_with_paths) as processor:\n            # Test parsing relative path\n            rel_paths, rel_missing = processor._parse_file_field(\"../pdf_subdir/relative_doc.pdf\")\n            self.assertEqual(len(rel_paths), 1, f\"Expected 1 path, got {len(rel_paths)}\")\n            self.assertEqual(rel_missing, 0, f\"Expected 0 missing paths, got {rel_missing}\")\n            self.assertTrue(\n                Path(rel_paths[0]).exists(),\n                f\"Path {rel_paths[0]} should exist, but it doesn't\"\n            )\n            self.assertEqual(\n                Path(rel_paths[0]).name, \n                \"relative_doc.pdf\",\n                f\"Expected filename 'relative_doc.pdf', got '{Path(rel_paths[0]).name}'\"\n            )\n            \n            # Test parsing absolute path\n            abs_paths, abs_missing = processor._parse_file_field(str(absolute_pdf))\n            self.assertEqual(len(abs_paths), 1, f\"Expected 1 path, got {len(abs_paths)}\")\n            self.assertEqual(abs_missing, 0, f\"Expected 0 missing paths, got {abs_missing}\")\n            self.assertTrue(\n                Path(abs_paths[0]).exists(),\n                f\"Path {abs_paths[0]} should exist, but it doesn't\"\n            )\n            self.assertEqual(\n                Path(abs_paths[0]).name, \n                \"absolute_doc.pdf\",\n                f\"Expected filename 'absolute_doc.pdf', got '{Path(abs_paths[0]).name}'\"\n            )\n            \n            # Test parsing non-existent path\n            none_paths, none_missing = processor._parse_file_field(\"non_existent_file.pdf\")\n            self.assertEqual(len(none_paths), 0, f\"Expected 0 paths, got {len(none_paths)}\")\n            self.assertEqual(none_missing, 1, f\"Expected 1 missing path, got {none_missing}\")"
            },
            "tests/test_process_bibliography.py::TestBibliographyProcessor::test_process_with_relative_paths": {
                "testid": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_process_with_relative_paths",
                "result": "passed",
                "test_implementation": "    def test_process_with_relative_paths(self):\n        \"\"\"Test processing a BibTeX file with relative paths from a different directory.\"\"\"\n        # Create a nested directory structure to test path resolution:\n        # temp_dir/\n        #  ├── run_from_here/      (directory to run the command from)\n        #  └── bib_data/\n        #      ├── bibliography.bib (contains relative paths to PDFs)\n        #      └── pdfs/\n        #          └── sample.pdf   (PDF referenced by relative path)\n        \n        # Create the directory structure\n        run_dir = Path(self.temp_dir) / \"run_from_here\"\n        bib_dir = Path(self.temp_dir) / \"bib_data\"\n        pdf_dir = bib_dir / \"pdfs\"\n        run_dir.mkdir(exist_ok=True)\n        bib_dir.mkdir(exist_ok=True)\n        pdf_dir.mkdir(exist_ok=True)\n        \n        # Create a simple test PDF file (just an empty file for testing)\n        pdf_file = pdf_dir / \"sample.pdf\"\n        pdf_file.touch()\n        \n        # Create a BibTeX file with a relative path to the PDF\n        bib_file = bib_dir / \"bibliography.bib\"\n        with open(bib_file, \"w\") as f:\n            f.write(\"\"\"@article{RelativePath2023,\n  title = {Test with Relative Path},\n  author = {Test, Author},\n  year = {2023},\n  journal = {Test Journal},\n  file = {pdfs/sample.pdf}\n}\n\"\"\")\n        \n        # Save the original working directory\n        original_cwd = os.getcwd()\n        \n        try:\n            # Change to the run_from_here directory to simulate running the command from there\n            os.chdir(run_dir)\n            \n            # Create a BibliographyProcessor instance with the BibTeX file\n            # This should be able to resolve the relative paths correctly\n            processor = BibliographyProcessor(bib_file, dry_run=True)\n            \n            # In dry run mode, we just need to verify that the processor successfully resolves the path\n            with processor:\n                # Force processing to ensure it runs\n                processor.process_all(force=True, num_processes=1)\n                \n                # For a more direct test of the path resolution capability,\n                # let's directly check if the relative path can be resolved\n                rel_paths, missing = processor._parse_file_field(\"pdfs/sample.pdf\")\n                self.assertEqual(len(rel_paths), 1, \n                              f\"Expected 1 path to be resolved, got {len(rel_paths)}\")\n                self.assertEqual(missing, 0, \n                              f\"Expected 0 missing paths, got {missing}\")\n                \n                # Verify the path points to the correct file\n                self.assertEqual(Path(rel_paths[0]).name, \"sample.pdf\", \n                              f\"Expected path to point to 'sample.pdf', got '{Path(rel_paths[0]).name}'\")\n        finally:\n            # Restore the original working directory\n            os.chdir(original_cwd)"
            },
            "tests/test_process_bibliography.py::TestBibliographyProcessor::test_standalone_parse_file_field": {
                "testid": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_standalone_parse_file_field",
                "result": "passed",
                "test_implementation": "    def test_standalone_parse_file_field(self):\n        \"\"\"Test the standalone parse_file_field function with relative and absolute paths.\"\"\"\n        from bib4llm.process_bibliography import standalone_process_entry\n        \n        # Create a test file structure similar to the previous test\n        bib_subdir = Path(self.temp_dir) / \"bib_subdir_standalone\"\n        pdf_subdir = Path(self.temp_dir) / \"pdf_subdir_standalone\"\n        bib_subdir.mkdir(exist_ok=True)\n        pdf_subdir.mkdir(exist_ok=True)\n        \n        # Create test PDF files (just empty files for testing)\n        relative_pdf = pdf_subdir / \"relative_doc.pdf\"\n        absolute_pdf = Path(self.temp_dir) / \"absolute_doc_standalone.pdf\"\n        relative_pdf.touch()\n        absolute_pdf.touch()\n        \n        # Create BibTeX file with both relative and absolute paths\n        bib_file_with_paths = bib_subdir / \"test_paths_standalone.bib\"\n        with open(bib_file_with_paths, \"w\") as f:\n            f.write(f\"\"\"@article{{RelativePathTest,\n  title = {{Test with Relative Path}},\n  author = {{Test, Author}},\n  year = {{2023}},\n  journal = {{Test Journal}},\n  file = {{../pdf_subdir_standalone/relative_doc.pdf}}\n}}\n\"\"\")\n        \n        # Create a test entry and args for standalone_process_entry\n        entry = {\n            \"ID\": \"TestEntry\",\n            \"file\": \"../pdf_subdir_standalone/relative_doc.pdf\"\n        }\n        output_dir = Path(self.temp_dir) / \"output_standalone\"\n        args = (entry, output_dir, bib_file_with_paths)\n        \n        try:\n            # Create a directory for standalone process entry to use\n            output_dir.mkdir(exist_ok=True)\n            \n            # Rather than trying to run a more complex test with changing directories,\n            # let's simplify and just verify the key functionality:\n            # 1. Can we resolve paths relative to the BibTeX file location?\n            \n            # Get the path as we would have in the process_entry function\n            file_field = \"../pdf_subdir_standalone/relative_doc.pdf\"\n            \n            # Directly test resolving it relative to BibTeX file directory\n            bib_dir = Path(bib_file_with_paths).parent\n            rel_path = (bib_dir / file_field).resolve()\n            \n            # This is the key assertion - the path should exist when resolved relative to bib file\n            self.assertTrue(\n                rel_path.exists(),\n                f\"Path {rel_path} should exist when resolved relative to bib file, but it doesn't\"\n            )\n            self.assertEqual(\n                rel_path.name, \n                \"relative_doc.pdf\",\n                f\"Expected filename 'relative_doc.pdf', got '{rel_path.name}'\"\n            )\n            \n            # Test that absolute paths work as expected\n            abs_path = Path(absolute_pdf)\n            self.assertTrue(\n                abs_path.exists(),\n                f\"Absolute path {abs_path} should exist, but it doesn't\"\n            )\n            \n            # Make a bogus path that shouldn't resolve\n            bogus_path = Path(\"some/nonexistent/path.pdf\")\n            self.assertFalse(\n                bogus_path.exists(),\n                f\"Bogus path {bogus_path} should not exist\"\n            )\n                \n        except Exception as e:\n            self.fail(f\"Test failed with exception: {e}\")"
            },
            "tests/test_watcher.py::TestWatcher::test_directory_watcher": {
                "testid": "tests/test_watcher.py::TestWatcher::test_directory_watcher",
                "result": "passed",
                "test_implementation": "    def test_directory_watcher(self):\n        \"\"\"\n        Test that the directory watcher script can be created and run.\n        \n        This test:\n        1. Creates a script that simulates the watcher functionality\n        2. Runs the script in a subprocess\n        3. Checks that the script completes successfully\n        \"\"\"\n        # Create a script to simulate the watcher\n        watcher_script = Path(self.temp_dir) / \"run_watcher.py\"\n        with open(watcher_script, 'w') as f:\n            f.write(\"\"\"\nimport sys\nimport time\nfrom pathlib import Path\n\nif __name__ == \"__main__\":\n    directory = Path(sys.argv[1])\n    output_dir = Path(sys.argv[2])\n    \n    # Create a file to indicate the watcher ran\n    with open(directory / \"watcher_ran\", \"w\") as f:\n        f.write(\"Watcher completed\")\n    \n    # Create the output directory to simulate processing\n    output_dir.mkdir(exist_ok=True)\n    \n    # Exit with success\n    sys.exit(0)\n\"\"\")\n        \n        # Create an output directory within the temp directory\n        output_dir = Path(self.temp_dir) / \"watcher_output\"\n        \n        # Start the watcher in a subprocess\n        watcher_process = subprocess.Popen(\n            [sys.executable, str(watcher_script), str(self.pdf_dir), str(output_dir)],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        \n        try:\n            # Wait for the process to complete\n            watcher_process.wait(timeout=5)\n            \n            # Check that the watcher ran\n            watcher_ran_file = self.pdf_dir / \"watcher_ran\"\n            self.assertTrue(\n                watcher_ran_file.exists(),\n                \"Watcher did not run to completion\"\n            )\n            \n            # Check that the output directory was created\n            self.assertTrue(\n                output_dir.exists(),\n                f\"Output directory {output_dir} was not created\"\n            )\n            \n        finally:\n            # Make sure the process is terminated\n            if watcher_process.poll() is None:\n                watcher_process.terminate()\n                watcher_process.wait(timeout=2)"
            }
        },
        "SRS_document": "# Software Requirements Specification: bib4llm\n\n## 1. Introduction\n\n### 1.1 Purpose\nThis Software Requirements Specification (SRS) document defines the functional and non-functional requirements for the `bib4llm` software system. Its primary goal is to serve as the sole specification for software developers tasked with implementing the system. The clarity, comprehensiveness, and appropriate level of abstraction in this document are critical for assessing the developers' ability to interpret requirements and build a functional product that passes a full suite of public and private test cases.\n\n### 1.2 Scope\n`bib4llm` is a command-line utility designed to convert research paper libraries, typically in PDF format and optionally managed via BibTeX files, into a format more amenable for ingestion and processing by Large Language Models (LLMs).\nThe system's core capabilities include:\n*   Processing individual PDF files, BibTeX library files, or entire directories containing such files.\n*   Extracting textual content from PDFs and converting it into Markdown.\n*   Extracting images from PDFs and saving them as PNG files, with references embedded in the Markdown output.\n*   Organizing the output into a structured directory hierarchy.\n*   Optionally watching input files or directories for changes and automatically re-processing them.\n*   Providing a mechanism to clean (remove) generated output.\n*   Tracking processed files to avoid redundant work unless forced.\n\nThe system focuses on content extraction and formatting, not on Retrieval-Augmented Generation (RAG) or other advanced LLM interaction techniques.\n\n### 1.3 Definitions, Acronyms, and Abbreviations\n*   **CLI:** Command-Line Interface\n*   **FR:** Functional Requirement\n*   **LLM:** Large Language Model\n*   **NFR:** Non-Functional Requirement\n*   **PDF:** Portable Document Format\n*   **PNG:** Portable Network Graphics\n*   **SRS:** Software Requirements Specification\n*   **BibTeX:** A bibliographic reference management software and file format.\n*   **Markdown:** A lightweight markup language with plain-text-formatting syntax.\n*   **RAG:** Retrieval-Augmented Generation\n*   **CWD:** Current Working Directory\n\n### 1.4 References\n*   `README.md` (provided as input material)\n*   Original source code and test cases (for LLM contextual understanding during SRS generation only)\n\n### 1.5 Overview\nThis SRS document is organized as follows:\n*   Section 1: Introduction - Provides an overview of the SRS, its purpose, scope, definitions, and references.\n*   Section 2: Overall Description - Describes the general factors affecting the product and its requirements, including product perspective, functions, user characteristics, and constraints.\n*   Section 3: Specific Requirements - Details all requirements, including functional, non-functional, external interface, and data handling requirements. Each requirement is uniquely identified and, where applicable, traced to test cases or source code analysis.\n\n## 2. Overall Description\n\n### 2.1 Product Perspective\n`bib4llm` is a standalone, command-line executable Python application. It is intended to be installed and run in a user's local environment. It interacts with the file system for input (PDFs, BibTeX files) and output (Markdown files, PNG images, log files, database). It utilizes external libraries for PDF processing and file system watching.\n\n### 2.2 Product Functions\nThe primary functions of `bib4llm` are:\n1.  **Conversion:** To take BibTeX files, individual PDF files, or directories of these as input and convert the content of the associated PDF documents.\n2.  **Extraction:** To extract text and images from PDF files. Text is converted to Markdown, and images are saved as PNG files.\n3.  **Output Generation:** To create a structured output directory containing the extracted Markdown text, PNG images, a copy/link to the original PDF, a processing log, and a database for tracking processed items.\n4.  **Watching:** To monitor specified input files or directories for modifications or additions and automatically trigger the conversion process for affected items.\n5.  **Cleaning:** To remove the generated output directory and its contents for a given input.\n6.  **Change Management:** To intelligently process only new or modified files/entries, minimizing redundant operations, unless forced to reprocess.\n7.  **User Interaction:** To provide a command-line interface for users to invoke its functions and configure its behavior using options.\n\n### 2.3 User Characteristics\nThe primary users of `bib4llm` are software developers, researchers, or students who utilize LLMs for research assistance and need to make their PDF-based literature accessible to these models. Users are expected to be comfortable with command-line tools and have a basic understanding of BibTeX and PDF file formats.\n\n### 2.4 Constraints\n*   The system must operate within a standard Python environment where it can be installed via `pip`.\n*   Input PDF files must be readable and not heavily encrypted or corrupted to allow text and image extraction.\n*   The file system must allow creation of directories, files, and symbolic links (with fallback to copying if symlinks fail).\n\n### 2.5 Assumptions and Dependencies\n*   Users have Python installed and can install `bib4llm` and its dependencies.\n*   Input files (PDFs, BibTeX) and directories specified by the user exist and are accessible with appropriate read permissions.\n*   The system has write permissions to create output directories and files in the designated locations (typically alongside the input file/directory or in the CWD if paths are relative and ambiguous).\n*   The effectiveness of text and image extraction is dependent on the quality and structure of the source PDF files and the capabilities of the underlying PDF processing library (whose specific choice is a design decision for the developer, but the output characteristics related to Markdown and PNGs are specified).\n\n## 3. Specific Requirements\n\n### 3.1 Functional Requirements\n\n#### 3.1.1 System Core Functions\n*   **FR-SYS-001:** The system shall convert PDF documents into LLM-readable formats, specifically Markdown for text and PNG for images.\n*   **FR-SYS-002:** The system shall accept input from three sources: a single BibTeX file, a single PDF file, or a directory containing BibTeX and/or PDF files.\n*   **FR-SYS-003:** The system shall provide a \"watch\" mode to monitor specified inputs for changes and automatically trigger re-processing.\n*   **FR-SYS-004:** The system shall provide a \"clean\" mode to remove all generated output associated with a given input BibTeX or PDF file.\n\n#### 3.1.2 Command-Line Interface (CLI)\n*   **FR-CLI-001:** The system shall provide a main command `bib4llm`.\n*   **FR-CLI-002:** The system shall support a `convert` subcommand for one-time processing of inputs.\n*   **FR-CLI-003:** The system shall support a `watch` subcommand for continuous monitoring and processing of inputs.\n*   **FR-CLI-004:** The system shall support a `clean` subcommand for removing generated outputs.\n*   **FR-CLI-005:** The system shall validate that the provided `input_path` argument for all commands exists on the file system. If not, it shall report an error and terminate.\n*   **FR-CLI-006:** The system shall support a `--help` option for the main command and each subcommand, displaying usage information.\n\n#### 3.1.3 `convert` Command Functionality\n*   **FR-CONV-001:** The `convert` command shall accept a single `input_path` argument specifying the BibTeX file, PDF file, or directory to process.\n*   **FR-CONV-002:** The `convert` command shall support a `--force` (or `-f`) option. When specified, all entries/files shall be reprocessed, regardless of their previous processing status or detected changes.\n*   **FR-CONV-003:** The `convert` command shall support a `--processes` (or `-p`) option specifying the number of parallel processes to use. If not specified, it should default to a system-appropriate number (e.g., CPU core count).\n*   **FR-CONV-004:** The `convert` command shall support a `--dry-run` (or `-n`) option. When specified, the system shall log what actions it would take (e.g., files to be processed, output directories to be created) without actually performing file modifications or creating outputs.\n*   **FR-CONV-005:** The `convert` command shall support a `--quiet` (or `-q`) option to suppress informational console output, showing only warnings and errors.\n*   **FR-CONV-006:** The `convert` command shall support a `--debug` (or `-d`) option to enable detailed debug logging to the console.\n*   **FR-CONV-007:** When `input_path` is a directory, the `convert` command shall support a `--no-recursive` (or `-R`) option to disable recursive processing of subdirectories. By default, processing shall be recursive.\n\n#### 3.1.4 `watch` Command Functionality\n*   **FR-WATCH-001:** The `watch` command shall accept a single `input_path` argument specifying the BibTeX file, PDF file, or directory to monitor.\n*   **FR-WATCH-002:** Upon starting, the `watch` command shall perform an initial processing of the specified `input_path` similar to the `convert` command.\n*   **FR-WATCH-003:** When monitoring a BibTeX or PDF file, the `watch` command shall detect modifications to the watched file and trigger reprocessing of that file.\n*   **FR-WATCH-004:** When monitoring a directory, the `watch` command shall detect creation or modification of BibTeX or PDF files within the directory (and subdirectories if recursive watching is active) and trigger appropriate processing.\n*   **FR-WATCH-005:** The `watch` command shall debounce rapid succession of file modification events to avoid redundant processing. A minimum delay (e.g., 1 second) between processing triggers for the same event type should be implemented.\n*   **FR-WATCH-006:** The `watch` command shall support a `--processes` (or `-p`) option for processing, similar to the `convert` command.\n*   **FR-WATCH-007:** The `watch` command shall support `--quiet` (or `-q`) and `--debug` (or `-d`) options for logging control, similar to the `convert` command.\n*   **FR-WATCH-008:** When `input_path` is a directory, the `watch` command shall support a `--no-recursive` (or `-R`) option to disable recursive watching of subdirectories. By default, watching shall be recursive.\n*   **FR-WATCH-009:** If a watched file is not found during a processing attempt (e.g., it was deleted after the watch started), the `watch` command shall log an error and terminate gracefully.\n\n#### 3.1.5 `clean` Command Functionality\n*   **FR-CLEAN-001:** The `clean` command shall accept an `input_path` argument specifying the BibTeX file or PDF file whose generated output directory should be removed.\n*   **FR-CLEAN-002:** The `clean` command shall remove the entire output directory (e.g., `<input_stem>-bib4llm`) associated with the specified `input_path`.\n*   **FR-CLEAN-003:** The `clean` command shall support a `--dry-run` (or `-n`) option. When specified, the system shall log which directory and files would be removed without actually deleting them.\n*   **FR-CLEAN-004:** The `clean` command shall not support directory inputs directly. It operates based on an input BibTeX or PDF file to identify the corresponding output directory.\n\n#### 3.1.6 PDF Content Extraction\n*   **FR-PDF-001:** The system shall extract textual content from PDF files.\n*   **FR-PDF-002:** The system shall convert the extracted textual content into Markdown format.\n*   **FR-PDF-003:** The system shall extract images from PDF files and save them as PNG files.\n*   **FR-PDF-004:** The generated Markdown content shall include references to the extracted PNG images using Markdown image syntax (e.g., `![image_alt_text](path/to/image.png)`).\n*   **FR-PDF-005:** For each processed PDF, the system shall place a corresponding PDF file (e.g., `[citation_key].pdf` or `[original_pdf_stem].pdf`) within the specific output sub-directory for that item. This file shall be a symbolic link to the original PDF file if possible; otherwise, it shall be a copy of the original PDF file.\n*   **FR-PDF-006:** The system shall capture and log any warnings or errors generated by the underlying PDF processing library (MuPDF) during extraction.\n\n#### 3.1.7 BibTeX Processing\n*   **FR-BIB-001:** The system shall parse BibTeX files (`.bib`, `.bibtex`) to extract bibliographic entries.\n*   **FR-BIB-002:** For each BibTeX entry, the system shall identify associated file paths from the 'file' field.\n*   **FR-BIB-003:** The system shall correctly parse 'file' fields containing multiple semicolon-separated file paths.\n*   **FR-BIB-004:** The system shall correctly parse Zotero-style 'file' fields (e.g., `description:filepath:mimetype`). It should extract the `filepath` component.\n*   **FR-BIB-005:** The system shall resolve relative file paths specified in BibTeX 'file' fields. Path resolution shall be relative to the location of the BibTeX file itself.\n*   **FR-BIB-006:** The system shall correctly handle absolute file paths specified in BibTeX 'file' fields.\n*   **FR-BIB-007:** BibTeX entries lacking an 'ID' (citation key) shall be skipped with a warning.\n*   **FR-BIB-008:** If a BibTeX entry links to non-PDF text files (e.g., `.txt`, `.md`), the system shall copy their content into the output Markdown file, wrapped in appropriate Markdown code blocks.\n\n#### 3.1.8 Directory Processing\n*   **FR-DIR-001:** When the input is a directory, the system shall find and process all BibTeX (`.bib`, `.bibtex`) files and PDF (`.pdf`) files within that directory.\n*   **FR-DIR-002:** By default, directory processing shall be recursive, including files in subdirectories.\n*   **FR-DIR-003:** If non-recursive processing is specified for a directory, only files directly within the top-level directory shall be processed.\n\n#### 3.1.9 Output Generation and Structure\n*   **FR-OUT-001:** For a given input file (e.g., `input.bib` or `input.pdf`), the system shall create a main output directory named `<input_stem>-bib4llm` in the same location as the input file.\n*   **FR-OUT-002:** For an input directory (e.g., `input_dir`), the system shall create a main output directory named `input_dir-bib4llm` in the same location as the input directory.\n*   **FR-OUT-003:** When processing a BibTeX file, each processed entry shall have its own subdirectory within the main output directory, named after the entry's citation key (ID).\n*   **FR-OUT-004:** When processing a single PDF file directly (e.g. `paper.pdf`), its output (Markdown, images) shall be placed in a subdirectory named `paper-bib4llm/paper/`.\n*   **FR-OUT-005:** When processing a directory of PDF files, the output for each PDF shall be placed in a subdirectory structure within the main output directory (`<input_dir>-bib4llm`) that mirrors the PDF's relative path from the input directory. Each PDF's specific outputs will be in a folder named after the PDF's stem.\n    *   Example: `input_dir/sub/paper.pdf` -> `<input_dir>-bib4llm/sub/paper/paper.md`\n*   **FR-OUT-006:** Within each entry's/PDF's specific output subdirectory, the extracted text shall be saved in a Markdown file named `[citation_key_or_pdf_stem].md`.\n*   **FR-OUT-007:** Extracted images (PNGs) shall be saved in the same specific output subdirectory as their corresponding Markdown file.\n*   **FR-OUT-008:** The Markdown file shall contain a header indicating the citation key (e.g., `# Citation Key: [citation_key]`).\n\n#### 3.1.10 Change Detection and Caching\n*   **FR-CHNG-001:** The system shall compute and store a hash (e.g., SHA-256) for each source PDF file associated with a BibTeX entry or processed directly.\n*   **FR-CHNG-002:** The system shall compute and store a hash representing the contents of each output subdirectory (for a BibTeX entry or a PDF). This hash should consider file names, contents of generated files (Markdown, PNGs), and linked PDF.\n*   **FR-CHNG-003:** The system shall avoid reprocessing a BibTeX entry or PDF file if its source file hash(es) and its output directory hash have not changed since the last successful processing, unless the `--force` option is used.\n*   **FR-CHNG-004:** Information about processed items, including citation keys, source file hashes, and output directory hashes, shall be stored persistently.\n\n#### 3.1.11 Data Handling Requirements (Database)\n*   **FR-DATA-001:** The system shall use a SQLite database named `processed_files.db` located in the root of the main output directory to store processing metadata.\n*   **FR-DATA-002:** The database shall contain a table (e.g., `processed_items`) to store records of processed bibliography entries or PDF files.\n*   **FR-DATA-003:** Each record in the `processed_items` table shall store at least the citation key (or an equivalent identifier for direct PDFs), a representation of source file hashes (e.g., JSON string), the output directory hash, and a timestamp of the last processing.\n\n#### 3.1.12 Logging\n*   **FR-LOG-001:** The system shall generate a log file named `processing.log` in the main output directory.\n*   **FR-LOG-002:** All log messages, including debug level, shall be written to the `processing.log` file.\n*   **FR-LOG-003:** Console logging level shall be configurable: INFO by default, WARNING if `--quiet` is specified, DEBUG if `--debug` is specified.\n*   **FR-LOG-004:** The system shall log important events, such as initialization, files being processed, successes, failures, warnings (including MuPDF warnings), and summaries of operations.\n\n#### 3.1.13 Error Handling and Robustness\n*   **FR-ERR-001:** The system shall handle missing PDF files referenced in BibTeX entries by logging a warning and skipping the missing file, continuing to process other available files for that entry or other entries.\n*   **FR-ERR-002:** The system shall handle references to unsupported file types in BibTeX entries by logging a warning and skipping those files.\n*   **FR-ERR-003:** On Windows, the system shall validate output paths for common issues like excessive length or invalid characters and log errors if such paths would be problematic, potentially skipping the problematic item.\n*   **FR-ERR-004:** The `BibliographyProcessor` class shall be usable as a Python context manager, ensuring resources like database connections are properly managed.\n\n#### 3.1.14 Parallel Processing\n*   **FR-PROC-001:** The system shall support parallel processing of multiple bibliography entries or PDF files to improve performance on multi-core systems.\n\n#### 3.1.15 User Interface / Experience\n*   **FR-UI-001:** The system shall display a progress indicator for long-running batch processing operations (e.g., converting multiple BibTeX entries or PDF files in a directory).\n\n### 3.2 Non-Functional Requirements\n\n#### 3.2.1 Usability\n*   **NFR-USAB-001:** The system's main command (`bib4llm`) must provide a user-friendly help message when invoked with the `--help` option, outlining its purpose and available subcommands.\n*   **NFR-USAB-002:** The `convert` subcommand must provide a user-friendly help message when invoked with `--help`, detailing its arguments and options.\n*   **NFR-USAB-003:** The `watch` subcommand must provide a user-friendly help message when invoked with `--help`, detailing its arguments and options.\n*   **NFR-USAB-004:** The `clean` subcommand must provide a user-friendly help message when invoked with `--help`, detailing its arguments and options.\n\n### 3.3 External Interface Requirements\nThis section is largely covered by the CLI functional requirements (Section 3.1.2, 3.1.3, 3.1.4, 3.1.5). The CLI is the primary external interface. File formats (PDF, BibTeX for input; Markdown, PNG, SQLite DB for output) also constitute external interfaces.\n\n*   **EIR-001: Input File Formats**\n    *   The system shall accept BibTeX files encoded in UTF-8, adhering to common BibTeX syntax.\n    *   The system shall accept PDF files that are not password-protected for content extraction.\n*   **EIR-002: Output File Formats**\n    *   Textual output shall be in Markdown format, encoded in UTF-8.\n    *   Image output shall be in PNG format.\n    *   Database output shall be a SQLite version 3 database file.\n*   **EIR-003: Command-Line Arguments**\n    *   All command-line arguments and options defined in sections 3.1.2, 3.1.3, 3.1.4, and 3.1.5 must be implemented as specified.\n\n### 3.4 Other Requirements\n(None specified beyond Functional and Non-Functional requirements relevant for this assessment context)",
        "structured_requirements": [
            {
                "requirement_id": "FR-SYS-001",
                "requirement_description": "The system shall convert PDF documents into LLM-readable formats, specifically Markdown for text and PNG for images.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": ""
                    },
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_compare_to_example",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SYS-002",
                "requirement_description": "The system shall accept input from three sources: a single BibTeX file, a single PDF file, or a directory containing BibTeX and/or PDF files.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py",
                        "description": "(implicit via command executions)"
                    },
                    {
                        "id": "tests/test_conversion.py",
                        "description": ""
                    },
                    {
                        "id": "tests/test_pdf_processing.py",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SYS-003",
                "requirement_description": "The system shall provide a \"watch\" mode to monitor specified inputs for changes and automatically trigger re-processing.",
                "test_traceability": [
                    {
                        "id": "tests/test_watcher.py::TestWatcher::test_directory_watcher",
                        "description": "(simulated)"
                    },
                    {
                        "id": "README \"watch\" command description.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/watcher.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SYS-004",
                "requirement_description": "The system shall provide a \"clean\" mode to remove all generated output associated with a given input BibTeX or PDF file.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_clean_removes_output_dir",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(clean command logic)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-001",
                "requirement_description": "The system shall provide a main command `bib4llm`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_help",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-002",
                "requirement_description": "The system shall support a `convert` subcommand for one-time processing of inputs.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_convert_help",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-003",
                "requirement_description": "The system shall support a `watch` subcommand for continuous monitoring and processing of inputs.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_watch_help",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-004",
                "requirement_description": "The system shall support a `clean` subcommand for removing generated outputs.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_clean_help",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-005",
                "requirement_description": "The system shall validate that the provided `input_path` argument for all commands exists on the file system. If not, it shall report an error and terminate.",
                "test_traceability": [
                    {
                        "id": "Source code analysis",
                        "description": "(implicit expectation from file operations)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(path validation)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-006",
                "requirement_description": "The system shall support a `--help` option for the main command and each subcommand, displaying usage information.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_help",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cli.py::TestCLI::test_convert_help",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cli.py::TestCLI::test_watch_help",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cli.py::TestCLI::test_clean_help",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CONV-001",
                "requirement_description": "The `convert` command shall accept a single `input_path` argument specifying the BibTeX file, PDF file, or directory to process.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_convert_help",
                        "description": "(argument presence)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(convert_parser setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CONV-002",
                "requirement_description": "The `convert` command shall support a `--force` (or `-f`) option. When specified, all entries/files shall be reprocessed, regardless of their previous processing status or detected changes.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing_with_custom_options",
                        "description": ""
                    },
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_using_cli",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(passes `force` to processors)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CONV-003",
                "requirement_description": "The `convert` command shall support a `--processes` (or `-p`) option specifying the number of parallel processes to use. If not specified, it should default to a system-appropriate number (e.g., CPU core count).",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_convert_with_processes",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cli.py::TestCLI::test_processes_option",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(passes `processes` to processors)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CONV-004",
                "requirement_description": "The `convert` command shall support a `--dry-run` (or `-n`) option. When specified, the system shall log what actions it would take (e.g., files to be processed, output directories to be created) without actually performing file modifications or creating outputs.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_convert_dry_run",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(passes `dry_run` to processors)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CONV-005",
                "requirement_description": "The `convert` command shall support a `--quiet` (or `-q`) option to suppress informational console output, showing only warnings and errors.",
                "test_traceability": [
                    {
                        "id": "Source code analysis",
                        "description": "(logging setup based on `quiet`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::setup_logging",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CONV-006",
                "requirement_description": "The `convert` command shall support a `--debug` (or `-d`) option to enable detailed debug logging to the console.",
                "test_traceability": [
                    {
                        "id": "Source code analysis",
                        "description": "(logging setup based on `debug`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::setup_logging",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CONV-007",
                "requirement_description": "When `input_path` is a directory, the `convert` command shall support a `--no-recursive` (or `-R`) option to disable recursive processing of subdirectories. By default, processing shall be recursive.",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_pdf_subdirectory_conversion",
                        "description": "(tests recursive behavior)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(passes `no_recursive` to `DirectoryProcessor`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-001",
                "requirement_description": "The `watch` command shall accept a single `input_path` argument specifying the BibTeX file, PDF file, or directory to monitor.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_watch_help",
                        "description": "(argument presence)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(watch_parser setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-002",
                "requirement_description": "Upon starting, the `watch` command shall perform an initial processing of the specified `input_path` similar to the `convert` command.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/watcher.py::BibTexHandler::__init__",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/watcher.py::PDFHandler::__init__",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/watcher.py::DirectoryHandler::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-003",
                "requirement_description": "When monitoring a BibTeX or PDF file, the `watch` command shall detect modifications to the watched file and trigger reprocessing of that file.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/watcher.py::BibTexHandler::on_modified",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/watcher.py::PDFHandler::on_modified",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-004",
                "requirement_description": "When monitoring a directory, the `watch` command shall detect creation or modification of BibTeX or PDF files within the directory (and subdirectories if recursive watching is active) and trigger appropriate processing.",
                "test_traceability": [
                    {
                        "id": "tests/test_watcher.py::TestWatcher::test_directory_watcher",
                        "description": "(simulates a watcher creating a file, indirectly testing the concept of acting on file system events). Direct test of `watchdog` integration is not present."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/watcher.py::DirectoryHandler::on_created",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/watcher.py::DirectoryHandler::on_modified",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-005",
                "requirement_description": "The `watch` command shall debounce rapid succession of file modification events to avoid redundant processing. A minimum delay (e.g., 1 second) between processing triggers for the same event type should be implemented.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/watcher.py::BibTexHandler::on_modified",
                        "description": "(and similar in other handlers)"
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-006",
                "requirement_description": "The `watch` command shall support a `--processes` (or `-p`) option for processing, similar to the `convert` command.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_watch_help",
                        "description": "(option presence)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(passes `processes` to watch functions)"
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-007",
                "requirement_description": "The `watch` command shall support `--quiet` (or `-q`) and `--debug` (or `-d`) options for logging control, similar to the `convert` command.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::setup_logging",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-008",
                "requirement_description": "When `input_path` is a directory, the `watch` command shall support a `--no-recursive` (or `-R`) option to disable recursive watching of subdirectories. By default, watching shall be recursive.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_watch_help",
                        "description": "(option presence)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(passes `no_recursive` to `watch_directory`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-WATCH-009",
                "requirement_description": "If a watched file is not found during a processing attempt (e.g., it was deleted after the watch started), the `watch` command shall log an error and terminate gracefully.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/watcher.py::BibTexHandler::_process",
                        "description": "(catches `FileNotFoundError` and raises `SystemExit`)"
                    },
                    {
                        "id": "bib4llm/watcher.py::PDFHandler::_process",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CLEAN-001",
                "requirement_description": "The `clean` command shall accept an `input_path` argument specifying the BibTeX file or PDF file whose generated output directory should be removed.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_clean_help",
                        "description": "(argument presence)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(clean_parser setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CLEAN-002",
                "requirement_description": "The `clean` command shall remove the entire output directory (e.g., `<input_stem>-bib4llm`) associated with the specified `input_path`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_clean_removes_output_dir",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(uses `shutil.rmtree`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CLEAN-003",
                "requirement_description": "The `clean` command shall support a `--dry-run` (or `-n`) option. When specified, the system shall log which directory and files would be removed without actually deleting them.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_clean_dry_run",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(dry-run logic for clean)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CLEAN-004",
                "requirement_description": "The `clean` command shall not support directory inputs directly. It operates based on an input BibTeX or PDF file to identify the corresponding output directory.",
                "test_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": "(logs error for directory input to clean)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::main",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PDF-001",
                "requirement_description": "The system shall extract textual content from PDF files.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": "(checks for title in Markdown)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": "(use of `pymupdf4llm.to_markdown`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-PDF-002",
                "requirement_description": "The system shall convert the extracted textual content into Markdown format.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": "(checks for `.md` file existence and content)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PDF-003",
                "requirement_description": "The system shall extract images from PDF files and save them as PNG files.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": "(checks for `*.png` files)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": "(use of `pymupdf4llm.to_markdown` with `write_images=True`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-PDF-004",
                "requirement_description": "The generated Markdown content shall include references to the extracted PNG images using Markdown image syntax (e.g., `![image_alt_text](path/to/image.png)`).",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": "(checks for `![` in Markdown)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": "(output of `pymupdf4llm.to_markdown`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-PDF-005",
                "requirement_description": "For each processed PDF, the system shall place a corresponding PDF file (e.g., `[citation_key].pdf` or `[original_pdf_stem].pdf`) within the specific output sub-directory for that item. This file shall be a symbolic link to the original PDF file if possible; otherwise, it shall be a copy of the original PDF file.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": "(checks for `pdf_file.exists()`)"
                    },
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::_process_pdf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PDF-006",
                "requirement_description": "The system shall capture and log any warnings or errors generated by the underlying PDF processing library (MuPDF) during extraction.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": "(use of `pymupdf.TOOLS.mupdf_warnings`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-BIB-001",
                "requirement_description": "The system shall parse BibTeX files (`.bib`, `.bibtex`) to extract bibliographic entries.",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_output_structure",
                        "description": "(implies parsing)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::process_all",
                        "description": "(use of `bibtexparser`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-BIB-002",
                "requirement_description": "For each BibTeX entry, the system shall identify associated file paths from the 'file' field.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::_parse_file_field",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BIB-003",
                "requirement_description": "The system shall correctly parse 'file' fields containing multiple semicolon-separated file paths.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::_parse_file_field",
                        "description": "(splits by ';')"
                    }
                ]
            },
            {
                "requirement_id": "FR-BIB-004",
                "requirement_description": "The system shall correctly parse Zotero-style 'file' fields (e.g., `description:filepath:mimetype`). It should extract the `filepath` component.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths",
                        "description": "(indirectly, as it handles general `name:value` if `:` is present)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::_parse_file_field",
                        "description": "(splits by ':', takes second part)"
                    }
                ]
            },
            {
                "requirement_id": "FR-BIB-005",
                "requirement_description": "The system shall resolve relative file paths specified in BibTeX 'file' fields. Path resolution shall be relative to the location of the BibTeX file itself.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths",
                        "description": ""
                    },
                    {
                        "id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_process_with_relative_paths",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::_parse_file_field",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BIB-006",
                "requirement_description": "The system shall correctly handle absolute file paths specified in BibTeX 'file' fields.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::_parse_file_field",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BIB-007",
                "requirement_description": "BibTeX entries lacking an 'ID' (citation key) shall be skipped with a warning.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::process_all",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BIB-008",
                "requirement_description": "If a BibTeX entry links to non-PDF text files (e.g., `.txt`, `.md`), the system shall copy their content into the output Markdown file, wrapped in appropriate Markdown code blocks.",
                "test_traceability": [
                    {
                        "id": "Source code analysis",
                        "description": "(mime type check for `text/`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-DIR-001",
                "requirement_description": "When the input is a directory, the system shall find and process all BibTeX (`.bib`, `.bibtex`) files and PDF (`.pdf`) files within that directory.",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion",
                        "description": ""
                    },
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_convert_pdf_directory",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::find_files",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::process_directory",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-DIR-002",
                "requirement_description": "By default, directory processing shall be recursive, including files in subdirectories.",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_pdf_subdirectory_conversion",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::find_files",
                        "description": "(uses `**/*` glob pattern by default)"
                    }
                ]
            },
            {
                "requirement_id": "FR-DIR-003",
                "requirement_description": "If non-recursive processing is specified for a directory, only files directly within the top-level directory shall be processed.",
                "test_traceability": [
                    {
                        "id": "Related to FR-CONV-007 and FR-WATCH-008;",
                        "description": "direct test for non-recursive output not explicitly found but option implies behavior."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::find_files",
                        "description": "(glob pattern changes if not recursive)"
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-001",
                "requirement_description": "For a given input file (e.g., `input.bib` or `input.pdf`), the system shall create a main output directory named `<input_stem>-bib4llm` in the same location as the input file.",
                "test_traceability": [
                    {
                        "id": "tests/test_basic.py::TestBibliographyProcessor::test_get_output_dir",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cli.py::TestCLI::test_convert_creates_output_dir",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::get_output_dir",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-002",
                "requirement_description": "For an input directory (e.g., `input_dir`), the system shall create a main output directory named `input_dir-bib4llm` in the same location as the input directory.",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-003",
                "requirement_description": "When processing a BibTeX file, each processed entry shall have its own subdirectory within the main output directory, named after the entry's citation key (ID).",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_output_structure",
                        "description": ""
                    },
                    {
                        "id": "README output structure.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": "(creates `entry_dir`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-004",
                "requirement_description": "When processing a single PDF file directly (e.g. `paper.pdf`), its output (Markdown, images) shall be placed in a subdirectory named `paper-bib4llm/paper/`.",
                "test_traceability": [
                    {
                        "id": "README output structure",
                        "description": "(output structure for single PDF is effectively `output_dir/pdf_stem/`)"
                    },
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::process_pdf",
                        "description": "(creates `entry_dir = self.output_dir / citation_key`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-005",
                "requirement_description": "When processing a directory of PDF files, the output for each PDF shall be placed in a subdirectory structure within the main output directory (`<input_dir>-bib4llm`) that mirrors the PDF's relative path from the input directory. Each PDF's specific outputs will be in a folder named after the PDF's stem.",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_pdf_directory_conversion",
                        "description": ""
                    },
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_pdf_subdirectory_conversion",
                        "description": ""
                    },
                    {
                        "id": "README output structure.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::process_directory",
                        "description": ""
                    },
                    {
                        "id": "_process_pdf_wrapper",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-006",
                "requirement_description": "Within each entry's/PDF's specific output subdirectory, the extracted text shall be saved in a Markdown file named `[citation_key_or_pdf_stem].md`.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": ""
                    },
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_compare_to_example",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::_process_pdf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-007",
                "requirement_description": "Extracted images (PNGs) shall be saved in the same specific output subdirectory as their corresponding Markdown file.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": "(image_path parameter)"
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-008",
                "requirement_description": "The Markdown file shall contain a header indicating the citation key (e.g., `# Citation Key: [citation_key]`).",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing",
                        "description": "(checks for title, but structure is `\"# Citation Key: {citation_key}\\n\\n---\\n\\n{md_text}\"`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::_process_pdf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CHNG-001",
                "requirement_description": "The system shall compute and store a hash (e.g., SHA-256) for each source PDF file associated with a BibTeX entry or processed directly.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestProcessingResult::test_init",
                        "description": "(file_hashes)"
                    },
                    {
                        "id": "tests/test_example_conversion.py::TestExampleConversion::test_example_conversion",
                        "description": "(DB interaction implies hash storage)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::_compute_file_hash",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CHNG-002",
                "requirement_description": "The system shall compute and store a hash representing the contents of each output subdirectory (for a BibTeX entry or a PDF). This hash should consider file names, contents of generated files (Markdown, PNGs), and linked PDF.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestProcessingResult::test_init",
                        "description": "(dir_hash)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::_compute_dir_hash",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CHNG-003",
                "requirement_description": "The system shall avoid reprocessing a BibTeX entry or PDF file if its source file hash(es) and its output directory hash have not changed since the last successful processing, unless the `--force` option is used.",
                "test_traceability": [
                    {
                        "id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing_with_custom_options",
                        "description": "(force ensures reprocessing)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::process_all",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::process_pdf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CHNG-004",
                "requirement_description": "Information about processed items, including citation keys, source file hashes, and output directory hashes, shall be stored persistently.",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_output_structure",
                        "description": "(checks for `processed_files.db`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor",
                        "description": "(database interaction)"
                    }
                ]
            },
            {
                "requirement_id": "FR-DATA-001",
                "requirement_description": "The system shall use a SQLite database named `processed_files.db` located in the root of the main output directory to store processing metadata.",
                "test_traceability": [
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_output_structure",
                        "description": ""
                    },
                    {
                        "id": "tests/test_example_conversion.py::TestExampleConversion::test_example_conversion",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-DATA-002",
                "requirement_description": "The database shall contain a table (e.g., `processed_items`) to store records of processed bibliography entries or PDF files.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_context_manager",
                        "description": "(checks for `processed_items` table)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-DATA-003",
                "requirement_description": "Each record in the `processed_items` table shall store at least the citation key (or an equivalent identifier for direct PDFs), a representation of source file hashes (e.g., JSON string), the output directory hash, and a timestamp of the last processing.",
                "test_traceability": [
                    {
                        "id": "tests/test_example_conversion.py::TestExampleConversion::test_example_conversion",
                        "description": "(compares table structure implicitly by comparing DBs, and source defines schema including `last_processed`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::__init__",
                        "description": "(table schema definition)"
                    }
                ]
            },
            {
                "requirement_id": "FR-LOG-001",
                "requirement_description": "The system shall generate a log file named `processing.log` in the main output directory.",
                "test_traceability": [
                    {
                        "id": "tests/test_basic.py::TestBibliographyProcessor::test_get_log_file",
                        "description": ""
                    },
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_output_structure",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::get_log_file",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/cli.py::setup_logging",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-LOG-002",
                "requirement_description": "All log messages, including debug level, shall be written to the `processing.log` file.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::setup_logging",
                        "description": "(file_handler level is DEBUG)"
                    }
                ]
            },
            {
                "requirement_id": "FR-LOG-003",
                "requirement_description": "Console logging level shall be configurable: INFO by default, WARNING if `--quiet` is specified, DEBUG if `--debug` is specified.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/cli.py::setup_logging",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-LOG-004",
                "requirement_description": "The system shall log important events, such as initialization, files being processed, successes, failures, warnings (including MuPDF warnings), and summaries of operations.",
                "test_traceability": [
                    {
                        "id": "Implicitly tested by observing log outputs during manual runs or if tests checked log content (not explicitly done in provided tests for specific messages).",
                        "description": "Derived from source code logging statements."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Various `logger` calls throughout the codebase.",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-001",
                "requirement_description": "The system shall handle missing PDF files referenced in BibTeX entries by logging a warning and skipping the missing file, continuing to process other available files for that entry or other entries.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths",
                        "description": "(checks `not_found` count)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::_parse_file_field",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-002",
                "requirement_description": "The system shall handle references to unsupported file types in BibTeX entries by logging a warning and skipping those files.",
                "test_traceability": [
                    {
                        "id": "Source code analysis.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": "(logs warning for unsupported mime_type)"
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-003",
                "requirement_description": "On Windows, the system shall validate output paths for common issues like excessive length or invalid characters and log errors if such paths would be problematic, potentially skipping the problematic item.",
                "test_traceability": [
                    {
                        "id": "Source code analysis",
                        "description": "(presence of `validate_windows_path`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::validate_windows_path",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::standalone_process_entry",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-004",
                "requirement_description": "The `BibliographyProcessor` class shall be usable as a Python context manager, ensuring resources like database connections are properly managed.",
                "test_traceability": [
                    {
                        "id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_context_manager",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::__enter__",
                        "description": ""
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::__exit__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PROC-001",
                "requirement_description": "The system shall support parallel processing of multiple bibliography entries or PDF files to improve performance on multi-core systems.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_convert_with_processes",
                        "description": ""
                    },
                    {
                        "id": "tests/test_conversion.py::TestConversion::test_conversion_using_cli",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::process_all",
                        "description": "(use of `process_map`)"
                    },
                    {
                        "id": "bib4llm/cli.py",
                        "description": "(processes option)"
                    }
                ]
            },
            {
                "requirement_id": "FR-UI-001",
                "requirement_description": "The system shall display a progress indicator for long-running batch processing operations (e.g., converting multiple BibTeX entries or PDF files in a directory).",
                "test_traceability": [
                    {
                        "id": "Derived from source code analysis;",
                        "description": "not directly asserted in tests."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "bib4llm/process_bibliography.py::BibliographyProcessor::process_all",
                        "description": "(use of `tqdm.contrib.concurrent.process_map`)"
                    },
                    {
                        "id": "bib4llm/process_bibliography.py::DirectoryProcessor::process_directory",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "NFR-USAB-001",
                "requirement_description": "The system's main command (`bib4llm`) must provide a user-friendly help message when invoked with the `--help` option, outlining its purpose and available subcommands.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_help",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "NFR-USAB-002",
                "requirement_description": "The `convert` subcommand must provide a user-friendly help message when invoked with `--help`, detailing its arguments and options.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_convert_help",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "NFR-USAB-003",
                "requirement_description": "The `watch` subcommand must provide a user-friendly help message when invoked with `--help`, detailing its arguments and options.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_watch_help",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "NFR-USAB-004",
                "requirement_description": "The `clean` subcommand must provide a user-friendly help message when invoked with `--help`, detailing its arguments and options.",
                "test_traceability": [
                    {
                        "id": "tests/test_cli.py::TestCLI::test_clean_help",
                        "description": ""
                    }
                ],
                "code_traceability": []
            }
        ],
        "commit_sha": "f0496bca017089f52ae8e498b032966f1f096f3c",
        "full_code_skeleton": "--- File: bib4llm/process_bibliography.py ---\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple, Union\nfrom pathlib import Path\n\n@dataclass\nclass ProcessingResult:\n    \"\"\"Result of processing a bibliography entry.\n    \n    Attributes:\n        citation_key: The citation key from the bibliography entry\n        file_hashes: Dictionary mapping file paths to their SHA-256 hashes\n        dir_hash: Hash of the directory contents (excluding linked file contents)\n        success: Whether the processing was successful\n        mupdf_warning_count: Number of MuPDF warnings encountered during processing\n    \"\"\"\n    citation_key: str\n    file_hashes: Dict[str, str]\n    dir_hash: str\n    success: bool\n    mupdf_warning_count: int = 0\n\ndef convert_to_extended_path(path: Path) -> Path:\n    \"\"\"Convert a path to Windows extended-length format if needed.\n    \n    Args:\n        path: Path to convert\n        \n    Returns:\n        Path: Converted path with extended-length format if on Windows and needed\n    \"\"\"\n    pass\n\ndef validate_windows_path(path: Path) -> Optional[str]:\n    \"\"\"Validate a path for Windows compatibility.\n    \n    Args:\n        path: Path to validate\n        \n    Returns:\n        Optional[str]: Error message if path is invalid, None if valid\n    \"\"\"\n    pass\n\ndef standalone_process_entry(args):\n    \"\"\"Process a single bibliography entry in a separate process.\n    \n    Args:\n        args: Tuple of (entry, output_dir, bib_file_path=None)\n            entry: Dictionary containing the bibliography entry data\n            output_dir: Path to the output directory\n            bib_file_path: Optional Path to the BibTeX file (to resolve relative paths)\n            \n    Returns:\n        ProcessingResult: Object containing processing results and status\n    \"\"\"\n    def parse_file_field(file_field: str) -> tuple[list[Path], int]:\n        pass\n    def compute_file_hash(filepath: str) -> str:\n        pass\n    def compute_dir_hash(directory: Path) -> str:\n        pass\n    pass\n\nclass BibliographyProcessor:\n    @staticmethod\n    def get_output_dir(input_file: Path | str) -> Path:\n        \"\"\"Get the output directory path for a bibliography file.\n        \n        Args:\n            input_file: Path to the bibliography file or PDF file\n            \n        Returns:\n            Path to the output directory\n        \"\"\"\n        pass\n\n    @staticmethod\n    def get_log_file(input_file: Path | str) -> Path:\n        \"\"\"Get the log file path for a bibliography file or PDF file.\n        \n        Args:\n            input_file: Path to the bibliography file or PDF file\n            \n        Returns:\n            Path to the log file\n        \"\"\"\n        pass\n\n    @staticmethod\n    def is_pdf_file(file_path: Path) -> bool:\n        \"\"\"Check if the file is a PDF.\n        \n        Args:\n            file_path: Path to check\n            \n        Returns:\n            bool: True if the file is a PDF, False otherwise\n        \"\"\"\n        pass\n\n    def __init__(self, input_path: Union[str, Path], dry_run: bool = False, quiet: bool = False):\n        \"\"\"Initialize the bibliography processor.\n        \n        Args:\n            input_path: Path to the bibliography file, PDF file, or directory to process\n            dry_run: If True, show what would be processed without actually doing it\n            quiet: If True, suppress all output except warnings and errors\n            \n        The processor will create an output directory named '{input_file_stem}-bib4llm'\n        and initialize a SQLite database to track processed files.\n        \"\"\"\n        pass\n\n    def __enter__(self):\n        \"\"\"Context manager entry point - opens database connection.\"\"\"\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit point - closes database connection.\"\"\"\n        pass\n\n    def _compute_file_hash(self, filepath: str) -> str:\n        \"\"\"Compute SHA-256 hash of a file.\n        \n        Args:\n            filepath: Path to the file to hash\n            \n        Returns:\n            str: Hex digest of the file's SHA-256 hash, or empty string on error\n        \"\"\"\n        pass\n\n    def _compute_dir_hash(self, directory: Path) -> str:\n        \"\"\"Compute a hash of a directory's contents.\n        \n        This function hashes:\n        - The relative paths of all files\n        - For regular files: their contents\n        - For symbolic links: their target paths (not the linked content)\n        \n        Args:\n            directory: Path to the directory to hash\n            \n        Returns:\n            str: Hex digest of the directory's SHA-256 hash, or empty string if directory doesn't exist\n        \"\"\"\n        pass\n\n    def _parse_file_field(self, file_field: str) -> tuple[list[Path], int]:\n        \"\"\"Parse the file field from bibtex entry.\n        \n        Handles both standard file paths and Zotero-style file fields\n        (description:filepath format).\n        \n        Args:\n            file_field: The file field string from bibtex entry\n            \n        Returns:\n            tuple: (List[Path], int) where:\n                - List[Path] contains Path objects for files that exist\n                - int is the count of files that were not found\n        \"\"\"\n        pass\n\n    def process_pdf(self, pdf_path: Path, force: bool = False, citation_key: str = None) -> ProcessingResult:\n        \"\"\"Process a single PDF file directly (no BibTeX entry).\n        \n        Args:\n            pdf_path: Path to the PDF file\n            force: Whether to force reprocessing\n            citation_key: Optional citation key to use (default: file stem)\n            \n        Returns:\n            ProcessingResult: Object containing processing results and status\n        \"\"\"\n        pass\n\n    def process_all(self, force: bool = False, num_processes: int = None):\n        \"\"\"Process all entries in the bibliography file or a single PDF.\n        \n        Args:\n            force: Whether to force reprocessing of all entries\n            num_processes: Number of parallel processes to use (default: number of CPU cores)\n        \"\"\"\n        pass\n\nclass DirectoryProcessor:\n    \"\"\"Processor for directories containing BibTeX and PDF files.\"\"\"\n    \n    def __init__(self, directory_path: Union[str, Path], dry_run: bool = False, quiet: bool = False):\n        \"\"\"Initialize the directory processor.\n        \n        Args:\n            directory_path: Path to the directory to process\n            dry_run: If True, show what would be processed without actually doing it\n            quiet: If True, suppress all output except warnings and errors\n        \"\"\"\n        pass\n    \n    def find_files(self, recursive: bool = True) -> Tuple[List[Path], List[Path]]:\n        \"\"\"Find BibTeX and PDF files in the directory.\n        \n        Args:\n            recursive: Whether to search recursively into subdirectories\n            \n        Returns:\n            Tuple containing two lists:\n            - List of BibTeX file paths\n            - List of PDF file paths\n        \"\"\"\n        pass\n    \n    def process_directory(self, recursive: bool = True, force: bool = False, num_processes: int = None) -> Dict:\n        \"\"\"Process all BibTeX and PDF files in the directory.\n        \n        Args:\n            recursive: Whether to recurse into subdirectories\n            force: Whether to force reprocessing of all entries\n            num_processes: Number of parallel processes to use\n            \n        Returns:\n            Dict: Summary of processing results\n        \"\"\"\n        pass\n    \n    def _process_pdf_wrapper(self, args):\n        \"\"\"Wrapper for _process_pdf to use with process_map.\n        \n        Args:\n            args: Tuple of (pdf_path, output_dir, force)\n            \n        Returns:\n            ProcessingResult: Object containing processing results and status\n        \"\"\"\n        pass\n\n    def _process_pdf(self, pdf_path: Path, output_dir: Path, force: bool = False) -> ProcessingResult:\n        \"\"\"Process a single PDF file.\n        \n        Args:\n            pdf_path: Path to the PDF file\n            output_dir: Directory to store the output\n            force: Whether to force reprocessing\n            \n        Returns:\n            ProcessingResult: Object containing processing results and status\n        \"\"\"\n        pass\n\n    def _compute_file_hash(self, filepath: str) -> str:\n        \"\"\"Compute SHA-256 hash of a file.\n        \n        Args:\n            filepath: Path to the file to hash\n            \n        Returns:\n            str: Hex digest of the file's SHA-256 hash, or empty string on error\n        \"\"\"\n        pass\n\n    def _compute_dir_hash(self, directory: Path) -> str:\n        \"\"\"Compute a hash of a directory's contents.\n        \n        This function hashes:\n        - The relative paths of all files\n        - For regular files: their contents\n        - For symbolic links: their target paths (not the linked content)\n        \n        Args:\n            directory: Path to the directory to hash\n            \n        Returns:\n            str: Hex digest of the directory's SHA-256 hash, or empty string if directory doesn't exist\n        \"\"\"\n        pass\n```\n--- File: bib4llm/watcher.py ---\n```python\nfrom pathlib import Path\nfrom watchdog.events import FileSystemEventHandler\n\nclass BibTexHandler(FileSystemEventHandler):\n    def __init__(self, bib_file: Path, num_processes: int = None):\n        pass\n\n    def on_modified(self, event):\n        pass\n\n    def _process(self):\n        \"\"\"Process the bibliography file.\"\"\"\n        pass\n\nclass PDFHandler(FileSystemEventHandler):\n    def __init__(self, pdf_file: Path, num_processes: int = None):\n        pass\n\n    def on_modified(self, event):\n        pass\n\n    def _process(self):\n        \"\"\"Process the PDF file.\"\"\"\n        pass\n\nclass DirectoryHandler(FileSystemEventHandler):\n    def __init__(self, directory_path: Path, recursive: bool = True, num_processes: int = None):\n        pass\n\n    def on_created(self, event):\n        \"\"\"Handle file creation events.\"\"\"\n        pass\n\n    def on_modified(self, event):\n        \"\"\"Handle file modification events.\"\"\"\n        pass\n            \n    def _process_file(self, file_path: Path):\n        \"\"\"Process a single file if it's a PDF or BibTeX file.\"\"\"\n        pass\n    \n    def _process_directory(self):\n        \"\"\"Process the entire directory.\"\"\"\n        pass\n\ndef watch_bibtex(bib_file: Path, num_processes: int = None):\n    \"\"\"Watch a BibTeX file for changes and process it automatically.\n    \n    Args:\n        bib_file: Path to the BibTeX file to watch\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n\ndef watch_pdf(pdf_file: Path, num_processes: int = None):\n    \"\"\"Watch a PDF file for changes and process it automatically.\n    \n    Args:\n        pdf_file: Path to the PDF file to watch\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n\ndef watch_directory(directory_path: Path, recursive: bool = True, num_processes: int = None):\n    \"\"\"Watch a directory for changes and process files automatically.\n    \n    Args:\n        directory_path: Path to the directory to watch\n        recursive: Whether to watch subdirectories recursively (default: True)\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n```\n--- File: bib4llm/cli.py ---\n```python\nfrom pathlib import Path\n\ndef setup_logging(debug: bool, quiet: bool, input_path: Path, log_file: Path = None):\n    \"\"\"Set up logging configuration with separate handlers for console and file.\n    \n    Args:\n        debug: Whether to show debug messages in console\n        quiet: Whether to suppress info messages in console\n        input_path: Path to the input file or directory, used to determine log file location\n        log_file: Optional path to log file. If not provided, will use default location\n    \"\"\"\n    pass\n\ndef main():\n    pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "bib4llm/process_bibliography.py",
                "code": "from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple, Union\nfrom pathlib import Path\n\n@dataclass\nclass ProcessingResult:\n    \"\"\"Result of processing a bibliography entry.\n    \n    Attributes:\n        citation_key: The citation key from the bibliography entry\n        file_hashes: Dictionary mapping file paths to their SHA-256 hashes\n        dir_hash: Hash of the directory contents (excluding linked file contents)\n        success: Whether the processing was successful\n        mupdf_warning_count: Number of MuPDF warnings encountered during processing\n    \"\"\"\n    citation_key: str\n    file_hashes: Dict[str, str]\n    dir_hash: str\n    success: bool\n    mupdf_warning_count: int = 0\n\ndef convert_to_extended_path(path: Path) -> Path:\n    \"\"\"Convert a path to Windows extended-length format if needed.\n    \n    Args:\n        path: Path to convert\n        \n    Returns:\n        Path: Converted path with extended-length format if on Windows and needed\n    \"\"\"\n    pass\n\ndef validate_windows_path(path: Path) -> Optional[str]:\n    \"\"\"Validate a path for Windows compatibility.\n    \n    Args:\n        path: Path to validate\n        \n    Returns:\n        Optional[str]: Error message if path is invalid, None if valid\n    \"\"\"\n    pass\n\ndef standalone_process_entry(args):\n    \"\"\"Process a single bibliography entry in a separate process.\n    \n    Args:\n        args: Tuple of (entry, output_dir, bib_file_path=None)\n            entry: Dictionary containing the bibliography entry data\n            output_dir: Path to the output directory\n            bib_file_path: Optional Path to the BibTeX file (to resolve relative paths)\n            \n    Returns:\n        ProcessingResult: Object containing processing results and status\n    \"\"\"\n    def parse_file_field(file_field: str) -> tuple[list[Path], int]:\n        pass\n    def compute_file_hash(filepath: str) -> str:\n        pass\n    def compute_dir_hash(directory: Path) -> str:\n        pass\n    pass\n\nclass BibliographyProcessor:\n    @staticmethod\n    def get_output_dir(input_file: Path | str) -> Path:\n        \"\"\"Get the output directory path for a bibliography file.\n        \n        Args:\n            input_file: Path to the bibliography file or PDF file\n            \n        Returns:\n            Path to the output directory\n        \"\"\"\n        pass\n\n    @staticmethod\n    def get_log_file(input_file: Path | str) -> Path:\n        \"\"\"Get the log file path for a bibliography file or PDF file.\n        \n        Args:\n            input_file: Path to the bibliography file or PDF file\n            \n        Returns:\n            Path to the log file\n        \"\"\"\n        pass\n\n    @staticmethod\n    def is_pdf_file(file_path: Path) -> bool:\n        \"\"\"Check if the file is a PDF.\n        \n        Args:\n            file_path: Path to check\n            \n        Returns:\n            bool: True if the file is a PDF, False otherwise\n        \"\"\"\n        pass\n\n    def __init__(self, input_path: Union[str, Path], dry_run: bool = False, quiet: bool = False):\n        \"\"\"Initialize the bibliography processor.\n        \n        Args:\n            input_path: Path to the bibliography file, PDF file, or directory to process\n            dry_run: If True, show what would be processed without actually doing it\n            quiet: If True, suppress all output except warnings and errors\n            \n        The processor will create an output directory named '{input_file_stem}-bib4llm'\n        and initialize a SQLite database to track processed files.\n        \"\"\"\n        pass\n\n    def __enter__(self):\n        \"\"\"Context manager entry point - opens database connection.\"\"\"\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit point - closes database connection.\"\"\"\n        pass\n\n    def _compute_file_hash(self, filepath: str) -> str:\n        \"\"\"Compute SHA-256 hash of a file.\n        \n        Args:\n            filepath: Path to the file to hash\n            \n        Returns:\n            str: Hex digest of the file's SHA-256 hash, or empty string on error\n        \"\"\"\n        pass\n\n    def _compute_dir_hash(self, directory: Path) -> str:\n        \"\"\"Compute a hash of a directory's contents.\n        \n        This function hashes:\n        - The relative paths of all files\n        - For regular files: their contents\n        - For symbolic links: their target paths (not the linked content)\n        \n        Args:\n            directory: Path to the directory to hash\n            \n        Returns:\n            str: Hex digest of the directory's SHA-256 hash, or empty string if directory doesn't exist\n        \"\"\"\n        pass\n\n    def _parse_file_field(self, file_field: str) -> tuple[list[Path], int]:\n        \"\"\"Parse the file field from bibtex entry.\n        \n        Handles both standard file paths and Zotero-style file fields\n        (description:filepath format).\n        \n        Args:\n            file_field: The file field string from bibtex entry\n            \n        Returns:\n            tuple: (List[Path], int) where:\n                - List[Path] contains Path objects for files that exist\n                - int is the count of files that were not found\n        \"\"\"\n        pass\n\n    def process_pdf(self, pdf_path: Path, force: bool = False, citation_key: str = None) -> ProcessingResult:\n        \"\"\"Process a single PDF file directly (no BibTeX entry).\n        \n        Args:\n            pdf_path: Path to the PDF file\n            force: Whether to force reprocessing\n            citation_key: Optional citation key to use (default: file stem)\n            \n        Returns:\n            ProcessingResult: Object containing processing results and status\n        \"\"\"\n        pass\n\n    def process_all(self, force: bool = False, num_processes: int = None):\n        \"\"\"Process all entries in the bibliography file or a single PDF.\n        \n        Args:\n            force: Whether to force reprocessing of all entries\n            num_processes: Number of parallel processes to use (default: number of CPU cores)\n        \"\"\"\n        pass\n\nclass DirectoryProcessor:\n    \"\"\"Processor for directories containing BibTeX and PDF files.\"\"\"\n    \n    def __init__(self, directory_path: Union[str, Path], dry_run: bool = False, quiet: bool = False):\n        \"\"\"Initialize the directory processor.\n        \n        Args:\n            directory_path: Path to the directory to process\n            dry_run: If True, show what would be processed without actually doing it\n            quiet: If True, suppress all output except warnings and errors\n        \"\"\"\n        pass\n    \n    def find_files(self, recursive: bool = True) -> Tuple[List[Path], List[Path]]:\n        \"\"\"Find BibTeX and PDF files in the directory.\n        \n        Args:\n            recursive: Whether to search recursively into subdirectories\n            \n        Returns:\n            Tuple containing two lists:\n            - List of BibTeX file paths\n            - List of PDF file paths\n        \"\"\"\n        pass\n    \n    def process_directory(self, recursive: bool = True, force: bool = False, num_processes: int = None) -> Dict:\n        \"\"\"Process all BibTeX and PDF files in the directory.\n        \n        Args:\n            recursive: Whether to recurse into subdirectories\n            force: Whether to force reprocessing of all entries\n            num_processes: Number of parallel processes to use\n            \n        Returns:\n            Dict: Summary of processing results\n        \"\"\"\n        pass\n    \n    def _process_pdf_wrapper(self, args):\n        \"\"\"Wrapper for _process_pdf to use with process_map.\n        \n        Args:\n            args: Tuple of (pdf_path, output_dir, force)\n            \n        Returns:\n            ProcessingResult: Object containing processing results and status\n        \"\"\"\n        pass\n\n    def _process_pdf(self, pdf_path: Path, output_dir: Path, force: bool = False) -> ProcessingResult:\n        \"\"\"Process a single PDF file.\n        \n        Args:\n            pdf_path: Path to the PDF file\n            output_dir: Directory to store the output\n            force: Whether to force reprocessing\n            \n        Returns:\n            ProcessingResult: Object containing processing results and status\n        \"\"\"\n        pass\n\n    def _compute_file_hash(self, filepath: str) -> str:\n        \"\"\"Compute SHA-256 hash of a file.\n        \n        Args:\n            filepath: Path to the file to hash\n            \n        Returns:\n            str: Hex digest of the file's SHA-256 hash, or empty string on error\n        \"\"\"\n        pass\n\n    def _compute_dir_hash(self, directory: Path) -> str:\n        \"\"\"Compute a hash of a directory's contents.\n        \n        This function hashes:\n        - The relative paths of all files\n        - For regular files: their contents\n        - For symbolic links: their target paths (not the linked content)\n        \n        Args:\n            directory: Path to the directory to hash\n            \n        Returns:\n            str: Hex digest of the directory's SHA-256 hash, or empty string if directory doesn't exist\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "bib4llm/watcher.py",
                "code": "from pathlib import Path\nfrom watchdog.events import FileSystemEventHandler\n\nclass BibTexHandler(FileSystemEventHandler):\n    def __init__(self, bib_file: Path, num_processes: int = None):\n        pass\n\n    def on_modified(self, event):\n        pass\n\n    def _process(self):\n        \"\"\"Process the bibliography file.\"\"\"\n        pass\n\nclass PDFHandler(FileSystemEventHandler):\n    def __init__(self, pdf_file: Path, num_processes: int = None):\n        pass\n\n    def on_modified(self, event):\n        pass\n\n    def _process(self):\n        \"\"\"Process the PDF file.\"\"\"\n        pass\n\nclass DirectoryHandler(FileSystemEventHandler):\n    def __init__(self, directory_path: Path, recursive: bool = True, num_processes: int = None):\n        pass\n\n    def on_created(self, event):\n        \"\"\"Handle file creation events.\"\"\"\n        pass\n\n    def on_modified(self, event):\n        \"\"\"Handle file modification events.\"\"\"\n        pass\n            \n    def _process_file(self, file_path: Path):\n        \"\"\"Process a single file if it's a PDF or BibTeX file.\"\"\"\n        pass\n    \n    def _process_directory(self):\n        \"\"\"Process the entire directory.\"\"\"\n        pass\n\ndef watch_bibtex(bib_file: Path, num_processes: int = None):\n    \"\"\"Watch a BibTeX file for changes and process it automatically.\n    \n    Args:\n        bib_file: Path to the BibTeX file to watch\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n\ndef watch_pdf(pdf_file: Path, num_processes: int = None):\n    \"\"\"Watch a PDF file for changes and process it automatically.\n    \n    Args:\n        pdf_file: Path to the PDF file to watch\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n\ndef watch_directory(directory_path: Path, recursive: bool = True, num_processes: int = None):\n    \"\"\"Watch a directory for changes and process files automatically.\n    \n    Args:\n        directory_path: Path to the directory to watch\n        recursive: Whether to watch subdirectories recursively (default: True)\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "bib4llm/cli.py",
                "code": "from pathlib import Path\n\ndef setup_logging(debug: bool, quiet: bool, input_path: Path, log_file: Path = None):\n    \"\"\"Set up logging configuration with separate handlers for console and file.\n    \n    Args:\n        debug: Whether to show debug messages in console\n        quiet: Whether to suppress info messages in console\n        input_path: Path to the input file or directory, used to determine log file location\n        log_file: Optional path to log file. If not provided, will use default location\n    \"\"\"\n    pass\n\ndef main():\n    pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: bib4llm/cli.py ---\n```python\ndef main():\n    pass\n```\n--- File: bib4llm/process_bibliography.py ---\n```python\nfrom dataclasses import dataclass # This import is technically against the rules for the skeleton itself, but @dataclass is part of the class definition structure. The prompt is a bit ambiguous here. Given \"Class definitions (signature only, as extracted)\" and dataclass defines the implicit signature, it's more useful to include it. For a stricter interpretation, one might remove this import and the decorator, and expect the user to define __init__ etc. manually. However, showing it as a dataclass is more faithful to the original structure's intent for the skeleton's user. I will proceed with including it for clarity of the class structure.\n# If the above import and decorator are disallowed, the ProcessingResult class would be:\n# class ProcessingResult:\n# \"\"\"Result of processing a bibliography entry.\n# ...\n# \"\"\"\n# def __init__(self, citation_key: str, file_hashes: Dict[str, str], dir_hash: str, success: bool, mupdf_warning_count: int = 0):\n#         pass\n\n# To strictly follow \"NO Import Statements\", the user of the skeleton would add `from dataclasses import dataclass`\n# and `from typing import ...` and `from pathlib import Path`.\n# For the skeleton itself, I will include the decorators like @dataclass and @staticmethod\n# as they are part of the definition syntax.\n\n@dataclass\nclass ProcessingResult:\n    \"\"\"Result of processing a bibliography entry.\n    \n    Attributes:\n        citation_key: The citation key from the bibliography entry\n        file_hashes: Dictionary mapping file paths to their SHA-256 hashes\n        dir_hash: Hash of the directory contents (excluding linked file contents)\n        success: Whether the processing was successful\n        mupdf_warning_count: Number of MuPDF warnings encountered during processing\n    \"\"\"\n    citation_key: str\n    file_hashes: Dict[str, str]\n    dir_hash: str\n    success: bool\n    mupdf_warning_count: int = 0\n\ndef standalone_process_entry(args):\n    \"\"\"Process a single bibliography entry in a separate process.\n    \n    Args:\n        args: Tuple of (entry, output_dir, bib_file_path=None)\n            entry: Dictionary containing the bibliography entry data\n            output_dir: Path to the output directory\n            bib_file_path: Optional Path to the BibTeX file (to resolve relative paths)\n            \n    Returns:\n        ProcessingResult: Object containing processing results and status\n    \"\"\"\n    pass\n\nclass BibliographyProcessor:\n    @staticmethod\n    def get_output_dir(input_file: Path | str) -> Path:\n        \"\"\"Get the output directory path for a bibliography file.\n        \n        Args:\n            input_file: Path to the bibliography file or PDF file\n            \n        Returns:\n            Path to the output directory\n        \"\"\"\n        pass\n\n    @staticmethod\n    def get_log_file(input_file: Path | str) -> Path:\n        \"\"\"Get the log file path for a bibliography file or PDF file.\n        \n        Args:\n            input_file: Path to the bibliography file or PDF file\n            \n        Returns:\n            Path to the log file\n        \"\"\"\n        pass\n\n    @staticmethod\n    def is_pdf_file(file_path: Path) -> bool:\n        \"\"\"Check if the file is a PDF.\n        \n        Args:\n            file_path: Path to check\n            \n        Returns:\n            bool: True if the file is a PDF, False otherwise\n        \"\"\"\n        pass\n\n    def __init__(self, input_path: Union[str, Path], dry_run: bool = False, quiet: bool = False):\n        \"\"\"Initialize the bibliography processor.\n        \n        Args:\n            input_path: Path to the bibliography file, PDF file, or directory to process\n            dry_run: If True, show what would be processed without actually doing it\n            quiet: If True, suppress all output except warnings and errors\n            \n        The processor will create an output directory named '{input_file_stem}-bib4llm'\n        and initialize a SQLite database to track processed files.\n        \"\"\"\n        pass\n\n    def __enter__(self):\n        \"\"\"Context manager entry point - opens database connection.\"\"\"\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit point - closes database connection.\"\"\"\n        pass\n\n    def _parse_file_field(self, file_field: str) -> tuple[list[Path], int]:\n        \"\"\"Parse the file field from bibtex entry.\n        \n        Handles both standard file paths and Zotero-style file fields\n        (description:filepath format).\n        \n        Args:\n            file_field: The file field string from bibtex entry\n            \n        Returns:\n            tuple: (List[Path], int) where:\n                - List[Path] contains Path objects for files that exist\n                - int is the count of files that were not found\n        \"\"\"\n        pass\n\n    def process_pdf(self, pdf_path: Path, force: bool = False, citation_key: str = None) -> ProcessingResult:\n        \"\"\"Process a single PDF file directly (no BibTeX entry).\n        \n        Args:\n            pdf_path: Path to the PDF file\n            force: Whether to force reprocessing\n            citation_key: Optional citation key to use (default: file stem)\n            \n        Returns:\n            ProcessingResult: Object containing processing results and status\n        \"\"\"\n        pass\n\n    def process_all(self, force: bool = False, num_processes: int = None):\n        \"\"\"Process all entries in the bibliography file or a single PDF.\n        \n        Args:\n            force: Whether to force reprocessing of all entries\n            num_processes: Number of parallel processes to use (default: number of CPU cores)\n        \"\"\"\n        pass\n\nclass DirectoryProcessor:\n    \"\"\"Processor for directories containing BibTeX and PDF files.\"\"\"\n\n    def __init__(self, directory_path: Union[str, Path], dry_run: bool = False, quiet: bool = False):\n        \"\"\"Initialize the directory processor.\n        \n        Args:\n            directory_path: Path to the directory to process\n            dry_run: If True, show what would be processed without actually doing it\n            quiet: If True, suppress all output except warnings and errors\n        \"\"\"\n        pass\n    \n    def process_directory(self, recursive: bool = True, force: bool = False, num_processes: int = None) -> Dict:\n        \"\"\"Process all BibTeX and PDF files in the directory.\n        \n        Args:\n            recursive: Whether to recurse into subdirectories\n            force: Whether to force reprocessing of all entries\n            num_processes: Number of parallel processes to use\n            \n        Returns:\n            Dict: Summary of processing results\n        \"\"\"\n        pass\n```\n--- File: bib4llm/watcher.py ---\n```python\ndef watch_bibtex(bib_file: Path, num_processes: int = None):\n    \"\"\"Watch a BibTeX file for changes and process it automatically.\n    \n    Args:\n        bib_file: Path to the BibTeX file to watch\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n\ndef watch_pdf(pdf_file: Path, num_processes: int = None):\n    \"\"\"Watch a PDF file for changes and process it automatically.\n    \n    Args:\n        pdf_file: Path to the PDF file to watch\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n\ndef watch_directory(directory_path: Path, recursive: bool = True, num_processes: int = None):\n    \"\"\"Watch a directory for changes and process files automatically.\n    \n    Args:\n        directory_path: Path to the directory to watch\n        recursive: Whether to watch subdirectories recursively (default: True)\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "bib4llm/cli.py",
                "code": "def main():\n    pass\n"
            },
            {
                "file_path": "bib4llm/process_bibliography.py",
                "code": "from dataclasses import dataclass # This import is technically against the rules for the skeleton itself, but @dataclass is part of the class definition structure. The prompt is a bit ambiguous here. Given \"Class definitions (signature only, as extracted)\" and dataclass defines the implicit signature, it's more useful to include it. For a stricter interpretation, one might remove this import and the decorator, and expect the user to define __init__ etc. manually. However, showing it as a dataclass is more faithful to the original structure's intent for the skeleton's user. I will proceed with including it for clarity of the class structure.\n# If the above import and decorator are disallowed, the ProcessingResult class would be:\n# class ProcessingResult:\n# \"\"\"Result of processing a bibliography entry.\n# ...\n# \"\"\"\n# def __init__(self, citation_key: str, file_hashes: Dict[str, str], dir_hash: str, success: bool, mupdf_warning_count: int = 0):\n#         pass\n\n# To strictly follow \"NO Import Statements\", the user of the skeleton would add `from dataclasses import dataclass`\n# and `from typing import ...` and `from pathlib import Path`.\n# For the skeleton itself, I will include the decorators like @dataclass and @staticmethod\n# as they are part of the definition syntax.\n\n@dataclass\nclass ProcessingResult:\n    \"\"\"Result of processing a bibliography entry.\n    \n    Attributes:\n        citation_key: The citation key from the bibliography entry\n        file_hashes: Dictionary mapping file paths to their SHA-256 hashes\n        dir_hash: Hash of the directory contents (excluding linked file contents)\n        success: Whether the processing was successful\n        mupdf_warning_count: Number of MuPDF warnings encountered during processing\n    \"\"\"\n    citation_key: str\n    file_hashes: Dict[str, str]\n    dir_hash: str\n    success: bool\n    mupdf_warning_count: int = 0\n\ndef standalone_process_entry(args):\n    \"\"\"Process a single bibliography entry in a separate process.\n    \n    Args:\n        args: Tuple of (entry, output_dir, bib_file_path=None)\n            entry: Dictionary containing the bibliography entry data\n            output_dir: Path to the output directory\n            bib_file_path: Optional Path to the BibTeX file (to resolve relative paths)\n            \n    Returns:\n        ProcessingResult: Object containing processing results and status\n    \"\"\"\n    pass\n\nclass BibliographyProcessor:\n    @staticmethod\n    def get_output_dir(input_file: Path | str) -> Path:\n        \"\"\"Get the output directory path for a bibliography file.\n        \n        Args:\n            input_file: Path to the bibliography file or PDF file\n            \n        Returns:\n            Path to the output directory\n        \"\"\"\n        pass\n\n    @staticmethod\n    def get_log_file(input_file: Path | str) -> Path:\n        \"\"\"Get the log file path for a bibliography file or PDF file.\n        \n        Args:\n            input_file: Path to the bibliography file or PDF file\n            \n        Returns:\n            Path to the log file\n        \"\"\"\n        pass\n\n    @staticmethod\n    def is_pdf_file(file_path: Path) -> bool:\n        \"\"\"Check if the file is a PDF.\n        \n        Args:\n            file_path: Path to check\n            \n        Returns:\n            bool: True if the file is a PDF, False otherwise\n        \"\"\"\n        pass\n\n    def __init__(self, input_path: Union[str, Path], dry_run: bool = False, quiet: bool = False):\n        \"\"\"Initialize the bibliography processor.\n        \n        Args:\n            input_path: Path to the bibliography file, PDF file, or directory to process\n            dry_run: If True, show what would be processed without actually doing it\n            quiet: If True, suppress all output except warnings and errors\n            \n        The processor will create an output directory named '{input_file_stem}-bib4llm'\n        and initialize a SQLite database to track processed files.\n        \"\"\"\n        pass\n\n    def __enter__(self):\n        \"\"\"Context manager entry point - opens database connection.\"\"\"\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit point - closes database connection.\"\"\"\n        pass\n\n    def _parse_file_field(self, file_field: str) -> tuple[list[Path], int]:\n        \"\"\"Parse the file field from bibtex entry.\n        \n        Handles both standard file paths and Zotero-style file fields\n        (description:filepath format).\n        \n        Args:\n            file_field: The file field string from bibtex entry\n            \n        Returns:\n            tuple: (List[Path], int) where:\n                - List[Path] contains Path objects for files that exist\n                - int is the count of files that were not found\n        \"\"\"\n        pass\n\n    def process_pdf(self, pdf_path: Path, force: bool = False, citation_key: str = None) -> ProcessingResult:\n        \"\"\"Process a single PDF file directly (no BibTeX entry).\n        \n        Args:\n            pdf_path: Path to the PDF file\n            force: Whether to force reprocessing\n            citation_key: Optional citation key to use (default: file stem)\n            \n        Returns:\n            ProcessingResult: Object containing processing results and status\n        \"\"\"\n        pass\n\n    def process_all(self, force: bool = False, num_processes: int = None):\n        \"\"\"Process all entries in the bibliography file or a single PDF.\n        \n        Args:\n            force: Whether to force reprocessing of all entries\n            num_processes: Number of parallel processes to use (default: number of CPU cores)\n        \"\"\"\n        pass\n\nclass DirectoryProcessor:\n    \"\"\"Processor for directories containing BibTeX and PDF files.\"\"\"\n\n    def __init__(self, directory_path: Union[str, Path], dry_run: bool = False, quiet: bool = False):\n        \"\"\"Initialize the directory processor.\n        \n        Args:\n            directory_path: Path to the directory to process\n            dry_run: If True, show what would be processed without actually doing it\n            quiet: If True, suppress all output except warnings and errors\n        \"\"\"\n        pass\n    \n    def process_directory(self, recursive: bool = True, force: bool = False, num_processes: int = None) -> Dict:\n        \"\"\"Process all BibTeX and PDF files in the directory.\n        \n        Args:\n            recursive: Whether to recurse into subdirectories\n            force: Whether to force reprocessing of all entries\n            num_processes: Number of parallel processes to use\n            \n        Returns:\n            Dict: Summary of processing results\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "bib4llm/watcher.py",
                "code": "def watch_bibtex(bib_file: Path, num_processes: int = None):\n    \"\"\"Watch a BibTeX file for changes and process it automatically.\n    \n    Args:\n        bib_file: Path to the BibTeX file to watch\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n\ndef watch_pdf(pdf_file: Path, num_processes: int = None):\n    \"\"\"Watch a PDF file for changes and process it automatically.\n    \n    Args:\n        pdf_file: Path to the PDF file to watch\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n\ndef watch_directory(directory_path: Path, recursive: bool = True, num_processes: int = None):\n    \"\"\"Watch a directory for changes and process files automatically.\n    \n    Args:\n        directory_path: Path to the directory to watch\n        recursive: Whether to watch subdirectories recursively (default: True)\n        num_processes: Number of parallel processes to use (default: number of CPU cores)\n    \"\"\"\n    pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_init",
                "covers": [
                    "bib4llm.process_bibliography.BibliographyProcessor.__init__ - happy path, default options initialization"
                ]
            },
            {
                "test_id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_init_with_dry_run",
                "covers": [
                    "bib4llm.process_bibliography.BibliographyProcessor.__init__ - initialization with dry_run=True"
                ]
            },
            {
                "test_id": "tests/test_basic.py::TestBibliographyProcessor::test_get_output_dir",
                "covers": [
                    "bib4llm.process_bibliography.BibliographyProcessor.get_output_dir - happy path, various input types"
                ]
            },
            {
                "test_id": "tests/test_basic.py::TestBibliographyProcessor::test_get_log_file",
                "covers": [
                    "bib4llm.process_bibliography.BibliographyProcessor.get_log_file - happy path, various input types"
                ]
            },
            {
                "test_id": "tests/test_conversion.py::TestConversion::test_conversion_output_structure",
                "covers": [
                    "bib4llm.process_bibliography.BibliographyProcessor.process_all - BibTeX processing with force and multiprocessing options",
                    "bib4llm.process_bibliography.BibliographyProcessor.__init__ - initialization for BibTeX processing"
                ]
            },
            {
                "test_id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_context_manager",
                "covers": [
                    "bib4llm.process_bibliography.BibliographyProcessor - context manager usage (__enter__, __exit__)",
                    "bib4llm.process_bibliography.BibliographyProcessor.__init__ - initialization within context manager"
                ]
            },
            {
                "test_id": "tests/test_process_bibliography.py::TestBibliographyProcessor::test_parse_file_field_with_paths",
                "covers": [
                    "bib4llm.process_bibliography.BibliographyProcessor._parse_file_field - testing private method for path parsing (as exercised by tests)",
                    "bib4llm.process_bibliography.BibliographyProcessor.__init__ - initialization for path parsing test"
                ]
            },
            {
                "test_id": "tests/test_conversion.py::TestConversion::test_pdf_subdirectory_conversion",
                "covers": [
                    "bib4llm.process_bibliography.DirectoryProcessor.process_directory - happy path, recursive PDF directory processing",
                    "bib4llm.process_bibliography.DirectoryProcessor.__init__ - initialization for directory processing",
                    "bib4llm.process_bibliography.DirectoryProcessor.find_files - (indirectly) finding files in directory",
                    "bib4llm.process_bibliography.BibliographyProcessor.is_pdf_file - (indirectly) checking PDF files during directory scan"
                ]
            },
            {
                "test_id": "tests/test_pdf_processing.py::TestPDFProcessing::test_pdf_processing_with_custom_options",
                "covers": [
                    "bib4llm.process_bibliography.DirectoryProcessor.process_directory - PDF directory processing with force=True option",
                    "bib4llm.process_bibliography.DirectoryProcessor.__init__ - initialization for directory processing"
                ]
            },
            {
                "test_id": "tests/test_cli.py::TestCLI::test_processes_option",
                "covers": [
                    "bib4llm.process_bibliography.DirectoryProcessor.__init__ - initialization (exercised via CLI for directory processing)",
                    "bib4llm.process_bibliography.DirectoryProcessor.process_directory - num_processes parameter usage (exercised via CLI)"
                ]
            },
            {
                "test_id": "tests/test_process_bibliography.py::TestProcessingResult::test_init",
                "covers": [
                    "bib4llm.process_bibliography.ProcessingResult.__init__ - dataclass structure and initialization"
                ]
            }
        ]
    },
    {
        "idx": 104980,
        "repo_name": "Maxim-Mushizky_cstructpy",
        "url": "https://github.com/Maxim-Mushizky/cstructpy",
        "description": "A package that allows to create data models classes for c like structs that eases sending and reading binary messages",
        "stars": 19,
        "forks": 1,
        "language": "python",
        "size": 63,
        "created_at": "2024-10-22T13:00:21+00:00",
        "updated_at": "2025-02-03T15:15:53+00:00",
        "pypi_info": {
            "name": "cstructpy",
            "version": "0.2.2",
            "url": "https://files.pythonhosted.org/packages/a3/81/21bd5e50e84b65ede2bcc48e39720740421095e39af23249b97fcf8ea058/cstructpy-0.2.2.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 7,
            "comment_ratio": 0.12731229597388466,
            "pyfile_content_length": 31813,
            "pyfile_code_lines": 919,
            "test_file_exist": true,
            "test_file_content_length": 14703,
            "pytest_framework": true,
            "test_case_num": 32,
            "metadata_path": [
                "pyproject.toml"
            ],
            "readme_content_length": 5198,
            "llm_reason": "The project 'cstructpy' is a strong candidate for an AI 'Build from Scratch' benchmark.\n\nPositive Aspects:\n*   **Self-Contained & Independent:** The core functionality of serializing/deserializing binary data using Python's `struct` module is entirely self-contained. It does not require internet access, external APIs, or complex external applications for its operation or testing. The AI-rebuilt solution would also be self-contained.\n*   **Clear & Well-Defined Functionality:** The README provides a clear explanation of the project's purpose (C-like binary data structures), features (primitive types, arrays, validation, packing/unpacking), and usage examples. This forms a good basis for the AI's task specification.\n*   **Testable & Verifiable Output:** The project includes a comprehensive suite of unit tests (`unit_tests/test_data_models.py` using `pytest`). These tests cover various data types, arrays, validation, and edge cases. The output (packed binary data or unpacked structures) is programmatically verifiable.\n*   **No Graphical User Interface (GUI):** The project is a library, intended for programmatic use, making it suitable for automated testing without GUI interaction.\n*   **Appropriate Complexity & Scope (Medium):** The project is non-trivial. It requires implementing a `GenericStruct` base class that likely uses introspection on type hints, a hierarchy of `PrimitiveType` classes with specific behaviors (format characters, validation, packing/unpacking logic), and support for fixed-size arrays (e.g., `UINT64[3]`, likely using `__class_getitem__`). This involves moderately complex object-oriented design and understanding of Python's type system and `struct` module. The codebase (approx. 3 main source files) is manageable for an AI to replicate within a reasonable timeframe (hours to a few days for a human equivalent).\n*   **Well-Understood Problem Domain:** Binary data serialization and C-like structures are common concepts in programming.\n*   **Predominantly Code-Based Solution:** The task is to generate Python code for the library's functionality.\n\nNegative Aspects or Concerns:\n*   **Moderately Advanced Python Features:** The dynamic nature of `GenericStruct` (processing type annotations to define struct fields) and the implementation of array types (e.g., `INT16[4]`) using mechanisms like `__class_getitem__` in `PrimitiveType` are moderately advanced Python concepts. While well-defined by the existing project, replicating this exact dynamic behavior from scratch could be challenging for an AI and would require careful attention to the original project's API and behavior as the specification. This, however, also contributes to its value as a 'Medium' difficulty benchmark.\n*   **Clarity on Pydantic Mention:** The README mentions that the package is 'best when used alongside pydantic.BaseModel or dataclasses.dataclass'. It should be clear that the task for the AI is to rebuild `cstructpy` itself, which does not depend on Pydantic for its own core implementation, but rather offers a similar declarative style. This is a minor point for specification clarity.\n\nOverall, the project is well-suited because it presents a realistic, self-contained coding challenge of moderate complexity with excellent testability.",
            "llm_project_type": "Python library for binary data serialization and deserialization with C-like type definitions and validation",
            "llm_rating": 85,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "Maxim-Mushizky_cstructpy",
            "finish_test": true,
            "test_case_result": {
                "unit_tests/test_data_models.py::TestInt16Type::test_valid_values": "passed",
                "unit_tests/test_data_models.py::TestInt16Type::test_invalid_values": "passed",
                "unit_tests/test_data_models.py::TestBooleanType::test_valid_values": "passed",
                "unit_tests/test_data_models.py::TestBooleanType::test_invalid_values": "passed",
                "unit_tests/test_data_models.py::TestBooleanType::test_pack_unpack": "passed",
                "unit_tests/test_data_models.py::TestCharType::test_valid_values": "passed",
                "unit_tests/test_data_models.py::TestCharType::test_invalid_values": "passed",
                "unit_tests/test_data_models.py::TestCharType::test_pack_unpack": "passed",
                "unit_tests/test_data_models.py::TestCharArrayType::test_valid_values": "passed",
                "unit_tests/test_data_models.py::TestCharArrayType::test_invalid_values": "passed",
                "unit_tests/test_data_models.py::TestCharArrayType::test_pack_unpack_with_padding": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT8--128-127-1]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT8-0-255-1]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT16--32768-32767-2]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT16-0-65535-2]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT32--2147483648-2147483647-4]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT32-0-4294967295-4]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT64--9223372036854775808-9223372036854775807-8]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT64-0-18446744073709551615-8]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT8--128-127-1]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT8-0-255-1]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT16--32768-32767-2]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT16-0-65535-2]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT32--2147483648-2147483647-4]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT32-0-4294967295-4]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT64--9223372036854775808-9223372036854775807-8]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT64-0-18446744073709551615-8]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT8--128-127-1]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT8-0-255-1]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT16--32768-32767-2]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT16-0-65535-2]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT32--2147483648-2147483647-4]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT32-0-4294967295-4]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT64--9223372036854775808-9223372036854775807-8]": "passed",
                "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT64-0-18446744073709551615-8]": "passed",
                "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[FLOAT-4-6]": "passed",
                "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[DOUBLE-8-15]": "passed",
                "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values[FLOAT]": "passed",
                "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values[DOUBLE]": "passed",
                "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_creation": "passed",
                "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_pack_unpack": "passed",
                "unit_tests/test_data_models.py::TestErrorHandling::test_invalid_field_name": "passed",
                "unit_tests/test_data_models.py::TestErrorHandling::test_missing_required_field": "passed",
                "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives": "passed",
                "unit_tests/test_data_models.py::TestArrayCreation::test_char_not_used_as_array": "passed",
                "unit_tests/test_data_models.py::TestArrayCreation::test_array_length_fixed": "passed",
                "unit_tests/test_data_models.py::TestClassDefaults::test_default_creation": "passed",
                "unit_tests/test_data_models.py::TestClassDefaults::test_defaults_raise_exceptions": "passed",
                "unit_tests/test_data_models.py::TestClassDefaults::test_variables_change_from_defaults": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_to_dict": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_padding_ignored_in_dict": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-1470]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-22279]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-24436]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[2529]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[4398]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[18601]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-27797]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[32408]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-32116]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-2797]": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_equality_for_complex_generic_struct": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_char_array_generic_structs": "passed",
                "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_generic_structs": "passed"
            },
            "success_count": 64,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "unknown_count": 0,
            "total_count": 64,
            "success_rate": 1.0,
            "coverage_report": {
                "covered_lines": 519,
                "num_statements": 538,
                "percent_covered": 95.42483660130719,
                "percent_covered_display": "95",
                "missing_lines": 19,
                "excluded_lines": 0,
                "num_branches": 74,
                "num_partial_branches": 9,
                "covered_branches": 65,
                "missing_branches": 9
            },
            "coverage_result": {}
        },
        "codelines_count": 919,
        "codefiles_count": 7,
        "code_length": 31813,
        "test_files_count": 1,
        "test_code_length": 14703,
        "class_diagram": "@startuml\nclass TestInt16Type {\n    test_valid_values(int16_struct): void\n    test_invalid_values(int16_struct): void\n}\nclass TestBooleanType {\n    test_valid_values(bool_struct): void\n    test_invalid_values(bool_struct): void\n    test_pack_unpack(bool_struct): void\n}\nclass TestCharType {\n    test_valid_values(char_struct): void\n    test_invalid_values(char_struct): void\n    test_pack_unpack(char_struct): void\n}\nclass TestCharArrayType {\n    test_valid_values(string_struct): void\n    test_invalid_values(string_struct): void\n    test_pack_unpack_with_padding(string_struct): void\n}\nclass TestIntegerTypes {\n    test_valid_range(type_class, min_val, max_val, size): void\n    test_invalid_range(type_class, min_val, max_val, size): void\n    test_pack_unpack(type_class, min_val, max_val, size): void\n}\nclass TestFloatingTypes {\n    test_pack_unpack_precision(type_class, size, places): void\n    test_invalid_values(type_class): void\n}\nclass TestComplexStructs {\n    test_mixed_struct_creation(mixed_struct): void\n    test_mixed_struct_pack_unpack(mixed_struct): void\n}\nclass TestErrorHandling {\n    test_invalid_field_name(mixed_struct): void\n    test_missing_required_field(mixed_struct): void\n}\nclass TestArrayCreation {\n    test_array_creation_for_primitives(arrays_struct_6): void\n    test_char_not_used_as_array(): void\n    test_array_length_fixed(): void\n}\nclass TestClassDefaults {\n    test_default_creation(): void\n    test_defaults_raise_exceptions(): void\n    test_variables_change_from_defaults(): void\n}\nclass TestUtilities {\n    test_to_dict(mixed_struct): void\n    test_padding_ignored_in_dict(): void\n    test_equality_between_generic_structs(value, int16_struct): void\n    test_equality_for_complex_generic_struct(complex_struct): void\n    test_inequality_between_char_array_generic_structs(string_struct): void\n    test_inequality_between_generic_structs(int16_struct): void\n}\nclass GenericStruct {\n    __init__(): void\n    __setattr__(name, value): void\n    pack(): bytes\n    unpack(cls, data): void\n    to_dict(): Dict[str, Any]\n    __eq__(other): bool\n    __repr__(): void\n}\nclass PrimitiveType {\n    __init__(format_char, min_value, max_value, size, python_dtypes): Unknown\n    format_char(): void\n    min_value(): void\n    max_value(): void\n    size(): void\n    __class_getitem__(cls, array_size): void\n    validate(value): bool\n    _validate_for_single_value(value): bool\n    pack(value): bytes\n    unpack(data): Any\n}\nclass INT8 {\n    __init__(): void\n}\nclass UINT8 {\n    __init__(): void\n}\nclass INT16 {\n    __init__(): void\n}\nclass UINT16 {\n    __init__(): void\n}\nclass INT32 {\n    __init__(): void\n}\nclass UINT32 {\n    __init__(): void\n}\nclass INT64 {\n    __init__(): void\n}\nclass UINT64 {\n    __init__(): void\n}\nclass FLOAT {\n    __init__(): void\n}\nclass DOUBLE {\n    __init__(): void\n}\nclass CHAR {\n    __init__(): void\n    __class_getitem__(cls, item): void\n    validate(value): bool\n    pack(value): bytes\n    unpack(data): str\n}\nclass CharArray {\n    __init__(length): void\n    validate(value): bool\n    pack(value): bytes\n    unpack(data): str\n}\nclass BOOL {\n    __init__(): void\n    validate(value): bool\n}\nclass PADDING {\n    __init__(size): void\n    validate(value): bool\n    pack(value): bytes\n    unpack(data): Unknown\n}\nclass ArraySizeError {\n    __init__(message): void\n}\nclass CharArrayError {\n    __init__(message): void\n}\nPrimitiveType <|-- INT64\nPrimitiveType <|-- UINT64\nPrimitiveType <|-- INT32\nPrimitiveType <|-- CharArray\nPrimitiveType <|-- BOOL\nPrimitiveType <|-- INT16\nPrimitiveType <|-- UINT32\nPrimitiveType <|-- FLOAT\nPrimitiveType <|-- UINT16\nPrimitiveType <|-- UINT8\nPrimitiveType <|-- DOUBLE\nPrimitiveType <|-- PADDING\nPrimitiveType <|-- INT8\nPrimitiveType <|-- CHAR\n@enduml",
        "structure": [
            {
                "file": "unit_tests/test_data_models.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestInt16Type",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "int16_struct"
                                ]
                            },
                            {
                                "name": "test_invalid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "int16_struct"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestBooleanType",
                        "docstring": null,
                        "comments": "Test primitive types",
                        "methods": [
                            {
                                "name": "test_valid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "bool_struct"
                                ]
                            },
                            {
                                "name": "test_invalid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "bool_struct"
                                ]
                            },
                            {
                                "name": "test_pack_unpack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "bool_struct"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestCharType",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "char_struct"
                                ]
                            },
                            {
                                "name": "test_invalid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "char_struct"
                                ]
                            },
                            {
                                "name": "test_pack_unpack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "char_struct"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestCharArrayType",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "string_struct"
                                ]
                            },
                            {
                                "name": "test_invalid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "string_struct"
                                ]
                            },
                            {
                                "name": "test_pack_unpack_with_padding",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "string_struct"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestIntegerTypes",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_range",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "type_class",
                                    "min_val",
                                    "max_val",
                                    "size"
                                ]
                            },
                            {
                                "name": "test_invalid_range",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "type_class",
                                    "min_val",
                                    "max_val",
                                    "size"
                                ]
                            },
                            {
                                "name": "test_pack_unpack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "type_class",
                                    "min_val",
                                    "max_val",
                                    "size"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestFloatingTypes",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_pack_unpack_precision",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "type_class",
                                    "size",
                                    "places"
                                ]
                            },
                            {
                                "name": "test_invalid_values",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "type_class"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestComplexStructs",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_mixed_struct_creation",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "mixed_struct"
                                ]
                            },
                            {
                                "name": "test_mixed_struct_pack_unpack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "mixed_struct"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestErrorHandling",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_invalid_field_name",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "mixed_struct"
                                ]
                            },
                            {
                                "name": "test_missing_required_field",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "mixed_struct"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestArrayCreation",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_array_creation_for_primitives",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "arrays_struct_6"
                                ]
                            },
                            {
                                "name": "test_char_not_used_as_array",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_array_length_fixed",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestClassDefaults",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_default_creation",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_defaults_raise_exceptions",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_variables_change_from_defaults",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "TestUtilities",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_to_dict",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "mixed_struct"
                                ]
                            },
                            {
                                "name": "test_padding_ignored_in_dict",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_equality_between_generic_structs",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value",
                                    "int16_struct"
                                ]
                            },
                            {
                                "name": "test_equality_for_complex_generic_struct",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "complex_struct"
                                ]
                            },
                            {
                                "name": "test_inequality_between_char_array_generic_structs",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "string_struct"
                                ]
                            },
                            {
                                "name": "test_inequality_between_generic_structs",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "int16_struct"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "NumStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "NumStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "NumStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "FloatStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "FloatStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "FixedArrayInt",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "MixedStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "MixedStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "PaddedStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "LocalInt16Struct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "LocalComplexStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "LocalStringStruct",
                        "docstring": null,
                        "comments": "Arrays in different memory locations, thus they shouldn't be equal",
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "LocalInt16Struct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "BrokenCharArray",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "MixedStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "MixedStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "MixedStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "unit_tests/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "unit_tests/conftest.py",
                "functions": [
                    {
                        "name": "bool_struct",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "char_struct",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "string_struct",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "int16_struct",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "mixed_struct",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "complex_struct",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "arrays_struct_6",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": [
                    {
                        "name": "BoolStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "CharStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "StringStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "Int16Struct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "MixedStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "MixedStruct",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "ArraysStruct6",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/cstructpy/data_models.py",
                "functions": [],
                "classes": [
                    {
                        "name": "GenericStruct",
                        "docstring": "A base class for structured data that provides methods to pack and unpack binary data\nand to dynamically create fields based on type hints.\n\nAttributes:\n    _type_hints (dict): Type hints for the class attributes, used for field validation and packing.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes the GenericStruct instance, setting up fields based on type hints.\nOnly enforces defaults if explicitly provided in subclasses.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__setattr__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "name",
                                    "value"
                                ]
                            },
                            {
                                "name": "pack",
                                "docstring": "Packs the structure's fields into a binary representation using the defined types.\n\nReturns:\n    bytes: The packed binary data for the structure.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "unpack",
                                "docstring": "Unpacks binary data into a structure instance by reading field values according to their types.\n\nArgs:\n    data (bytes): The binary data to unpack.\n\nReturns:\n    GenericStruct: An instance of the class with field values set from the binary data.",
                                "comments": null,
                                "args": [
                                    "cls",
                                    "data"
                                ]
                            },
                            {
                                "name": "to_dict",
                                "docstring": "Converts the structure to a dictionary with field names as keys and their values.\n\nReturns:\n    dict: A dictionary representation of the structure.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__eq__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "other"
                                ]
                            },
                            {
                                "name": "__repr__",
                                "docstring": "Provides a string representation of the instance with all user-defined attributes and their values.\n\nReturns:\n    str: A string showing the class name and user-defined attribute names and values.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/cstructpy/primitives.py",
                "functions": [],
                "classes": [
                    {
                        "name": "PrimitiveType",
                        "docstring": "Abstract base class for primitive types that defines methods for validation, packing,\nand unpacking binary data.\n\nAttributes:\n    _format_char (str): The format character used by the `struct` module for packing/unpacking.\n    _min_value (int, optional): The minimum allowable value for the type.\n    _max_value (int, optional): The maximum allowable value for the type.\n    _size (int): The size of the type in bytes.",
                        "comments": "Base primitive type class (same as before)",
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes a PrimitiveType with the given format character, optional min/max values, and size.\n\nArgs:\n    format_char (str): Format character for the type (e.g., 'i', 'f').\n    min_value (int, optional): Minimum value allowed (for integer types).\n    max_value (int, optional): Maximum value allowed (for integer types).\n    size (int, optional): Size of the type in bytes.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "format_char",
                                    "min_value",
                                    "max_value",
                                    "size",
                                    "python_dtypes"
                                ]
                            },
                            {
                                "name": "format_char",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "min_value",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "max_value",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "size",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__class_getitem__",
                                "docstring": "Intercepts the [] operator, returning a new class that represents an array of this type.\n\nArgs:\n    array_size (int): The size of the array.\n\nReturns:\n    PrimitiveType: The instantiated class with augmented format_char and size to represent array",
                                "comments": null,
                                "args": [
                                    "cls",
                                    "array_size"
                                ]
                            },
                            {
                                "name": "validate",
                                "docstring": "Validates the given value against the type's constraints (min/max).\n\nArgs:\n    value (Any): The value to validate.\n\nRaises:\n    ValueError: If the value does not meet the type's constraints.\n\nReturns:\n    bool: True if the value is valid.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "_validate_for_single_value",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "pack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "unpack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "data"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "INT8",
                        "docstring": null,
                        "comments": "Integer types",
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "UINT8",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "INT16",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "UINT16",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "INT32",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "UINT32",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "INT64",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "UINT64",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "FLOAT",
                        "docstring": null,
                        "comments": "====== Special dtypes ======\nFloating point types",
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "DOUBLE",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "CHAR",
                        "docstring": null,
                        "comments": "Character types",
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__class_getitem__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls",
                                    "item"
                                ]
                            },
                            {
                                "name": "validate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "pack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "unpack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "data"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "CharArray",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "length"
                                ]
                            },
                            {
                                "name": "validate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "pack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "unpack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "data"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "BOOL",
                        "docstring": null,
                        "comments": "Boolean type",
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "validate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "PADDING",
                        "docstring": null,
                        "comments": "Padding type (for alignment)",
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "size"
                                ]
                            },
                            {
                                "name": "validate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "pack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "value"
                                ]
                            },
                            {
                                "name": "unpack",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "data"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/cstructpy/exceptions.py",
                "functions": [],
                "classes": [
                    {
                        "name": "ArraySizeError",
                        "docstring": "Array size errors custom exception",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "message"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "CharArrayError",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "message"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/cstructpy/__init__.py",
                "functions": [],
                "classes": []
            }
        ],
        "test_cases": {
            "unit_tests/test_data_models.py::TestInt16Type::test_valid_values": {
                "testid": "unit_tests/test_data_models.py::TestInt16Type::test_valid_values",
                "result": "passed",
                "test_implementation": "    def test_valid_values(self, string_struct):\n        s = string_struct(value=\"Hello\")\n        assert s.value == \"Hello\"\n\n        s.value = \"Hi\"  # Shorter string\n        assert s.value == \"Hi\""
            },
            "unit_tests/test_data_models.py::TestInt16Type::test_invalid_values": {
                "testid": "unit_tests/test_data_models.py::TestInt16Type::test_invalid_values",
                "result": "passed",
                "test_implementation": "    def test_invalid_values(self, type_class):\n        class FloatStruct(GenericStruct):\n            value: type_class\n\n        with pytest.raises(exceptions.ArraySizeError):\n            FloatStruct(value=\"3.14\")\n\n        s = FloatStruct(value=1.0)\n        with pytest.raises(exceptions.ArraySizeError):\n            s.value = \"invalid\""
            },
            "unit_tests/test_data_models.py::TestBooleanType::test_valid_values": {
                "testid": "unit_tests/test_data_models.py::TestBooleanType::test_valid_values",
                "result": "passed",
                "test_implementation": "    def test_valid_values(self, string_struct):\n        s = string_struct(value=\"Hello\")\n        assert s.value == \"Hello\"\n\n        s.value = \"Hi\"  # Shorter string\n        assert s.value == \"Hi\""
            },
            "unit_tests/test_data_models.py::TestBooleanType::test_invalid_values": {
                "testid": "unit_tests/test_data_models.py::TestBooleanType::test_invalid_values",
                "result": "passed",
                "test_implementation": "    def test_invalid_values(self, type_class):\n        class FloatStruct(GenericStruct):\n            value: type_class\n\n        with pytest.raises(exceptions.ArraySizeError):\n            FloatStruct(value=\"3.14\")\n\n        s = FloatStruct(value=1.0)\n        with pytest.raises(exceptions.ArraySizeError):\n            s.value = \"invalid\""
            },
            "unit_tests/test_data_models.py::TestBooleanType::test_pack_unpack": {
                "testid": "unit_tests/test_data_models.py::TestBooleanType::test_pack_unpack",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestCharType::test_valid_values": {
                "testid": "unit_tests/test_data_models.py::TestCharType::test_valid_values",
                "result": "passed",
                "test_implementation": "    def test_valid_values(self, string_struct):\n        s = string_struct(value=\"Hello\")\n        assert s.value == \"Hello\"\n\n        s.value = \"Hi\"  # Shorter string\n        assert s.value == \"Hi\""
            },
            "unit_tests/test_data_models.py::TestCharType::test_invalid_values": {
                "testid": "unit_tests/test_data_models.py::TestCharType::test_invalid_values",
                "result": "passed",
                "test_implementation": "    def test_invalid_values(self, type_class):\n        class FloatStruct(GenericStruct):\n            value: type_class\n\n        with pytest.raises(exceptions.ArraySizeError):\n            FloatStruct(value=\"3.14\")\n\n        s = FloatStruct(value=1.0)\n        with pytest.raises(exceptions.ArraySizeError):\n            s.value = \"invalid\""
            },
            "unit_tests/test_data_models.py::TestCharType::test_pack_unpack": {
                "testid": "unit_tests/test_data_models.py::TestCharType::test_pack_unpack",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestCharArrayType::test_valid_values": {
                "testid": "unit_tests/test_data_models.py::TestCharArrayType::test_valid_values",
                "result": "passed",
                "test_implementation": "    def test_valid_values(self, string_struct):\n        s = string_struct(value=\"Hello\")\n        assert s.value == \"Hello\"\n\n        s.value = \"Hi\"  # Shorter string\n        assert s.value == \"Hi\""
            },
            "unit_tests/test_data_models.py::TestCharArrayType::test_invalid_values": {
                "testid": "unit_tests/test_data_models.py::TestCharArrayType::test_invalid_values",
                "result": "passed",
                "test_implementation": "    def test_invalid_values(self, type_class):\n        class FloatStruct(GenericStruct):\n            value: type_class\n\n        with pytest.raises(exceptions.ArraySizeError):\n            FloatStruct(value=\"3.14\")\n\n        s = FloatStruct(value=1.0)\n        with pytest.raises(exceptions.ArraySizeError):\n            s.value = \"invalid\""
            },
            "unit_tests/test_data_models.py::TestCharArrayType::test_pack_unpack_with_padding": {
                "testid": "unit_tests/test_data_models.py::TestCharArrayType::test_pack_unpack_with_padding",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack_with_padding(self, string_struct):\n        s = string_struct(value=\"Hi\")\n        packed = s.pack()\n        assert len(packed) == 5  # Full length including padding\n\n        unpacked = string_struct.unpack(packed)\n        assert unpacked.value == \"Hi\"  # Padding should be stripped"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT8--128-127-1]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT8--128-127-1]",
                "result": "passed",
                "test_implementation": "    def test_valid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        s.value = min_val\n        assert s.value == min_val\n        s.value = max_val\n        assert s.value == max_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT8-0-255-1]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT8-0-255-1]",
                "result": "passed",
                "test_implementation": "    def test_valid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        s.value = min_val\n        assert s.value == min_val\n        s.value = max_val\n        assert s.value == max_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT16--32768-32767-2]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT16--32768-32767-2]",
                "result": "passed",
                "test_implementation": "    def test_valid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        s.value = min_val\n        assert s.value == min_val\n        s.value = max_val\n        assert s.value == max_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT16-0-65535-2]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT16-0-65535-2]",
                "result": "passed",
                "test_implementation": "    def test_valid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        s.value = min_val\n        assert s.value == min_val\n        s.value = max_val\n        assert s.value == max_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT32--2147483648-2147483647-4]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT32--2147483648-2147483647-4]",
                "result": "passed",
                "test_implementation": "    def test_valid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        s.value = min_val\n        assert s.value == min_val\n        s.value = max_val\n        assert s.value == max_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT32-0-4294967295-4]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT32-0-4294967295-4]",
                "result": "passed",
                "test_implementation": "    def test_valid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        s.value = min_val\n        assert s.value == min_val\n        s.value = max_val\n        assert s.value == max_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT64--9223372036854775808-9223372036854775807-8]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT64--9223372036854775808-9223372036854775807-8]",
                "result": "passed",
                "test_implementation": "    def test_valid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        s.value = min_val\n        assert s.value == min_val\n        s.value = max_val\n        assert s.value == max_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT64-0-18446744073709551615-8]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT64-0-18446744073709551615-8]",
                "result": "passed",
                "test_implementation": "    def test_valid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        s.value = min_val\n        assert s.value == min_val\n        s.value = max_val\n        assert s.value == max_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT8--128-127-1]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT8--128-127-1]",
                "result": "passed",
                "test_implementation": "    def test_invalid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        with pytest.raises(ValueError):\n            s.value = min_val - 1\n        with pytest.raises(ValueError):\n            s.value = max_val + 1"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT8-0-255-1]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT8-0-255-1]",
                "result": "passed",
                "test_implementation": "    def test_invalid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        with pytest.raises(ValueError):\n            s.value = min_val - 1\n        with pytest.raises(ValueError):\n            s.value = max_val + 1"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT16--32768-32767-2]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT16--32768-32767-2]",
                "result": "passed",
                "test_implementation": "    def test_invalid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        with pytest.raises(ValueError):\n            s.value = min_val - 1\n        with pytest.raises(ValueError):\n            s.value = max_val + 1"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT16-0-65535-2]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT16-0-65535-2]",
                "result": "passed",
                "test_implementation": "    def test_invalid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        with pytest.raises(ValueError):\n            s.value = min_val - 1\n        with pytest.raises(ValueError):\n            s.value = max_val + 1"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT32--2147483648-2147483647-4]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT32--2147483648-2147483647-4]",
                "result": "passed",
                "test_implementation": "    def test_invalid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        with pytest.raises(ValueError):\n            s.value = min_val - 1\n        with pytest.raises(ValueError):\n            s.value = max_val + 1"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT32-0-4294967295-4]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT32-0-4294967295-4]",
                "result": "passed",
                "test_implementation": "    def test_invalid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        with pytest.raises(ValueError):\n            s.value = min_val - 1\n        with pytest.raises(ValueError):\n            s.value = max_val + 1"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT64--9223372036854775808-9223372036854775807-8]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT64--9223372036854775808-9223372036854775807-8]",
                "result": "passed",
                "test_implementation": "    def test_invalid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        with pytest.raises(ValueError):\n            s.value = min_val - 1\n        with pytest.raises(ValueError):\n            s.value = max_val + 1"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT64-0-18446744073709551615-8]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT64-0-18446744073709551615-8]",
                "result": "passed",
                "test_implementation": "    def test_invalid_range(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        s = NumStruct(value=0)\n        with pytest.raises(ValueError):\n            s.value = min_val - 1\n        with pytest.raises(ValueError):\n            s.value = max_val + 1"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT8--128-127-1]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT8--128-127-1]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT8-0-255-1]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT8-0-255-1]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT16--32768-32767-2]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT16--32768-32767-2]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT16-0-65535-2]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT16-0-65535-2]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT32--2147483648-2147483647-4]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT32--2147483648-2147483647-4]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT32-0-4294967295-4]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT32-0-4294967295-4]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT64--9223372036854775808-9223372036854775807-8]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT64--9223372036854775808-9223372036854775807-8]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT64-0-18446744073709551615-8]": {
                "testid": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT64-0-18446744073709551615-8]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack(self, type_class, min_val, max_val, size):\n        class NumStruct(GenericStruct):\n            value: type_class\n\n        test_val = max_val // 2\n        s = NumStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = NumStruct.unpack(packed)\n        assert unpacked.value == test_val"
            },
            "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[FLOAT-4-6]": {
                "testid": "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[FLOAT-4-6]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack_precision(self, type_class, size, places):\n        class FloatStruct(GenericStruct):\n            value: type_class\n\n        test_val = 3.141592653589793\n        s = FloatStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = FloatStruct.unpack(packed)\n        assert unpacked.value == pytest.approx(unpacked.value,\n                                               rel=10 ** -places), f\"Unequal values  at {places} precision\""
            },
            "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[DOUBLE-8-15]": {
                "testid": "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[DOUBLE-8-15]",
                "result": "passed",
                "test_implementation": "    def test_pack_unpack_precision(self, type_class, size, places):\n        class FloatStruct(GenericStruct):\n            value: type_class\n\n        test_val = 3.141592653589793\n        s = FloatStruct(value=test_val)\n        packed = s.pack()\n        assert len(packed) == size\n\n        unpacked = FloatStruct.unpack(packed)\n        assert unpacked.value == pytest.approx(unpacked.value,\n                                               rel=10 ** -places), f\"Unequal values  at {places} precision\""
            },
            "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values[FLOAT]": {
                "testid": "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values[FLOAT]",
                "result": "passed",
                "test_implementation": "    def test_invalid_values(self, type_class):\n        class FloatStruct(GenericStruct):\n            value: type_class\n\n        with pytest.raises(exceptions.ArraySizeError):\n            FloatStruct(value=\"3.14\")\n\n        s = FloatStruct(value=1.0)\n        with pytest.raises(exceptions.ArraySizeError):\n            s.value = \"invalid\""
            },
            "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values[DOUBLE]": {
                "testid": "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values[DOUBLE]",
                "result": "passed",
                "test_implementation": "    def test_invalid_values(self, type_class):\n        class FloatStruct(GenericStruct):\n            value: type_class\n\n        with pytest.raises(exceptions.ArraySizeError):\n            FloatStruct(value=\"3.14\")\n\n        s = FloatStruct(value=1.0)\n        with pytest.raises(exceptions.ArraySizeError):\n            s.value = \"invalid\""
            },
            "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_creation": {
                "testid": "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_creation",
                "result": "passed",
                "test_implementation": "    def test_mixed_struct_creation(self, mixed_struct):\n        s = mixed_struct(\n            bool_val=True,\n            char_val='X',\n            int16_val=-1234,\n            float_val=3.14,\n            string_val=\"Hello\"\n        )\n\n        assert s.bool_val is True\n        assert s.char_val == 'X'\n        assert s.int16_val == -1234\n        assert s.float_val == pytest.approx(3.14, rel=10 ** -6)\n        assert s.string_val == \"Hello\""
            },
            "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_pack_unpack": {
                "testid": "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_pack_unpack",
                "result": "passed",
                "test_implementation": "    def test_mixed_struct_pack_unpack(self, mixed_struct):\n        original = mixed_struct(\n            bool_val=True,\n            char_val='X',\n            int16_val=-1234,\n            float_val=3.14,\n            string_val=\"Hello\"\n        )\n\n        packed = original.pack()\n        assert len(packed) == 18  # 1 + 1 + 2  + 4 + 10\n\n        unpacked = mixed_struct.unpack(packed)\n        assert unpacked.bool_val == original.bool_val\n        assert unpacked.char_val == original.char_val\n        assert unpacked.int16_val == original.int16_val\n        assert unpacked.float_val == pytest.approx(unpacked.float_val, rel=10 ** (-6))\n        assert unpacked.string_val == original.string_val"
            },
            "unit_tests/test_data_models.py::TestErrorHandling::test_invalid_field_name": {
                "testid": "unit_tests/test_data_models.py::TestErrorHandling::test_invalid_field_name",
                "result": "passed",
                "test_implementation": "    def test_invalid_field_name(self, mixed_struct):\n        with pytest.raises(ValueError, match=\"Unknown field\"):\n            mixed_struct(invalid_field=\"value\")"
            },
            "unit_tests/test_data_models.py::TestErrorHandling::test_missing_required_field": {
                "testid": "unit_tests/test_data_models.py::TestErrorHandling::test_missing_required_field",
                "result": "passed",
                "test_implementation": "    def test_missing_required_field(self, mixed_struct):\n        with pytest.raises(AttributeError):\n            mixed_struct(bool_val=True).pack()  # Missing other required fields"
            },
            "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives": {
                "testid": "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives",
                "result": "passed",
                "test_implementation": "    def test_array_creation_for_primitives(self, arrays_struct_6):\n        arrays_struct_obj = arrays_struct_6(\n            bool_array_6=[True, False, False, True, False, False],\n            int16_array_6=[1234, 23341, 12, 1451, 234, 11],\n            float_array_6=[3.141592, 123.141592, -1233.141592, 13.141391, -1001.141592, 10000.141592],\n            uint16_array_6=[1234, 43341, 12, 1451, 234, 11],\n            uint32_array_6=[1234, 23341, 12, 1451, 234, 11],\n            uint64_array_6=[1234, 23341, 12, 23123123, 234, 1844674407370955]\n        )\n\n        packed = arrays_struct_obj.pack()\n        assert len(packed) == (1 + 2 + 4 + 2 + 4 + 8) * 6, \"Checksum failed\"\n\n        arrays_struct_6_unpacked = arrays_struct_6.unpack(packed)\n\n        assert list(arrays_struct_6_unpacked.bool_array_6) == arrays_struct_obj.bool_array_6\n        assert list(arrays_struct_6_unpacked.int16_array_6) == arrays_struct_obj.int16_array_6\n        assert list(arrays_struct_6_unpacked.uint16_array_6) == arrays_struct_obj.uint16_array_6\n        assert list(arrays_struct_6_unpacked.uint32_array_6) == arrays_struct_obj.uint32_array_6\n        assert list(arrays_struct_6_unpacked.uint64_array_6) == arrays_struct_obj.uint64_array_6\n\n        # Check for float precision\n        for u_val, val in zip(arrays_struct_6_unpacked.float_array_6, arrays_struct_obj.float_array_6):\n            assert val == pytest.approx(u_val, rel=10 ** -6)"
            },
            "unit_tests/test_data_models.py::TestArrayCreation::test_char_not_used_as_array": {
                "testid": "unit_tests/test_data_models.py::TestArrayCreation::test_char_not_used_as_array",
                "result": "passed",
                "test_implementation": "    def test_char_not_used_as_array(self):\n        with pytest.raises(exceptions.CharArrayError):\n            class BrokenCharArray(GenericStruct):\n                char_array: CHAR[6]"
            },
            "unit_tests/test_data_models.py::TestArrayCreation::test_array_length_fixed": {
                "testid": "unit_tests/test_data_models.py::TestArrayCreation::test_array_length_fixed",
                "result": "passed",
                "test_implementation": "    def test_array_length_fixed(self):\n        class FixedArrayInt(GenericStruct):\n            values: INT16[4]\n\n        with pytest.raises(exceptions.ArraySizeError):\n            FixedArrayInt(values=[1, 2, 3])\n        with pytest.raises(exceptions.ArraySizeError):\n            FixedArrayInt(values=[1, 2, 3, 3, 1])\n\n        array_obj = FixedArrayInt(values=[1, 2, 3, 3])\n        assert len(array_obj.values) == 4, \"Array size isn't length expected of 4\""
            },
            "unit_tests/test_data_models.py::TestClassDefaults::test_default_creation": {
                "testid": "unit_tests/test_data_models.py::TestClassDefaults::test_default_creation",
                "result": "passed",
                "test_implementation": "    def test_default_creation(self):\n        class MixedStruct(GenericStruct):\n            bool_val: BOOL = True\n            uint32_array: UINT32[4] = [1, 2, 3, 4]\n            int32: INT32 = 324\n\n        try:\n            mixed_struct_obj = MixedStruct()\n        except Exception as e:\n            raise AssertionError(f\"Object from mixed struct causes and exception, when it shouldn't. {e}\")\n\n        assert mixed_struct_obj.bool_val is True\n        assert mixed_struct_obj.uint32_array == [1, 2, 3, 4]\n        assert mixed_struct_obj.int32 == 324\n\n        packed = mixed_struct_obj.pack()\n        unpacked = mixed_struct_obj.unpack(packed)\n\n        assert mixed_struct_obj.bool_val == unpacked.bool_val\n        assert mixed_struct_obj.uint32_array == list(unpacked.uint32_array)\n        assert mixed_struct_obj.int32 == unpacked.int32"
            },
            "unit_tests/test_data_models.py::TestClassDefaults::test_defaults_raise_exceptions": {
                "testid": "unit_tests/test_data_models.py::TestClassDefaults::test_defaults_raise_exceptions",
                "result": "passed",
                "test_implementation": "    def test_defaults_raise_exceptions(self):\n\n        with pytest.raises(ValueError):\n            class MixedStruct(GenericStruct):\n                bool_val: BOOL = 1\n                uint32_array: UINT32[4] = [1, 2, 3, 4]\n                int32: INT32 = 324\n\n            MixedStruct()\n        with pytest.raises(exceptions.ArraySizeError):\n            class MixedStruct(GenericStruct):\n                bool_val: BOOL = True\n                uint32_array: UINT32[4] = [1, 2, 3]\n                int32: INT32 = 324\n\n            MixedStruct()\n\n        with pytest.raises(ValueError):\n            class MixedStruct(GenericStruct):\n                bool_val: BOOL = True\n                uint32_array: UINT32[4] = [1, 2, 3, 4]\n                int16: INT16 = 2 ** 15\n\n            MixedStruct()"
            },
            "unit_tests/test_data_models.py::TestClassDefaults::test_variables_change_from_defaults": {
                "testid": "unit_tests/test_data_models.py::TestClassDefaults::test_variables_change_from_defaults",
                "result": "passed",
                "test_implementation": "    def test_variables_change_from_defaults(self):\n        class MixedStruct(GenericStruct):\n            bool_val: BOOL = True\n            uint32_array: UINT32[4] = [1, 2, 3, 4]\n            int32: INT32 = 324\n\n        mixed_struct_obj = MixedStruct(bool_val=False)\n        mixed_struct_obj.uint32_array = [12, 12, 12, 12]\n\n        assert mixed_struct_obj.bool_val is False\n        assert mixed_struct_obj.uint32_array == [12, 12, 12, 12]\n\n        packed = mixed_struct_obj.pack()\n        unpacked = mixed_struct_obj.unpack(packed)\n\n        assert mixed_struct_obj.bool_val == unpacked.bool_val\n        assert mixed_struct_obj.uint32_array == list(unpacked.uint32_array)\n        assert mixed_struct_obj.int32 == unpacked.int32"
            },
            "unit_tests/test_data_models.py::TestUtilities::test_to_dict": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_to_dict",
                "result": "passed",
                "test_implementation": "    def test_to_dict(self, mixed_struct):\n        s = mixed_struct(\n            bool_val=True,\n            char_val='X',\n            int16_val=-1234,\n            float_val=3.14,\n            string_val=\"Hello\"\n        )\n\n        d = s.to_dict()\n        assert d == {\n            'bool_val': True,\n            'char_val': 'X',\n            'int16_val': -1234,\n            'float_val': pytest.approx(3.14),\n            'string_val': \"Hello\"\n        }"
            },
            "unit_tests/test_data_models.py::TestUtilities::test_padding_ignored_in_dict": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_padding_ignored_in_dict",
                "result": "passed",
                "test_implementation": "    def test_padding_ignored_in_dict(self):\n        class PaddedStruct(GenericStruct):\n            value: INT16\n            next_value: INT16\n\n        s = PaddedStruct(value=1, next_value=2)\n        d = s.to_dict()\n        assert d == {'value': 1, 'next_value': 2}"
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-1470]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-1470]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-22279]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-22279]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-24436]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-24436]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[2529]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[2529]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[4398]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[4398]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[18601]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[18601]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-27797]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-27797]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[32408]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[32408]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-32116]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-32116]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-2797]": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-2797]",
                "result": "passed",
                "test_implementation": "    def test_equality_between_generic_structs(self, value, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=value)\n        local_int16_obj = LocalInt16Struct(value=value)\n        assert int16_obj == local_int16_obj, \"The objects should be equal\"\n        assert local_int16_obj == int16_obj, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_equality_for_complex_generic_struct": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_equality_for_complex_generic_struct",
                "result": "passed",
                "test_implementation": "    def test_equality_for_complex_generic_struct(self, complex_struct):\n        class LocalComplexStruct(GenericStruct):\n            bool_val: BOOL\n            char_val: CHAR\n            int16_val: INT16\n            float_val: FLOAT\n            uint16_val: UINT16\n            uint32_val: UINT32\n            uint64_val: UINT64\n\n        complex_obj = complex_struct(\n            bool_val=True,\n            char_val='X',\n            int16_val=-1234,\n            float_val=3.14,\n            uint16_val=int(2 ** 16 - 1),\n            uint32_val=int(2 ** 32 - 1),\n            uint64_val=int(2 ** 64 - 1)\n        )\n\n        local_complex_struct = LocalComplexStruct(\n            bool_val=True,\n            char_val='X',\n            int16_val=-1234,\n            float_val=3.14,\n            uint16_val=int(2 ** 16 - 1),\n            uint32_val=int(2 ** 32 - 1),\n            uint64_val=int(2 ** 64 - 1)\n        )\n\n        assert local_complex_struct == complex_obj, \"The objects should be equal\"\n        assert complex_obj == local_complex_struct, \"The objects should be equal\""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_char_array_generic_structs": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_char_array_generic_structs",
                "result": "passed",
                "test_implementation": "    def test_inequality_between_char_array_generic_structs(self, string_struct):\n        # Arrays in different memory locations, thus they shouldn't be equal\n        class LocalStringStruct(GenericStruct):\n            value: CharArray(5)\n\n        string_obj = string_struct(value='yes')\n        local_string_obj = LocalStringStruct(value='yes')\n\n        assert string_obj != local_string_obj, \"This objects should be different \"\n        assert local_string_obj != string_obj, \"This objects should be different \""
            },
            "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_generic_structs": {
                "testid": "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_generic_structs",
                "result": "passed",
                "test_implementation": "    def test_inequality_between_generic_structs(self, int16_struct):\n        class LocalInt16Struct(GenericStruct):\n            value: INT16\n\n        int16_obj = int16_struct(value=12)\n        local_int16_obj = LocalInt16Struct(value=12345)\n        assert int16_obj != local_int16_obj, \"The objects should not be equal\""
            }
        },
        "SRS_document": "**Software Requirements Specification: cstructpy**\n\n**Table of Contents:**\n1.  Introduction\n    1.1 Purpose\n    1.2 Scope\n    1.3 Definitions, Acronyms, and Abbreviations\n    1.4 References\n    1.5 Document Overview\n2.  Overall Description\n    2.1 Product Perspective\n    2.2 Product Functions\n    2.3 User Characteristics\n    2.4 Constraints\n    2.5 Assumptions and Dependencies\n3.  Specific Requirements\n    3.1 Functional Requirements\n        3.1.1 Struct Definition (`GenericStruct`)\n        3.1.2 Primitive Data Type Fields\n        3.1.3 Array Fields\n        3.1.4 Struct Instantiation\n        3.1.5 Field Assignment and Validation\n        3.1.6 Serialization (Packing)\n        3.1.7 Deserialization (Unpacking)\n        3.1.8 Data Conversion Utilities\n        3.1.9 Equality Comparison\n        3.1.10 Error Handling\n    3.2 Non-Functional Requirements\n    3.3 External Interface Requirements\n    3.4 Other Requirements\n\n---\n\n**1. Introduction**\n\n**1.1 Purpose**\nThis Software Requirements Specification (SRS) document provides a comprehensive description of the functionalities for the `cstructpy` library. It is intended to serve as the sole source of requirements for software developers tasked with implementing the library. Developer assessment will be based on their implementation's ability to pass a full suite of test cases (public and private), derived from the behavior specified herein.\n\n**1.2 Scope**\nThe `cstructpy` library is designed for binary serialization and deserialization of structured data using C-like primitive types. It allows users to define custom data structures with typed fields and provides mechanisms to convert instances of these structures to and from binary byte strings. The scope includes defining structures, specifying various primitive and array field types, data validation, packing, unpacking, and utility functions.\n\n**1.3 Definitions, Acronyms, and Abbreviations**\n*   **SRS:** Software Requirements Specification\n*   **Struct:** A user-defined composite data type that groups together variables (fields) under one name.\n*   **Primitive Type:** Basic data types such as integers, floating-point numbers, characters, booleans.\n*   **Serialization (Packing):** The process of converting a data structure or object state into a format (in this case, binary bytes) that can be stored or transmitted.\n*   **Deserialization (Unpacking):** The process of converting data from a stored/transmitted format back into a data structure or object.\n*   **ASCII:** American Standard Code for Information Interchange, a character encoding standard.\n\n**1.4 References**\n*   `cstructpy` README.md (for general context)\n*   Python `struct` module documentation (as an underlying mechanism, though its direct use is an implementation detail)\n\n**1.5 Document Overview**\nThis document is organized into three main sections:\n*   Section 1 (Introduction): Provides an overview of the SRS, its purpose, scope, and definitions.\n*   Section 2 (Overall Description): Describes the product, its functions, users, constraints, and assumptions at a high level.\n*   Section 3 (Specific Requirements): Details all functional and non-functional requirements of the software. This section is critical for implementation.\n\n---\n\n**2. Overall Description**\n\n**2.1 Product Perspective**\n`cstructpy` is a Python library module intended to be imported and used by other Python applications. It provides a way to define and manage C-like data structures, facilitating the conversion between Python objects and their binary representations. It aims to offer data validation through type annotations, similar in concept to libraries like Pydantic, but specifically for binary data formats.\n\n**2.2 Product Functions**\nThe `cstructpy` library shall provide the following core functionalities:\n*   Definition of custom structured data classes by inheriting from a base class (`GenericStruct`).\n*   Specification of fields within these structures using a predefined set of primitive data types (e.g., INT8, UINT16, FLOAT, CHAR, BOOL).\n*   Support for fixed-size arrays of primitive types as fields.\n*   Support for fixed-length character array fields, including padding and stripping of null terminators.\n*   Validation of field values against their defined types, constraints (e.g., min/max values for integers), and array sizes.\n*   Serialization of structure instances into binary byte strings.\n*   Deserialization of binary byte strings back into instances of the defined structures.\n*   Support for default values for fields in structure definitions.\n*   A utility to convert structure instances into Python dictionaries.\n\n**2.3 User Characteristics**\nThe primary users of `cstructpy` are Python developers who need to work with binary data formats, either for file I/O, network communication, or interoperability with systems that use C-style structures. Users are expected to be familiar with Python programming concepts, including classes, type hints, and basic data types.\n\n**2.4 Constraints**\n*   The system's binary packing and unpacking behavior for primitive types shall be compatible with the standard sizes and representations typically used in C and supported by Python's `struct` module (e.g., an INT32 is a 4-byte signed integer).\n*   Character encoding for CHAR and CHAR_ARRAY types shall be ASCII.\n*   Field definitions within structs shall rely on Python type annotations.\n\n**2.5 Assumptions and Dependencies**\n*   The system will be developed in Python.\n*   The system will operate correctly in Python environments where the `struct` module is available and functions as per standard Python distributions.\n*   Users will define structs by subclassing the provided `GenericStruct` base class.\n\n---\n\n**3. Specific Requirements**\n\n**3.1 Functional Requirements**\n\n**3.1.1 Struct Definition (`GenericStruct`)**\n\n*   **FR-GS-DEF-001:** The system shall allow users to define custom data structures by creating Python classes that inherit from a base class (`GenericStruct`).\n\n*   **FR-GS-DEF-002:** Fields within a user-defined struct shall be declared using Python type annotations.\n\n**3.1.2 Primitive Data Type Fields**\nThe system shall support the following primitive data types for fields within structs. For each integer type, validation shall ensure the value fits within the specified range. For all types, Python type validation shall be performed.\n\n*   **FR-PRIM-INT8-001 (INT8):**\n    *   Represents an 8-bit signed integer.\n    *   Valid range: -128 to 127, inclusive.\n    *   Python type: `int`.\n    *   Binary size: 1 byte.\n\n*   **FR-PRIM-UINT8-001 (UINT8):**\n    *   Represents an 8-bit unsigned integer.\n    *   Valid range: 0 to 255, inclusive.\n    *   Python type: `int`.\n    *   Binary size: 1 byte.\n\n*   **FR-PRIM-INT16-001 (INT16):**\n    *   Represents a 16-bit signed integer.\n    *   Valid range: -32768 to 32767, inclusive.\n    *   Python type: `int`.\n    *   Binary size: 2 bytes.\n\n*   **FR-PRIM-UINT16-001 (UINT16):**\n    *   Represents a 16-bit unsigned integer.\n    *   Valid range: 0 to 65535, inclusive.\n    *   Python type: `int`.\n    *   Binary size: 2 bytes.\n\n*   **FR-PRIM-INT32-001 (INT32):**\n    *   Represents a 32-bit signed integer.\n    *   Valid range: -2147483648 to 2147483647, inclusive.\n    *   Python type: `int`.\n    *   Binary size: 4 bytes.\n\n*   **FR-PRIM-UINT32-001 (UINT32):**\n    *   Represents a 32-bit unsigned integer.\n    *   Valid range: 0 to 4294967295, inclusive.\n    *   Python type: `int`.\n    *   Binary size: 4 bytes.\n\n*   **FR-PRIM-INT64-001 (INT64):**\n    *   Represents a 64-bit signed integer.\n    *   Valid range: -9223372036854775808 to 9223372036854775807, inclusive.\n    *   Python type: `int`.\n    *   Binary size: 8 bytes.\n\n*   **FR-PRIM-UINT64-001 (UINT64):**\n    *   Represents a 64-bit unsigned integer.\n    *   Valid range: 0 to 18446744073709551615, inclusive.\n    *   Python type: `int`.\n    *   Binary size: 8 bytes.\n\n*   **FR-PRIM-FLOAT-001 (FLOAT):**\n    *   Represents a 32-bit single-precision floating-point number.\n    *   Python type: `float` or `int`.\n    *   Binary size: 4 bytes.\n    *   Unpacked float values should maintain precision comparable to standard float representations (approx. 6-7 decimal digits).\n\n*   **FR-PRIM-DOUBLE-001 (DOUBLE):**\n    *   Represents a 64-bit double-precision floating-point number.\n    *   Python type: `float` or `int`.\n    *   Binary size: 8 bytes.\n    *   Unpacked double values should maintain precision comparable to standard double representations (approx. 15-16 decimal digits).\n\n*   **FR-PRIM-CHAR-001 (CHAR):**\n    *   Represents a single character, encoded as ASCII.\n    *   Python type: `str` or `bytes` (single character/byte).\n    *   Value must be a single character string or a single byte.\n    *   Binary size: 1 byte.\n    *   When packing a `str`, it shall be encoded to ASCII. When unpacking, bytes shall be decoded from ASCII to `str`.\n\n*   **FR-PRIM-CHARARRAY-001 (CharArray):**\n    *   Represents a fixed-length array of characters, encoded as ASCII. Defined as `CharArray(N)` where N is the length.\n    *   Python type: `str` or `bytes`.\n    *   The length of the assigned string/bytes value must not exceed the defined fixed length N.\n    *   Binary size: N bytes.\n    *   When packing, if the provided string/bytes is shorter than N, it shall be right-padded with null bytes (`\\x00`) to reach length N. Encoding to ASCII applies if input is `str`.\n    *   When unpacking, any trailing null bytes shall be stripped from the resulting string. Bytes shall be decoded from ASCII to `str`.\n\n*   **FR-PRIM-BOOL-001 (BOOL):**\n    *   Represents a boolean value.\n    *   Python type: `bool`.\n    *   Valid values: `True`, `False`.\n    *   Binary size: 1 byte.\n\n*   **FR-PRIM-PADDING-001 (PADDING):**\n    *   Represents a padding field of a specified size in bytes. Defined as `PADDING(N)` where N is the number of padding bytes.\n    *   This type is used for alignment and does not store a user-settable value.\n    *   Binary representation: N null bytes (`\\x00`).\n    *   When unpacked, it results in `None` or an equivalent non-data value.\n    *   Validation always passes for any assigned value (as it's effectively ignored).\n\n**3.1.3 Array Fields**\n\n*   **FR-ARRAY-DEF-001:** The system shall support defining fixed-size array fields for all primitive types except `CHAR` and `PADDING`. Array types are defined using square bracket notation, e.g., `INT16[3]` for an array of 3 `INT16` values.\n\n*   **FR-ARRAY-DEF-002:** The size specified for an array (e.g., N in `TYPE[N]`) must be a positive integer. Zero or negative sizes shall result in an error during type definition.\n\n*   **FR-ARRAY-VALIDATE-SIZE-001:** When assigning a value to an array field, the system shall validate that the provided sequence (e.g., list, tuple) has a length exactly matching the defined array size. A mismatch in length shall result in an `ArraySizeError`.\n\n*   **FR-ARRAY-VALIDATE-ELEMENTS-001:** Each element in a sequence assigned to an array field shall be validated against the base primitive type of the array (e.g., for `INT16[3]`, each element must be a valid `INT16`). An invalid element shall result in an appropriate error (e.g., `ValueError`, `TypeError`).\n\n*   **FR-ARRAY-PACK-001:** The system shall pack an array field by sequentially packing each element of the array according to its base primitive type. The total binary size shall be N * (size of base type), where N is the array length. Input for an array field can be a Python sequence (e.g., list or tuple).\n\n*   **FR-ARRAY-UNPACK-001:** The system shall unpack an array field by sequentially unpacking elements from the binary data according to the base primitive type and array length. Unpacked array fields shall be represented as Python tuples.\n\n*   **FR-ARRAY-CHAR-INVALID-001:** Attempting to define an array of `CHAR` using the square bracket notation (e.g., `CHAR[5]`) shall result in a `CharArrayError`. The `CharArray(N)` type should be used for fixed-length character arrays instead.\n\n**3.1.4 Struct Instantiation**\n\n*   **FR-INST-KWARGS-001:** The system shall allow instantiation of a `GenericStruct` subclass by providing field values as keyword arguments.\n\n*   **FR-INST-KWARGS-VALIDATE-001:** Field values provided as keyword arguments during instantiation shall be validated against their respective field type definitions. Invalid values shall raise an appropriate error (e.g., `ValueError`, `TypeError`, `ArraySizeError`).\n\n*   **FR-INST-UNKNOWN-FIELD-001:** If an unknown field name (one not defined in the struct class via type hints) is provided as a keyword argument during instantiation, the system shall raise a `ValueError`.\n\n*   **FR-INST-DEFAULT-VALUES-001:** The system shall support defining default values for fields directly in the `GenericStruct` subclass definition (e.g., `field1: INT8 = 16`).\n\n*   **FR-INST-DEFAULT-USE-001:** If a field with a defined default value is not provided as a keyword argument during instantiation, its default value shall be used for the instance.\n\n*   **FR-INST-DEFAULT-VALIDATE-001:** Default values defined in a `GenericStruct` subclass shall be validated against their respective field type definitions upon instantiation of the struct if that default value is used. An error (e.g., `ValueError`, `ArraySizeError`) shall be raised if a default value is invalid.\n\n*   **FR-INST-DEFAULT-OVERRIDE-001:** Default values can be overridden by providing a keyword argument for that field during instantiation.\n\n**3.1.5 Field Assignment and Validation**\n\n*   **FR-ASSIGN-VALIDATE-001:** After instantiation, assigning a value to a field of a `GenericStruct` instance shall trigger validation of the value against the field's defined type. An invalid value shall raise an appropriate error (e.g., `ValueError`, `TypeError`, `ArraySizeError`).\n\n*   **FR-ASSIGN-SEQ-TO-SCALAR-ERR-001:** Assigning a Python sequence (e.g., `str`, `bytes`) to a scalar (non-array) field of type BOOL, FLOAT, or DOUBLE shall result in an `ArraySizeError`.\n\n**3.1.6 Serialization (Packing)**\n\n*   **FR-PACK-MAIN-001:** The system shall provide a method (e.g., `pack()`) on `GenericStruct` instances to serialize the instance into a single `bytes` object.\n\n*   **FR-PACK-ORDER-001:** The `bytes` object produced by packing shall be formed by concatenating the binary representations of each of the struct's defined fields. The fields shall be processed and packed in the order they are defined (based on type hints) in the `GenericStruct` subclass.\n\n*   **FR-PACK-FIELD-TYPE-001:** Each field's value shall be packed according to its specific primitive type definition, including its size, format, and any array configurations (length, element type).\n\n*   **FR-PACK-MISSING-FIELD-001:** Attempting to pack a `GenericStruct` instance where one or more fields (that do not have defaults and were not set) are missing values shall result in an `AttributeError`.\n\n**3.1.7 Deserialization (Unpacking)**\n\n*   **FR-UNPACK-MAIN-001:** The system shall provide a class method (e.g., `unpack()`) on `GenericStruct` subclasses to deserialize a `bytes` object into a new instance of that subclass.\n\n*   **FR-UNPACK-PROCESS-001:** The system shall sequentially parse the input `bytes` object. Segments of bytes shall be interpreted according to the sequence, type, size, and array configuration of fields defined in the target `GenericStruct` subclass.\n\n*   **FR-UNPACK-INSTANCE-POPULATE-001:** The deserialized values obtained from the binary data shall be used to populate the fields of the newly created `GenericStruct` instance.\n\n*   **FR-UNPACK-DATA-LENGTH-001:** The input `bytes` object for unpacking must be of sufficient length to cover all defined fields in the struct. Insufficient data may lead to errors during unpacking (e.g., `struct.error`).\n\n**3.1.8 Data Conversion Utilities**\n\n*   **FR-UTIL-TODICT-001:** The system shall provide a method (e.g., `to_dict()`) on `GenericStruct` instances to convert the instance into a Python dictionary.\n\n*   **FR-UTIL-TODICT-CONTENT-001:** Each key in the dictionary returned by `to_dict()` shall be a string corresponding to a field name defined in the struct. The value for each key shall be the current Python value of that field in the instance.\n\n*   **FR-UTIL-TODICT-PADDING-001:** Fields defined with the `PADDING` type shall be included in the dictionary representation generated by `to_dict()`, and their value in the dictionary shall be `None`.\n\n**3.1.9 Equality Comparison**\n\n*   **FR-EQ-MAIN-001:** The system shall allow equality comparison (`==`) between `GenericStruct` instances.\n\n*   **FR-EQ-CRITERIA-001:** Two `GenericStruct` instances shall be considered equal if all their user-defined field values are equal, and their structural definitions (field names, types, order, and type configurations like array sizes or `CharArray` lengths) are equivalent.\n    *   This means instances of different `GenericStruct` subclasses can be equal if they meet these criteria.\n\n*   **FR-EQ-CRITERIA-CHARARRAY-INST-001:** For fields of type `CharArray(N)` or array types like `INT16[N]`, the equality comparison is sensitive to the specific instances of these type descriptors stored within the `_type_hints` attribute of the `GenericStruct`. If these type descriptor instances are different objects (e.g., created by separate `CharArray(N)` calls in different class definitions), the parent `GenericStruct` instances may be considered unequal even if all field values are identical.\n\n*   **FR-EQ-UNEQUAL-VALUES-001:** Two `GenericStruct` instances shall be considered unequal if any of their corresponding user-defined field values are different.\n\n**3.1.10 Error Handling**\nThe system shall raise specific exceptions for anticipated error conditions.\n\n*   **FR-ERR-VALUE-001:** A `ValueError` shall be raised for:\n    *   Assigning a value outside the valid range of an integer type.\n    *   Assigning a value of incorrect Python type to a field where a specific Python type is expected (and not covered by `TypeError` from a more general check).\n    *   Assigning a multi-character string or non-character value to a `CHAR` field.\n    *   Assigning a string longer than the defined length to a `CharArray` field.\n    *   Assigning a non-string/bytes value to a `CharArray` field.\n    *   Providing an unknown field name during `GenericStruct` instantiation.\n    *   Assigning a non-boolean value to a `BOOL` field.\n\n*   **FR-ERR-TYPE-001:** A `TypeError` may be raised for assigning a value of a fundamentally incompatible Python type to a field (e.g., if internal validation relies on isinstance for a specific Python type not covered by other checks).\n\n*   **FR-ERR-ATTRIBUTE-001:** An `AttributeError` shall be raised if `pack()` is called on an instance where required fields (without defaults) have not been assigned values.\n\n*   **FR-ERR-ARRAYSIZE-001:** An `ArraySizeError` (custom exception) shall be raised for:\n    *   Providing a sequence of incorrect length when assigning to a fixed-size array field.\n    *   Assigning a sequence (like `str` or `bytes`) to a scalar (non-array) field of types like `BOOL`, `FLOAT`, `DOUBLE`.\n\n*   **FR-ERR-CHARARRAY-001:** A `CharArrayError` (custom exception) shall be raised if an attempt is made to define an array of `CHAR` using the `CHAR[N]` syntax.\n\n\n**3.2 Non-Functional Requirements**\n*   **NFR-001:** No non-functional requirements are explicitly defined by dedicated original test cases. All provided test cases focus on functional correctness.\n\n**3.3 External Interface Requirements**\nThe primary external interface of the `cstructpy` library is its Python API, consisting of:\n*   The `GenericStruct` base class, intended for subclassing by users.\n*   Primitive type classes (`INT8`, `UINT8`, `INT16`, `UINT16`, `INT32`, `UINT32`, `INT64`, `UINT64`, `FLOAT`, `DOUBLE`, `CHAR`, `BOOL`).\n*   The `CharArray(N)` type class.\n*   The `PADDING(N)` type class.\n*   Methods on `GenericStruct` instances: `pack()`, `to_dict()`.\n*   Class method on `GenericStruct` subclasses: `unpack()`.\n*   Exceptions: `ArraySizeError`, `CharArrayError`.\nThese elements constitute the public interface that developers using the library will interact with.\n\n**3.4 Other Requirements**\nNone specified.\n\n---\n**End of Document**",
        "structured_requirements": [
            {
                "requirement_id": "FR-GS-DEF-001",
                "requirement_description": "The system shall allow users to define custom data structures by creating Python classes that inherit from a base class (`GenericStruct`).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestInt16Type::test_valid_values",
                        "description": "Implicitly used in all tests creating struct classes (e.g.,"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-GS-DEF-002",
                "requirement_description": "Fields within a user-defined struct shall be declared using Python type annotations.",
                "test_traceability": [
                    {
                        "id": "unit_tests/conftest.py::bool_struct",
                        "description": "Implicitly used in all fixture definitions (e.g.,"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": "(use of `get_type_hints`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-INT8-001",
                "requirement_description": "(INT8):\nRepresents an 8-bit signed integer.\nValid range: -128 to 127, inclusive.\nPython type: `int`.\nBinary size: 1 byte.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT8-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT8-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT8-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::INT8",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-UINT8-001",
                "requirement_description": "(UINT8):\nRepresents an 8-bit unsigned integer.\nValid range: 0 to 255, inclusive.\nPython type: `int`.\nBinary size: 1 byte.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT8-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT8-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT8-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::UINT8",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-INT16-001",
                "requirement_description": "(INT16):\nRepresents a 16-bit signed integer.\nValid range: -32768 to 32767, inclusive.\nPython type: `int`.\nBinary size: 2 bytes.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestInt16Type::test_valid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestInt16Type::test_invalid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT16-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT16-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT16-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::INT16",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-UINT16-001",
                "requirement_description": "(UINT16):\nRepresents a 16-bit unsigned integer.\nValid range: 0 to 65535, inclusive.\nPython type: `int`.\nBinary size: 2 bytes.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT16-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT16-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT16-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::UINT16",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-INT32-001",
                "requirement_description": "(INT32):\nRepresents a 32-bit signed integer.\nValid range: -2147483648 to 2147483647, inclusive.\nPython type: `int`.\nBinary size: 4 bytes.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT32-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT32-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT32-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::INT32",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-UINT32-001",
                "requirement_description": "(UINT32):\nRepresents a 32-bit unsigned integer.\nValid range: 0 to 4294967295, inclusive.\nPython type: `int`.\nBinary size: 4 bytes.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT32-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT32-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT32-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::UINT32",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-INT64-001",
                "requirement_description": "(INT64):\nRepresents a 64-bit signed integer.\nValid range: -9223372036854775808 to 9223372036854775807, inclusive.\nPython type: `int`.\nBinary size: 8 bytes.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[INT64-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[INT64-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT64-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::INT64",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-UINT64-001",
                "requirement_description": "(UINT64):\nRepresents a 64-bit unsigned integer.\nValid range: 0 to 18446744073709551615, inclusive.\nPython type: `int`.\nBinary size: 8 bytes.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_valid_range[UINT64-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_invalid_range[UINT64-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[UINT64-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::UINT64",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-FLOAT-001",
                "requirement_description": "(FLOAT):\nRepresents a 32-bit single-precision floating-point number.\nPython type: `float` or `int`.\nBinary size: 4 bytes.\nUnpacked float values should maintain precision comparable to standard float representations (approx. 6-7 decimal digits).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[FLOAT-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values[FLOAT-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::FLOAT",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-DOUBLE-001",
                "requirement_description": "(DOUBLE):\nRepresents a 64-bit double-precision floating-point number.\nPython type: `float` or `int`.\nBinary size: 8 bytes.\nUnpacked double values should maintain precision comparable to standard double representations (approx. 15-16 decimal digits).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[DOUBLE-...]",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values[DOUBLE-...]",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::DOUBLE",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-CHAR-001",
                "requirement_description": "(CHAR):\nRepresents a single character, encoded as ASCII.\nPython type: `str` or `bytes` (single character/byte).\nValue must be a single character string or a single byte.\nBinary size: 1 byte.\nWhen packing a `str`, it shall be encoded to ASCII. When unpacking, bytes shall be decoded from ASCII to `str`.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestCharType::test_valid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestCharType::test_invalid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestCharType::test_pack_unpack",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::CHAR",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-CHARARRAY-001",
                "requirement_description": "(CharArray):\nRepresents a fixed-length array of characters, encoded as ASCII. Defined as `CharArray(N)` where N is the length.\nPython type: `str` or `bytes`.\nThe length of the assigned string/bytes value must not exceed the defined fixed length N.\nBinary size: N bytes.\nWhen packing, if the provided string/bytes is shorter than N, it shall be right-padded with null bytes (`\\x00`) to reach length N. Encoding to ASCII applies if input is `str`.\nWhen unpacking, any trailing null bytes shall be stripped from the resulting string. Bytes shall be decoded from ASCII to `str`.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestCharArrayType::test_valid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestCharArrayType::test_invalid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestCharArrayType::test_pack_unpack_with_padding",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::CharArray",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-BOOL-001",
                "requirement_description": "(BOOL):\nRepresents a boolean value.\nPython type: `bool`.\nValid values: `True`, `False`.\nBinary size: 1 byte.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_valid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_invalid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_pack_unpack",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::BOOL",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PRIM-PADDING-001",
                "requirement_description": "(PADDING):\nRepresents a padding field of a specified size in bytes. Defined as `PADDING(N)` where N is the number of padding bytes.\nThis type is used for alignment and does not store a user-settable value.\nBinary representation: N null bytes (`\\x00`).\nWhen unpacked, it results in `None` or an equivalent non-data value.\nValidation always passes for any assigned value (as it's effectively ignored).",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PADDING",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARRAY-DEF-001",
                "requirement_description": "The system shall support defining fixed-size array fields for all primitive types except `CHAR` and `PADDING`. Array types are defined using square bracket notation, e.g., `INT16[3]` for an array of 3 `INT16` values.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::__class_getitem__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARRAY-DEF-002",
                "requirement_description": "The size specified for an array (e.g., N in `TYPE[N]`) must be a positive integer. Zero or negative sizes shall result in an error during type definition.",
                "test_traceability": [
                    {
                        "id": "",
                        "description": "(No explicit test for this exact definition error, but implied by valid array tests)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::__class_getitem__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARRAY-VALIDATE-SIZE-001",
                "requirement_description": "When assigning a value to an array field, the system shall validate that the provided sequence (e.g., list, tuple) has a length exactly matching the defined array size. A mismatch in length shall result in an `ArraySizeError`.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_length_fixed",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARRAY-VALIDATE-ELEMENTS-001",
                "requirement_description": "Each element in a sequence assigned to an array field shall be validated against the base primitive type of the array (e.g., for `INT16[3]`, each element must be a valid `INT16`). An invalid element shall result in an appropriate error (e.g., `ValueError`, `TypeError`).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives",
                        "description": "(valid values pass), and default value tests like"
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestClassDefaults::test_defaults_raise_exceptions",
                        "description": "(for array with invalid element default)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARRAY-PACK-001",
                "requirement_description": "The system shall pack an array field by sequentially packing each element of the array according to its base primitive type. The total binary size shall be N * (size of base type), where N is the array length. Input for an array field can be a Python sequence (e.g., list or tuple).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARRAY-UNPACK-001",
                "requirement_description": "The system shall unpack an array field by sequentially unpacking elements from the binary data according to the base primitive type and array length. Unpacked array fields shall be represented as Python tuples.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARRAY-CHAR-INVALID-001",
                "requirement_description": "Attempting to define an array of `CHAR` using the square bracket notation (e.g., `CHAR[5]`) shall result in a `CharArrayError`. The `CharArray(N)` type should be used for fixed-length character arrays instead.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_char_not_used_as_array",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::CHAR::__class_getitem__",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/exceptions.py::CharArrayError",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INST-KWARGS-001",
                "requirement_description": "The system shall allow instantiation of a `GenericStruct` subclass by providing field values as keyword arguments.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestInt16Type::test_valid_values",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INST-KWARGS-VALIDATE-001",
                "requirement_description": "Field values provided as keyword arguments during instantiation shall be validated against their respective field type definitions. Invalid values shall raise an appropriate error (e.g., `ValueError`, `TypeError`, `ArraySizeError`).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestInt16Type::test_invalid_values",
                        "description": "(instantiation with invalid value),"
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestCharArrayType::test_invalid_values",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__setattr__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INST-UNKNOWN-FIELD-001",
                "requirement_description": "If an unknown field name (one not defined in the struct class via type hints) is provided as a keyword argument during instantiation, the system shall raise a `ValueError`.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestErrorHandling::test_invalid_field_name",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INST-DEFAULT-VALUES-001",
                "requirement_description": "The system shall support defining default values for fields directly in the `GenericStruct` subclass definition (e.g., `field1: INT8 = 16`).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestClassDefaults::test_default_creation",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INST-DEFAULT-USE-001",
                "requirement_description": "If a field with a defined default value is not provided as a keyword argument during instantiation, its default value shall be used for the instance.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestClassDefaults::test_default_creation",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INST-DEFAULT-VALIDATE-001",
                "requirement_description": "Default values defined in a `GenericStruct` subclass shall be validated against their respective field type definitions upon instantiation of the struct if that default value is used. An error (e.g., `ValueError`, `ArraySizeError`) shall be raised if a default value is invalid.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestClassDefaults::test_defaults_raise_exceptions",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__setattr__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INST-DEFAULT-OVERRIDE-001",
                "requirement_description": "Default values can be overridden by providing a keyword argument for that field during instantiation.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestClassDefaults::test_variables_change_from_defaults",
                        "description": "(instantiation part)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ASSIGN-VALIDATE-001",
                "requirement_description": "After instantiation, assigning a value to a field of a `GenericStruct` instance shall trigger validation of the value against the field's defined type. An invalid value shall raise an appropriate error (e.g., `ValueError`, `TypeError`, `ArraySizeError`).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestInt16Type::test_invalid_values",
                        "description": "(assignment after creation),"
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_invalid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__setattr__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ASSIGN-SEQ-TO-SCALAR-ERR-001",
                "requirement_description": "Assigning a Python sequence (e.g., `str`, `bytes`) to a scalar (non-array) field of type BOOL, FLOAT, or DOUBLE shall result in an `ArraySizeError`.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_invalid_values",
                        "description": "(for `s.value = \"True\"`),"
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values",
                        "description": "(for `FloatStruct(value=\"3.14\")`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PACK-MAIN-001",
                "requirement_description": "The system shall provide a method (e.g., `pack()`) on `GenericStruct` instances to serialize the instance into a single `bytes` object.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_pack_unpack",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::pack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PACK-ORDER-001",
                "requirement_description": "The `bytes` object produced by packing shall be formed by concatenating the binary representations of each of the struct's defined fields. The fields shall be processed and packed in the order they are defined (based on type hints) in the `GenericStruct` subclass.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_pack_unpack",
                        "description": "(verified by successful unpack and length check)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::pack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PACK-FIELD-TYPE-001",
                "requirement_description": "Each field's value shall be packed according to its specific primitive type definition, including its size, format, and any array configurations (length, element type).",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::pack",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::pack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PACK-MISSING-FIELD-001",
                "requirement_description": "Attempting to pack a `GenericStruct` instance where one or more fields (that do not have defaults and were not set) are missing values shall result in an `AttributeError`.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestErrorHandling::test_missing_required_field",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::pack",
                        "description": "(due to `getattr`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-UNPACK-MAIN-001",
                "requirement_description": "The system shall provide a class method (e.g., `unpack()`) on `GenericStruct` subclasses to deserialize a `bytes` object into a new instance of that subclass.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_pack_unpack",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UNPACK-PROCESS-001",
                "requirement_description": "The system shall sequentially parse the input `bytes` object. Segments of bytes shall be interpreted according to the sequence, type, size, and array configuration of fields defined in the target `GenericStruct` subclass.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_pack_unpack",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UNPACK-INSTANCE-POPULATE-001",
                "requirement_description": "The deserialized values obtained from the binary data shall be used to populate the fields of the newly created `GenericStruct` instance.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestCharType::test_pack_unpack",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::unpack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UNPACK-DATA-LENGTH-001",
                "requirement_description": "The input `bytes` object for unpacking must be of sufficient length to cover all defined fields in the struct. Insufficient data may lead to errors during unpacking (e.g., `struct.error`).",
                "test_traceability": [
                    {
                        "id": "",
                        "description": "(Not explicitly tested for insufficient data, but implied by tests that provide exactly enough data)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::unpack",
                        "description": "(relies on `struct.unpack` behavior)"
                    }
                ]
            },
            {
                "requirement_id": "FR-UTIL-TODICT-001",
                "requirement_description": "The system shall provide a method (e.g., `to_dict()`) on `GenericStruct` instances to convert the instance into a Python dictionary.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestUtilities::test_to_dict",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::to_dict",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UTIL-TODICT-CONTENT-001",
                "requirement_description": "Each key in the dictionary returned by `to_dict()` shall be a string corresponding to a field name defined in the struct. The value for each key shall be the current Python value of that field in the instance.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestUtilities::test_to_dict",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::to_dict",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UTIL-TODICT-PADDING-001",
                "requirement_description": "Fields defined with the `PADDING` type shall be included in the dictionary representation generated by `to_dict()`, and their value in the dictionary shall be `None`.",
                "test_traceability": [
                    {
                        "id": "",
                        "description": "None (Test `test_padding_ignored_in_dict` does not cover this). This requirement is derived from source code analysis."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::to_dict",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/primitives.py::PADDING::unpack",
                        "description": "(which returns `None`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EQ-MAIN-001",
                "requirement_description": "The system shall allow equality comparison (`==`) between `GenericStruct` instances.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__eq__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EQ-CRITERIA-001",
                "requirement_description": "Two `GenericStruct` instances shall be considered equal if all their user-defined field values are equal, and their structural definitions (field names, types, order, and type configurations like array sizes or `CharArray` lengths) are equivalent.\nThis means instances of different `GenericStruct` subclasses can be equal if they meet these criteria.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestUtilities::test_equality_for_complex_generic_struct",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__eq__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EQ-CRITERIA-CHARARRAY-INST-001",
                "requirement_description": "For fields of type `CharArray(N)` or array types like `INT16[N]`, the equality comparison is sensitive to the specific instances of these type descriptors stored within the `_type_hints` attribute of the `GenericStruct`. If these type descriptor instances are different objects (e.g., created by separate `CharArray(N)` calls in different class definitions), the parent `GenericStruct` instances may be considered unequal even if all field values are identical.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_char_array_generic_structs",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__eq__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EQ-UNEQUAL-VALUES-001",
                "requirement_description": "Two `GenericStruct` instances shall be considered unequal if any of their corresponding user-defined field values are different.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_generic_structs",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__eq__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-VALUE-001",
                "requirement_description": "A `ValueError` shall be raised for:\nAssigning a value outside the valid range of an integer type.\nAssigning a value of incorrect Python type to a field where a specific Python type is expected (and not covered by `TypeError` from a more general check).\nAssigning a multi-character string or non-character value to a `CHAR` field.\nAssigning a string longer than the defined length to a `CharArray` field.\nAssigning a non-string/bytes value to a `CharArray` field.\nProviding an unknown field name during `GenericStruct` instantiation.\nAssigning a non-boolean value to a `BOOL` field.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestInt16Type::test_invalid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestCharType::test_invalid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestCharArrayType::test_invalid_values",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestErrorHandling::test_invalid_field_name",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_invalid_values",
                        "description": "(for `s.value = 1`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-TYPE-001",
                "requirement_description": "A `TypeError` may be raised for assigning a value of a fundamentally incompatible Python type to a field (e.g., if internal validation relies on isinstance for a specific Python type not covered by other checks).",
                "test_traceability": [
                    {
                        "id": "TestCharType::test_invalid_values",
                        "description": "(Some `ValueError` in tests cover type issues, e.g.,"
                    },
                    {
                        "id": "s.value = 123`).",
                        "description": "The distinction between `TypeError` and `ValueError` for type issues depends on specific `PrimitiveType._validate_for_single_value` implementation."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::_validate_for_single_value",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-ATTRIBUTE-001",
                "requirement_description": "An `AttributeError` shall be raised if `pack()` is called on an instance where required fields (without defaults) have not been assigned values.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestErrorHandling::test_missing_required_field",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/data_models.py::GenericStruct::pack",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-ARRAYSIZE-001",
                "requirement_description": "An `ArraySizeError` (custom exception) shall be raised for:\nProviding a sequence of incorrect length when assigning to a fixed-size array field.\nAssigning a sequence (like `str` or `bytes`) to a scalar (non-array) field of types like `BOOL`, `FLOAT`, `DOUBLE`.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_length_fixed",
                        "description": ""
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestBooleanType::test_invalid_values",
                        "description": "(for string/bytes to bool),"
                    },
                    {
                        "id": "unit_tests/test_data_models.py::TestFloatingTypes::test_invalid_values",
                        "description": "(for string to float)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::PrimitiveType::validate",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/exceptions.py::ArraySizeError",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-CHARARRAY-001",
                "requirement_description": "A `CharArrayError` (custom exception) shall be raised if an attempt is made to define an array of `CHAR` using the `CHAR[N]` syntax.",
                "test_traceability": [
                    {
                        "id": "unit_tests/test_data_models.py::TestArrayCreation::test_char_not_used_as_array",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/cstructpy/primitives.py::CHAR::__class_getitem__",
                        "description": ""
                    },
                    {
                        "id": "src/cstructpy/exceptions.py::CharArrayError",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "NFR-001",
                "requirement_description": "No non-functional requirements are explicitly defined by dedicated original test cases. All provided test cases focus on functional correctness.",
                "test_traceability": [],
                "code_traceability": []
            }
        ],
        "commit_sha": "4e2492d01968ff39649eec62705b36b308856a4e",
        "full_code_skeleton": "--- File: src/cstructpy/data_models.py ---\n```python\nclass GenericStruct:\n    \"\"\"\n    A base class for structured data that provides methods to pack and unpack binary data\n    and to dynamically create fields based on type hints.\n\n    Attributes:\n        _type_hints (dict): Type hints for the class attributes, used for field validation and packing.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the GenericStruct instance, setting up fields based on type hints.\n        Only enforces defaults if explicitly provided in subclasses.\n        \"\"\"\n        pass\n\n    def __setattr__(self, name: str, value: Any):\n        pass\n\n    def pack(self) -> bytes:\n        \"\"\"\n        Packs the structure's fields into a binary representation using the defined types.\n\n        Returns:\n            bytes: The packed binary data for the structure.\n       \"\"\"\n        pass\n\n    @classmethod\n    def unpack(cls, data: bytes):\n        \"\"\"\n        Unpacks binary data into a structure instance by reading field values according to their types.\n\n        Args:\n            data (bytes): The binary data to unpack.\n\n        Returns:\n            GenericStruct: An instance of the class with field values set from the binary data.\n        \"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Converts the structure to a dictionary with field names as keys and their values.\n\n        Returns:\n            dict: A dictionary representation of the structure.\n        \"\"\"\n        pass\n\n    def __eq__(self, other) -> bool:\n        pass\n\n    def __repr__(self):\n        \"\"\"\n        Provides a string representation of the instance with all user-defined attributes and their values.\n\n        Returns:\n            str: A string showing the class name and user-defined attribute names and values.\n        \"\"\"\n        pass\n```\n--- File: src/cstructpy/primitives.py ---\n```python\nclass PrimitiveType(ABC):\n    \"\"\"\n    Abstract base class for primitive types that defines methods for validation, packing,\n    and unpacking binary data.\n\n    Attributes:\n        _format_char (str): The format character used by the `struct` module for packing/unpacking.\n        _min_value (int, optional): The minimum allowable value for the type.\n        _max_value (int, optional): The maximum allowable value for the type.\n        _size (int): The size of the type in bytes.\n    \"\"\"\n\n    def __init__(self, format_char: str = '',\n                 min_value: Optional[int] = None,\n                 max_value: Optional[int] = None,\n                 size: int = 0,\n                 python_dtypes: Optional[Type | tuple[Type, ...]] = None\n                 ) -> None:\n        \"\"\"\n        Initializes a PrimitiveType with the given format character, optional min/max values, and size.\n\n        Args:\n            format_char (str): Format character for the type (e.g., 'i', 'f').\n            min_value (int, optional): Minimum value allowed (for integer types).\n            max_value (int, optional): Maximum value allowed (for integer types).\n            size (int, optional): Size of the type in bytes.\n        \"\"\"\n        pass\n\n    @property\n    def format_char(self):\n        pass\n\n    @property\n    def min_value(self):\n        pass\n\n    @property\n    def max_value(self):\n        pass\n\n    @property\n    def size(self):\n        pass\n\n    def __class_getitem__(cls, array_size: int):\n        \"\"\"\n        Intercepts the [] operator, returning a new class that represents an array of this type.\n\n        Args:\n            array_size (int): The size of the array.\n\n        Returns:\n            PrimitiveType: The instantiated class with augmented format_char and size to represent array\n        \"\"\"\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def _validate_for_single_value(self, value: Any) -> bool:\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> Any:\n        pass\n\nclass INT8(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT8(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT16(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT16(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT32(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT32(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT64(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT64(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass FLOAT(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass DOUBLE(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass CHAR(PrimitiveType):\n    def __init__(self):\n        pass\n\n    def __class_getitem__(cls, item):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> str:\n        pass\n\nclass CharArray(PrimitiveType):\n    def __init__(self, length: int):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> str:\n        pass\n\nclass BOOL(PrimitiveType):\n    def __init__(self):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\nclass PADDING(PrimitiveType):\n    def __init__(self, size: int):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def pack(self, value: Any = None) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> None:\n        pass\n```\n--- File: src/cstructpy/exceptions.py ---\n```python\nclass ArraySizeError(Exception):\n    \"\"\"\n    Array size errors custom exception\n    \"\"\"\n\n    def __init__(self, message: str):\n        pass\n\nclass CharArrayError(Exception):\n    def __init__(self, message: Optional[str] = None):\n        pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "src/cstructpy/data_models.py",
                "code": "class GenericStruct:\n    \"\"\"\n    A base class for structured data that provides methods to pack and unpack binary data\n    and to dynamically create fields based on type hints.\n\n    Attributes:\n        _type_hints (dict): Type hints for the class attributes, used for field validation and packing.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the GenericStruct instance, setting up fields based on type hints.\n        Only enforces defaults if explicitly provided in subclasses.\n        \"\"\"\n        pass\n\n    def __setattr__(self, name: str, value: Any):\n        pass\n\n    def pack(self) -> bytes:\n        \"\"\"\n        Packs the structure's fields into a binary representation using the defined types.\n\n        Returns:\n            bytes: The packed binary data for the structure.\n       \"\"\"\n        pass\n\n    @classmethod\n    def unpack(cls, data: bytes):\n        \"\"\"\n        Unpacks binary data into a structure instance by reading field values according to their types.\n\n        Args:\n            data (bytes): The binary data to unpack.\n\n        Returns:\n            GenericStruct: An instance of the class with field values set from the binary data.\n        \"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Converts the structure to a dictionary with field names as keys and their values.\n\n        Returns:\n            dict: A dictionary representation of the structure.\n        \"\"\"\n        pass\n\n    def __eq__(self, other) -> bool:\n        pass\n\n    def __repr__(self):\n        \"\"\"\n        Provides a string representation of the instance with all user-defined attributes and their values.\n\n        Returns:\n            str: A string showing the class name and user-defined attribute names and values.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "src/cstructpy/primitives.py",
                "code": "class PrimitiveType(ABC):\n    \"\"\"\n    Abstract base class for primitive types that defines methods for validation, packing,\n    and unpacking binary data.\n\n    Attributes:\n        _format_char (str): The format character used by the `struct` module for packing/unpacking.\n        _min_value (int, optional): The minimum allowable value for the type.\n        _max_value (int, optional): The maximum allowable value for the type.\n        _size (int): The size of the type in bytes.\n    \"\"\"\n\n    def __init__(self, format_char: str = '',\n                 min_value: Optional[int] = None,\n                 max_value: Optional[int] = None,\n                 size: int = 0,\n                 python_dtypes: Optional[Type | tuple[Type, ...]] = None\n                 ) -> None:\n        \"\"\"\n        Initializes a PrimitiveType with the given format character, optional min/max values, and size.\n\n        Args:\n            format_char (str): Format character for the type (e.g., 'i', 'f').\n            min_value (int, optional): Minimum value allowed (for integer types).\n            max_value (int, optional): Maximum value allowed (for integer types).\n            size (int, optional): Size of the type in bytes.\n        \"\"\"\n        pass\n\n    @property\n    def format_char(self):\n        pass\n\n    @property\n    def min_value(self):\n        pass\n\n    @property\n    def max_value(self):\n        pass\n\n    @property\n    def size(self):\n        pass\n\n    def __class_getitem__(cls, array_size: int):\n        \"\"\"\n        Intercepts the [] operator, returning a new class that represents an array of this type.\n\n        Args:\n            array_size (int): The size of the array.\n\n        Returns:\n            PrimitiveType: The instantiated class with augmented format_char and size to represent array\n        \"\"\"\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def _validate_for_single_value(self, value: Any) -> bool:\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> Any:\n        pass\n\nclass INT8(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT8(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT16(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT16(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT32(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT32(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT64(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT64(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass FLOAT(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass DOUBLE(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass CHAR(PrimitiveType):\n    def __init__(self):\n        pass\n\n    def __class_getitem__(cls, item):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> str:\n        pass\n\nclass CharArray(PrimitiveType):\n    def __init__(self, length: int):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> str:\n        pass\n\nclass BOOL(PrimitiveType):\n    def __init__(self):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\nclass PADDING(PrimitiveType):\n    def __init__(self, size: int):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def pack(self, value: Any = None) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> None:\n        pass\n"
            },
            {
                "file_path": "src/cstructpy/exceptions.py",
                "code": "class ArraySizeError(Exception):\n    \"\"\"\n    Array size errors custom exception\n    \"\"\"\n\n    def __init__(self, message: str):\n        pass\n\nclass CharArrayError(Exception):\n    def __init__(self, message: Optional[str] = None):\n        pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: src/cstructpy/data_models.py ---\n```python\nclass GenericStruct:\n    \"\"\"\n    A base class for structured data that provides methods to pack and unpack binary data\n    and to dynamically create fields based on type hints.\n\n    Attributes:\n        _type_hints (dict): Type hints for the class attributes, used for field validation and packing.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the GenericStruct instance, setting up fields based on type hints.\n        Only enforces defaults if explicitly provided in subclasses.\n        \"\"\"\n        pass\n\n    def __setattr__(self, name: str, value: Any):\n        pass\n\n    def pack(self) -> bytes:\n        \"\"\"\n        Packs the structure's fields into a binary representation using the defined types.\n\n        Returns:\n            bytes: The packed binary data for the structure.\n       \"\"\"\n        pass\n\n    @classmethod\n    def unpack(cls, data: bytes):\n        \"\"\"\n        Unpacks binary data into a structure instance by reading field values according to their types.\n\n        Args:\n            data (bytes): The binary data to unpack.\n\n        Returns:\n            GenericStruct: An instance of the class with field values set from the binary data.\n        \"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Converts the structure to a dictionary with field names as keys and their values.\n\n        Returns:\n            dict: A dictionary representation of the structure.\n        \"\"\"\n        pass\n\n    def __eq__(self, other) -> bool:\n        pass\n```\n--- File: src/cstructpy/primitives.py ---\n```python\nclass PrimitiveType(ABC):\n    \"\"\"\n    Abstract base class for primitive types that defines methods for validation, packing,\n    and unpacking binary data.\n\n    Attributes:\n        _format_char (str): The format character used by the `struct` module for packing/unpacking.\n        _min_value (int, optional): The minimum allowable value for the type.\n        _max_value (int, optional): The maximum allowable value for the type.\n        _size (int): The size of the type in bytes.\n    \"\"\"\n\n    def __init__(self, format_char: str = '',\n                 min_value: Optional[int] = None,\n                 max_value: Optional[int] = None,\n                 size: int = 0,\n                 python_dtypes: Optional[Type | tuple[Type, ...]] = None\n                 ) -> None:\n        \"\"\"\n        Initializes a PrimitiveType with the given format character, optional min/max values, and size.\n\n        Args:\n            format_char (str): Format character for the type (e.g., 'i', 'f').\n            min_value (int, optional): Minimum value allowed (for integer types).\n            max_value (int, optional): Maximum value allowed (for integer types).\n            size (int, optional): Size of the type in bytes.\n        \"\"\"\n        pass\n\n    @property\n    def size(self):\n        pass\n\n    def __class_getitem__(cls, array_size: int):\n        \"\"\"\n        Intercepts the [] operator, returning a new class that represents an array of this type.\n\n        Args:\n            array_size (int): The size of the array.\n\n        Returns:\n            PrimitiveType: The instantiated class with augmented format_char and size to represent array\n        \"\"\"\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> Any:\n        pass\n\nclass INT8(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT8(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT16(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT16(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT32(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT32(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT64(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT64(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass FLOAT(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass DOUBLE(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass CHAR(PrimitiveType):\n    def __init__(self):\n        pass\n\n    def __class_getitem__(cls, item):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> str:\n        pass\n\nclass CharArray(PrimitiveType):\n    def __init__(self, length: int):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> str:\n        pass\n\nclass BOOL(PrimitiveType):\n    def __init__(self):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        pass\n```\n--- File: src/cstructpy/exceptions.py ---\n```python\nclass ArraySizeError(Exception):\n    \"\"\"\n    Array size errors custom exception\n    \"\"\"\n\n    def __init__(self, message: str):\n        pass\n\nclass CharArrayError(Exception):\n    def __init__(self, message: Optional[str] = None):\n        pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "src/cstructpy/data_models.py",
                "code": "class GenericStruct:\n    \"\"\"\n    A base class for structured data that provides methods to pack and unpack binary data\n    and to dynamically create fields based on type hints.\n\n    Attributes:\n        _type_hints (dict): Type hints for the class attributes, used for field validation and packing.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the GenericStruct instance, setting up fields based on type hints.\n        Only enforces defaults if explicitly provided in subclasses.\n        \"\"\"\n        pass\n\n    def __setattr__(self, name: str, value: Any):\n        pass\n\n    def pack(self) -> bytes:\n        \"\"\"\n        Packs the structure's fields into a binary representation using the defined types.\n\n        Returns:\n            bytes: The packed binary data for the structure.\n       \"\"\"\n        pass\n\n    @classmethod\n    def unpack(cls, data: bytes):\n        \"\"\"\n        Unpacks binary data into a structure instance by reading field values according to their types.\n\n        Args:\n            data (bytes): The binary data to unpack.\n\n        Returns:\n            GenericStruct: An instance of the class with field values set from the binary data.\n        \"\"\"\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Converts the structure to a dictionary with field names as keys and their values.\n\n        Returns:\n            dict: A dictionary representation of the structure.\n        \"\"\"\n        pass\n\n    def __eq__(self, other) -> bool:\n        pass\n"
            },
            {
                "file_path": "src/cstructpy/primitives.py",
                "code": "class PrimitiveType(ABC):\n    \"\"\"\n    Abstract base class for primitive types that defines methods for validation, packing,\n    and unpacking binary data.\n\n    Attributes:\n        _format_char (str): The format character used by the `struct` module for packing/unpacking.\n        _min_value (int, optional): The minimum allowable value for the type.\n        _max_value (int, optional): The maximum allowable value for the type.\n        _size (int): The size of the type in bytes.\n    \"\"\"\n\n    def __init__(self, format_char: str = '',\n                 min_value: Optional[int] = None,\n                 max_value: Optional[int] = None,\n                 size: int = 0,\n                 python_dtypes: Optional[Type | tuple[Type, ...]] = None\n                 ) -> None:\n        \"\"\"\n        Initializes a PrimitiveType with the given format character, optional min/max values, and size.\n\n        Args:\n            format_char (str): Format character for the type (e.g., 'i', 'f').\n            min_value (int, optional): Minimum value allowed (for integer types).\n            max_value (int, optional): Maximum value allowed (for integer types).\n            size (int, optional): Size of the type in bytes.\n        \"\"\"\n        pass\n\n    @property\n    def size(self):\n        pass\n\n    def __class_getitem__(cls, array_size: int):\n        \"\"\"\n        Intercepts the [] operator, returning a new class that represents an array of this type.\n\n        Args:\n            array_size (int): The size of the array.\n\n        Returns:\n            PrimitiveType: The instantiated class with augmented format_char and size to represent array\n        \"\"\"\n        pass\n\n    def validate(self, value: Any) -> bool:\n        \"\"\"\n        Validates the given value against the type's constraints (min/max).\n\n        Args:\n            value (Any): The value to validate.\n\n        Raises:\n            ValueError: If the value does not meet the type's constraints.\n\n        Returns:\n            bool: True if the value is valid.\n        \"\"\"\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> Any:\n        pass\n\nclass INT8(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT8(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT16(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT16(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT32(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT32(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass INT64(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass UINT64(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass FLOAT(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass DOUBLE(PrimitiveType):\n    def __init__(self):\n        pass\n\nclass CHAR(PrimitiveType):\n    def __init__(self):\n        pass\n\n    def __class_getitem__(cls, item):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> str:\n        pass\n\nclass CharArray(PrimitiveType):\n    def __init__(self, length: int):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        pass\n\n    def pack(self, value: Any) -> bytes:\n        pass\n\n    def unpack(self, data: bytes) -> str:\n        pass\n\nclass BOOL(PrimitiveType):\n    def __init__(self):\n        pass\n\n    def validate(self, value: Any) -> bool:\n        pass\n"
            },
            {
                "file_path": "src/cstructpy/exceptions.py",
                "code": "class ArraySizeError(Exception):\n    \"\"\"\n    Array size errors custom exception\n    \"\"\"\n\n    def __init__(self, message: str):\n        pass\n\nclass CharArrayError(Exception):\n    def __init__(self, message: Optional[str] = None):\n        pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "unit_tests/test_data_models.py::TestBooleanType::test_pack_unpack",
                "covers": [
                    "cstructpy.GenericStruct - instantiation with BOOL field (happy path)",
                    "cstructpy.GenericStruct.pack - basic scenario with BOOL field",
                    "cstructpy.GenericStruct.unpack - basic scenario with BOOL field"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestCharType::test_pack_unpack",
                "covers": [
                    "cstructpy.GenericStruct - instantiation with CHAR field (happy path)",
                    "cstructpy.GenericStruct.pack - basic scenario with CHAR field",
                    "cstructpy.GenericStruct.unpack - basic scenario with CHAR field"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestCharArrayType::test_pack_unpack_with_padding",
                "covers": [
                    "cstructpy.primitives.CharArray - instantiation and usage as field type",
                    "cstructpy.GenericStruct - instantiation with CharArray field (happy path)",
                    "cstructpy.GenericStruct.pack - with CharArray field (handles padding)",
                    "cstructpy.GenericStruct.unpack - with CharArray field (handles padding)"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestIntegerTypes::test_pack_unpack[INT16--32768-32767-2]",
                "covers": [
                    "cstructpy.GenericStruct - instantiation with INT16 field (happy path, representative for integers)",
                    "cstructpy.GenericStruct.pack - with INT16 field",
                    "cstructpy.GenericStruct.unpack - with INT16 field"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[FLOAT-4-6]",
                "covers": [
                    "cstructpy.GenericStruct - instantiation with FLOAT field (happy path)",
                    "cstructpy.GenericStruct.pack - with FLOAT field",
                    "cstructpy.GenericStruct.unpack - with FLOAT field (precision check)"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestFloatingTypes::test_pack_unpack_precision[DOUBLE-8-15]",
                "covers": [
                    "cstructpy.GenericStruct - instantiation with DOUBLE field (happy path)",
                    "cstructpy.GenericStruct.pack - with DOUBLE field",
                    "cstructpy.GenericStruct.unpack - with DOUBLE field (precision check)"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestComplexStructs::test_mixed_struct_pack_unpack",
                "covers": [
                    "cstructpy.GenericStruct - instantiation with multiple mixed-type fields (happy path)",
                    "cstructpy.GenericStruct.pack - complex structure with various field types",
                    "cstructpy.GenericStruct.unpack - complex structure with various field types"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_creation_for_primitives",
                "covers": [
                    "cstructpy.primitives.PrimitiveType.__class_getitem__ - successful usage for defining array fields",
                    "cstructpy.GenericStruct - instantiation with various primitive array fields (happy path)",
                    "cstructpy.GenericStruct.pack - with primitive array fields",
                    "cstructpy.GenericStruct.unpack - with primitive array fields"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestClassDefaults::test_variables_change_from_defaults",
                "covers": [
                    "cstructpy.GenericStruct - instantiation with overridden and default field values",
                    "cstructpy.GenericStruct - attribute assignment post-instantiation (tests __setattr__ and validation)",
                    "cstructpy.GenericStruct.pack - with modified default values",
                    "cstructpy.GenericStruct.unpack - with modified default values"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestErrorHandling::test_invalid_field_name",
                "covers": [
                    "cstructpy.GenericStruct.__init__ - error handling for unknown field name during instantiation"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestErrorHandling::test_missing_required_field",
                "covers": [
                    "cstructpy.GenericStruct.pack - error handling when attempting to pack with missing required fields"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestUtilities::test_to_dict",
                "covers": [
                    "cstructpy.GenericStruct.to_dict - happy path conversion to dictionary"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestUtilities::test_equality_between_generic_structs[-1470]",
                "covers": [
                    "cstructpy.GenericStruct.__eq__ - verifying equality of two identical struct instances"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestUtilities::test_inequality_between_generic_structs",
                "covers": [
                    "cstructpy.GenericStruct.__eq__ - verifying inequality of struct instances with different field values"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestArrayCreation::test_array_length_fixed",
                "covers": [
                    "cstructpy.exceptions.ArraySizeError - raised when providing an array of incorrect length for a fixed-size array field",
                    "cstructpy.GenericStruct.__init__ - validation of fixed-size array field length"
                ]
            },
            {
                "test_id": "unit_tests/test_data_models.py::TestArrayCreation::test_char_not_used_as_array",
                "covers": [
                    "cstructpy.exceptions.CharArrayError - raised when attempting to use CHAR type with array syntax (e.g., CHAR[6])",
                    "cstructpy.primitives.CHAR.__class_getitem__ - correctly disallowing array usage via subscript"
                ]
            }
        ]
    },
    {
        "idx": 93623,
        "repo_name": "aio-libs_propcache",
        "url": "https://github.com/aio-libs/propcache",
        "description": "Fast property caching",
        "stars": 18,
        "forks": 7,
        "language": "python",
        "size": 202,
        "created_at": "2024-10-02T13:45:18+00:00",
        "updated_at": "2025-04-07T18:46:53+00:00",
        "pypi_info": {
            "name": "propcache",
            "version": "0.3.1",
            "url": "https://files.pythonhosted.org/packages/07/c8/fdc6686a986feae3541ea23dcaa661bd93972d3940460646c6bb96e21c40/propcache-0.3.1.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 19,
            "comment_ratio": 0.3801478352692714,
            "pyfile_content_length": 61104,
            "pyfile_code_lines": 1894,
            "test_file_exist": true,
            "test_file_content_length": 12726,
            "pytest_framework": true,
            "test_case_num": 26,
            "metadata_path": [
                "pyproject.toml"
            ],
            "readme_content_length": 3267,
            "llm_reason": "The project `propcache` aims to provide fast cached property implementations, similar to `functools.cached_property` but with an additional `under_cached_property` variant. \n\n**Positive Aspects for Direct Benchmark Use (targeting pure Python functionality):**\n*   **Self-Contained & Independent:** The core library functionality (property caching decorators), especially its pure Python version, is self-contained. It does not require internet access, external APIs, or specialized hardware for its operation or testing. Dependencies for the AI's rebuilt solution would be standard Python.\n*   **Clear & Well-Defined Functionality:** The goal is to implement two types of cached property decorators. The behavior of one is similar to the standard `functools.cached_property`, providing a clear reference. The other (`under_cached_property`) is also well-defined by its use of a `_cache` attribute.\n*   **Testable & Verifiable Output:** The project has a comprehensive test suite (e.g., `test_cached_property.py`, `test_under_cached_property.py`) that can be directly used or adapted to verify the AI's implementation. This is a strong positive.\n*   **No Graphical User Interface (GUI):** The project is a library, intended for programmatic use (decorators), making it suitable for non-GUI testing.\n*   **Appropriate Complexity & Scope (for pure Python):** Re-implementing the descriptor protocol for property caching (`__get__`, `__set_name__`, managing the cache) is a non-trivial task of medium complexity. It's more involved than a simple script but well within the capabilities of an advanced AI assistant to replicate in a reasonable timeframe.\n*   **Well-Understood Problem Domain:** Python descriptors and caching mechanisms are established programming concepts.\n*   **Predominantly Code-Based Solution:** The task is primarily about generating Python code for the decorators.\n*   **Pure Python Fallback:** The project explicitly supports a pure Python mode, which is ideal for the AI benchmark target, side-stepping C extension complexity.\n\n**Negative Aspects or Concerns (requiring careful benchmark specification):**\n*   **C Extensions and Build System:** The original project includes optional C extensions for performance and a sophisticated custom PEP 517 packaging/build system (in the `packaging/` directory). If the benchmark task were to replicate the *entire* project including these C extensions and the custom build logic, it would become 'Hard' to 'Very Hard / Unsuitable'.\n*   **Scope Clarification Needed:** To be a suitable benchmark, the task for the AI must be clearly scoped to re-implementing the *pure Python functionality* of the `cached_property` and `under_cached_property` decorators as defined by the project's tests. The C extensions and the project's own build/packaging infrastructure should be explicitly excluded from the AI's task.\n\n**Overall Assessment:**\nIf the benchmark is defined as \"recreate the pure-Python library functionality of `propcache` (the `cached_property` and `under_cached_property` decorators) to pass the existing functional tests,\" then this project is a good candidate. The difficulty is 'Medium' as it requires a solid understanding of Python's descriptor protocol and object model. The rating of 75 reflects its strong suitability under this specific scoping, with the slight deduction due to the need for careful specification to distinguish the core library task from the more complex C extension and build system aspects of the original repository.",
            "llm_project_type": "Python utility library for property caching",
            "llm_rating": 75,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "aio-libs_propcache",
            "finish_test": true,
            "test_case_result": {
                "tests/test_api.py::test_api": "passed",
                "tests/test_benchmarks.py::test_under_cached_property_cache_hit": "skipped",
                "tests/test_benchmarks.py::test_cached_property_cache_hit": "skipped",
                "tests/test_benchmarks.py::test_under_cached_property_cache_miss": "skipped",
                "tests/test_benchmarks.py::test_cached_property_cache_miss": "skipped",
                "tests/test_cached_property.py::test_cached_property[c-extension-module]": "passed",
                "tests/test_cached_property.py::test_cached_property_without_cache[c-extension-module]": "passed",
                "tests/test_cached_property.py::test_cached_property_check_without_cache[c-extension-module]": "passed",
                "tests/test_cached_property.py::test_cached_property_caching[c-extension-module]": "passed",
                "tests/test_cached_property.py::test_cached_property_class_docstring[c-extension-module]": "passed",
                "tests/test_cached_property.py::test_set_name[c-extension-module]": "passed",
                "tests/test_cached_property.py::test_get_without_set_name[c-extension-module]": "passed",
                "tests/test_cached_property.py::test_ensured_wrapped_function_is_accessible[c-extension-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property[c-extension-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_typeddict[c-extension-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_assignment[c-extension-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_without_cache[c-extension-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache[c-extension-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_caching[c-extension-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_class_docstring[c-extension-module]": "passed",
                "tests/test_under_cached_property.py::test_ensured_wrapped_function_is_accessible[c-extension-module]": "passed",
                "tests/test_cached_property.py::test_cached_property[pure-python-module]": "passed",
                "tests/test_cached_property.py::test_cached_property_without_cache[pure-python-module]": "passed",
                "tests/test_cached_property.py::test_cached_property_check_without_cache[pure-python-module]": "passed",
                "tests/test_cached_property.py::test_cached_property_caching[pure-python-module]": "passed",
                "tests/test_cached_property.py::test_cached_property_class_docstring[pure-python-module]": "passed",
                "tests/test_cached_property.py::test_set_name[pure-python-module]": "passed",
                "tests/test_cached_property.py::test_get_without_set_name[pure-python-module]": "passed",
                "tests/test_cached_property.py::test_ensured_wrapped_function_is_accessible[pure-python-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property[pure-python-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_typeddict[pure-python-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_assignment[pure-python-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_without_cache[pure-python-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache[pure-python-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_caching[pure-python-module]": "passed",
                "tests/test_under_cached_property.py::test_under_cached_property_class_docstring[pure-python-module]": "passed",
                "tests/test_under_cached_property.py::test_ensured_wrapped_function_is_accessible[pure-python-module]": "passed",
                "tests/test_init.py::test_api_at_top_level": "passed",
                "tests/test_init.py::test_public_api_is_discoverable_in_dir[cached_property]": "passed",
                "tests/test_init.py::test_public_api_is_discoverable_in_dir[under_cached_property]": "passed",
                "tests/test_init.py::test_importing_invalid_attr_raises": "passed",
                "tests/test_init.py::test_import_error_invalid_attr": "passed",
                "tests/test_init.py::test_no_wildcard_imports": "passed"
            },
            "success_count": 39,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 4,
            "unknown_count": 0,
            "total_count": 43,
            "success_rate": 0.9069767441860465,
            "coverage_report": {
                "covered_lines": 60,
                "num_statements": 72,
                "percent_covered": 80.0,
                "percent_covered_display": "80",
                "missing_lines": 12,
                "excluded_lines": 5,
                "num_branches": 18,
                "num_partial_branches": 6,
                "covered_branches": 12,
                "missing_branches": 6
            },
            "coverage_result": {}
        },
        "codelines_count": 1894,
        "codefiles_count": 19,
        "code_length": 61104,
        "test_files_count": 5,
        "test_code_length": 12726,
        "class_diagram": "@startuml\nclass APIProtocol {\n    under_cached_property(func): under_cached_property[_T_co]\n}\nclass PropcacheImplementation {\n    is_pure_python: bool\n    tag(): str\n    imported_module(): ModuleType\n    __str__(): str\n}\nclass Config {\n    env: dict[str, str]\n    flags: dict[str, bool]\n    kwargs: dict[str, str]\n    src: list[str]\n}\nclass _CacheImpl {\n    _cache: _Cache\n}\nclass under_cached_property {\n    __init__(wrapped): Unknown\n    __get__(inst, owner): Self\n    __get__(inst, owner): _T\n    __get__(inst, owner): Union[_T, Self]\n    __set__(inst, value): Unknown\n}\n@enduml",
        "structure": [
            {
                "file": "tests/test_cached_property.py",
                "functions": [
                    {
                        "name": "test_cached_property",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_cached_property_without_cache",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_cached_property_check_without_cache",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_cached_property_caching",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_cached_property_class_docstring",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_set_name",
                        "docstring": "Test that the __set_name__ method is called and checked.",
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_get_without_set_name",
                        "docstring": "Test that get without __set_name__ fails.",
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_ensured_wrapped_function_is_accessible",
                        "docstring": "Test that the wrapped function can be accessed from python.",
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "APIProtocol",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "cached_property",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "func"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Mock property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": [
                            "__slots__"
                        ]
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Mock property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": [
                            "__slots__"
                        ]
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Docstring.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Docstring.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "prop",
                                "docstring": "Docstring.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": "A class.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Docstring.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_benchmarks.py",
                "functions": [
                    {
                        "name": "test_under_cached_property_cache_hit",
                        "docstring": "Benchmark for under_cached_property cache hit.",
                        "comments": null,
                        "args": [
                            "benchmark"
                        ]
                    },
                    {
                        "name": "test_cached_property_cache_hit",
                        "docstring": "Benchmark for cached_property cache hit.",
                        "comments": null,
                        "args": [
                            "benchmark"
                        ]
                    },
                    {
                        "name": "test_under_cached_property_cache_miss",
                        "docstring": "Benchmark for under_cached_property cache miss.",
                        "comments": null,
                        "args": [
                            "benchmark"
                        ]
                    },
                    {
                        "name": "test_cached_property_cache_miss",
                        "docstring": "Benchmark for cached_property cache miss.",
                        "comments": null,
                        "args": [
                            "benchmark"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "Test",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Return the value of the property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "Test",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Return the value of the property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "Test",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Return the value of the property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "Test",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "prop",
                                "docstring": "Return the value of the property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/conftest.py",
                "functions": [
                    {
                        "name": "propcache_implementation",
                        "docstring": "Return a propcache variant facade.",
                        "comments": null,
                        "args": [
                            "request"
                        ]
                    },
                    {
                        "name": "propcache_module",
                        "docstring": "Return a pre-imported module containing a propcache variant.",
                        "comments": null,
                        "args": [
                            "propcache_implementation"
                        ]
                    },
                    {
                        "name": "pytest_addoption",
                        "docstring": "Define a new ``--c-extensions`` flag.\n\nThis lets the callers deselect tests executed against the C-extension\nversion of the ``propcache`` implementation.",
                        "comments": null,
                        "args": [
                            "parser",
                            "pluginmanager"
                        ]
                    },
                    {
                        "name": "pytest_collection_modifyitems",
                        "docstring": "Deselect tests against C-extensions when requested via CLI.",
                        "comments": null,
                        "args": [
                            "session",
                            "config",
                            "items"
                        ]
                    },
                    {
                        "name": "pytest_configure",
                        "docstring": "Declare the C-extension marker in config.",
                        "comments": null,
                        "args": [
                            "config"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "PropcacheImplementation",
                        "docstring": "A facade for accessing importable propcache module variants.\n\nAn instance essentially represents a c-extension or a pure-python module.\nThe actual underlying module is accessed dynamically through a property and\nis cached.\n\nIt also has a text tag depending on what variant it is, and a string\nrepresentation suitable for use in Pytest's test IDs via parametrization.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "tag",
                                "docstring": "Return a text representation of the pure-python attribute.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "imported_module",
                                "docstring": "Return a loaded importable containing a propcache variant.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": "Render the implementation facade instance as a string.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_init.py",
                "functions": [
                    {
                        "name": "test_api_at_top_level",
                        "docstring": "Verify the public API is accessible at top-level.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_public_api_is_discoverable_in_dir",
                        "docstring": "Verify the public API is discoverable programmatically.",
                        "comments": null,
                        "args": [
                            "prop_name"
                        ]
                    },
                    {
                        "name": "test_importing_invalid_attr_raises",
                        "docstring": "Verify importing an invalid attribute raises an AttributeError.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_import_error_invalid_attr",
                        "docstring": "Verify importing an invalid attribute raises an ImportError.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_no_wildcard_imports",
                        "docstring": "Verify wildcard imports are prohibited.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_under_cached_property.py",
                "functions": [
                    {
                        "name": "test_under_cached_property",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_under_cached_property_typeddict",
                        "docstring": "Test static typing passes with TypedDict.",
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_under_cached_property_assignment",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_under_cached_property_without_cache",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_under_cached_property_check_without_cache",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_under_cached_property_caching",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_under_cached_property_class_docstring",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    },
                    {
                        "name": "test_ensured_wrapped_function_is_accessible",
                        "docstring": "Test that the wrapped function can be accessed from python.",
                        "comments": null,
                        "args": [
                            "propcache_module"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "APIProtocol",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "under_cached_property",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "func"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop2",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "_Cache",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop2",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Mock property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Mock property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Mock property.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Docstring.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Docstring.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "A",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Init.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "prop",
                                "docstring": "Docstring.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_api.py",
                "functions": [
                    {
                        "name": "test_api",
                        "docstring": "Verify the public API is accessible.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "docs/conf.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "packaging/pep517_backend/_cython_configuration.py",
                "functions": [
                    {
                        "name": "get_local_cython_config",
                        "docstring": "Grab optional build dependencies from pyproject.toml config.\n\n:returns: config section from ``pyproject.toml``\n:rtype: dict\n\nThis basically reads entries from::\n\n    [tool.local.cythonize]\n    # Env vars provisioned during cythonize call\n    src = [\"src/**/*.pyx\"]\n\n    [tool.local.cythonize.env]\n    # Env vars provisioned during cythonize call\n    LDFLAGS = \"-lssh\"\n\n    [tool.local.cythonize.flags]\n    # This section can contain the following booleans:\n    # * annotate — generate annotated HTML page for source files\n    # * build — build extension modules using distutils\n    # * inplace — build extension modules in place using distutils (implies -b)\n    # * force — force recompilation\n    # * quiet — be less verbose during compilation\n    # * lenient — increase Python compat by ignoring some compile time errors\n    # * keep-going — compile as much as possible, ignore compilation failures\n    annotate = false\n    build = false\n    inplace = true\n    force = true\n    quiet = false\n    lenient = false\n    keep-going = false\n\n    [tool.local.cythonize.kwargs]\n    # This section can contain args that have values:\n    # * exclude=PATTERN      exclude certain file patterns from the compilation\n    # * parallel=N    run builds in N parallel jobs (default: calculated per system)\n    exclude = \"**.py\"\n    parallel = 12\n\n    [tool.local.cythonize.kwargs.directives]\n    # This section can contain compiler directives\n    # NAME = \"VALUE\"\n\n    [tool.local.cythonize.kwargs.compile-time-env]\n    # This section can contain compile time env vars\n    # NAME = \"VALUE\"\n\n    [tool.local.cythonize.kwargs.options]\n    # This section can contain cythonize options\n    # NAME = \"VALUE\"",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "make_cythonize_cli_args_from_config",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "config"
                        ]
                    },
                    {
                        "name": "patched_env",
                        "docstring": "Temporary set given env vars.\n\n:param env: tmp env vars to set\n:type env: dict\n\n:yields: None",
                        "comments": null,
                        "args": [
                            "env",
                            "cython_line_tracing_requested"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "Config",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "packaging/pep517_backend/_backend.py",
                "functions": [
                    {
                        "name": "_is_truthy_setting_value",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "setting_value"
                        ]
                    },
                    {
                        "name": "_get_setting_value",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "config_settings",
                            "config_setting_name",
                            "env_var_name"
                        ]
                    },
                    {
                        "name": "_make_pure_python",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "config_settings"
                        ]
                    },
                    {
                        "name": "_include_cython_line_tracing",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "config_settings"
                        ]
                    },
                    {
                        "name": "patched_distutils_cmd_install",
                        "docstring": "Make `install_lib` of `install` cmd always use `platlib`.\n\n:yields: None",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "patched_dist_has_ext_modules",
                        "docstring": "Make `has_ext_modules` of `Distribution` always return `True`.\n\n:yields: None",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "patched_dist_get_long_description",
                        "docstring": "Make `has_ext_modules` of `Distribution` always return `True`.\n\n:yields: None",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "_exclude_dir_path",
                        "docstring": "Prevent recursive directory traversal.",
                        "comments": null,
                        "args": [
                            "excluded_dir_path",
                            "visited_directory",
                            "_visited_dir_contents"
                        ]
                    },
                    {
                        "name": "_in_temporary_directory",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "src_dir"
                        ]
                    },
                    {
                        "name": "maybe_prebuild_c_extensions",
                        "docstring": "Pre-build C-extensions in a temporary directory, when needed.\n\nThis context manager also patches metadata, setuptools and distutils.\n\n:param build_inplace: Whether to copy and chdir to a temporary location.\n:param config_settings: :pep:`517` config settings mapping.",
                        "comments": null,
                        "args": [
                            "line_trace_cython_when_unset",
                            "build_inplace",
                            "config_settings"
                        ]
                    },
                    {
                        "name": "build_wheel",
                        "docstring": "Produce a built wheel.\n\nThis wraps the corresponding ``setuptools``' build backend hook.\n\n:param wheel_directory: Directory to put the resulting wheel in.\n:param config_settings: :pep:`517` config settings mapping.\n:param metadata_directory: :file:`.dist-info` directory path.",
                        "comments": null,
                        "args": [
                            "wheel_directory",
                            "config_settings",
                            "metadata_directory"
                        ]
                    },
                    {
                        "name": "build_editable",
                        "docstring": "Produce a built wheel for editable installs.\n\nThis wraps the corresponding ``setuptools``' build backend hook.\n\n:param wheel_directory: Directory to put the resulting wheel in.\n:param config_settings: :pep:`517` config settings mapping.\n:param metadata_directory: :file:`.dist-info` directory path.",
                        "comments": null,
                        "args": [
                            "wheel_directory",
                            "config_settings",
                            "metadata_directory"
                        ]
                    },
                    {
                        "name": "get_requires_for_build_wheel",
                        "docstring": "Determine additional requirements for building wheels.\n\n:param config_settings: :pep:`517` config settings mapping.",
                        "comments": null,
                        "args": [
                            "config_settings"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "packaging/pep517_backend/hooks.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "packaging/pep517_backend/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "packaging/pep517_backend/__main__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "packaging/pep517_backend/_compat.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "packaging/pep517_backend/cli.py",
                "functions": [
                    {
                        "name": "run_main_program",
                        "docstring": "Invoke ``translate-cython`` or fail.",
                        "comments": null,
                        "args": [
                            "argv"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "packaging/pep517_backend/_transformers.py",
                "functions": [
                    {
                        "name": "_emit_opt_pairs",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "opt_pair"
                        ]
                    },
                    {
                        "name": "get_cli_kwargs_from_config",
                        "docstring": "Make a list of options with values from config.",
                        "comments": null,
                        "args": [
                            "kwargs_map"
                        ]
                    },
                    {
                        "name": "get_enabled_cli_flags_from_config",
                        "docstring": "Make a list of enabled boolean flags from config.",
                        "comments": null,
                        "args": [
                            "flags_map"
                        ]
                    },
                    {
                        "name": "sanitize_rst_roles",
                        "docstring": "Replace RST roles with inline highlighting.",
                        "comments": null,
                        "args": [
                            "rst_source_text"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/propcache/_helpers_py.py",
                "functions": [],
                "classes": [
                    {
                        "name": "_CacheImpl",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "under_cached_property",
                        "docstring": "Use as a class method decorator.\n\nIt operates almost exactly like\nthe Python `@property` decorator, but it puts the result of the\nmethod it decorates into the instance dict after the first call,\neffectively replacing the function it decorates with an instance\nvariable.  It is, in Python parlance, a data descriptor.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "wrapped"
                                ]
                            },
                            {
                                "name": "__get__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "inst",
                                    "owner"
                                ]
                            },
                            {
                                "name": "__get__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "inst",
                                    "owner"
                                ]
                            },
                            {
                                "name": "__get__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "inst",
                                    "owner"
                                ]
                            },
                            {
                                "name": "__set__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "inst",
                                    "value"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/propcache/_helpers.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/propcache/api.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/propcache/__init__.py",
                "functions": [
                    {
                        "name": "_import_facade",
                        "docstring": "Import the public API from the `api` module.",
                        "comments": null,
                        "args": [
                            "attr"
                        ]
                    },
                    {
                        "name": "_dir_facade",
                        "docstring": "Include the public API in the module's dir() output.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            }
        ],
        "test_cases": {
            "tests/test_api.py::test_api": {
                "testid": "tests/test_api.py::test_api",
                "result": "passed",
                "test_implementation": "def test_api() -> None:\n    \"\"\"Verify the public API is accessible.\"\"\"\n    assert api.cached_property is not None\n    assert api.under_cached_property is not None\n    assert api.cached_property is _helpers.cached_property\n    assert api.under_cached_property is _helpers.under_cached_property"
            },
            "tests/test_benchmarks.py::test_under_cached_property_cache_hit": {
                "testid": "tests/test_benchmarks.py::test_under_cached_property_cache_hit",
                "result": "skipped",
                "test_implementation": "def test_under_cached_property_cache_hit(benchmark: \"BenchmarkFixture\") -> None:\n    \"\"\"Benchmark for under_cached_property cache hit.\"\"\"\n\n    class Test:\n        def __init__(self) -> None:\n            self._cache = {\"prop\": 42}\n\n        @under_cached_property\n        def prop(self) -> int:\n            \"\"\"Return the value of the property.\"\"\"\n            raise NotImplementedError\n\n    t = Test()\n\n    @benchmark\n    def _run() -> None:\n        for _ in range(100):\n            t.prop"
            },
            "tests/test_benchmarks.py::test_cached_property_cache_hit": {
                "testid": "tests/test_benchmarks.py::test_cached_property_cache_hit",
                "result": "skipped",
                "test_implementation": "def test_cached_property_cache_hit(benchmark: \"BenchmarkFixture\") -> None:\n    \"\"\"Benchmark for cached_property cache hit.\"\"\"\n\n    class Test:\n        def __init__(self) -> None:\n            self.__dict__[\"prop\"] = 42\n\n        @cached_property\n        def prop(self) -> int:\n            \"\"\"Return the value of the property.\"\"\"\n            raise NotImplementedError\n\n    t = Test()\n\n    @benchmark\n    def _run() -> None:\n        for _ in range(100):\n            t.prop"
            },
            "tests/test_benchmarks.py::test_under_cached_property_cache_miss": {
                "testid": "tests/test_benchmarks.py::test_under_cached_property_cache_miss",
                "result": "skipped",
                "test_implementation": "def test_under_cached_property_cache_miss(benchmark: \"BenchmarkFixture\") -> None:\n    \"\"\"Benchmark for under_cached_property cache miss.\"\"\"\n\n    class Test:\n        def __init__(self) -> None:\n            self._cache: dict[str, int] = {}\n\n        @under_cached_property\n        def prop(self) -> int:\n            \"\"\"Return the value of the property.\"\"\"\n            return 42\n\n    t = Test()\n    cache = t._cache\n\n    @benchmark\n    def _run() -> None:\n        for _ in range(100):\n            cache.pop(\"prop\", None)\n            t.prop"
            },
            "tests/test_benchmarks.py::test_cached_property_cache_miss": {
                "testid": "tests/test_benchmarks.py::test_cached_property_cache_miss",
                "result": "skipped",
                "test_implementation": "def test_cached_property_cache_miss(benchmark: \"BenchmarkFixture\") -> None:\n    \"\"\"Benchmark for cached_property cache miss.\"\"\"\n\n    class Test:\n\n        @cached_property\n        def prop(self) -> int:\n            \"\"\"Return the value of the property.\"\"\"\n            return 42\n\n    t = Test()\n    cache = t.__dict__\n\n    @benchmark\n    def _run() -> None:\n        for _ in range(100):\n            cache.pop(\"prop\", None)\n            t.prop"
            },
            "tests/test_cached_property.py::test_cached_property[c-extension-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> int:\n            return 1\n\n    a = A()\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop, int)\n    assert a.prop == 1"
            },
            "tests/test_cached_property.py::test_cached_property_without_cache[c-extension-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property_without_cache[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property_without_cache(propcache_module: APIProtocol) -> None:\n    class A:\n\n        __slots__ = ()\n\n        def __init__(self) -> None:\n            pass\n\n        @propcache_module.cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n\n    with pytest.raises(AttributeError):\n        a.prop = 123  # type: ignore[assignment]"
            },
            "tests/test_cached_property.py::test_cached_property_check_without_cache[c-extension-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property_check_without_cache[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property_check_without_cache(propcache_module: APIProtocol) -> None:\n    class A:\n\n        __slots__ = ()\n\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n    with pytest.raises((TypeError, AttributeError)):\n        assert a.prop == 1"
            },
            "tests/test_cached_property.py::test_cached_property_caching[c-extension-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property_caching[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property_caching(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> int:\n            \"\"\"Docstring.\"\"\"\n            return 1\n\n    a = A()\n    assert a.prop == 1"
            },
            "tests/test_cached_property.py::test_cached_property_class_docstring[c-extension-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property_class_docstring[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property_class_docstring(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> None:\n            \"\"\"Docstring.\"\"\"\n\n    if TYPE_CHECKING:\n        assert isinstance(A.prop, cached_property)\n    else:\n        assert isinstance(A.prop, propcache_module.cached_property)\n    assert \"Docstring.\" == A.prop.__doc__"
            },
            "tests/test_cached_property.py::test_set_name[c-extension-module]": {
                "testid": "tests/test_cached_property.py::test_set_name[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_set_name(propcache_module: APIProtocol) -> None:\n    \"\"\"Test that the __set_name__ method is called and checked.\"\"\"\n\n    class A:\n\n        @propcache_module.cached_property\n        def prop(self) -> None:\n            \"\"\"Docstring.\"\"\"\n\n    A.prop.__set_name__(A, \"prop\")\n\n    match = r\"Cannot assign the same cached_property to two \"\n    with pytest.raises(TypeError, match=match):\n        A.prop.__set_name__(A, \"something_else\")"
            },
            "tests/test_cached_property.py::test_get_without_set_name[c-extension-module]": {
                "testid": "tests/test_cached_property.py::test_get_without_set_name[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_get_without_set_name(propcache_module: APIProtocol) -> None:\n    \"\"\"Test that get without __set_name__ fails.\"\"\"\n    cp = propcache_module.cached_property(not_)\n\n    class A:\n        \"\"\"A class.\"\"\"\n\n    A.cp = cp  # type: ignore[attr-defined]\n    match = r\"Cannot use cached_property instance \"\n    with pytest.raises(TypeError, match=match):\n        _ = A().cp  # type: ignore[attr-defined]"
            },
            "tests/test_cached_property.py::test_ensured_wrapped_function_is_accessible[c-extension-module]": {
                "testid": "tests/test_cached_property.py::test_ensured_wrapped_function_is_accessible[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_ensured_wrapped_function_is_accessible(propcache_module: APIProtocol) -> None:\n    \"\"\"Test that the wrapped function can be accessed from python.\"\"\"\n\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> int:\n            \"\"\"Docstring.\"\"\"\n            return 1\n\n    a = A()\n    assert A.prop.func(a) == 1"
            },
            "tests/test_under_cached_property.py::test_under_cached_property[c-extension-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            self._cache: dict[str, int] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> int:\n            return 1\n\n        @propcache_module.under_cached_property\n        def prop2(self) -> str:\n            return \"foo\"\n\n    a = A()\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop, int)\n    assert a.prop == 1\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop2, str)\n    assert a.prop2 == \"foo\""
            },
            "tests/test_under_cached_property.py::test_under_cached_property_typeddict[c-extension-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_typeddict[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_typeddict(propcache_module: APIProtocol) -> None:\n    \"\"\"Test static typing passes with TypedDict.\"\"\"\n\n    class _Cache(TypedDict, total=False):\n        prop: int\n        prop2: str\n\n    class A:\n        def __init__(self) -> None:\n            self._cache: _Cache = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> int:\n            return 1\n\n        @propcache_module.under_cached_property\n        def prop2(self) -> str:\n            return \"foo\"\n\n    a = A()\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop, int)\n    assert a.prop == 1\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop2, str)\n    assert a.prop2 == \"foo\""
            },
            "tests/test_under_cached_property.py::test_under_cached_property_assignment[c-extension-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_assignment[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_assignment(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            self._cache: dict[str, Any] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n\n    with pytest.raises(AttributeError):\n        a.prop = 123  # type: ignore[assignment]"
            },
            "tests/test_under_cached_property.py::test_under_cached_property_without_cache[c-extension-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_without_cache[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_without_cache(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n            self._cache: dict[str, int] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n\n    with pytest.raises(AttributeError):\n        a.prop = 123  # type: ignore[assignment]"
            },
            "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache[c-extension-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_check_without_cache(\n    propcache_module: APIProtocol,\n) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n            # Note that self._cache is intentionally missing\n            # here to verify AttributeError\n\n        @propcache_module.under_cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n    with pytest.raises(AttributeError):\n        _ = a.prop  # type: ignore[call-overload]"
            },
            "tests/test_under_cached_property.py::test_under_cached_property_caching[c-extension-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_caching[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_caching(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            self._cache: dict[str, int] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> int:\n            \"\"\"Docstring.\"\"\"\n            return 1\n\n    a = A()\n    assert a.prop == 1"
            },
            "tests/test_under_cached_property.py::test_under_cached_property_class_docstring[c-extension-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_class_docstring[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_class_docstring(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.under_cached_property\n        def prop(self) -> None:\n            \"\"\"Docstring.\"\"\"\n\n    if TYPE_CHECKING:\n        # At type checking, the fixture doesn't represent the real module, so\n        # we use the global-level imported module to verify the isinstance() check here\n        # matches the behaviour users would see in real code.\n        assert isinstance(A.prop, under_cached_property)\n    else:\n        assert isinstance(A.prop, propcache_module.under_cached_property)\n    assert \"Docstring.\" == A.prop.__doc__"
            },
            "tests/test_under_cached_property.py::test_ensured_wrapped_function_is_accessible[c-extension-module]": {
                "testid": "tests/test_under_cached_property.py::test_ensured_wrapped_function_is_accessible[c-extension-module]",
                "result": "passed",
                "test_implementation": "def test_ensured_wrapped_function_is_accessible(propcache_module: APIProtocol) -> None:\n    \"\"\"Test that the wrapped function can be accessed from python.\"\"\"\n\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n            self._cache: dict[str, int] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> int:\n            \"\"\"Docstring.\"\"\"\n            return 1\n\n    a = A()\n    assert A.prop.wrapped(a) == 1"
            },
            "tests/test_cached_property.py::test_cached_property[pure-python-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> int:\n            return 1\n\n    a = A()\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop, int)\n    assert a.prop == 1"
            },
            "tests/test_cached_property.py::test_cached_property_without_cache[pure-python-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property_without_cache[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property_without_cache(propcache_module: APIProtocol) -> None:\n    class A:\n\n        __slots__ = ()\n\n        def __init__(self) -> None:\n            pass\n\n        @propcache_module.cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n\n    with pytest.raises(AttributeError):\n        a.prop = 123  # type: ignore[assignment]"
            },
            "tests/test_cached_property.py::test_cached_property_check_without_cache[pure-python-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property_check_without_cache[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property_check_without_cache(propcache_module: APIProtocol) -> None:\n    class A:\n\n        __slots__ = ()\n\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n    with pytest.raises((TypeError, AttributeError)):\n        assert a.prop == 1"
            },
            "tests/test_cached_property.py::test_cached_property_caching[pure-python-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property_caching[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property_caching(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> int:\n            \"\"\"Docstring.\"\"\"\n            return 1\n\n    a = A()\n    assert a.prop == 1"
            },
            "tests/test_cached_property.py::test_cached_property_class_docstring[pure-python-module]": {
                "testid": "tests/test_cached_property.py::test_cached_property_class_docstring[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_cached_property_class_docstring(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> None:\n            \"\"\"Docstring.\"\"\"\n\n    if TYPE_CHECKING:\n        assert isinstance(A.prop, cached_property)\n    else:\n        assert isinstance(A.prop, propcache_module.cached_property)\n    assert \"Docstring.\" == A.prop.__doc__"
            },
            "tests/test_cached_property.py::test_set_name[pure-python-module]": {
                "testid": "tests/test_cached_property.py::test_set_name[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_set_name(propcache_module: APIProtocol) -> None:\n    \"\"\"Test that the __set_name__ method is called and checked.\"\"\"\n\n    class A:\n\n        @propcache_module.cached_property\n        def prop(self) -> None:\n            \"\"\"Docstring.\"\"\"\n\n    A.prop.__set_name__(A, \"prop\")\n\n    match = r\"Cannot assign the same cached_property to two \"\n    with pytest.raises(TypeError, match=match):\n        A.prop.__set_name__(A, \"something_else\")"
            },
            "tests/test_cached_property.py::test_get_without_set_name[pure-python-module]": {
                "testid": "tests/test_cached_property.py::test_get_without_set_name[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_get_without_set_name(propcache_module: APIProtocol) -> None:\n    \"\"\"Test that get without __set_name__ fails.\"\"\"\n    cp = propcache_module.cached_property(not_)\n\n    class A:\n        \"\"\"A class.\"\"\"\n\n    A.cp = cp  # type: ignore[attr-defined]\n    match = r\"Cannot use cached_property instance \"\n    with pytest.raises(TypeError, match=match):\n        _ = A().cp  # type: ignore[attr-defined]"
            },
            "tests/test_cached_property.py::test_ensured_wrapped_function_is_accessible[pure-python-module]": {
                "testid": "tests/test_cached_property.py::test_ensured_wrapped_function_is_accessible[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_ensured_wrapped_function_is_accessible(propcache_module: APIProtocol) -> None:\n    \"\"\"Test that the wrapped function can be accessed from python.\"\"\"\n\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.cached_property\n        def prop(self) -> int:\n            \"\"\"Docstring.\"\"\"\n            return 1\n\n    a = A()\n    assert A.prop.func(a) == 1"
            },
            "tests/test_under_cached_property.py::test_under_cached_property[pure-python-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            self._cache: dict[str, int] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> int:\n            return 1\n\n        @propcache_module.under_cached_property\n        def prop2(self) -> str:\n            return \"foo\"\n\n    a = A()\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop, int)\n    assert a.prop == 1\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop2, str)\n    assert a.prop2 == \"foo\""
            },
            "tests/test_under_cached_property.py::test_under_cached_property_typeddict[pure-python-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_typeddict[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_typeddict(propcache_module: APIProtocol) -> None:\n    \"\"\"Test static typing passes with TypedDict.\"\"\"\n\n    class _Cache(TypedDict, total=False):\n        prop: int\n        prop2: str\n\n    class A:\n        def __init__(self) -> None:\n            self._cache: _Cache = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> int:\n            return 1\n\n        @propcache_module.under_cached_property\n        def prop2(self) -> str:\n            return \"foo\"\n\n    a = A()\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop, int)\n    assert a.prop == 1\n    if sys.version_info >= (3, 11):\n        assert_type(a.prop2, str)\n    assert a.prop2 == \"foo\""
            },
            "tests/test_under_cached_property.py::test_under_cached_property_assignment[pure-python-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_assignment[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_assignment(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            self._cache: dict[str, Any] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n\n    with pytest.raises(AttributeError):\n        a.prop = 123  # type: ignore[assignment]"
            },
            "tests/test_under_cached_property.py::test_under_cached_property_without_cache[pure-python-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_without_cache[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_without_cache(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n            self._cache: dict[str, int] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n\n    with pytest.raises(AttributeError):\n        a.prop = 123  # type: ignore[assignment]"
            },
            "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache[pure-python-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_check_without_cache(\n    propcache_module: APIProtocol,\n) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n            # Note that self._cache is intentionally missing\n            # here to verify AttributeError\n\n        @propcache_module.under_cached_property\n        def prop(self) -> None:\n            \"\"\"Mock property.\"\"\"\n\n    a = A()\n    with pytest.raises(AttributeError):\n        _ = a.prop  # type: ignore[call-overload]"
            },
            "tests/test_under_cached_property.py::test_under_cached_property_caching[pure-python-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_caching[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_caching(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            self._cache: dict[str, int] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> int:\n            \"\"\"Docstring.\"\"\"\n            return 1\n\n    a = A()\n    assert a.prop == 1"
            },
            "tests/test_under_cached_property.py::test_under_cached_property_class_docstring[pure-python-module]": {
                "testid": "tests/test_under_cached_property.py::test_under_cached_property_class_docstring[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_under_cached_property_class_docstring(propcache_module: APIProtocol) -> None:\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n\n        @propcache_module.under_cached_property\n        def prop(self) -> None:\n            \"\"\"Docstring.\"\"\"\n\n    if TYPE_CHECKING:\n        # At type checking, the fixture doesn't represent the real module, so\n        # we use the global-level imported module to verify the isinstance() check here\n        # matches the behaviour users would see in real code.\n        assert isinstance(A.prop, under_cached_property)\n    else:\n        assert isinstance(A.prop, propcache_module.under_cached_property)\n    assert \"Docstring.\" == A.prop.__doc__"
            },
            "tests/test_under_cached_property.py::test_ensured_wrapped_function_is_accessible[pure-python-module]": {
                "testid": "tests/test_under_cached_property.py::test_ensured_wrapped_function_is_accessible[pure-python-module]",
                "result": "passed",
                "test_implementation": "def test_ensured_wrapped_function_is_accessible(propcache_module: APIProtocol) -> None:\n    \"\"\"Test that the wrapped function can be accessed from python.\"\"\"\n\n    class A:\n        def __init__(self) -> None:\n            \"\"\"Init.\"\"\"\n            self._cache: dict[str, int] = {}\n\n        @propcache_module.under_cached_property\n        def prop(self) -> int:\n            \"\"\"Docstring.\"\"\"\n            return 1\n\n    a = A()\n    assert A.prop.wrapped(a) == 1"
            },
            "tests/test_init.py::test_api_at_top_level": {
                "testid": "tests/test_init.py::test_api_at_top_level",
                "result": "passed",
                "test_implementation": "def test_api_at_top_level() -> None:\n    \"\"\"Verify the public API is accessible at top-level.\"\"\"\n    assert propcache.cached_property is not None\n    assert propcache.under_cached_property is not None\n    assert propcache.cached_property is _helpers.cached_property\n    assert propcache.under_cached_property is _helpers.under_cached_property"
            },
            "tests/test_init.py::test_public_api_is_discoverable_in_dir[cached_property]": {
                "testid": "tests/test_init.py::test_public_api_is_discoverable_in_dir[cached_property]",
                "result": "passed",
                "test_implementation": "def test_public_api_is_discoverable_in_dir(prop_name: str) -> None:\n    \"\"\"Verify the public API is discoverable programmatically.\"\"\"\n    assert prop_name in dir(propcache)"
            },
            "tests/test_init.py::test_public_api_is_discoverable_in_dir[under_cached_property]": {
                "testid": "tests/test_init.py::test_public_api_is_discoverable_in_dir[under_cached_property]",
                "result": "passed",
                "test_implementation": "def test_public_api_is_discoverable_in_dir(prop_name: str) -> None:\n    \"\"\"Verify the public API is discoverable programmatically.\"\"\"\n    assert prop_name in dir(propcache)"
            },
            "tests/test_init.py::test_importing_invalid_attr_raises": {
                "testid": "tests/test_init.py::test_importing_invalid_attr_raises",
                "result": "passed",
                "test_implementation": "def test_importing_invalid_attr_raises() -> None:\n    \"\"\"Verify importing an invalid attribute raises an AttributeError.\"\"\"\n    match = r\"^module 'propcache' has no attribute 'invalid_attr'$\"\n    with pytest.raises(AttributeError, match=match):\n        propcache.invalid_attr"
            },
            "tests/test_init.py::test_import_error_invalid_attr": {
                "testid": "tests/test_init.py::test_import_error_invalid_attr",
                "result": "passed",
                "test_implementation": "def test_import_error_invalid_attr() -> None:\n    \"\"\"Verify importing an invalid attribute raises an ImportError.\"\"\"\n    # No match here because the error is raised by the import system\n    # and may vary between Python versions.\n    with pytest.raises(ImportError):\n        from propcache import invalid_attr  # noqa: F401"
            },
            "tests/test_init.py::test_no_wildcard_imports": {
                "testid": "tests/test_init.py::test_no_wildcard_imports",
                "result": "passed",
                "test_implementation": "def test_no_wildcard_imports() -> None:\n    \"\"\"Verify wildcard imports are prohibited.\"\"\"\n    assert not propcache.__all__"
            }
        },
        "SRS_document": "# Software Requirements Specification: propcache\n\n## 1. Introduction\n\n### 1.1 Purpose\nThis Software Requirements Specification (SRS) document defines the functional and non-functional requirements for the `propcache` Python library. Its primary purpose is to serve as a definitive guide for software developers who will be assessed on their ability to develop a complete, functional software project based solely on this SRS. The assessment will measure success by passing a comprehensive set of original test cases, including both public (provided with this SRS) and private tests. Therefore, this document emphasizes clarity, comprehensiveness, and appropriate abstraction to allow for independent design and implementation choices while ensuring all specified functionalities are met.\n\n### 1.2 Scope\nThe `propcache` library provides specialized property caching mechanisms for Python classes. The scope of this SRS covers two main property caching decorators: `cached_property` and `under_cached_property`. It also includes requirements related to the library's API accessibility, build and distribution options, and a command-line utility for Cython translation. The library is intended for Python 3.9 and later versions.\n\n### 1.3 Definitions, Acronyms, and Abbreviations\n*   **SRS**: Software Requirements Specification\n*   **API**: Application Programming Interface\n*   **CLI**: Command-Line Interface\n*   **Decorator**: A Python design pattern that allows adding new functionality to an existing object without modifying its structure.\n*   **Property**: In Python, a special kind of attribute that can have behavior (getter, setter, deleter) associated with its access.\n*   **Caching**: Storing the result of an expensive computation so that it can be retrieved quickly on subsequent requests.\n*   **Instance**: An object created from a class.\n*   **`__dict__`**: A dictionary or other mapping object used to store an object’s (writable) attributes.\n*   **`_cache`**: A specific dictionary attribute expected on an instance for use by `under_cached_property`.\n*   **PEP**: Python Enhancement Proposal.\n*   **Cython**: An optimizing static compiler for both Python and the extended Cython programming language.\n\n### 1.4 References\n*   Original README.md for `propcache` (provided as input).\n*   Original source code of `propcache` (provided as input for LLM contextual understanding).\n*   Original test case implementations for `propcache` (provided as input).\n*   Python `functools.cached_property` documentation (behavioral reference for `propcache.cached_property`).\n\n### 1.5 Overview\nThis SRS is organized into three main sections:\n*   **Section 1 (Introduction):** Provides purpose, scope, definitions, references, and an overview of the document.\n*   **Section 2 (Overall Description):** Describes the general factors affecting the product and its requirements, including product perspective, functions, user characteristics, constraints, and assumptions.\n*   **Section 3 (Specific Requirements):** Details all requirements, including functional requirements for the core library, build/distribution, CLI, and any non-functional requirements. Each requirement is uniquely identified and, where applicable, traced to original test cases or source code analysis.\n\n## 2. Overall Description\n\n### 2.1 Product Perspective\n`propcache` is a Python library designed to offer high-performance property caching solutions for Python classes. It can be used as a drop-in enhancement or alternative to standard Python property mechanisms when caching of computed values is desired. The library provides implementations that can leverage C-extensions for speed on CPython, with a pure Python fallback mechanism.\n\n### 2.2 Product Functions\nThe primary functions of the `propcache` library are:\n*   **`cached_property` Decorator:** Provides a property whose value is computed once per instance and then cached in the instance's `__dict__` (or equivalent standard attribute storage). Its behavior is designed to be equivalent to Python's `functools.cached_property`.\n*   **`under_cached_property` Decorator:** Provides a property whose value is computed once per instance and then cached in a specific instance attribute named `_cache` (which must be a dictionary). This property is strictly read-only.\n*   **Selectable Implementations:** The library supports both C-extension-based and pure Python implementations, with mechanisms for users to select the pure Python version during installation or runtime.\n*   **Cython Translation CLI:** A command-line utility is provided to facilitate the translation of Cython (`.pyx`) source files to C/C++ files as part of a build process.\n\n### 2.3 User Characteristics\nThe intended users of this library are Python developers who need to:\n*   Optimize class property access by caching computationally intensive results.\n*   Have fine-grained control over where cached values are stored (standard `__dict__` vs. a dedicated `_cache` attribute).\n*   Optionally leverage C-extensions for performance gains in CPython environments.\n\n### 2.4 Constraints\n*   **C1:** The software must be compatible with Python 3.9 and newer versions.\n*   **C2:** The C-extension, if built, relies on a C compiler and Python headers being available during installation from a source distribution.\n*   **C3:** The pure Python implementation is always used on non-CPython interpreters (e.g., PyPy).\n\n### 2.5 Assumptions and Dependencies\n*   **A1:** Users of `under_cached_property` are responsible for initializing the `_cache` attribute (as a dictionary) on their class instances before the property is first accessed.\n*   **A2:** For C-extension builds, it is assumed that if Cython is used, a compatible version of Cython is available as a build dependency.\n\n## 3. Specific Requirements\n\n### 3.1 Core Library Functional Requirements\n\n#### 3.1.1 `cached_property` Decorator\nThe `cached_property` decorator transforms a class method into a property whose value is computed on first access and then cached.\n\n*   **FR-CP-001:** The system's `cached_property` shall provide functionality equivalent to Python's standard `functools.cached_property`.\n*   **FR-CP-002:** Upon first access of a `cached_property` on an instance, the system shall compute the value by calling the original wrapped method.\n*   **FR-CP-003:** The computed value of a `cached_property` shall be stored in the instance's standard attribute storage mechanism (typically `__dict__`), using the property's name as the key.\n*   **FR-CP-004:** Subsequent accesses to the `cached_property` on the same instance shall return the stored value without recomputing it.\n*   **FR-CP-005:** Attempting to assign a value to an instance's `cached_property` attribute (e.g., `instance.prop = value`) after its value has been computed and cached shall raise an `AttributeError`.\n    *   This describes the behavior of the `__set__` method of the `cached_property` descriptor itself.\n*   **FR-CP-006:** If an instance does not have a dictionary as its `__dict__` attribute, or if the `__dict__` attribute is missing (e.g., for a class using `__slots__` without `__dict__`), accessing the `cached_property` shall raise a `TypeError` or `AttributeError`.\n*   **FR-CP-007:** The `cached_property` descriptor shall preserve the `__doc__` string of the original wrapped method.\n    *   The docstring should be accessible via `ClassName.property_name.__doc__`.\n*   **FR-CP-008:** The original wrapped method for a `cached_property` shall be accessible via the `func` attribute of the property object on the class (e.g., `ClassName.property_name.func`).\n*   **FR-CP-009:** The `cached_property` descriptor must correctly implement the `__set_name__` protocol method.\n    *   This method is called automatically by Python when the class is created, setting the name of the property.\n*   **FR-CP-010:** Attempting to re-assign the name of a `cached_property` instance using its `__set_name__` method after it has already been named shall raise a `TypeError`.\n    *   Example: Calling `A.prop.__set_name__(A, \"new_name\")` after `A.prop` was already established.\n*   **FR-CP-011:** Accessing a `cached_property` that was assigned to a class attribute manually without Python's standard descriptor binding process (and thus `__set_name__` was not properly called) shall raise a `TypeError`.\n\n#### 3.1.2 `under_cached_property` Decorator\nThe `under_cached_property` decorator transforms a class method into a property whose value is computed on first access and then cached in a dedicated `_cache` dictionary on the instance.\n\n*   **FR-UCP-001:** Upon first access of an `under_cached_property` on an instance, the system shall compute the value by calling the original wrapped method.\n    *   The instance must have an attribute `_cache` that is a dictionary.\n*   **FR-UCP-002:** The computed value of an `under_cached_property` shall be stored in the instance's `_cache` dictionary attribute. The key used in the `_cache` dictionary shall be the name of the property.\n    *   The property name is determined from the name of the wrapped function at the time of decoration.\n*   **FR-UCP-003:** Subsequent accesses to the `under_cached_property` on the same instance shall return the stored value from the `_cache` dictionary without recomputing it.\n*   **FR-UCP-004:** The `under_cached_property` shall be strictly read-only. Attempting to assign a value to an instance's `under_cached_property` attribute (e.g., `instance.prop = value`) shall raise an `AttributeError`.\n*   **FR-UCP-005:** If an instance does not have an attribute named `_cache`, or if `_cache` is not a dictionary-like object supporting item access and assignment, accessing the `under_cached_property` shall raise an `AttributeError` (if `_cache` is missing) or potentially other errors like `TypeError` (if `_cache` is not a dict and item access fails). The primary tested case is missing `_cache`.\n*   **FR-UCP-006:** The `under_cached_property` decorator shall preserve the `__doc__` string of the original wrapped method.\n    *   The docstring should be accessible via `ClassName.property_name.__doc__`.\n*   **FR-UCP-007:** The original wrapped method for an `under_cached_property` shall be accessible via the `wrapped` attribute of the property object on the class (e.g., `ClassName.property_name.wrapped`).\n*   **FR-UCP-008:** The `_cache` attribute on an instance can be a standard Python `dict` or a `typing.TypedDict`.\n\n#### 3.1.3 General Property Caching Behaviors (Applicable to both decorators)\n\n*   **FR-GEN-001:** When a decorated property is accessed on the class itself (e.g., `ClassName.property_name`) rather than an instance, the system shall return the descriptor object itself.\n\n#### 3.1.4 Module API and Import Behavior\n\n*   **FR-MOD-001:** The public API components `cached_property` and `under_cached_property` shall be directly accessible from the top-level `propcache` package.\n    *   Example: `import propcache; propcache.cached_property`\n*   **FR-MOD-002:** The public API components `cached_property` and `under_cached_property` shall also be accessible from the `propcache.api` module.\n    *   Example: `from propcache.api import cached_property`\n*   **FR-MOD-003:** The public API components (`cached_property`, `under_cached_property`) shall be discoverable through Python's built-in `dir()` function when applied to the `propcache` module.\n*   **FR-MOD-004:** Attempting to access a non-existent attribute on the `propcache` module directly (e.g., `propcache.invalid_attr`) shall raise an `AttributeError`.\n*   **FR-MOD-005:** Attempting to import a non-existent attribute from the `propcache` module (e.g., `from propcache import invalid_attr`) shall raise an `ImportError`.\n*   **FR-MOD-006:** The system shall ensure that wildcard imports (`from propcache import *`) do not import the public API components (`cached_property`, `under_cached_property`).\n    *   This is typically achieved by not defining `__all__` or setting it to an empty list/tuple.\n\n### 3.2 Build and Distribution Requirements\n\n*   **FR-BLD-001:** The system shall be buildable and installable with C-extensions for CPython environments to provide accelerated performance.\n*   **FR-BLD-002:** The system shall provide a mechanism to be built and installed as a pure Python package, without C-extensions.\n*   **FR-BLD-003:** The selection of a pure Python build/installation shall be controllable by setting the environment variable `PROPCACHE_NO_EXTENSIONS` to a non-empty value.\n*   **FR-BLD-004:** The selection of a pure Python build/installation shall be controllable via a PEP 517 configuration setting named `pure-python`.\n*   **FR-BLD-005:** The system's build process shall support enabling Cython line tracing for debugging C-extensions. This shall be controllable via the environment variable `PROPCACHE_CYTHON_TRACING` or a PEP 517 configuration setting `with-cython-tracing`.\n\n### 3.3 Command-Line Interface (CLI) Requirements\n\n*   **FR-CLI-001:** The system shall provide a command-line interface accessible via `python -m propcache.pep517_backend`.\n*   **FR-CLI-002:** The CLI shall support a subcommand `translate-cython`.\n*   **FR-CLI-003:** The `translate-cython` subcommand shall read Cython build configuration (such as source files, environment variables, flags, and options) from the `pyproject.toml` file in the current working directory, under the `[tool.local.cythonize]` section.\n*   **FR-CLI-004:** The `translate-cython` subcommand shall process the specified Cython source files (`.pyx`) and generate corresponding C/C++ files.\n    *   It effectively invokes Cython's compilation (translation) logic.\n    *   Line tracing shall be enabled for this translation.\n*   **FR-CLI-005:** If the `translate-cython` subcommand is invoked with an incorrect number of arguments or an invalid subcommand name, it shall return an error message and a non-zero exit code.\n\n### 3.4 Non-Functional Requirements\nNo non-functional requirements (NFRs) have been identified that are explicitly and directly validated by specific assertions within the provided original test cases. Performance benchmarks exist but do not assert specific performance targets. Therefore, per the project guidelines, no NFRs are listed here.",
        "structured_requirements": [
            {
                "requirement_id": "C1",
                "requirement_description": "The software must be compatible with Python 3.9 and newer versions.",
                "test_traceability": [
                    {
                        "id": "README.md",
                        "description": "Implicitly tested by all tests, as environment setup targets these Python versions."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "docs/conf.py",
                        "description": "(mentions Python 3 in intersphinx)"
                    },
                    {
                        "id": "packaging/pep517_backend/_cython_configuration.py::(`py_ver_arg = f'-{_python_version_tuple.major!s}'`)",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "C2",
                "requirement_description": "The C-extension, if built, relies on a C compiler and Python headers being available during installation from a source distribution.",
                "test_traceability": [
                    {
                        "id": "README.md",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/*",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "C3",
                "requirement_description": "The pure Python implementation is always used on non-CPython interpreters (e.g., PyPy).",
                "test_traceability": [
                    {
                        "id": "README.md",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers.py",
                        "description": "(`if sys.implementation.name != \"cpython\": NO_EXTENSIONS = True`)"
                    }
                ]
            },
            {
                "requirement_id": "A1",
                "requirement_description": "Users of `under_cached_property` are responsible for initializing the `_cache` attribute (as a dictionary) on their class instances before the property is first accessed.",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "A2",
                "requirement_description": "For C-extension builds, it is assumed that if Cython is used, a compatible version of Cython is available as a build dependency.",
                "test_traceability": [
                    {
                        "id": "packaging/pep517_backend/_backend.py::get_requires_for_build_wheel",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/_backend.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-001",
                "requirement_description": "The system's `cached_property` shall provide functionality equivalent to Python's standard `functools.cached_property`.",
                "test_traceability": [
                    {
                        "id": "All tests in `tests/test_cached_property.py`",
                        "description": "implicitly verify this equivalence for the aspects they cover."
                    },
                    {
                        "id": "README.md",
                        "description": "states \"nearly identical\"."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py",
                        "description": "(imports and re-exports `functools.cached_property`)"
                    },
                    {
                        "id": "src/propcache/_helpers.py",
                        "description": "(selects Python or C version)"
                    },
                    {
                        "id": "_helpers_c",
                        "description": "The C-extension (`_helpers_c`) must mirror this."
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-002",
                "requirement_description": "Upon first access of a `cached_property` on an instance, the system shall compute the value by calling the original wrapped method.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_cached_property",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cached_property.py::test_cached_property_caching",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-003",
                "requirement_description": "The computed value of a `cached_property` shall be stored in the instance's standard attribute storage mechanism (typically `__dict__`), using the property's name as the key.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_cached_property",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cached_property.py::test_cached_property_caching",
                        "description": "The use of `__dict__` is inferred from `functools.cached_property` behavior and tests like `test_cached_property_without_cache`."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-004",
                "requirement_description": "Subsequent accesses to the `cached_property` on the same instance shall return the stored value without recomputing it.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_cached_property_caching",
                        "description": "(verified by ensuring the method isn't called multiple times, though test doesn't explicitly check call count, it's implied by caching behavior)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-005",
                "requirement_description": "Attempting to assign a value to an instance's `cached_property` attribute (e.g., `instance.prop = value`) after its value has been computed and cached shall raise an `AttributeError`.\nThis describes the behavior of the `__set__` method of the `cached_property` descriptor itself.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_cached_property_without_cache",
                        "description": "(although named \"without_cache\", it attempts assignment: `a.prop = 123`)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property.__set__`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-006",
                "requirement_description": "If an instance does not have a dictionary as its `__dict__` attribute, or if the `__dict__` attribute is missing (e.g., for a class using `__slots__` without `__dict__`), accessing the `cached_property` shall raise a `TypeError` or `AttributeError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_cached_property_check_without_cache",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-007",
                "requirement_description": "The `cached_property` descriptor shall preserve the `__doc__` string of the original wrapped method.\nThe docstring should be accessible via `ClassName.property_name.__doc__`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_cached_property_class_docstring",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-008",
                "requirement_description": "The original wrapped method for a `cached_property` shall be accessible via the `func` attribute of the property object on the class (e.g., `ClassName.property_name.func`).",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_ensured_wrapped_function_is_accessible",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-009",
                "requirement_description": "The `cached_property` descriptor must correctly implement the `__set_name__` protocol method.\nThis method is called automatically by Python when the class is created, setting the name of the property.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_set_name",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property.__set_name__`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-010",
                "requirement_description": "Attempting to re-assign the name of a `cached_property` instance using its `__set_name__` method after it has already been named shall raise a `TypeError`.\nExample: Calling `A.prop.__set_name__(A, \"new_name\")` after `A.prop` was already established.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_set_name",
                        "description": "(the second part of the test)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property.__set_name__`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CP-011",
                "requirement_description": "Accessing a `cached_property` that was assigned to a class attribute manually without Python's standard descriptor binding process (and thus `__set_name__` was not properly called) shall raise a `TypeError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_get_without_set_name",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Behavior of `functools.cached_property`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UCP-001",
                "requirement_description": "Upon first access of an `under_cached_property` on an instance, the system shall compute the value by calling the original wrapped method.\nThe instance must have an attribute `_cache` that is a dictionary.",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property",
                        "description": ""
                    },
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_caching",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__get__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UCP-002",
                "requirement_description": "The computed value of an `under_cached_property` shall be stored in the instance's `_cache` dictionary attribute. The key used in the `_cache` dictionary shall be the name of the property.\nThe property name is determined from the name of the wrapped function at the time of decoration.",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property",
                        "description": ""
                    },
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_caching",
                        "description": "(verified by behavior, cache key implicitly tested)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__init__",
                        "description": "(for name)"
                    },
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__get__",
                        "description": "(for storage)"
                    }
                ]
            },
            {
                "requirement_id": "FR-UCP-003",
                "requirement_description": "Subsequent accesses to the `under_cached_property` on the same instance shall return the stored value from the `_cache` dictionary without recomputing it.",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_caching",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__get__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UCP-004",
                "requirement_description": "The `under_cached_property` shall be strictly read-only. Attempting to assign a value to an instance's `under_cached_property` attribute (e.g., `instance.prop = value`) shall raise an `AttributeError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_assignment",
                        "description": ""
                    },
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_without_cache",
                        "description": "(this test also checks assignment)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__set__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UCP-005",
                "requirement_description": "If an instance does not have an attribute named `_cache`, or if `_cache` is not a dictionary-like object supporting item access and assignment, accessing the `under_cached_property` shall raise an `AttributeError` (if `_cache` is missing) or potentially other errors like `TypeError` (if `_cache` is not a dict and item access fails). The primary tested case is missing `_cache`.",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache",
                        "description": "(tests missing `_cache` leading to `AttributeError`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__get__",
                        "description": "(accesses `inst._cache`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-UCP-006",
                "requirement_description": "The `under_cached_property` decorator shall preserve the `__doc__` string of the original wrapped method.\nThe docstring should be accessible via `ClassName.property_name.__doc__`.",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_class_docstring",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UCP-007",
                "requirement_description": "The original wrapped method for an `under_cached_property` shall be accessible via the `wrapped` attribute of the property object on the class (e.g., `ClassName.property_name.wrapped`).",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_ensured_wrapped_function_is_accessible",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UCP-008",
                "requirement_description": "The `_cache` attribute on an instance can be a standard Python `dict` or a `typing.TypedDict`.",
                "test_traceability": [
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_typeddict",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py",
                        "description": "(type hint `_Cache = TypeVar(\"_Cache\", bound=Mapping[str, Any])`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-GEN-001",
                "requirement_description": "When a decorated property is accessed on the class itself (e.g., `ClassName.property_name`) rather than an instance, the system shall return the descriptor object itself.",
                "test_traceability": [
                    {
                        "id": "tests/test_cached_property.py::test_cached_property_class_docstring",
                        "description": ""
                    },
                    {
                        "id": "tests/test_under_cached_property.py::test_under_cached_property_class_docstring",
                        "description": "(verified by `isinstance` checks on `A.prop`)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers_py.py::under_cached_property::__get__",
                        "description": "Standard descriptor protocol behavior (`__get__` method's `inst is None` check)."
                    },
                    {
                        "id": "functools.cached_property",
                        "description": "also follows this."
                    }
                ]
            },
            {
                "requirement_id": "FR-MOD-001",
                "requirement_description": "The public API components `cached_property` and `under_cached_property` shall be directly accessible from the top-level `propcache` package.\nExample: `import propcache; propcache.cached_property`",
                "test_traceability": [
                    {
                        "id": "tests/test_init.py::test_api_at_top_level",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/__init__.py",
                        "description": "(`_import_facade`, `__getattr__`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-MOD-002",
                "requirement_description": "The public API components `cached_property` and `under_cached_property` shall also be accessible from the `propcache.api` module.\nExample: `from propcache.api import cached_property`",
                "test_traceability": [
                    {
                        "id": "tests/test_api.py::test_api",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/api.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-MOD-003",
                "requirement_description": "The public API components (`cached_property`, `under_cached_property`) shall be discoverable through Python's built-in `dir()` function when applied to the `propcache` module.",
                "test_traceability": [
                    {
                        "id": "tests/test_init.py::test_public_api_is_discoverable_in_dir",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/__init__.py",
                        "description": "(`_dir_facade`, `__dir__`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-MOD-004",
                "requirement_description": "Attempting to access a non-existent attribute on the `propcache` module directly (e.g., `propcache.invalid_attr`) shall raise an `AttributeError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_init.py::test_importing_invalid_attr_raises",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/__init__.py",
                        "description": "(`_import_facade` raising `AttributeError`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-MOD-005",
                "requirement_description": "Attempting to import a non-existent attribute from the `propcache` module (e.g., `from propcache import invalid_attr`) shall raise an `ImportError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_init.py::test_import_error_invalid_attr",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Python's import machinery",
                        "description": "reacting to `AttributeError` from `__getattr__`."
                    }
                ]
            },
            {
                "requirement_id": "FR-MOD-006",
                "requirement_description": "The system shall ensure that wildcard imports (`from propcache import *`) do not import the public API components (`cached_property`, `under_cached_property`).\nThis is typically achieved by not defining `__all__` or setting it to an empty list/tuple.",
                "test_traceability": [
                    {
                        "id": "tests/test_init.py::test_no_wildcard_imports",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/__init__.py",
                        "description": "(`__all__ = ()`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-BLD-001",
                "requirement_description": "The system shall be buildable and installable with C-extensions for CPython environments to provide accelerated performance.",
                "test_traceability": [
                    {
                        "id": "README.md",
                        "description": ""
                    },
                    {
                        "id": "tests/conftest.py",
                        "description": "(parametrization for C-extension tests)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/*",
                        "description": ""
                    },
                    {
                        "id": "pyproject.toml",
                        "description": "(not provided but implied by packaging code)."
                    }
                ]
            },
            {
                "requirement_id": "FR-BLD-002",
                "requirement_description": "The system shall provide a mechanism to be built and installed as a pure Python package, without C-extensions.",
                "test_traceability": [
                    {
                        "id": "README.md",
                        "description": ""
                    },
                    {
                        "id": "tests/conftest.py",
                        "description": "(parametrization for pure Python tests)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers.py",
                        "description": "(logic for `NO_EXTENSIONS`)"
                    },
                    {
                        "id": "packaging/pep517_backend/_backend.py",
                        "description": "(handles `pure-python` config)."
                    }
                ]
            },
            {
                "requirement_id": "FR-BLD-003",
                "requirement_description": "The selection of a pure Python build/installation shall be controllable by setting the environment variable `PROPCACHE_NO_EXTENSIONS` to a non-empty value.",
                "test_traceability": [
                    {
                        "id": "README.md",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/propcache/_helpers.py",
                        "description": "(reads `PROPCACHE_NO_EXTENSIONS`)"
                    },
                    {
                        "id": "packaging/pep517_backend/_backend.py",
                        "description": "(reads `PURE_PYTHON_ENV_VAR`)."
                    }
                ]
            },
            {
                "requirement_id": "FR-BLD-004",
                "requirement_description": "The selection of a pure Python build/installation shall be controllable via a PEP 517 configuration setting named `pure-python`.",
                "test_traceability": [
                    {
                        "id": "README.md",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/_backend.py",
                        "description": "(reads `PURE_PYTHON_CONFIG_SETTING`)."
                    }
                ]
            },
            {
                "requirement_id": "FR-BLD-005",
                "requirement_description": "The system's build process shall support enabling Cython line tracing for debugging C-extensions. This shall be controllable via the environment variable `PROPCACHE_CYTHON_TRACING` or a PEP 517 configuration setting `with-cython-tracing`.",
                "test_traceability": [
                    {
                        "id": "None",
                        "description": "None direct test for this feature's outcome, but code exists. Derived from source code analysis."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/_backend.py",
                        "description": "(`_include_cython_line_tracing`)"
                    },
                    {
                        "id": "packaging/pep517_backend/_cython_configuration.py",
                        "description": "(`patched_env` adds CFLAGS for tracing)."
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-001",
                "requirement_description": "The system shall provide a command-line interface accessible via `python -m propcache.pep517_backend`.",
                "test_traceability": [
                    {
                        "id": "Derived from source code analysis",
                        "description": "Derived from source code analysis."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/__main__.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-002",
                "requirement_description": "The CLI shall support a subcommand `translate-cython`.",
                "test_traceability": [
                    {
                        "id": "Derived from source code analysis",
                        "description": "Derived from source code analysis."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/cli.py::run_main_program",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-003",
                "requirement_description": "The `translate-cython` subcommand shall read Cython build configuration (such as source files, environment variables, flags, and options) from the `pyproject.toml` file in the current working directory, under the `[tool.local.cythonize]` section.",
                "test_traceability": [
                    {
                        "id": "Derived from source code analysis",
                        "description": "Derived from source code analysis."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/cli.py::run_main_program",
                        "description": "(calls `_get_local_cython_config`)"
                    },
                    {
                        "id": "packaging/pep517_backend/_cython_configuration.py::get_local_cython_config",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-004",
                "requirement_description": "The `translate-cython` subcommand shall process the specified Cython source files (`.pyx`) and generate corresponding C/C++ files.\nIt effectively invokes Cython's compilation (translation) logic.\nLine tracing shall be enabled for this translation.",
                "test_traceability": [
                    {
                        "id": "Derived from source code analysis",
                        "description": "Derived from source code analysis."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/cli.py::run_main_program",
                        "description": "(calls `_translate_cython_cli_cmd` with `cython_line_tracing_requested=True`)."
                    }
                ]
            },
            {
                "requirement_id": "FR-CLI-005",
                "requirement_description": "If the `translate-cython` subcommand is invoked with an incorrect number of arguments or an invalid subcommand name, it shall return an error message and a non-zero exit code.",
                "test_traceability": [
                    {
                        "id": "Derived from source code analysis",
                        "description": "Derived from source code analysis."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "packaging/pep517_backend/cli.py::run_main_program",
                        "description": ""
                    }
                ]
            }
        ],
        "commit_sha": "a414b2bfb1f5689b2e70195ee5e827c52194a08b",
        "full_code_skeleton": "--- File: packaging/pep517_backend/_cython_configuration.py ---\n```python\nfrom typing import TypedDict\nfrom collections.abc import Iterator\nfrom pathlib import Path\n\nclass Config(TypedDict):\n    env: dict[str, str]\n    flags: dict[str, bool]\n    kwargs: dict[str, str]\n    src: list[str]\n\ndef get_local_cython_config() -> Config:\n    \"\"\"Grab optional build dependencies from pyproject.toml config.\n\n    :returns: config section from ``pyproject.toml``\n    :rtype: dict\n\n    This basically reads entries from::\n\n        [tool.local.cythonize]\n        # Env vars provisioned during cythonize call\n        src = [\"src/**/*.pyx\"]\n\n        [tool.local.cythonize.env]\n        # Env vars provisioned during cythonize call\n        LDFLAGS = \"-lssh\"\n\n        [tool.local.cythonize.flags]\n        # This section can contain the following booleans:\n        # * annotate — generate annotated HTML page for source files\n        # * build — build extension modules using distutils\n        # * inplace — build extension modules in place using distutils (implies -b)\n        # * force — force recompilation\n        # * quiet — be less verbose during compilation\n        # * lenient — increase Python compat by ignoring some compile time errors\n        # * keep-going — compile as much as possible, ignore compilation failures\n        annotate = false\n        build = false\n        inplace = true\n        force = true\n        quiet = false\n        lenient = false\n        keep-going = false\n\n        [tool.local.cythonize.kwargs]\n        # This section can contain args that have values:\n        # * exclude=PATTERN      exclude certain file patterns from the compilation\n        # * parallel=N    run builds in N parallel jobs (default: calculated per system)\n        exclude = \"**.py\"\n        parallel = 12\n\n        [tool.local.cythonize.kwargs.directives]\n        # This section can contain compiler directives\n        # NAME = \"VALUE\"\n\n        [tool.local.cythonize.kwargs.compile-time-env]\n        # This section can contain compile time env vars\n        # NAME = \"VALUE\"\n\n        [tool.local.cythonize.kwargs.options]\n        # This section can contain cythonize options\n        # NAME = \"VALUE\"\n    \"\"\"\n    pass\n\ndef make_cythonize_cli_args_from_config(config: Config) -> list[str]:\n    pass\n\n@contextmanager\ndef patched_env(env: dict[str, str], cython_line_tracing_requested: bool) -> Iterator[None]:\n    \"\"\"Temporary set given env vars.\n\n    :param env: tmp env vars to set\n    :type env: dict\n\n    :yields: None\n    \"\"\"\n    pass\n```\n--- File: packaging/pep517_backend/_backend.py ---\n```python\nimport typing as t\nfrom collections.abc import Iterator\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom distutils.command.install import install as _distutils_install_cmd\nfrom distutils.dist import DistributionMetadata as _DistutilsDistributionMetadata\n\n_ConfigDict = t.Dict[str, t.Union[str, t.List[str], None]]\n\ndef _is_truthy_setting_value(setting_value: str) -> bool:\n    pass\n\ndef _get_setting_value(\n        config_settings: _ConfigDict | None = None,\n        config_setting_name: str | None = None,\n        env_var_name: str | None = None,\n        *,\n        default: bool = False,\n) -> bool:\n    pass\n\ndef _make_pure_python(config_settings: _ConfigDict | None = None) -> bool:\n    pass\n\ndef _include_cython_line_tracing(\n        config_settings: _ConfigDict | None = None,\n        *,\n        default: bool = False,\n) -> bool:\n    pass\n\n@contextmanager\ndef patched_distutils_cmd_install() -> Iterator[None]:\n    \"\"\"Make `install_lib` of `install` cmd always use `platlib`.\n\n    :yields: None\n    \"\"\"\n    pass\n\n@contextmanager\ndef patched_dist_has_ext_modules() -> Iterator[None]:\n    \"\"\"Make `has_ext_modules` of `Distribution` always return `True`.\n\n    :yields: None\n    \"\"\"\n    pass\n\n@contextmanager\ndef patched_dist_get_long_description() -> Iterator[None]:\n    \"\"\"Make `has_ext_modules` of `Distribution` always return `True`.\n\n    :yields: None\n    \"\"\"\n    pass\n\ndef _exclude_dir_path(\n    excluded_dir_path: Path,\n    visited_directory: str,\n    _visited_dir_contents: list[str],\n) -> list[str]:\n    \"\"\"Prevent recursive directory traversal.\"\"\"\n    pass\n\n@contextmanager\ndef _in_temporary_directory(src_dir: Path) -> t.Iterator[None]:\n    pass\n\n@contextmanager\ndef maybe_prebuild_c_extensions(\n        line_trace_cython_when_unset: bool = False,\n        build_inplace: bool = False,\n        config_settings: _ConfigDict | None = None,\n) -> t.Generator[None, t.Any, t.Any]:\n    \"\"\"Pre-build C-extensions in a temporary directory, when needed.\n\n    This context manager also patches metadata, setuptools and distutils.\n\n    :param build_inplace: Whether to copy and chdir to a temporary location.\n    :param config_settings: :pep:`517` config settings mapping.\n\n    \"\"\"\n    pass\n\n@patched_dist_get_long_description()\ndef build_wheel(\n        wheel_directory: str,\n        config_settings: _ConfigDict | None = None,\n        metadata_directory: str | None = None,\n) -> str:\n    \"\"\"Produce a built wheel.\n\n    This wraps the corresponding ``setuptools``' build backend hook.\n\n    :param wheel_directory: Directory to put the resulting wheel in.\n    :param config_settings: :pep:`517` config settings mapping.\n    :param metadata_directory: :file:`.dist-info` directory path.\n\n    \"\"\"\n    pass\n\n@patched_dist_get_long_description()\ndef build_editable(\n        wheel_directory: str,\n        config_settings: _ConfigDict | None = None,\n        metadata_directory: str | None = None,\n) -> str:\n    \"\"\"Produce a built wheel for editable installs.\n\n    This wraps the corresponding ``setuptools``' build backend hook.\n\n    :param wheel_directory: Directory to put the resulting wheel in.\n    :param config_settings: :pep:`517` config settings mapping.\n    :param metadata_directory: :file:`.dist-info` directory path.\n\n    \"\"\"\n    pass\n\ndef get_requires_for_build_wheel(\n        config_settings: _ConfigDict | None = None,\n) -> list[str]:\n    \"\"\"Determine additional requirements for building wheels.\n\n    :param config_settings: :pep:`517` config settings mapping.\n\n    \"\"\"\n    pass\n```\n--- File: packaging/pep517_backend/_compat.py ---\n```python\nimport os\nfrom collections.abc import Iterator\nfrom contextlib import contextmanager\n\n@contextmanager\ndef chdir_cm(path: os.PathLike) -> Iterator[None]:\n    \"\"\"Temporarily change the current directory, recovering on exit.\"\"\"\n    pass\n```\n--- File: packaging/pep517_backend/cli.py ---\n```python\nfrom collections.abc import Sequence\n\ndef run_main_program(argv: Sequence[str]) -> int | str:\n    \"\"\"Invoke ``translate-cython`` or fail.\"\"\"\n    pass\n```\n--- File: packaging/pep517_backend/_transformers.py ---\n```python\nfrom collections.abc import Iterator, Mapping\nfrom typing import Union\n\ndef _emit_opt_pairs(opt_pair: tuple[str, Union[dict[str, str], str]]) -> Iterator[str]:\n    pass\n\ndef get_cli_kwargs_from_config(kwargs_map: dict[str, str]) -> list[str]:\n    \"\"\"Make a list of options with values from config.\"\"\"\n    pass\n\ndef get_enabled_cli_flags_from_config(flags_map: Mapping[str, bool]) -> list[str]:\n    \"\"\"Make a list of enabled boolean flags from config.\"\"\"\n    pass\n\ndef sanitize_rst_roles(rst_source_text: str) -> str:\n    \"\"\"Replace RST roles with inline highlighting.\"\"\"\n    pass\n```\n--- File: src/propcache/_helpers_py.py ---\n```python\nfrom collections.abc import Mapping\nfrom typing import Any, Callable, Generic, Optional, Protocol, TypeVar, Union, overload\n\n_T = TypeVar(\"_T\")\n_Cache = TypeVar(\"_Cache\", bound=Mapping[str, Any])\nSelf = Any\n\nclass _CacheImpl(Protocol[_Cache]):\n    _cache: _Cache\n\nclass under_cached_property(Generic[_T]):\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n    def __init__(self, wrapped: Callable[[Any], _T]) -> None:\n        pass\n\n    @overload\n    def __get__(self, inst: None, owner: Optional[type[object]] = None) -> Self:\n        pass\n\n    @overload\n    def __get__(self, inst: _CacheImpl[Any], owner: Optional[type[object]] = None) -> _T:  # type: ignore[misc]\n        pass\n\n    def __get__(\n        self, inst: Optional[_CacheImpl[Any]], owner: Optional[type[object]] = None\n    ) -> Union[_T, Self]:\n        pass\n\n    def __set__(self, inst: _CacheImpl[Any], value: _T) -> None:\n        pass\n```\n--- File: src/propcache/__init__.py ---\n```python\ndef _import_facade(attr: str) -> object:\n    \"\"\"Import the public API from the `api` module.\"\"\"\n    pass\n\ndef _dir_facade() -> list[str]:\n    \"\"\"Include the public API in the module's dir() output.\"\"\"\n    pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "packaging/pep517_backend/_cython_configuration.py",
                "code": "from typing import TypedDict\nfrom collections.abc import Iterator\nfrom pathlib import Path\n\nclass Config(TypedDict):\n    env: dict[str, str]\n    flags: dict[str, bool]\n    kwargs: dict[str, str]\n    src: list[str]\n\ndef get_local_cython_config() -> Config:\n    \"\"\"Grab optional build dependencies from pyproject.toml config.\n\n    :returns: config section from ``pyproject.toml``\n    :rtype: dict\n\n    This basically reads entries from::\n\n        [tool.local.cythonize]\n        # Env vars provisioned during cythonize call\n        src = [\"src/**/*.pyx\"]\n\n        [tool.local.cythonize.env]\n        # Env vars provisioned during cythonize call\n        LDFLAGS = \"-lssh\"\n\n        [tool.local.cythonize.flags]\n        # This section can contain the following booleans:\n        # * annotate — generate annotated HTML page for source files\n        # * build — build extension modules using distutils\n        # * inplace — build extension modules in place using distutils (implies -b)\n        # * force — force recompilation\n        # * quiet — be less verbose during compilation\n        # * lenient — increase Python compat by ignoring some compile time errors\n        # * keep-going — compile as much as possible, ignore compilation failures\n        annotate = false\n        build = false\n        inplace = true\n        force = true\n        quiet = false\n        lenient = false\n        keep-going = false\n\n        [tool.local.cythonize.kwargs]\n        # This section can contain args that have values:\n        # * exclude=PATTERN      exclude certain file patterns from the compilation\n        # * parallel=N    run builds in N parallel jobs (default: calculated per system)\n        exclude = \"**.py\"\n        parallel = 12\n\n        [tool.local.cythonize.kwargs.directives]\n        # This section can contain compiler directives\n        # NAME = \"VALUE\"\n\n        [tool.local.cythonize.kwargs.compile-time-env]\n        # This section can contain compile time env vars\n        # NAME = \"VALUE\"\n\n        [tool.local.cythonize.kwargs.options]\n        # This section can contain cythonize options\n        # NAME = \"VALUE\"\n    \"\"\"\n    pass\n\ndef make_cythonize_cli_args_from_config(config: Config) -> list[str]:\n    pass\n\n@contextmanager\ndef patched_env(env: dict[str, str], cython_line_tracing_requested: bool) -> Iterator[None]:\n    \"\"\"Temporary set given env vars.\n\n    :param env: tmp env vars to set\n    :type env: dict\n\n    :yields: None\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "packaging/pep517_backend/_backend.py",
                "code": "import typing as t\nfrom collections.abc import Iterator\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom distutils.command.install import install as _distutils_install_cmd\nfrom distutils.dist import DistributionMetadata as _DistutilsDistributionMetadata\n\n_ConfigDict = t.Dict[str, t.Union[str, t.List[str], None]]\n\ndef _is_truthy_setting_value(setting_value: str) -> bool:\n    pass\n\ndef _get_setting_value(\n        config_settings: _ConfigDict | None = None,\n        config_setting_name: str | None = None,\n        env_var_name: str | None = None,\n        *,\n        default: bool = False,\n) -> bool:\n    pass\n\ndef _make_pure_python(config_settings: _ConfigDict | None = None) -> bool:\n    pass\n\ndef _include_cython_line_tracing(\n        config_settings: _ConfigDict | None = None,\n        *,\n        default: bool = False,\n) -> bool:\n    pass\n\n@contextmanager\ndef patched_distutils_cmd_install() -> Iterator[None]:\n    \"\"\"Make `install_lib` of `install` cmd always use `platlib`.\n\n    :yields: None\n    \"\"\"\n    pass\n\n@contextmanager\ndef patched_dist_has_ext_modules() -> Iterator[None]:\n    \"\"\"Make `has_ext_modules` of `Distribution` always return `True`.\n\n    :yields: None\n    \"\"\"\n    pass\n\n@contextmanager\ndef patched_dist_get_long_description() -> Iterator[None]:\n    \"\"\"Make `has_ext_modules` of `Distribution` always return `True`.\n\n    :yields: None\n    \"\"\"\n    pass\n\ndef _exclude_dir_path(\n    excluded_dir_path: Path,\n    visited_directory: str,\n    _visited_dir_contents: list[str],\n) -> list[str]:\n    \"\"\"Prevent recursive directory traversal.\"\"\"\n    pass\n\n@contextmanager\ndef _in_temporary_directory(src_dir: Path) -> t.Iterator[None]:\n    pass\n\n@contextmanager\ndef maybe_prebuild_c_extensions(\n        line_trace_cython_when_unset: bool = False,\n        build_inplace: bool = False,\n        config_settings: _ConfigDict | None = None,\n) -> t.Generator[None, t.Any, t.Any]:\n    \"\"\"Pre-build C-extensions in a temporary directory, when needed.\n\n    This context manager also patches metadata, setuptools and distutils.\n\n    :param build_inplace: Whether to copy and chdir to a temporary location.\n    :param config_settings: :pep:`517` config settings mapping.\n\n    \"\"\"\n    pass\n\n@patched_dist_get_long_description()\ndef build_wheel(\n        wheel_directory: str,\n        config_settings: _ConfigDict | None = None,\n        metadata_directory: str | None = None,\n) -> str:\n    \"\"\"Produce a built wheel.\n\n    This wraps the corresponding ``setuptools``' build backend hook.\n\n    :param wheel_directory: Directory to put the resulting wheel in.\n    :param config_settings: :pep:`517` config settings mapping.\n    :param metadata_directory: :file:`.dist-info` directory path.\n\n    \"\"\"\n    pass\n\n@patched_dist_get_long_description()\ndef build_editable(\n        wheel_directory: str,\n        config_settings: _ConfigDict | None = None,\n        metadata_directory: str | None = None,\n) -> str:\n    \"\"\"Produce a built wheel for editable installs.\n\n    This wraps the corresponding ``setuptools``' build backend hook.\n\n    :param wheel_directory: Directory to put the resulting wheel in.\n    :param config_settings: :pep:`517` config settings mapping.\n    :param metadata_directory: :file:`.dist-info` directory path.\n\n    \"\"\"\n    pass\n\ndef get_requires_for_build_wheel(\n        config_settings: _ConfigDict | None = None,\n) -> list[str]:\n    \"\"\"Determine additional requirements for building wheels.\n\n    :param config_settings: :pep:`517` config settings mapping.\n\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "packaging/pep517_backend/_compat.py",
                "code": "import os\nfrom collections.abc import Iterator\nfrom contextlib import contextmanager\n\n@contextmanager\ndef chdir_cm(path: os.PathLike) -> Iterator[None]:\n    \"\"\"Temporarily change the current directory, recovering on exit.\"\"\"\n    pass\n"
            },
            {
                "file_path": "packaging/pep517_backend/cli.py",
                "code": "from collections.abc import Sequence\n\ndef run_main_program(argv: Sequence[str]) -> int | str:\n    \"\"\"Invoke ``translate-cython`` or fail.\"\"\"\n    pass\n"
            },
            {
                "file_path": "packaging/pep517_backend/_transformers.py",
                "code": "from collections.abc import Iterator, Mapping\nfrom typing import Union\n\ndef _emit_opt_pairs(opt_pair: tuple[str, Union[dict[str, str], str]]) -> Iterator[str]:\n    pass\n\ndef get_cli_kwargs_from_config(kwargs_map: dict[str, str]) -> list[str]:\n    \"\"\"Make a list of options with values from config.\"\"\"\n    pass\n\ndef get_enabled_cli_flags_from_config(flags_map: Mapping[str, bool]) -> list[str]:\n    \"\"\"Make a list of enabled boolean flags from config.\"\"\"\n    pass\n\ndef sanitize_rst_roles(rst_source_text: str) -> str:\n    \"\"\"Replace RST roles with inline highlighting.\"\"\"\n    pass\n"
            },
            {
                "file_path": "src/propcache/_helpers_py.py",
                "code": "from collections.abc import Mapping\nfrom typing import Any, Callable, Generic, Optional, Protocol, TypeVar, Union, overload\n\n_T = TypeVar(\"_T\")\n_Cache = TypeVar(\"_Cache\", bound=Mapping[str, Any])\nSelf = Any\n\nclass _CacheImpl(Protocol[_Cache]):\n    _cache: _Cache\n\nclass under_cached_property(Generic[_T]):\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n    def __init__(self, wrapped: Callable[[Any], _T]) -> None:\n        pass\n\n    @overload\n    def __get__(self, inst: None, owner: Optional[type[object]] = None) -> Self:\n        pass\n\n    @overload\n    def __get__(self, inst: _CacheImpl[Any], owner: Optional[type[object]] = None) -> _T:  # type: ignore[misc]\n        pass\n\n    def __get__(\n        self, inst: Optional[_CacheImpl[Any]], owner: Optional[type[object]] = None\n    ) -> Union[_T, Self]:\n        pass\n\n    def __set__(self, inst: _CacheImpl[Any], value: _T) -> None:\n        pass\n"
            },
            {
                "file_path": "src/propcache/__init__.py",
                "code": "def _import_facade(attr: str) -> object:\n    \"\"\"Import the public API from the `api` module.\"\"\"\n    pass\n\ndef _dir_facade() -> list[str]:\n    \"\"\"Include the public API in the module's dir() output.\"\"\"\n    pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: src/propcache/__init__.py ---\n```python\n__all__ = ()\n\ndef __getattr__(attr: str) -> object:\n    \"\"\"Import the public API from the `api` module.\"\"\"\n    pass\n\ndef __dir__() -> list[str]:\n    \"\"\"Include the public API in the module's dir() output.\"\"\"\n    pass\n```\n--- File: src/propcache/api.py ---\n```python\n# Note: The original file uses imports. To make symbols available for tests\n# without imports in the skeleton, full class/method signatures are replicated here.\n# The developer will need to replace these with appropriate aliasing/imports.\n# Necessary imports for type hints like Callable, Optional, Any should be added by the developer.\n\nclass cached_property:\n    def __init__(self, func: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __set_name__(self, owner: type, name: str) -> None:\n        pass\n\n    def __get__(self, instance: \"Optional[object]\", owner: \"Optional[type]\" = None) -> \"Any\":\n        pass\n\n    def __set__(self, instance: object, value: \"Any\") -> None:\n        pass\n\nclass under_cached_property:\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n    def __init__(self, wrapped: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __get__(self, inst: \"Optional[Any]\", owner: \"Optional[type[object]]\" = None) -> \"Any\":\n        pass\n\n    def __set__(self, inst: \"Any\", value: \"Any\") -> None:\n        pass\n```\n--- File: src/propcache/_helpers.py ---\n```python\n# Note: The original file uses conditional imports. To make symbols available for tests\n# without imports in the skeleton, full class/method signatures are replicated here.\n# The developer will need to replace these with appropriate aliasing/imports from ._helpers_py.\n# Necessary imports for type hints like Callable, Optional, Any should be added by the developer.\n\n__all__ = (\"cached_property\", \"under_cached_property\")\n\nclass cached_property:\n    def __init__(self, func: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __set_name__(self, owner: type, name: str) -> None:\n        pass\n\n    def __get__(self, instance: \"Optional[object]\", owner: \"Optional[type]\" = None) -> \"Any\":\n        pass\n\n    def __set__(self, instance: object, value: \"Any\") -> None:\n        pass\n\nclass under_cached_property:\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n    def __init__(self, wrapped: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __get__(self, inst: \"Optional[Any]\", owner: \"Optional[type[object]]\" = None) -> \"Any\":\n        pass\n\n    def __set__(self, inst: \"Any\", value: \"Any\") -> None:\n        pass\n```\n--- File: src/propcache/_helpers_py.py ---\n```python\n# Necessary imports for type hints like Callable, Optional, Any, Self (if used) should be added by the developer.\n\n__all__ = (\"under_cached_property\", \"cached_property\")\n\nclass under_cached_property:\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n    def __init__(self, wrapped: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __get__(self, inst: \"Optional[Any]\", owner: \"Optional[type[object]]\" = None) -> \"Any\":\n        # In the original, 'inst' might be 'Optional[_CacheImpl[Any]]' and return 'Union[_T, Self]'\n        # This is simplified due to skeleton rules.\n        pass\n\n    def __set__(self, inst: \"Any\", value: \"Any\") -> None:\n        pass\n\n# The original 'cached_property' in this file is an alias to 'functools.cached_property'.\n# The skeleton below provides a class structure matching the interface of\n# functools.cached_property as used by the tests.\nclass cached_property:\n    def __init__(self, func: \"Callable[[Any], Any]\") -> None:\n        # The implementation should store 'func' and set self.__doc__ = func.__doc__\n        pass\n\n    def __set_name__(self, owner: type, name: str) -> None:\n        # The implementation should store 'name' for use in __get__\n        pass\n\n    def __get__(self, instance: \"Optional[object]\", owner: \"Optional[type]\" = None) -> \"Any\":\n        # If instance is None, return self.\n        # Otherwise, compute value using stored func, cache it on instance, and return value.\n        # The actual return type for non-None instance would be '_T' (func's return type).\n        # If instance is None, return type is 'Self' (i.e., cached_property instance).\n        pass\n\n    def __set__(self, instance: object, value: \"Any\") -> None:\n        # This method should raise an AttributeError, as cached_property is read-only.\n        pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "src/propcache/__init__.py",
                "code": "__all__ = ()\n\ndef __getattr__(attr: str) -> object:\n    \"\"\"Import the public API from the `api` module.\"\"\"\n    pass\n\ndef __dir__() -> list[str]:\n    \"\"\"Include the public API in the module's dir() output.\"\"\"\n    pass\n"
            },
            {
                "file_path": "src/propcache/api.py",
                "code": "# Note: The original file uses imports. To make symbols available for tests\n# without imports in the skeleton, full class/method signatures are replicated here.\n# The developer will need to replace these with appropriate aliasing/imports.\n# Necessary imports for type hints like Callable, Optional, Any should be added by the developer.\n\nclass cached_property:\n    def __init__(self, func: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __set_name__(self, owner: type, name: str) -> None:\n        pass\n\n    def __get__(self, instance: \"Optional[object]\", owner: \"Optional[type]\" = None) -> \"Any\":\n        pass\n\n    def __set__(self, instance: object, value: \"Any\") -> None:\n        pass\n\nclass under_cached_property:\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n    def __init__(self, wrapped: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __get__(self, inst: \"Optional[Any]\", owner: \"Optional[type[object]]\" = None) -> \"Any\":\n        pass\n\n    def __set__(self, inst: \"Any\", value: \"Any\") -> None:\n        pass\n"
            },
            {
                "file_path": "src/propcache/_helpers.py",
                "code": "# Note: The original file uses conditional imports. To make symbols available for tests\n# without imports in the skeleton, full class/method signatures are replicated here.\n# The developer will need to replace these with appropriate aliasing/imports from ._helpers_py.\n# Necessary imports for type hints like Callable, Optional, Any should be added by the developer.\n\n__all__ = (\"cached_property\", \"under_cached_property\")\n\nclass cached_property:\n    def __init__(self, func: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __set_name__(self, owner: type, name: str) -> None:\n        pass\n\n    def __get__(self, instance: \"Optional[object]\", owner: \"Optional[type]\" = None) -> \"Any\":\n        pass\n\n    def __set__(self, instance: object, value: \"Any\") -> None:\n        pass\n\nclass under_cached_property:\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n    def __init__(self, wrapped: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __get__(self, inst: \"Optional[Any]\", owner: \"Optional[type[object]]\" = None) -> \"Any\":\n        pass\n\n    def __set__(self, inst: \"Any\", value: \"Any\") -> None:\n        pass\n"
            },
            {
                "file_path": "src/propcache/_helpers_py.py",
                "code": "# Necessary imports for type hints like Callable, Optional, Any, Self (if used) should be added by the developer.\n\n__all__ = (\"under_cached_property\", \"cached_property\")\n\nclass under_cached_property:\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n    def __init__(self, wrapped: \"Callable[[Any], Any]\") -> None:\n        pass\n\n    def __get__(self, inst: \"Optional[Any]\", owner: \"Optional[type[object]]\" = None) -> \"Any\":\n        # In the original, 'inst' might be 'Optional[_CacheImpl[Any]]' and return 'Union[_T, Self]'\n        # This is simplified due to skeleton rules.\n        pass\n\n    def __set__(self, inst: \"Any\", value: \"Any\") -> None:\n        pass\n\n# The original 'cached_property' in this file is an alias to 'functools.cached_property'.\n# The skeleton below provides a class structure matching the interface of\n# functools.cached_property as used by the tests.\nclass cached_property:\n    def __init__(self, func: \"Callable[[Any], Any]\") -> None:\n        # The implementation should store 'func' and set self.__doc__ = func.__doc__\n        pass\n\n    def __set_name__(self, owner: type, name: str) -> None:\n        # The implementation should store 'name' for use in __get__\n        pass\n\n    def __get__(self, instance: \"Optional[object]\", owner: \"Optional[type]\" = None) -> \"Any\":\n        # If instance is None, return self.\n        # Otherwise, compute value using stored func, cache it on instance, and return value.\n        # The actual return type for non-None instance would be '_T' (func's return type).\n        # If instance is None, return type is 'Self' (i.e., cached_property instance).\n        pass\n\n    def __set__(self, instance: object, value: \"Any\") -> None:\n        # This method should raise an AttributeError, as cached_property is read-only.\n        pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "tests/test_init.py::test_api_at_top_level",
                "covers": [
                    "propcache.cached_property - top-level import availability",
                    "propcache.under_cached_property - top-level import availability"
                ]
            },
            {
                "test_id": "tests/test_cached_property.py::test_cached_property[pure-python-module]",
                "covers": [
                    "propcache.cached_property.__init__ - decorator instantiation",
                    "propcache.cached_property.__get__ - happy path, first value retrieval"
                ]
            },
            {
                "test_id": "tests/test_cached_property.py::test_cached_property_caching[pure-python-module]",
                "covers": [
                    "propcache.cached_property.__get__ - caching behavior (value retained on subsequent access)"
                ]
            },
            {
                "test_id": "tests/test_cached_property.py::test_cached_property_without_cache[pure-python-module]",
                "covers": [
                    "propcache.cached_property - read-only behavior (attempting to set, interaction with __slots__)"
                ]
            },
            {
                "test_id": "tests/test_cached_property.py::test_set_name[pure-python-module]",
                "covers": [
                    "propcache.cached_property.__set_name__ - descriptor protocol method"
                ]
            },
            {
                "test_id": "tests/test_cached_property.py::test_ensured_wrapped_function_is_accessible[pure-python-module]",
                "covers": [
                    "propcache.cached_property.func - accessing the wrapped function"
                ]
            },
            {
                "test_id": "tests/test_cached_property.py::test_cached_property_class_docstring[pure-python-module]",
                "covers": [
                    "propcache.cached_property.__doc__ - docstring propagation"
                ]
            },
            {
                "test_id": "tests/test_cached_property.py::test_cached_property_check_without_cache[pure-python-module]",
                "covers": [
                    "propcache.cached_property.__get__ - error on access with __slots__ (no __dict__ for caching)"
                ]
            },
            {
                "test_id": "tests/test_cached_property.py::test_get_without_set_name[pure-python-module]",
                "covers": [
                    "propcache.cached_property.__get__ - error on access if __set_name__ not called (relevant for C-impl style)"
                ]
            },
            {
                "test_id": "tests/test_under_cached_property.py::test_under_cached_property[pure-python-module]",
                "covers": [
                    "propcache.under_cached_property.__init__ - decorator instantiation",
                    "propcache.under_cached_property.__get__ - happy path, first value retrieval"
                ]
            },
            {
                "test_id": "tests/test_under_cached_property.py::test_under_cached_property_caching[pure-python-module]",
                "covers": [
                    "propcache.under_cached_property.__get__ - caching behavior (value retained on subsequent access)"
                ]
            },
            {
                "test_id": "tests/test_under_cached_property.py::test_under_cached_property_assignment[pure-python-module]",
                "covers": [
                    "propcache.under_cached_property.__set__ - read-only behavior (raises AttributeError)"
                ]
            },
            {
                "test_id": "tests/test_under_cached_property.py::test_ensured_wrapped_function_is_accessible[pure-python-module]",
                "covers": [
                    "propcache.under_cached_property.wrapped - accessing the wrapped function"
                ]
            },
            {
                "test_id": "tests/test_under_cached_property.py::test_under_cached_property_class_docstring[pure-python-module]",
                "covers": [
                    "propcache.under_cached_property.__doc__ - docstring propagation"
                ]
            },
            {
                "test_id": "tests/test_under_cached_property.py::test_under_cached_property_check_without_cache[pure-python-module]",
                "covers": [
                    "propcache.under_cached_property.__get__ - error on access if instance._cache attribute is missing"
                ]
            },
            {
                "test_id": "tests/test_api.py::test_api",
                "covers": [
                    "propcache.api.cached_property - import from propcache.api module",
                    "propcache.api.under_cached_property - import from propcache.api module"
                ]
            },
            {
                "test_id": "tests/test_init.py::test_public_api_is_discoverable_in_dir[cached_property]",
                "covers": [
                    "propcache module - dir() discoverability of public API (via __dir__ facade)"
                ]
            },
            {
                "test_id": "tests/test_init.py::test_importing_invalid_attr_raises",
                "covers": [
                    "propcache module - AttributeError on invalid attribute direct access (via __getattr__ facade)"
                ]
            },
            {
                "test_id": "tests/test_init.py::test_import_error_invalid_attr",
                "covers": [
                    "propcache module - ImportError on invalid attribute import (via __getattr__ facade)"
                ]
            },
            {
                "test_id": "tests/test_init.py::test_no_wildcard_imports",
                "covers": [
                    "propcache module - __all__ is empty (prohibits wildcard import)"
                ]
            }
        ]
    },
    {
        "idx": 33112,
        "repo_name": "vigsun19_smartprofiler",
        "url": "https://github.com/vigsun19/smartprofiler",
        "description": "A lightweight, thread-safe Python library and 1-stop shop for profiling execution time, memory usage, CPU time, and function call counts.",
        "stars": 16,
        "forks": 0,
        "language": "python",
        "size": 27,
        "created_at": "2024-10-12T20:41:47+00:00",
        "updated_at": "2025-02-03T15:19:01+00:00",
        "pypi_info": {
            "name": "smartprofiler",
            "version": "0.3.2",
            "url": "https://files.pythonhosted.org/packages/03/64/c44e950abb834c5a50e8211c67921df3e3dbddd420b9cc24794ea08c2495/smartprofiler-0.3.2.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 20,
            "comment_ratio": 0.2219804134929271,
            "pyfile_content_length": 37705,
            "pyfile_code_lines": 919,
            "test_file_exist": true,
            "test_file_content_length": 11883,
            "pytest_framework": true,
            "test_case_num": 24,
            "metadata_path": [
                "setup.py"
            ],
            "readme_content_length": 5955,
            "llm_reason": "The project, SmartProfiler, is a Python library designed for profiling execution time, memory usage, CPU time, function call counts, disk usage, and network usage. It is generally well-suited to be used directly as an AI 'Build from Scratch' benchmark.\n\nPositive Aspects:\n1.  **Clear & Well-Defined Functionality:** The library's purpose and features (profiling via decorators and context managers) are clearly described in the README and demonstrated with examples. It has a modular structure (`time.py`, `memory.py`, etc.).\n2.  **Appropriate Complexity (Medium):** Replicating the profilers involves understanding and using Python's `time`, `tracemalloc`, `threading` (for thread-safety), `functools`, and `logging` modules, and potentially `psutil` or OS-level calls for disk/network stats. This is non-trivial but achievable.\n3.  **Testable & Verifiable Output:** The project includes unit tests for core features (time, memory, CPU, function tracking) using `unittest` and `unittest.mock`. These tests check log outputs and can be adapted/extended for the AI-rebuilt version.\n4.  **Self-Contained (with clarification):** The core profiling functionalities (time, memory, CPU, function calls) are self-contained. For disk and network usage profiling, it's assumed they would rely on system-level statistics (e.g., via an acceptable dependency like `psutil`) or by timing I/O operations, rather than the profiler itself making external network calls or having complex external dependencies for its operation. The `requests` library shown in a README example for network profiling is considered part of the *user's code being profiled*, not a direct dependency of the profiler's measurement mechanism.\n5.  **No GUI:** The project is a library, suitable for non-GUI testing.\n6.  **Well-Understood Problem Domain:** Code profiling is a common software engineering task.\n\nNegative Aspects/Concerns:\n1.  **Clarity of Network/Disk Profiling Implementation:** While assumed to be self-contained (e.g., using `psutil`), the exact mechanism for disk/network usage profiling isn't detailed. The README's `network_usage` example using `requests.get` to an external site could be misinterpreted. The specification for the AI should clarify that the profiler measures local stats/time and does not itself initiate external communications for its core logic or testing.\n2.  **Completeness of Provided Tests:** Test files for `disk_usage` and `network_usage` were not provided in the input, though tests for other modules exist. The AI would need to ensure comprehensive test coverage for all features, potentially by inferring from existing tests or if the original project has them.\n\nOverall, the project is a good candidate. The AI would be tasked to rebuild this profiling library, including all its modules. Minor specification refinements, particularly around the testing strategy for disk and network profiling (ensuring use of mocked or local I/O), would make it an effective benchmark.",
            "llm_project_type": "Python Profiling Library",
            "llm_rating": 75,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "vigsun19_smartprofiler",
            "finish_test": true,
            "test_case_result": {
                "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block": "passed",
                "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block_invalid_type": "passed",
                "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_cpu_time_decorator": "passed",
                "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line": "passed",
                "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line_invalid_type": "passed",
                "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count": "passed",
                "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_multiple_calls": "passed",
                "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_thread_safety": "passed",
                "tests/test_memory.py::TestMemoryProfiler::test_profile_block": "passed",
                "tests/test_memory.py::TestMemoryProfiler::test_profile_block_invalid_type": "passed",
                "tests/test_memory.py::TestMemoryProfiler::test_profile_line": "passed",
                "tests/test_memory.py::TestMemoryProfiler::test_profile_line_invalid_type": "passed",
                "tests/test_memory.py::TestMemoryProfiler::test_profile_memory_decorator": "passed",
                "tests/test_time.py::TimeProfilerTests::test_profile_block": "passed",
                "tests/test_time.py::TimeProfilerTests::test_profile_block_invalid_type": "passed",
                "tests/test_time.py::TimeProfilerTests::test_profile_line": "passed",
                "tests/test_time.py::TimeProfilerTests::test_profile_line_invalid_type": "passed",
                "tests/test_time.py::TimeProfilerTests::test_profile_time_decorator": "passed"
            },
            "success_count": 18,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "unknown_count": 0,
            "total_count": 18,
            "success_rate": 1.0,
            "coverage_report": {
                "covered_lines": 136,
                "num_statements": 202,
                "percent_covered": 68.46846846846847,
                "percent_covered_display": "68",
                "missing_lines": 66,
                "excluded_lines": 0,
                "num_branches": 20,
                "num_partial_branches": 4,
                "covered_branches": 16,
                "missing_branches": 4
            },
            "coverage_result": {}
        },
        "codelines_count": 919,
        "codefiles_count": 20,
        "code_length": 37705,
        "test_files_count": 4,
        "test_code_length": 11883,
        "class_diagram": "@startuml\nclass TestCPUTimeProfiler {\n    test_profile_cpu_time_decorator(): void\n    test_profile_block(): void\n    test_profile_line(): void\n    test_profile_block_invalid_type(): void\n    test_profile_line_invalid_type(): void\n}\nclass TestMemoryProfiler {\n    test_profile_memory_decorator(): void\n    test_profile_block(): void\n    test_profile_line(): void\n    test_profile_block_invalid_type(): void\n    test_profile_line_invalid_type(): void\n}\nclass TestCallCountProfiler {\n    test_profile_call_count(): void\n    test_profile_call_count_multiple_calls(): void\n    test_profile_call_count_thread_safety(): void\n}\nclass TimeProfilerTests {\n    test_profile_time_decorator(mock_time, mock_log): void\n    test_profile_block(mock_time, mock_log): void\n    test_profile_line(mock_time, mock_log): void\n    test_profile_block_invalid_type(mock_time, mock_error): void\n    test_profile_line_invalid_type(mock_time, mock_error): void\n}\n@enduml",
        "structure": [
            {
                "file": "setup.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/test_cpu_time.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestCPUTimeProfiler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_profile_cpu_time_decorator",
                                "docstring": "Test the profile_cpu_time decorator.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_block",
                                "docstring": "Test the profile_block context manager.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_line",
                                "docstring": "Test the profile_line context manager.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_block_invalid_type",
                                "docstring": "Test the profile_block context manager with an invalid profile_type.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_line_invalid_type",
                                "docstring": "Test the profile_line context manager with an invalid profile_type.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_memory.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestMemoryProfiler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_profile_memory_decorator",
                                "docstring": "Test the profile_memory decorator.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_block",
                                "docstring": "Test the profile_block context manager.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_line",
                                "docstring": "Test the profile_line context manager.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_block_invalid_type",
                                "docstring": "Test the profile_block context manager with an invalid profile_type.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_line_invalid_type",
                                "docstring": "Test the profile_line context manager with an invalid profile_type.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/test_function_tracking.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestCallCountProfiler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_profile_call_count",
                                "docstring": "Test the profile_call_count decorator.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_call_count_multiple_calls",
                                "docstring": "Test that call count increments correctly for multiple calls.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_profile_call_count_thread_safety",
                                "docstring": "Test that the call count is thread-safe.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_time.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TimeProfilerTests",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_profile_time_decorator",
                                "docstring": "Test the profile_time decorator.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "mock_time",
                                    "mock_log"
                                ]
                            },
                            {
                                "name": "test_profile_block",
                                "docstring": "Test the profile_block context manager.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "mock_time",
                                    "mock_log"
                                ]
                            },
                            {
                                "name": "test_profile_line",
                                "docstring": "Test the profile_line context manager.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "mock_time",
                                    "mock_log"
                                ]
                            },
                            {
                                "name": "test_profile_block_invalid_type",
                                "docstring": "Test the profile_block context manager with an invalid profile_type.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "mock_time",
                                    "mock_error"
                                ]
                            },
                            {
                                "name": "test_profile_line_invalid_type",
                                "docstring": "Test the profile_line context manager with an invalid profile_type.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "mock_time",
                                    "mock_error"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "examples/cpu_time_profiling_usage.py",
                "functions": [
                    {
                        "name": "cpu_time_function",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "example_block",
                        "docstring": null,
                        "comments": "Example block to demonstrate block profiling for CPU time in a single-threaded environment",
                        "args": []
                    },
                    {
                        "name": "example_line",
                        "docstring": null,
                        "comments": "Example line profiling for CPU time in a single-threaded environment",
                        "args": []
                    },
                    {
                        "name": "run_multithreaded_cpu_time_example",
                        "docstring": null,
                        "comments": "Multithreaded example for CPU time profiling",
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "examples/network_profile_usage.py",
                "functions": [
                    {
                        "name": "simulate_network_activity",
                        "docstring": "Simulate network activity by waiting (no actual request).",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "perform_network_operations",
                        "docstring": "Simulate multiple network operations inside a code block.",
                        "comments": "Example of Block-Level Profiling with the Context Manager",
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "examples/time_profiler_usage.py",
                "functions": [
                    {
                        "name": "time_function",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "example_block",
                        "docstring": null,
                        "comments": "Example block to demonstrate block profiling for time in a single-threaded environment",
                        "args": []
                    },
                    {
                        "name": "example_line",
                        "docstring": null,
                        "comments": "Example line profiling for time in a single-threaded environment",
                        "args": []
                    },
                    {
                        "name": "run_multithreaded_time_example",
                        "docstring": null,
                        "comments": "Multithreaded example for time profiling",
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "examples/multi_profiling_usage.py",
                "functions": [
                    {
                        "name": "example_function",
                        "docstring": "Example function that performs some work to demonstrate profiling.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "examples/disk_profile_usage.py",
                "functions": [
                    {
                        "name": "perform_disk_operations",
                        "docstring": "Simulate multiple disk I/O operations inside a code block.",
                        "comments": "Example function to simulate multiple disk I/O operations",
                        "args": []
                    },
                    {
                        "name": "write_to_file",
                        "docstring": "Simulate writing to a file and performing some disk I/O.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "examples/function_tracking_usage.py",
                "functions": [
                    {
                        "name": "sample_function",
                        "docstring": "A simple function to demonstrate call count tracking.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "another_function",
                        "docstring": "Another function to demonstrate call count tracking.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "multithreaded_function",
                        "docstring": "A simple function for multithreaded execution.",
                        "comments": null,
                        "args": [
                            "thread_name"
                        ]
                    },
                    {
                        "name": "run_multithreaded_example",
                        "docstring": null,
                        "comments": "Function to run the function across multiple threads",
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "examples/memory_profiler_usage.py",
                "functions": [
                    {
                        "name": "memory_function",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "example_block",
                        "docstring": null,
                        "comments": "Example block to demonstrate block profiling for memory in a single-threaded environment",
                        "args": []
                    },
                    {
                        "name": "example_line",
                        "docstring": null,
                        "comments": "Example line profiling for memory in a single-threaded environment",
                        "args": []
                    },
                    {
                        "name": "run_multithreaded_memory_example",
                        "docstring": null,
                        "comments": "Multithreaded example for memory profiling",
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "smartprofiler/function_tracking.py",
                "functions": [
                    {
                        "name": "profile_call_count",
                        "docstring": "Decorator to track the number of times a function is called, thread-safe.",
                        "comments": null,
                        "args": [
                            "func"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "smartprofiler/disk_usage.py",
                "functions": [
                    {
                        "name": "profile_method",
                        "docstring": "Decorator to profile disk usage of a method/function.",
                        "comments": null,
                        "args": [
                            "func"
                        ]
                    },
                    {
                        "name": "profile_block",
                        "docstring": "Context manager to profile disk I/O usage.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "smartprofiler/memory.py",
                "functions": [
                    {
                        "name": "profile_memory",
                        "docstring": "Decorator to profile the memory usage of a function using tracemalloc.",
                        "comments": null,
                        "args": [
                            "func"
                        ]
                    },
                    {
                        "name": "profile_block",
                        "docstring": "Context manager to profile a block of code (memory).",
                        "comments": null,
                        "args": [
                            "profile_type"
                        ]
                    },
                    {
                        "name": "profile_line",
                        "docstring": "Context manager to profile specific lines of code (memory).",
                        "comments": null,
                        "args": [
                            "profile_type"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "smartprofiler/cpu_time.py",
                "functions": [
                    {
                        "name": "profile_cpu_time",
                        "docstring": "Decorator to profile the CPU time used by a function.",
                        "comments": null,
                        "args": [
                            "func"
                        ]
                    },
                    {
                        "name": "profile_block",
                        "docstring": "Context manager to profile a block of code (CPU time).",
                        "comments": null,
                        "args": [
                            "profile_type"
                        ]
                    },
                    {
                        "name": "profile_line",
                        "docstring": "Context manager to profile a specific line of code (CPU time).",
                        "comments": null,
                        "args": [
                            "profile_type"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "smartprofiler/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "smartprofiler/time.py",
                "functions": [
                    {
                        "name": "profile_time",
                        "docstring": "Decorator to profile the execution time of a function.",
                        "comments": null,
                        "args": [
                            "func"
                        ]
                    },
                    {
                        "name": "profile_block",
                        "docstring": "Context manager to profile a block of code (time).",
                        "comments": null,
                        "args": [
                            "profile_type"
                        ]
                    },
                    {
                        "name": "profile_line",
                        "docstring": "Context manager to profile specific lines of code (time).",
                        "comments": null,
                        "args": [
                            "profile_type"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "smartprofiler/network_usage.py",
                "functions": [
                    {
                        "name": "profile_method",
                        "docstring": "Decorator to profile network usage of a method/function.",
                        "comments": null,
                        "args": [
                            "func"
                        ]
                    },
                    {
                        "name": "profile_block",
                        "docstring": "Context manager to profile network usage.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            }
        ],
        "test_cases": {
            "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block": {
                "testid": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block",
                "result": "passed",
                "test_implementation": "    def test_profile_block(self):\n        \"\"\"Test the profile_block context manager.\"\"\"\n        # Use the context manager to profile a block of code\n        with profile_block(profile_type='cpu_time'):\n            # Simulate CPU work\n            time.sleep(0.1)  # Simulate a small delay\n\n        # Check the logs\n        with self.assertLogs(level='INFO') as log:\n            with profile_block(profile_type='cpu_time'):\n                pass\n\n        log_output = log.output[0]\n        self.assertIn(\"Code block took\", log_output)\n        self.assertIn(\"seconds of CPU time\", log_output)"
            },
            "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block_invalid_type": {
                "testid": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block_invalid_type",
                "result": "passed",
                "test_implementation": "    def test_profile_block_invalid_type(self):\n        \"\"\"Test the profile_block context manager with an invalid profile_type.\"\"\"\n        with self.assertRaises(ValueError):\n            with profile_block(profile_type='invalid_type'):\n                pass"
            },
            "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_cpu_time_decorator": {
                "testid": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_cpu_time_decorator",
                "result": "passed",
                "test_implementation": "    def test_profile_cpu_time_decorator(self):\n        \"\"\"Test the profile_cpu_time decorator.\"\"\"\n        # Define a function to test the CPU time decorator\n        @profile_cpu_time\n        def test_function():\n            # Simulate CPU work\n            time.sleep(0.1)  # Simulate a small delay\n\n        # Call the decorated function\n        with self.assertLogs(level='INFO') as log:\n            test_function()\n\n        # Ensure that the CPU time logging occurred\n        log_output = log.output[0]\n        self.assertIn(\"Function 'test_function' used\", log_output)\n        self.assertIn(\"seconds of CPU time\", log_output)"
            },
            "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line": {
                "testid": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line",
                "result": "passed",
                "test_implementation": "    def test_profile_line(self):\n        \"\"\"Test the profile_line context manager.\"\"\"\n\n        # Use the context manager to profile a specific line of code\n        with profile_line(profile_type='cpu_time'):\n            # Simulate CPU work\n            time.sleep(0.1)  # Simulate a small delay\n\n        # Check the logs\n        with self.assertLogs(level='INFO') as log:\n            with profile_line(profile_type='cpu_time'):\n                pass\n\n        log_output = log.output[0]\n        self.assertIn(\"Line(s) took\", log_output)\n        self.assertIn(\"seconds of CPU time\", log_output)"
            },
            "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line_invalid_type": {
                "testid": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line_invalid_type",
                "result": "passed",
                "test_implementation": "    def test_profile_line_invalid_type(self):\n        \"\"\"Test the profile_line context manager with an invalid profile_type.\"\"\"\n        with self.assertRaises(ValueError):\n            with profile_line(profile_type='invalid_type'):\n                pass"
            },
            "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count": {
                "testid": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count",
                "result": "passed",
                "test_implementation": "    def test_profile_call_count(self):\n        \"\"\"Test the profile_call_count decorator.\"\"\"\n        # Define a simple function to apply the decorator to\n        @profile_call_count\n        def test_function():\n            return \"Hello, World!\"\n\n        # Call the decorated function multiple times\n        with self.assertLogs(level='INFO') as log:\n            test_function()\n            test_function()\n\n        # Verify that the call count was logged correctly\n        log_output = log.output\n        self.assertIn(\"Function 'test_function' has been called 1 times\", log_output[0])\n        self.assertIn(\"Function 'test_function' has been called 2 times\", log_output[1])"
            },
            "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_multiple_calls": {
                "testid": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_multiple_calls",
                "result": "passed",
                "test_implementation": "    def test_profile_call_count_multiple_calls(self):\n        \"\"\"Test that call count increments correctly for multiple calls.\"\"\"\n        # Define the function\n        @profile_call_count\n        def test_function():\n            return \"Test\"\n\n        # Call it multiple times\n        with self.assertLogs(level='INFO') as log:\n            for _ in range(5):\n                test_function()\n\n        # Check if the call count was logged properly\n        log_output = log.output\n        self.assertIn(\"Function 'test_function' has been called 5 times\", log_output[-1])"
            },
            "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_thread_safety": {
                "testid": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_thread_safety",
                "result": "passed",
                "test_implementation": "    def test_profile_call_count_thread_safety(self):\n        \"\"\"Test that the call count is thread-safe.\"\"\"\n        # Define the function\n        @profile_call_count\n        def test_function():\n            return \"Test\"\n\n        # Use threads to call the function concurrently\n        def thread_function():\n            for _ in range(3):\n                test_function()\n\n        threads = [threading.Thread(target=thread_function) for _ in range(3)]\n\n        with self.assertLogs(level='INFO') as log:\n            # Start the threads\n            for thread in threads:\n                thread.start()\n\n            # Wait for them to finish\n            for thread in threads:\n                thread.join()\n\n        # Check if the function was called the correct number of times\n        log_output = log.output\n        self.assertIn(\"Function 'test_function' has been called 9 times\", log_output[-1])"
            },
            "tests/test_memory.py::TestMemoryProfiler::test_profile_block": {
                "testid": "tests/test_memory.py::TestMemoryProfiler::test_profile_block",
                "result": "passed",
                "test_implementation": "    def test_profile_block(self):\n        \"\"\"Test the profile_block context manager.\"\"\"\n        # Use the context manager to profile a block of code\n        with profile_block(profile_type='memory'):\n            # Simulate memory usage\n            data = [1] * (10**6)  # Allocate memory\n            time.sleep(0.1)  # Simulate some work\n\n        # Check the logs\n        with self.assertLogs(level='INFO') as log:\n            with profile_block(profile_type='memory'):\n                pass\n\n        log_output = log.output[0]\n        self.assertIn(\"Code block used\", log_output)\n        self.assertIn(\"KB of memory (peak)\", log_output)"
            },
            "tests/test_memory.py::TestMemoryProfiler::test_profile_block_invalid_type": {
                "testid": "tests/test_memory.py::TestMemoryProfiler::test_profile_block_invalid_type",
                "result": "passed",
                "test_implementation": "    def test_profile_block_invalid_type(self):\n        \"\"\"Test the profile_block context manager with an invalid profile_type.\"\"\"\n        with self.assertRaises(ValueError):\n            with profile_block(profile_type='invalid_type'):\n                pass"
            },
            "tests/test_memory.py::TestMemoryProfiler::test_profile_line": {
                "testid": "tests/test_memory.py::TestMemoryProfiler::test_profile_line",
                "result": "passed",
                "test_implementation": "    def test_profile_line(self):\n        \"\"\"Test the profile_line context manager.\"\"\"\n\n        # Use the context manager to profile a specific line of code\n        with profile_line(profile_type='memory'):\n            # Simulate memory usage\n            data = [1] * (10**6)  # Allocate memory\n            time.sleep(0.1)  # Simulate some work\n\n        # Check the logs\n        with self.assertLogs(level='INFO') as log:\n            with profile_line(profile_type='memory'):\n                pass\n\n        log_output = log.output[0]\n        self.assertIn(\"Line(s) used\", log_output)\n        self.assertIn(\"KB of memory (peak)\", log_output)"
            },
            "tests/test_memory.py::TestMemoryProfiler::test_profile_line_invalid_type": {
                "testid": "tests/test_memory.py::TestMemoryProfiler::test_profile_line_invalid_type",
                "result": "passed",
                "test_implementation": "    def test_profile_line_invalid_type(self):\n        \"\"\"Test the profile_line context manager with an invalid profile_type.\"\"\"\n        with self.assertRaises(ValueError):\n            with profile_line(profile_type='invalid_type'):\n                pass"
            },
            "tests/test_memory.py::TestMemoryProfiler::test_profile_memory_decorator": {
                "testid": "tests/test_memory.py::TestMemoryProfiler::test_profile_memory_decorator",
                "result": "passed",
                "test_implementation": "    def test_profile_memory_decorator(self):\n        \"\"\"Test the profile_memory decorator.\"\"\"\n\n        # Define a function to test the memory decorator\n        @profile_memory\n        def test_function():\n            # Simulate memory usage\n            data = [1] * (10**6)  # Allocate memory\n            time.sleep(0.1)  # Simulate some work\n            return data\n\n        # Call the decorated function\n        with self.assertLogs(level='INFO') as log:\n            test_function()\n\n        # Ensure memory logging occurs and check the log message\n        log_output = log.output[0]\n        self.assertIn(\"Function 'test_function' used\", log_output)\n        self.assertIn(\"KB of memory (peak)\", log_output)"
            },
            "tests/test_time.py::TimeProfilerTests::test_profile_block": {
                "testid": "tests/test_time.py::TimeProfilerTests::test_profile_block",
                "result": "passed",
                "test_implementation": "    def test_profile_block(self, mock_time, mock_log):\n        \"\"\"Test the profile_block context manager.\"\"\"\n\n        # Simulate the start and end times for the block\n        mock_time.side_effect = [1.0, 2.0]  # Block should take 1 second\n\n        # Use the context manager\n        with profile_block(profile_type='time'):\n            time.sleep(0.5)  # Simulate work\n\n        # Verify that the logging.info function was called to log execution time\n        mock_log.assert_called_with(\"Code block took 1.0000 seconds\")"
            },
            "tests/test_time.py::TimeProfilerTests::test_profile_block_invalid_type": {
                "testid": "tests/test_time.py::TimeProfilerTests::test_profile_block_invalid_type",
                "result": "passed",
                "test_implementation": "    def test_profile_block_invalid_type(self, mock_time, mock_error):\n        \"\"\"Test the profile_block context manager with an invalid profile_type.\"\"\"\n\n        # Attempt to use the context manager with an invalid profile_type\n        with self.assertRaises(ValueError):\n            with profile_block(profile_type='invalid_type'):\n                pass\n\n        # Verify that an error was logged when an invalid profile_type is used\n        mock_error.assert_called_with(\"Unknown profile_type: 'invalid_type', use 'time'\")"
            },
            "tests/test_time.py::TimeProfilerTests::test_profile_line": {
                "testid": "tests/test_time.py::TimeProfilerTests::test_profile_line",
                "result": "passed",
                "test_implementation": "    def test_profile_line(self, mock_time, mock_log):\n        \"\"\"Test the profile_line context manager.\"\"\"\n\n        # Simulate the start and end times for the line\n        mock_time.side_effect = [1.0, 1.5]  # Line should take 0.5 seconds\n\n        # Use the context manager\n        with profile_line(profile_type='time'):\n            time.sleep(0.5)  # Simulate work on a line\n\n        # Verify that the logging.info function was called to log execution time\n        mock_log.assert_called_with(\"Line(s) took 0.5000 seconds\")"
            },
            "tests/test_time.py::TimeProfilerTests::test_profile_line_invalid_type": {
                "testid": "tests/test_time.py::TimeProfilerTests::test_profile_line_invalid_type",
                "result": "passed",
                "test_implementation": "    def test_profile_line_invalid_type(self, mock_time, mock_error):\n        \"\"\"Test the profile_line context manager with an invalid profile_type.\"\"\"\n\n        # Attempt to use the context manager with an invalid profile_type\n        with self.assertRaises(ValueError):\n            with profile_line(profile_type='invalid_type'):\n                pass\n\n        # Verify that an error was logged when an invalid profile_type is used\n        mock_error.assert_called_with(\"Unknown profile_type: 'invalid_type', use 'time'\")"
            },
            "tests/test_time.py::TimeProfilerTests::test_profile_time_decorator": {
                "testid": "tests/test_time.py::TimeProfilerTests::test_profile_time_decorator",
                "result": "passed",
                "test_implementation": "    def test_profile_time_decorator(self, mock_time, mock_log):\n        \"\"\"Test the profile_time decorator.\"\"\"\n\n        # Simulate the start and end times\n        mock_time.side_effect = [1.0, 2.0]  # Function should take 1 second\n\n        # Define a simple function to apply the decorator to\n        @profile_time\n        def test_function():\n            time.sleep(0.5)  # Simulate work\n\n        # Call the decorated function\n        test_function()\n\n        # Verify that the logging.info function was called to log execution time\n        mock_log.assert_called_with(\"Function 'test_function' took 1.0000 seconds\")"
            }
        },
        "SRS_document": "**Software Requirements Specification: SmartProfiler**\n\n**Table of Contents**\n1.  Introduction\n    1.1 Purpose\n    1.2 Scope\n    1.3 Definitions, Acronyms, and Abbreviations\n    1.4 References\n    1.5 Overview\n2.  Overall Description\n    2.1 Product Perspective\n    2.2 Product Features\n    2.3 User Characteristics\n    2.4 Constraints\n    2.5 Assumptions and Dependencies\n3.  Specific Requirements\n    3.1 Functional Requirements\n        3.1.1 General Profiling Requirements\n        3.1.2 Execution Time Profiling\n        3.1.3 Memory Usage Profiling\n        3.1.4 CPU Time Profiling\n        3.1.5 Function Call Counting\n        3.1.6 Disk Usage Profiling\n        3.1.7 Network Usage Profiling\n        3.1.8 Error Handling\n    3.2 Non-Functional Requirements\n    3.3 External Interface Requirements\n4.  Appendices (Optional)\n\n---\n\n**1. Introduction**\n\n**1.1 Purpose**\nThis Software Requirements Specification (SRS) document defines the requirements for the SmartProfiler library. Its primary goal is to serve as the basis for assessing software developers. Developers will use this SRS and a subset of original test cases to build the SmartProfiler library. Their success will be measured by passing all original test cases, including private ones. Therefore, this document emphasizes clarity, functional comprehensiveness, and appropriate abstraction to allow for independent design and implementation while ensuring all required functionalities are met.\n\n**1.2 Scope**\nThe SmartProfiler library is a Python tool designed for profiling various aspects of Python code, including execution time, memory usage, CPU time, function call counts, disk I/O, and network I/O. It provides mechanisms for profiling entire functions/methods (via decorators) and specific code blocks or lines (via context managers). The library is intended to be lightweight, easy to use, and suitable for multithreaded environments.\n\nThis SRS covers all functional capabilities necessary to replicate the behavior of the original SmartProfiler library as inferable from its source code, test cases, and documentation. It focuses on externally observable behaviors, inputs, outputs, and core processing logic.\n\n**1.3 Definitions, Acronyms, and Abbreviations**\n*   **SRS:** Software Requirements Specification\n*   **CPU:** Central Processing Unit\n*   **I/O:** Input/Output\n*   **KB:** Kilobytes\n*   **GB:** Gigabytes\n*   **Decorator:** A Python design pattern that allows adding new functionality to an existing object without modifying its structure.\n*   **Context Manager:** A Python object that defines the methods `__enter__()` and `__exit__()`, used with the `with` statement to manage resources.\n\n**1.4 References**\n*   SmartProfiler README.md (provided)\n*   SmartProfiler Source Code (provided for LLM contextual understanding only)\n*   SmartProfiler Test Cases (provided for LLM contextual understanding and for developer's public tests)\n\n**1.5 Overview**\nThis document is organized into three main sections:\n*   **Section 1 (Introduction):** Provides an overview of the SRS, its purpose, scope, and definitions.\n*   **Section 2 (Overall Description):** Describes the general factors affecting the product and its requirements, including product perspective, features, user characteristics, constraints, and dependencies.\n*   **Section 3 (Specific Requirements):** Details all functional and non-functional requirements, external interfaces, and other specific constraints. This section is critical for developers implementing the system.\n\n---\n\n**2. Overall Description**\n\n**2.1 Product Perspective**\nSmartProfiler is a Python library intended to be imported and used within other Python applications. It enhances standard Python development by providing tools to monitor and analyze code performance and resource consumption. It relies on standard Python features, the `psutil` library for system-level metrics, and Python's built-in `logging` module for output.\n\n**2.2 Product Features**\nThe major features of SmartProfiler are:\n*   **Execution Time Profiling:** Measure and log the wall-clock time taken by functions or code blocks.\n*   **Memory Usage Profiling:** Measure and log the peak memory consumption of functions or code blocks.\n*   **CPU Time Profiling:** Measure and log the CPU time consumed by functions or code blocks.\n*   **Function Call Counting:** Track and log the number of times a specific function is called.\n*   **Disk Usage Profiling:** Measure and log disk I/O (bytes read/written, operation counts) and disk space usage during the execution of functions or code blocks.\n*   **Network Usage Profiling:** Measure and log network I/O (bytes sent/received, packets sent/received) during the execution of functions or code blocks.\n*   **Multiple Profiling Mechanisms:** Offers decorators for function-level profiling and context managers for block/line-level profiling.\n*   **Logging Integration:** Outputs profiling information via Python's standard `logging` framework.\n*   **Thread-Safe Operation:** Designed to provide correct profiling information in multithreaded applications for applicable profilers.\n\n**2.3 User Characteristics**\nThe users of SmartProfiler are Python developers who need to:\n*   Optimize the performance of their applications.\n*   Debug memory leaks or high memory usage.\n*   Understand CPU utilization of specific code parts.\n*   Track function invocation frequency.\n*   Monitor disk and network activity related to specific code sections.\n\nUsers are expected to be familiar with Python programming, including decorators and context managers, and basic software profiling concepts.\n\n**2.4 Constraints**\n*   **CON-1:** The system shall be implemented in Python.\n*   **CON-2:** The system shall be compatible with Python versions 3.6 and higher.\n*   **CON-3:** The system shall utilize the `psutil` library for metrics requiring system-level information (e.g., disk I/O, network I/O, CPU times for some implementations if `time.process_time` is not sufficient).\n*   **CON-4:** Profiling output shall be directed to Python's standard `logging` system.\n\n**2.5 Assumptions and Dependencies**\n*   **DEP-1:** The `psutil` library must be installed in the Python environment where SmartProfiler is used.\n*   **ASSUM-1:** The user's Python environment will have the `logging` module configured appropriately to capture `INFO` and `ERROR` level messages if they wish to see profiler output. The library itself is not required to perform a global logging configuration beyond what is necessary for its own operation if any. (Note: Source code shows `logging.basicConfig` in modules, which can have side effects if the user also configures logging. For assessment, developers should be aware that the library's logging calls must work with standard logger instances.)\n\n---\n\n**3. Specific Requirements**\n\n**3.1 Functional Requirements**\n\n**3.1.1 General Profiling Requirements**\n*   **FR-G-1:** The system shall provide decorator-based mechanisms to profile entire functions or methods.\n*   **FR-G-2:** The system shall provide context manager-based mechanisms to profile arbitrary blocks of code.\n*   **FR-G-3:** The system shall provide context manager-based mechanisms to profile specific lines of code (conceptually similar to blocks, but potentially logged with different phrasing).\n*   **FR-G-4:** Decorated functions shall retain their original name and docstring.\n*   **FR-G-5:** Profiling operations should function correctly when used in multithreaded applications, ensuring that measurements are not intermingled or corrupted across threads for profilers designed to be thread-safe (e.g., time, CPU time, function call count, memory).\n    *   **Description:** For instance, if two threads simultaneously execute code profiled by context managers using thread-local storage, each thread's profiling result should reflect only its own execution. For memory profiling, concurrent execution of decorated functions should yield correct peak memory for each call context if possible, or manage shared `tracemalloc` state appropriately.\n\n**3.1.2 Execution Time Profiling**\n*   **FR-T-1:** The system shall provide a decorator (`profile_time`) to measure and log the execution (wall-clock) time of a function.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Function '{function_name}' took {duration:.4f} seconds`\n*   **FR-T-2:** The system shall provide a context manager (`profile_block` from `smartprofiler.time`) to measure and log the execution (wall-clock) time of a code block.\n    *   This context manager must be initialized with `profile_type='time'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Code block took {duration:.4f} seconds`\n*   **FR-T-3:** The system shall provide a context manager (`profile_line` from `smartprofiler.time`) to measure and log the execution (wall-clock) time of one or more lines of code.\n    *   This context manager must be initialized with `profile_type='time'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Line(s) took {duration:.4f} seconds`\n\n**3.1.3 Memory Usage Profiling**\n*   **FR-M-1:** The system shall provide a decorator (`profile_memory`) to measure and log the peak memory usage (in Kilobytes) attributed to a function's execution.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Function '{function_name}' used {peak_memory_kb:.2f} KB of memory (peak)`\n*   **FR-M-2:** The system shall provide a context manager (`profile_block` from `smartprofiler.memory`) to measure and log the peak memory usage (in Kilobytes) of a code block.\n    *   This context manager must be initialized with `profile_type='memory'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Code block used {peak_memory_kb:.2f} KB of memory (peak)`\n*   **FR-M-3:** The system shall provide a context manager (`profile_line` from `smartprofiler.memory`) to measure and log the peak memory usage (in Kilobytes) of one or more lines of code.\n    *   This context manager must be initialized with `profile_type='memory'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Line(s) used {peak_memory_kb:.2f} KB of memory (peak)`\n\n**3.1.4 CPU Time Profiling**\n*   **FR-CPU-1:** The system shall provide a decorator (`profile_cpu_time`) to measure and log the CPU time (in seconds) consumed by a function's execution.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Function '{function_name}' used {cpu_time:.4f} seconds of CPU time`\n*   **FR-CPU-2:** The system shall provide a context manager (`profile_block` from `smartprofiler.cpu_time`) to measure and log the CPU time (in seconds) consumed by a code block.\n    *   This context manager must be initialized with `profile_type='cpu_time'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Code block took {cpu_time:.4f} seconds of CPU time`\n*   **FR-CPU-3:** The system shall provide a context manager (`profile_line` from `smartprofiler.cpu_time`) to measure and log the CPU time (in seconds) consumed by one or more lines of code.\n    *   This context manager must be initialized with `profile_type='cpu_time'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Line(s) took {cpu_time:.4f} seconds of CPU time`\n\n**3.1.5 Function Call Counting**\n*   **FR-FCC-1:** The system shall provide a decorator (`profile_call_count`) to track and log the cumulative number of times a specific function has been called.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Function '{function_name}' has been called {count} times`\n    *   The count must be specific to the decorated function.\n\n**3.1.6 Disk Usage Profiling**\n*   **FR-DU-1:** The system shall provide a decorator (`profile_method` from `smartprofiler.disk_usage`) to profile and log disk I/O and disk space usage related to a function's execution.\n    *   It shall log the difference in total bytes read and written by the system during the function's execution at `INFO` level. Log message format: `Function '{function_name}' disk read bytes: {read_bytes_diff}, write bytes: {write_bytes_diff}`\n    *   It shall log the system's disk space usage (Total, Used, Free in GB, formatted to two decimal places) for the root partition ('/') before the function execution at `INFO` level. Log message format: `Disk space before: Total={total_gb:.2f}GB, Used={used_gb:.2f}GB, Free={free_gb:.2f}GB`\n    *   It shall log the system's disk space usage (Total, Used, Free in GB, formatted to two decimal places) for the root partition ('/') after the function execution at `INFO` level. Log message format: `Disk space after: Total={total_gb:.2f}GB, Used={used_gb:.2f}GB, Free={free_gb:.2f}GB`\n*   **FR-DU-2:** The system shall provide a context manager (`profile_block` from `smartprofiler.disk_usage`) to profile and log disk I/O and disk space usage related to a code block's execution.\n    *   It shall log the raw `psutil.disk_io_counters()` object before and after the block execution at `INFO` level. Log message formats: `Disk I/O before: {disk_io_object_before}`, `Disk I/O after: {disk_io_object_after}`\n    *   It shall log the difference in total bytes read, bytes written, read operations, and write operations by the system during the block's execution at `INFO` level. Log message formats: `Bytes read: {read_bytes_diff}`, `Bytes written: {write_bytes_diff}`, `Read operations: {read_ops_diff}`, `Write operations: {write_ops_diff}`\n    *   It shall log the system's disk space usage (Total, Used, Free in GB, formatted to two decimal places) for the root partition ('/') before the block execution at `INFO` level. Log message format: `Disk space before: Total={total_gb:.2f}GB, Used={used_gb:.2f}GB, Free={free_gb:.2f}GB`\n    *   It shall log the system's disk space usage (Total, Used, Free in GB, formatted to two decimal places) for the root partition ('/') after the block execution at `INFO` level. Log message format: `Disk space after: Total={total_gb:.2f}GB, Used={used_gb:.2f}GB, Free={free_gb:.2f}GB`\n\n**3.1.7 Network Usage Profiling**\n*   **FR-NU-1:** The system shall provide a decorator (`profile_method` from `smartprofiler.network_usage`) to profile and log network I/O related to a function's execution.\n    *   It shall log the difference in total bytes sent and received by the system during the function's execution at `INFO` level.\n    *   Log message format: `Function '{function_name}' sent {bytes_sent_diff} bytes, received {bytes_recv_diff} bytes`\n*   **FR-NU-2:** The system shall provide a context manager (`profile_block` from `smartprofiler.network_usage`) to profile and log network I/O related to a code block's execution.\n    *   It shall log the raw `psutil.net_io_counters()` object before and after the block execution at `INFO` level. Log message formats: `Network usage before: {net_io_object_before}`, `Network usage after: {net_io_object_after}`\n    *   It shall log the difference in total bytes sent, bytes received, packets sent, and packets received by the system during the block's execution at `INFO` level. Log message formats: `Bytes sent: {bytes_sent_diff}`, `Bytes received: {bytes_recv_diff}`, `Packets sent: {packets_sent_diff}`, `Packets received: {packets_recv_diff}`\n\n**3.1.8 Error Handling**\n*   **FR-ERR-1:** The time profiling context managers (`profile_block` and `profile_line` from `smartprofiler.time`) shall raise a `ValueError` if initialized with a `profile_type` argument other than `'time'`.\n    *   An error message shall be logged at `ERROR` level: `Unknown profile_type: '{invalid_type}', use 'time'`\n*   **FR-ERR-2:** The memory profiling context managers (`profile_block` and `profile_line` from `smartprofiler.memory`) shall raise a `ValueError` if initialized with a `profile_type` argument other than `'memory'`.\n    *   An error message shall be logged at `ERROR` level: `Unknown profile_type: '{invalid_type}', use 'memory'`\n*   **FR-ERR-3:** The CPU time profiling context managers (`profile_block` and `profile_line` from `smartprofiler.cpu_time`) shall raise a `ValueError` if initialized with a `profile_type` argument other than `'cpu_time'`.\n    *   An error message shall be logged at `ERROR` level: `Unknown profile_type: '{invalid_type}', use 'cpu_time'`\n\n**3.2 Non-Functional Requirements**\n*   **NFR-1:** The function call counter (`profile_call_count`) shall be thread-safe.\n    *   **Description:** Concurrent calls to a function decorated with `profile_call_count` from multiple threads must result in an accurate, monotonically increasing call count without race conditions.\n\n**3.3 External Interface Requirements**\n*   **EIR-1:** The system interacts with the `psutil` library to retrieve disk, network, and potentially CPU usage statistics.\n*   **EIR-2:** The system outputs all profiling information and error messages using Python's standard `logging` module.\n    *   Profiling information is logged at `INFO` level.\n    *   Specific error conditions (e.g., invalid `profile_type`) are logged at `ERROR` level.\n\n---\n**4. Appendices**\n(Not applicable for this assessment-focused SRS)\n</END OF REQUIREMENT FILE>",
        "structured_requirements": [
            {
                "requirement_id": "CON-1",
                "requirement_description": "The system shall be implemented in Python.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "CON-2",
                "requirement_description": "The system shall be compatible with Python versions 3.6 and higher.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "setup.py::python_requires",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "CON-3",
                "requirement_description": "The system shall utilize the `psutil` library for metrics requiring system-level information (e.g., disk I/O, network I/O, CPU times for some implementations if `time.process_time` is not sufficient).",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "setup.py::install_requires",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/disk_usage.py",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/network_usage.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "CON-4",
                "requirement_description": "Profiling output shall be directed to Python's standard `logging` system.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_cpu_time_decorator",
                        "description": "Verified by log assertions in most test cases"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/memory.py",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/cpu_time.py",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/function_tracking.py",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/disk_usage.py",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/network_usage.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "DEP-1",
                "requirement_description": "The `psutil` library must be installed in the Python environment where SmartProfiler is used.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "setup.py::install_requires",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "ASSUM-1",
                "requirement_description": "The user's Python environment will have the `logging` module configured appropriately to capture `INFO` and `ERROR` level messages if they wish to see profiler output. The library itself is not required to perform a global logging configuration beyond what is necessary for its own operation if any. (Note: Source code shows `logging.basicConfig` in modules, which can have side effects if the user also configures logging. For assessment, developers should be aware that the library's logging calls must work with standard logger instances.)",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "FR-G-1",
                "requirement_description": "The system shall provide decorator-based mechanisms to profile entire functions or methods.",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_time_decorator",
                        "description": ""
                    },
                    {
                        "id": "tests/test_memory.py::TestMemoryProfiler::test_profile_memory_decorator",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_cpu_time_decorator",
                        "description": ""
                    },
                    {
                        "id": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py::profile_time",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/memory.py::profile_memory",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/cpu_time.py::profile_cpu_time",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/function_tracking.py::profile_call_count",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/disk_usage.py::profile_method",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/network_usage.py::profile_method",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-G-2",
                "requirement_description": "The system shall provide context manager-based mechanisms to profile arbitrary blocks of code.",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_block",
                        "description": ""
                    },
                    {
                        "id": "tests/test_memory.py::TestMemoryProfiler::test_profile_block",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py::profile_block",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/memory.py::profile_block",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/cpu_time.py::profile_block",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/disk_usage.py::profile_block",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/network_usage.py::profile_block",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-G-3",
                "requirement_description": "The system shall provide context manager-based mechanisms to profile specific lines of code (conceptually similar to blocks, but potentially logged with different phrasing).",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_line",
                        "description": ""
                    },
                    {
                        "id": "tests/test_memory.py::TestMemoryProfiler::test_profile_line",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py::profile_line",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/memory.py::profile_line",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/cpu_time.py::profile_line",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-G-4",
                "requirement_description": "Decorated functions shall retain their original name and docstring.",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_time_decorator",
                        "description": "Implicitly through `func.__name__` usage in log messages. Explicit test not provided but `functools.wraps` is used."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py::profile_time",
                        "description": "(and other decorators using `functools.wraps`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-G-5",
                "requirement_description": "Profiling operations should function correctly when used in multithreaded applications, ensuring that measurements are not intermingled or corrupted across threads for profilers designed to be thread-safe (e.g., time, CPU time, function call count, memory).\n    *   **Description:** For instance, if two threads simultaneously execute code profiled by context managers using thread-local storage, each thread's profiling result should reflect only its own execution. For memory profiling, concurrent execution of decorated functions should yield correct peak memory for each call context if possible, or manage shared `tracemalloc` state appropriately.",
                "test_traceability": [
                    {
                        "id": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_thread_safety",
                        "description": ""
                    },
                    {
                        "id": "examples/cpu_time_profiling_usage.py::run_multithreaded_cpu_time_example",
                        "description": "General multithreaded usage is shown in README examples"
                    },
                    {
                        "id": "examples/memory_profiler_usage.py::run_multithreaded_memory_example",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/function_tracking.py::_lock",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/time.py::_thread_local",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/cpu_time.py::_thread_local",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/memory.py",
                        "description": "uses global `tracemalloc` state modification."
                    }
                ]
            },
            {
                "requirement_id": "FR-T-1",
                "requirement_description": "The system shall provide a decorator (`profile_time`) to measure and log the execution (wall-clock) time of a function.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Function '{function_name}' took {duration:.4f} seconds`",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_time_decorator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py::profile_time",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-T-2",
                "requirement_description": "The system shall provide a context manager (`profile_block` from `smartprofiler.time`) to measure and log the execution (wall-clock) time of a code block.\n    *   This context manager must be initialized with `profile_type='time'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Code block took {duration:.4f} seconds`",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_block",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py::profile_block",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-T-3",
                "requirement_description": "The system shall provide a context manager (`profile_line` from `smartprofiler.time`) to measure and log the execution (wall-clock) time of one or more lines of code.\n    *   This context manager must be initialized with `profile_type='time'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Line(s) took {duration:.4f} seconds`",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_line",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py::profile_line",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-M-1",
                "requirement_description": "The system shall provide a decorator (`profile_memory`) to measure and log the peak memory usage (in Kilobytes) attributed to a function's execution.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Function '{function_name}' used {peak_memory_kb:.2f} KB of memory (peak)`",
                "test_traceability": [
                    {
                        "id": "tests/test_memory.py::TestMemoryProfiler::test_profile_memory_decorator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/memory.py::profile_memory",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-M-2",
                "requirement_description": "The system shall provide a context manager (`profile_block` from `smartprofiler.memory`) to measure and log the peak memory usage (in Kilobytes) of a code block.\n    *   This context manager must be initialized with `profile_type='memory'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Code block used {peak_memory_kb:.2f} KB of memory (peak)`",
                "test_traceability": [
                    {
                        "id": "tests/test_memory.py::TestMemoryProfiler::test_profile_block",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/memory.py::profile_block",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-M-3",
                "requirement_description": "The system shall provide a context manager (`profile_line` from `smartprofiler.memory`) to measure and log the peak memory usage (in Kilobytes) of one or more lines of code.\n    *   This context manager must be initialized with `profile_type='memory'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Line(s) used {peak_memory_kb:.2f} KB of memory (peak)`",
                "test_traceability": [
                    {
                        "id": "tests/test_memory.py::TestMemoryProfiler::test_profile_line",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/memory.py::profile_line",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPU-1",
                "requirement_description": "The system shall provide a decorator (`profile_cpu_time`) to measure and log the CPU time (in seconds) consumed by a function's execution.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Function '{function_name}' used {cpu_time:.4f} seconds of CPU time`",
                "test_traceability": [
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_cpu_time_decorator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/cpu_time.py::profile_cpu_time",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPU-2",
                "requirement_description": "The system shall provide a context manager (`profile_block` from `smartprofiler.cpu_time`) to measure and log the CPU time (in seconds) consumed by a code block.\n    *   This context manager must be initialized with `profile_type='cpu_time'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Code block took {cpu_time:.4f} seconds of CPU time`",
                "test_traceability": [
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/cpu_time.py::profile_block",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPU-3",
                "requirement_description": "The system shall provide a context manager (`profile_line` from `smartprofiler.cpu_time`) to measure and log the CPU time (in seconds) consumed by one or more lines of code.\n    *   This context manager must be initialized with `profile_type='cpu_time'`.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Line(s) took {cpu_time:.4f} seconds of CPU time`",
                "test_traceability": [
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/cpu_time.py::profile_line",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-FCC-1",
                "requirement_description": "The system shall provide a decorator (`profile_call_count`) to track and log the cumulative number of times a specific function has been called.\n    *   The logged message shall be at `INFO` level.\n    *   The log message format shall be: `Function '{function_name}' has been called {count} times`\n    *   The count must be specific to the decorated function.",
                "test_traceability": [
                    {
                        "id": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count",
                        "description": ""
                    },
                    {
                        "id": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_multiple_calls",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/function_tracking.py::profile_call_count",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-DU-1",
                "requirement_description": "The system shall provide a decorator (`profile_method` from `smartprofiler.disk_usage`) to profile and log disk I/O and disk space usage related to a function's execution.\n    *   It shall log the difference in total bytes read and written by the system during the function's execution at `INFO` level. Log message format: `Function '{function_name}' disk read bytes: {read_bytes_diff}, write bytes: {write_bytes_diff}`\n    *   It shall log the system's disk space usage (Total, Used, Free in GB, formatted to two decimal places) for the root partition ('/') before the function execution at `INFO` level. Log message format: `Disk space before: Total={total_gb:.2f}GB, Used={used_gb:.2f}GB, Free={free_gb:.2f}GB`\n    *   It shall log the system's disk space usage (Total, Used, Free in GB, formatted to two decimal places) for the root partition ('/') after the function execution at `INFO` level. Log message format: `Disk space after: Total={total_gb:.2f}GB, Used={used_gb:.2f}GB, Free={free_gb:.2f}GB`",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "smartprofiler/disk_usage.py::profile_method",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-DU-2",
                "requirement_description": "The system shall provide a context manager (`profile_block` from `smartprofiler.disk_usage`) to profile and log disk I/O and disk space usage related to a code block's execution.\n    *   It shall log the raw `psutil.disk_io_counters()` object before and after the block execution at `INFO` level. Log message formats: `Disk I/O before: {disk_io_object_before}`, `Disk I/O after: {disk_io_object_after}`\n    *   It shall log the difference in total bytes read, bytes written, read operations, and write operations by the system during the block's execution at `INFO` level. Log message formats: `Bytes read: {read_bytes_diff}`, `Bytes written: {write_bytes_diff}`, `Read operations: {read_ops_diff}`, `Write operations: {write_ops_diff}`\n    *   It shall log the system's disk space usage (Total, Used, Free in GB, formatted to two decimal places) for the root partition ('/') before the block execution at `INFO` level. Log message format: `Disk space before: Total={total_gb:.2f}GB, Used={used_gb:.2f}GB, Free={free_gb:.2f}GB`\n    *   It shall log the system's disk space usage (Total, Used, Free in GB, formatted to two decimal places) for the root partition ('/') after the block execution at `INFO` level. Log message format: `Disk space after: Total={total_gb:.2f}GB, Used={used_gb:.2f}GB, Free={free_gb:.2f}GB`",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "smartprofiler/disk_usage.py::profile_block",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-NU-1",
                "requirement_description": "The system shall provide a decorator (`profile_method` from `smartprofiler.network_usage`) to profile and log network I/O related to a function's execution.\n    *   It shall log the difference in total bytes sent and received by the system during the function's execution at `INFO` level.\n    *   Log message format: `Function '{function_name}' sent {bytes_sent_diff} bytes, received {bytes_recv_diff} bytes`",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "smartprofiler/network_usage.py::profile_method",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-NU-2",
                "requirement_description": "The system shall provide a context manager (`profile_block` from `smartprofiler.network_usage`) to profile and log network I/O related to a code block's execution.\n    *   It shall log the raw `psutil.net_io_counters()` object before and after the block execution at `INFO` level. Log message formats: `Network usage before: {net_io_object_before}`, `Network usage after: {net_io_object_after}`\n    *   It shall log the difference in total bytes sent, bytes received, packets sent, and packets received by the system during the block's execution at `INFO` level. Log message formats: `Bytes sent: {bytes_sent_diff}`, `Bytes received: {bytes_recv_diff}`, `Packets sent: {packets_sent_diff}`, `Packets received: {packets_recv_diff}`",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "smartprofiler/network_usage.py::profile_block",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-1",
                "requirement_description": "The time profiling context managers (`profile_block` and `profile_line` from `smartprofiler.time`) shall raise a `ValueError` if initialized with a `profile_type` argument other than `'time'`.\n    *   An error message shall be logged at `ERROR` level: `Unknown profile_type: '{invalid_type}', use 'time'`",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_block_invalid_type",
                        "description": ""
                    },
                    {
                        "id": "tests/test_time.py::TimeProfilerTests::test_profile_line_invalid_type",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/time.py::profile_block",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/time.py::profile_line",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-2",
                "requirement_description": "The memory profiling context managers (`profile_block` and `profile_line` from `smartprofiler.memory`) shall raise a `ValueError` if initialized with a `profile_type` argument other than `'memory'`.\n    *   An error message shall be logged at `ERROR` level: `Unknown profile_type: '{invalid_type}', use 'memory'`",
                "test_traceability": [
                    {
                        "id": "tests/test_memory.py::TestMemoryProfiler::test_profile_block_invalid_type",
                        "description": ""
                    },
                    {
                        "id": "tests/test_memory.py::TestMemoryProfiler::test_profile_line_invalid_type",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/memory.py::profile_block",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/memory.py::profile_line",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ERR-3",
                "requirement_description": "The CPU time profiling context managers (`profile_block` and `profile_line` from `smartprofiler.cpu_time`) shall raise a `ValueError` if initialized with a `profile_type` argument other than `'cpu_time'`.\n    *   An error message shall be logged at `ERROR` level: `Unknown profile_type: '{invalid_type}', use 'cpu_time'`",
                "test_traceability": [
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block_invalid_type",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line_invalid_type",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/cpu_time.py::profile_block",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/cpu_time.py::profile_line",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "NFR-1",
                "requirement_description": "The function call counter (`profile_call_count`) shall be thread-safe.\n    *   **Description:** Concurrent calls to a function decorated with `profile_call_count` from multiple threads must result in an accurate, monotonically increasing call count without race conditions.",
                "test_traceability": [
                    {
                        "id": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count_thread_safety",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "smartprofiler/function_tracking.py::profile_call_count",
                        "description": "(specifically the use of `_lock`)"
                    }
                ]
            },
            {
                "requirement_id": "EIR-1",
                "requirement_description": "The system interacts with the `psutil` library to retrieve disk, network, and potentially CPU usage statistics.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "smartprofiler/disk_usage.py",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/network_usage.py",
                        "description": ""
                    },
                    {
                        "id": "smartprofiler/memory.py",
                        "description": "(uses `tracemalloc`, not `psutil` for memory)."
                    }
                ]
            },
            {
                "requirement_id": "EIR-2",
                "requirement_description": "The system outputs all profiling information and error messages using Python's standard `logging` module.\n    *   Profiling information is logged at `INFO` level.\n    *   Specific error conditions (e.g., invalid `profile_type`) are logged at `ERROR` level.",
                "test_traceability": [
                    {
                        "id": "tests/test_time.py",
                        "description": "Log assertions in all test files (e.g., `tests/test_time.py`, `tests/test_memory.py`, etc.)."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "All smartprofiler/*.py modules",
                        "description": "using `logging`."
                    }
                ]
            }
        ],
        "commit_sha": "ea6e15a1d5dff78f6e195869f61f137a9412fe3f",
        "full_code_skeleton": "--- File: examples/cpu_time_profiling_usage.py ---\n```python\n@profile_cpu_time\ndef cpu_time_function():\n    pass\n\ndef example_block():\n    pass\n\ndef example_line():\n    pass\n\ndef run_multithreaded_cpu_time_example():\n    pass\n```\n--- File: examples/network_profile_usage.py ---\n```python\n@profile_method\ndef simulate_network_activity():\n    \"\"\"Simulate network activity by waiting (no actual request).\"\"\"\n    pass\n\ndef perform_network_operations():\n    \"\"\"Simulate multiple network operations inside a code block.\"\"\"\n    pass\n```\n--- File: examples/time_profiler_usage.py ---\n```python\n@profile_time\ndef time_function():\n    pass\n\ndef example_block():\n    pass\n\ndef example_line():\n    pass\n\ndef run_multithreaded_time_example():\n    pass\n```\n--- File: examples/multi_profiling_usage.py ---\n```python\n@profile_time\n@profile_memory\n@profile_cpu_time\n@profile_call_count\ndef example_function():\n    \"\"\"Example function that performs some work to demonstrate profiling.\"\"\"\n    pass\n```\n--- File: examples/disk_profile_usage.py ---\n```python\ndef perform_disk_operations():\n    \"\"\"Simulate multiple disk I/O operations inside a code block.\"\"\"\n    pass\n\n@profile_method\ndef write_to_file():\n    \"\"\"Simulate writing to a file and performing some disk I/O.\"\"\"\n    pass\n```\n--- File: examples/function_tracking_usage.py ---\n```python\n@profile_call_count\ndef sample_function():\n    \"\"\"A simple function to demonstrate call count tracking.\"\"\"\n    pass\n\n@profile_call_count\ndef another_function():\n    \"\"\"Another function to demonstrate call count tracking.\"\"\"\n    pass\n\n@profile_call_count\ndef multithreaded_function(thread_name):\n    \"\"\"A simple function for multithreaded execution.\"\"\"\n    pass\n\ndef run_multithreaded_example():\n    pass\n```\n--- File: examples/memory_profiler_usage.py ---\n```python\n@profile_memory\ndef memory_function():\n    pass\n\ndef example_block():\n    pass\n\ndef example_line():\n    pass\n\ndef run_multithreaded_memory_example():\n    pass\n```\n--- File: smartprofiler/function_tracking.py ---\n```python\ndef profile_call_count(func):\n    \"\"\"Decorator to track the number of times a function is called, thread-safe.\"\"\"\n    pass\n```\n--- File: smartprofiler/disk_usage.py ---\n```python\ndef profile_method(func):\n    \"\"\"Decorator to profile disk usage of a method/function.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block():\n    \"\"\"Context manager to profile disk I/O usage.\"\"\"\n    pass\n```\n--- File: smartprofiler/memory.py ---\n```python\ndef profile_memory(func):\n    \"\"\"Decorator to profile the memory usage of a function using tracemalloc.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block(profile_type='memory'):\n    \"\"\"Context manager to profile a block of code (memory).\"\"\"\n    pass\n\n@contextmanager\ndef profile_line(profile_type='memory'):\n    \"\"\"Context manager to profile specific lines of code (memory).\"\"\"\n    pass\n```\n--- File: smartprofiler/cpu_time.py ---\n```python\ndef profile_cpu_time(func):\n    \"\"\"Decorator to profile the CPU time used by a function.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block(profile_type='cpu_time'):\n    \"\"\"Context manager to profile a block of code (CPU time).\"\"\"\n    pass\n\n@contextmanager\ndef profile_line(profile_type='cpu_time'):\n    \"\"\"Context manager to profile a specific line of code (CPU time).\"\"\"\n    pass\n```\n--- File: smartprofiler/__init__.py ---\n```python\n```\n--- File: smartprofiler/time.py ---\n```python\ndef profile_time(func):\n    \"\"\"Decorator to profile the execution time of a function.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block(profile_type='time'):\n    \"\"\"Context manager to profile a block of code (time).\"\"\"\n    pass\n\n@contextmanager\ndef profile_line(profile_type='time'):\n    \"\"\"Context manager to profile specific lines of code (time).\"\"\"\n    pass\n```\n--- File: smartprofiler/network_usage.py ---\n```python\ndef profile_method(func):\n    \"\"\"Decorator to profile network usage of a method/function.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block():\n    \"\"\"Context manager to profile network usage.\"\"\"\n    pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "examples/cpu_time_profiling_usage.py",
                "code": "@profile_cpu_time\ndef cpu_time_function():\n    pass\n\ndef example_block():\n    pass\n\ndef example_line():\n    pass\n\ndef run_multithreaded_cpu_time_example():\n    pass\n"
            },
            {
                "file_path": "examples/network_profile_usage.py",
                "code": "@profile_method\ndef simulate_network_activity():\n    \"\"\"Simulate network activity by waiting (no actual request).\"\"\"\n    pass\n\ndef perform_network_operations():\n    \"\"\"Simulate multiple network operations inside a code block.\"\"\"\n    pass\n"
            },
            {
                "file_path": "examples/time_profiler_usage.py",
                "code": "@profile_time\ndef time_function():\n    pass\n\ndef example_block():\n    pass\n\ndef example_line():\n    pass\n\ndef run_multithreaded_time_example():\n    pass\n"
            },
            {
                "file_path": "examples/multi_profiling_usage.py",
                "code": "@profile_time\n@profile_memory\n@profile_cpu_time\n@profile_call_count\ndef example_function():\n    \"\"\"Example function that performs some work to demonstrate profiling.\"\"\"\n    pass\n"
            },
            {
                "file_path": "examples/disk_profile_usage.py",
                "code": "def perform_disk_operations():\n    \"\"\"Simulate multiple disk I/O operations inside a code block.\"\"\"\n    pass\n\n@profile_method\ndef write_to_file():\n    \"\"\"Simulate writing to a file and performing some disk I/O.\"\"\"\n    pass\n"
            },
            {
                "file_path": "examples/function_tracking_usage.py",
                "code": "@profile_call_count\ndef sample_function():\n    \"\"\"A simple function to demonstrate call count tracking.\"\"\"\n    pass\n\n@profile_call_count\ndef another_function():\n    \"\"\"Another function to demonstrate call count tracking.\"\"\"\n    pass\n\n@profile_call_count\ndef multithreaded_function(thread_name):\n    \"\"\"A simple function for multithreaded execution.\"\"\"\n    pass\n\ndef run_multithreaded_example():\n    pass\n"
            },
            {
                "file_path": "examples/memory_profiler_usage.py",
                "code": "@profile_memory\ndef memory_function():\n    pass\n\ndef example_block():\n    pass\n\ndef example_line():\n    pass\n\ndef run_multithreaded_memory_example():\n    pass\n"
            },
            {
                "file_path": "smartprofiler/function_tracking.py",
                "code": "def profile_call_count(func):\n    \"\"\"Decorator to track the number of times a function is called, thread-safe.\"\"\"\n    pass\n"
            },
            {
                "file_path": "smartprofiler/disk_usage.py",
                "code": "def profile_method(func):\n    \"\"\"Decorator to profile disk usage of a method/function.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block():\n    \"\"\"Context manager to profile disk I/O usage.\"\"\"\n    pass\n"
            },
            {
                "file_path": "smartprofiler/memory.py",
                "code": "def profile_memory(func):\n    \"\"\"Decorator to profile the memory usage of a function using tracemalloc.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block(profile_type='memory'):\n    \"\"\"Context manager to profile a block of code (memory).\"\"\"\n    pass\n\n@contextmanager\ndef profile_line(profile_type='memory'):\n    \"\"\"Context manager to profile specific lines of code (memory).\"\"\"\n    pass\n"
            },
            {
                "file_path": "smartprofiler/cpu_time.py",
                "code": "def profile_cpu_time(func):\n    \"\"\"Decorator to profile the CPU time used by a function.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block(profile_type='cpu_time'):\n    \"\"\"Context manager to profile a block of code (CPU time).\"\"\"\n    pass\n\n@contextmanager\ndef profile_line(profile_type='cpu_time'):\n    \"\"\"Context manager to profile a specific line of code (CPU time).\"\"\"\n    pass\n"
            },
            {
                "file_path": "smartprofiler/__init__.py",
                "code": ""
            },
            {
                "file_path": "smartprofiler/time.py",
                "code": "def profile_time(func):\n    \"\"\"Decorator to profile the execution time of a function.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block(profile_type='time'):\n    \"\"\"Context manager to profile a block of code (time).\"\"\"\n    pass\n\n@contextmanager\ndef profile_line(profile_type='time'):\n    \"\"\"Context manager to profile specific lines of code (time).\"\"\"\n    pass\n"
            },
            {
                "file_path": "smartprofiler/network_usage.py",
                "code": "def profile_method(func):\n    \"\"\"Decorator to profile network usage of a method/function.\"\"\"\n    pass\n\n@contextmanager\ndef profile_block():\n    \"\"\"Context manager to profile network usage.\"\"\"\n    pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: smartprofiler/cpu_time.py ---\n```python\ndef profile_cpu_time(func):\n    \"\"\"Decorator to profile the CPU time used by a function.\"\"\"\n    pass\n\ndef profile_block(profile_type='cpu_time'):\n    \"\"\"Context manager to profile a block of code (CPU time).\"\"\"\n    pass\n\ndef profile_line(profile_type='cpu_time'):\n    \"\"\"Context manager to profile a specific line of code (CPU time).\"\"\"\n    pass\n```\n--- File: smartprofiler/memory.py ---\n```python\ndef profile_memory(func):\n    \"\"\"Decorator to profile the memory usage of a function using tracemalloc.\"\"\"\n    pass\n\ndef profile_block(profile_type='memory'):\n    \"\"\"Context manager to profile a block of code (memory).\"\"\"\n    pass\n\ndef profile_line(profile_type='memory'):\n    \"\"\"Context manager to profile specific lines of code (memory).\"\"\"\n    pass\n```\n--- File: smartprofiler/function_tracking.py ---\n```python\ndef profile_call_count(func):\n    \"\"\"Decorator to track the number of times a function is called, thread-safe.\"\"\"\n    pass\n```\n--- File: smartprofiler/time.py ---\n```python\ndef profile_time(func):\n    \"\"\"Decorator to profile the execution time of a function.\"\"\"\n    pass\n\ndef profile_block(profile_type='time'):\n    \"\"\"Context manager to profile a block of code (time).\"\"\"\n    pass\n\ndef profile_line(profile_type='time'):\n    \"\"\"Context manager to profile specific lines of code (time).\"\"\"\n    pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "smartprofiler/cpu_time.py",
                "code": "def profile_cpu_time(func):\n    \"\"\"Decorator to profile the CPU time used by a function.\"\"\"\n    pass\n\ndef profile_block(profile_type='cpu_time'):\n    \"\"\"Context manager to profile a block of code (CPU time).\"\"\"\n    pass\n\ndef profile_line(profile_type='cpu_time'):\n    \"\"\"Context manager to profile a specific line of code (CPU time).\"\"\"\n    pass\n"
            },
            {
                "file_path": "smartprofiler/memory.py",
                "code": "def profile_memory(func):\n    \"\"\"Decorator to profile the memory usage of a function using tracemalloc.\"\"\"\n    pass\n\ndef profile_block(profile_type='memory'):\n    \"\"\"Context manager to profile a block of code (memory).\"\"\"\n    pass\n\ndef profile_line(profile_type='memory'):\n    \"\"\"Context manager to profile specific lines of code (memory).\"\"\"\n    pass\n"
            },
            {
                "file_path": "smartprofiler/function_tracking.py",
                "code": "def profile_call_count(func):\n    \"\"\"Decorator to track the number of times a function is called, thread-safe.\"\"\"\n    pass\n"
            },
            {
                "file_path": "smartprofiler/time.py",
                "code": "def profile_time(func):\n    \"\"\"Decorator to profile the execution time of a function.\"\"\"\n    pass\n\ndef profile_block(profile_type='time'):\n    \"\"\"Context manager to profile a block of code (time).\"\"\"\n    pass\n\ndef profile_line(profile_type='time'):\n    \"\"\"Context manager to profile specific lines of code (time).\"\"\"\n    pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_cpu_time_decorator",
                "covers": [
                    "smartprofiler.cpu_time.profile_cpu_time - happy path for decorator"
                ]
            },
            {
                "test_id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_block",
                "covers": [
                    "smartprofiler.cpu_time.profile_block - happy path for context manager"
                ]
            },
            {
                "test_id": "tests/test_cpu_time.py::TestCPUTimeProfiler::test_profile_line",
                "covers": [
                    "smartprofiler.cpu_time.profile_line - happy path for context manager"
                ]
            },
            {
                "test_id": "tests/test_function_tracking.py::TestCallCountProfiler::test_profile_call_count",
                "covers": [
                    "smartprofiler.function_tracking.profile_call_count - happy path for decorator, basic call count"
                ]
            },
            {
                "test_id": "tests/test_memory.py::TestMemoryProfiler::test_profile_memory_decorator",
                "covers": [
                    "smartprofiler.memory.profile_memory - happy path for decorator"
                ]
            },
            {
                "test_id": "tests/test_memory.py::TestMemoryProfiler::test_profile_block",
                "covers": [
                    "smartprofiler.memory.profile_block - happy path for context manager"
                ]
            },
            {
                "test_id": "tests/test_memory.py::TestMemoryProfiler::test_profile_line",
                "covers": [
                    "smartprofiler.memory.profile_line - happy path for context manager"
                ]
            },
            {
                "test_id": "tests/test_time.py::TimeProfilerTests::test_profile_time_decorator",
                "covers": [
                    "smartprofiler.time.profile_time - happy path for decorator"
                ]
            },
            {
                "test_id": "tests/test_time.py::TimeProfilerTests::test_profile_block",
                "covers": [
                    "smartprofiler.time.profile_block - happy path for context manager"
                ]
            },
            {
                "test_id": "tests/test_time.py::TimeProfilerTests::test_profile_line",
                "covers": [
                    "smartprofiler.time.profile_line - happy path for context manager"
                ]
            }
        ]
    },
    {
        "idx": 60111,
        "repo_name": "amaziahub_mimicker",
        "url": "https://github.com/amaziahub/mimicker",
        "description": "Your lightweight, Python-native HTTP mocking server",
        "stars": 11,
        "forks": 2,
        "language": "python",
        "size": 199,
        "created_at": "2024-10-26T12:55:37+00:00",
        "updated_at": "2025-04-10T03:06:33+00:00",
        "pypi_info": {
            "name": "mimicker",
            "version": "2.0.3",
            "url": "https://files.pythonhosted.org/packages/53/2c/b5255387fd7587605419f4b43aa22ac94b9c8f37081c2d41a6510b7303fa/mimicker-2.0.3.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 17,
            "comment_ratio": 0.1349206349206349,
            "pyfile_content_length": 37403,
            "pyfile_code_lines": 1134,
            "test_file_exist": true,
            "test_file_content_length": 19982,
            "pytest_framework": true,
            "test_case_num": 56,
            "metadata_path": [
                "pyproject.toml"
            ],
            "readme_content_length": 7625,
            "llm_reason": "The project 'Mimicker' is a lightweight Python HTTP mocking server. It is well-suited to be used directly as a benchmark for an AI to rebuild from scratch.\n\n**Positive Aspects:**\n*   **Self-Contained & Independent:** The core `mimicker` library is designed to have no third-party runtime dependencies, relying on Python's standard library (e.g., `http.server`, `socketserver`). This is excellent for reproducibility. While tests use `requests` and `hamcrest`, these are standard test dependencies and don't affect the core library's self-containment.\n*   **Clear & Well-Defined Functionality:** The README clearly outlines the project's purpose: creating HTTP stubs for various methods, mocking responses (status, headers, body), handling path parameters, response delays, and dynamic responses via a callback function. This provides a clear specification for the AI.\n*   **Testable & Verifiable Output:** The project includes a comprehensive suite of tests (e.g., `test_mimicker_get.py`, `test_mimicker_post.py`, `test_route.py`). These tests make HTTP requests to the locally running mock server and verify responses, making them ideal for validating the AI-generated solution.\n*   **No Graphical User Interface (GUI):** Interaction is through library usage in Python code and standard HTTP requests, fitting the benchmark criteria.\n*   **Appropriate Complexity (Medium):** Rebuilding Mimicker involves understanding HTTP protocols, implementing routing logic (including path parameter extraction), managing stub configurations, creating a fluent API for defining routes, and running a threaded HTTP server. This is non-trivial, offering a meaningful challenge, but not excessively complex or requiring esoteric knowledge. The codebase size appears manageable.\n*   **Well-Understood Problem Domain:** HTTP mocking is a common concept in software development and testing.\n*   **Predominantly Code-Based Solution:** The task is to generate Python code for the library.\n*   **Good Documentation:** The README provides clear usage examples that can serve as part of the specification.\n\n**Negative Aspects or Concerns (Considered Minor):**\n*   **Standard Library Knowledge:** Replicating the server component requires familiarity with Python's `http.server` and `socketserver` modules.\n*   **Design Replication:** The AI needs to replicate the specific fluent API design (e.g., `get(...).body(...).status(...)`) and the logic for path parameter parsing.\n*   **Dynamic Responses:** The `response_func` feature, allowing request data to influence the response, adds a layer of complexity to implement correctly.\n\nOverall, Mimicker presents a realistic, well-scoped, and appropriately challenging task. Its self-contained nature for core functionality and excellent existing test suite make it a strong candidate for an AI 'build from scratch' benchmark. The 'negatives' are features that contribute to its 'Medium' difficulty rather than being disqualifiers.",
            "llm_project_type": "HTTP mocking server library",
            "llm_rating": 85,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "amaziahub_mimicker",
            "finish_test": true,
            "test_case_result": {
                "tests/test_mimicker_delete.py::test_delete_404": "passed",
                "tests/test_mimicker_delete.py::test_delete_default_200_status_code": "passed",
                "tests/test_mimicker_delete.py::test_delete_picked_status_code": "passed",
                "tests/test_mimicker_delete.py::test_delete_empty_response": "passed",
                "tests/test_mimicker_get.py::test_get_404": "passed",
                "tests/test_mimicker_get.py::test_get_default_200_status_code": "passed",
                "tests/test_mimicker_get.py::test_get_picked_status_code": "passed",
                "tests/test_mimicker_get.py::test_get_body_as_text": "passed",
                "tests/test_mimicker_get.py::test_get_body_as_json": "passed",
                "tests/test_mimicker_get.py::test_get_path_param": "passed",
                "tests/test_mimicker_get.py::test_get_empty_response": "passed",
                "tests/test_mimicker_get.py::test_get_headers": "passed",
                "tests/test_mimicker_get.py::test_get_with_delay[0.1]": "passed",
                "tests/test_mimicker_get.py::test_get_with_delay[0.2]": "passed",
                "tests/test_mimicker_get.py::test_get_with_delay[0.5]": "passed",
                "tests/test_mimicker_get.py::test_get_with_timedout_delay": "passed",
                "tests/test_mimicker_get.py::test_get_delays_should_affect_only_their_respective_call": "passed",
                "tests/test_mimicker_patch.py::test_patch_404": "passed",
                "tests/test_mimicker_patch.py::test_patch_default_200_status_code": "passed",
                "tests/test_mimicker_patch.py::test_patch_picked_status_code": "passed",
                "tests/test_mimicker_patch.py::test_patch_body_as_text": "passed",
                "tests/test_mimicker_patch.py::test_patch_body_as_json": "passed",
                "tests/test_mimicker_patch.py::test_patch_empty_response": "passed",
                "tests/test_mimicker_patch.py::test_patch_text_content": "passed",
                "tests/test_mimicker_patch.py::test_patch_json_content": "passed",
                "tests/test_mimicker_post.py::test_post_404": "passed",
                "tests/test_mimicker_post.py::test_post_default_200_status_code": "passed",
                "tests/test_mimicker_post.py::test_post_picked_status_code": "passed",
                "tests/test_mimicker_post.py::test_post_body_as_text": "passed",
                "tests/test_mimicker_post.py::test_post_body_as_json": "passed",
                "tests/test_mimicker_post.py::test_post_reuse_request_body": "passed",
                "tests/test_mimicker_post.py::test_post_empty_response": "passed",
                "tests/test_mimicker_post.py::test_post_body_with_text_content": "passed",
                "tests/test_mimicker_post.py::test_post_body_with_json_content": "passed",
                "tests/test_mimicker_post.py::test_post_file_upload": "passed",
                "tests/test_mimicker_put.py::test_put_404": "passed",
                "tests/test_mimicker_put.py::test_put_default_200_status_code": "passed",
                "tests/test_mimicker_put.py::test_put_picked_status_code": "passed",
                "tests/test_mimicker_put.py::test_put_body_as_text": "passed",
                "tests/test_mimicker_put.py::test_put_body_as_json": "passed",
                "tests/test_mimicker_put.py::test_put_empty_response": "passed",
                "tests/test_mimicker_put.py::test_put_text_content": "passed",
                "tests/test_mimicker_put.py::test_put_json_content": "passed",
                "tests/test_route.py::test_route_initialization": "passed",
                "tests/test_route.py::test_route_body": "passed",
                "tests/test_route.py::test_route_status": "passed",
                "tests/test_route.py::test_route_with_path_parameter": "passed",
                "tests/test_route.py::test_route_build": "passed",
                "tests/test_route.py::test_route_response_func": "passed",
                "tests/test_stub_group.py::test_none_given_no_stubs": "passed",
                "tests/test_stub_group.py::test_no_match": "passed",
                "tests/test_stub_group.py::test_match": "passed",
                "tests/test_stub_group.py::test_none_on_partial_match": "passed",
                "tests/test_stub_group.py::test_match_w_path_param": "passed",
                "tests/test_stub_group.py::test_match_w_delay": "passed",
                "tests/test_stub_group.py::test_match_stub_with_response_func": "passed",
                "tests/test_stub_group.py::test_match_given_unexpected_header": "passed",
                "tests/test_stub_group.py::test_match_given_partial_expected_headers": "passed"
            },
            "success_count": 58,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "unknown_count": 0,
            "total_count": 58,
            "success_rate": 1.0,
            "coverage_report": {
                "covered_lines": 176,
                "num_statements": 179,
                "percent_covered": 97.1830985915493,
                "percent_covered_display": "97",
                "missing_lines": 3,
                "excluded_lines": 0,
                "num_branches": 34,
                "num_partial_branches": 3,
                "covered_branches": 31,
                "missing_branches": 3
            },
            "coverage_result": {}
        },
        "codelines_count": 1134,
        "codefiles_count": 17,
        "code_length": 37403,
        "test_files_count": 7,
        "test_code_length": 19982,
        "class_diagram": "@startuml\nclass Client {\n    __init__(root): void\n    get(path, headers): void\n    post_as_json(path, body, headers): void\n    post_as_text(path, body, headers): void\n    post_as_image(path, body, headers): void\n    post_as_file(path, body, headers): void\n    put_as_json(path, body, headers): void\n    put_as_text(path, body, headers): void\n    put_as_file(path, body, headers): void\n    put(path, body, headers): void\n    delete(path, headers): void\n    patch(path): void\n    patch_as_json(path, body): void\n    patch_as_text(path, body): void\n}\nclass MimickerHandler {\n    __init__(stub_matcher): void\n    do_GET(): void\n    do_POST(): void\n    do_PUT(): void\n    do_DELETE(): void\n    do_PATCH(): void\n    _handle_request(method): void\n    _send_response(matched_stub, path_params, request_body, request_headers): void\n    _set_headers(headers): void\n    _write_response(response, path_params): void\n    _send_404_response(method): void\n    _format_response(response, path_params): void\n    _get_request_body(): Optional[dict]\n}\nclass Route {\n    __init__(method, path): void\n    delay(delay): void\n    body(response): void\n    status(status_code): void\n    headers(headers): void\n    response_func(func): void\n    build(): void\n}\nclass Stub {\n    status_code: int\n    delay: float\n    response: Any\n    response_func: Optional[Callable]\n    headers: Optional[List[Tuple[str, str]]]\n}\nclass StubGroup {\n    __init__(): void\n    add(method, pattern, status_code, response, delay, response_func, headers): void\n    match(method, path, request_headers): Tuple[Stub, Dict[str, str]]\n}\nclass ReusableAddressThreadingTCPServer {\n    allow_reuse_address: Unknown\n}\nclass MimickerServer {\n    __init__(port): void\n    _handler_factory(): void\n    routes(): void\n    start(): void\n    shutdown(): void\n}\nStubGroup --> Stub\nMimickerServer --> StubGroup\nMimickerHandler --> Stub\nMimickerServer --> ReusableAddressThreadingTCPServer\nMimickerHandler --> StubGroup\nMimickerHandler ..> StubGroup\nStubGroup ..> Stub\nMimickerServer ..> ReusableAddressThreadingTCPServer\nMimickerHandler ..> Stub\nMimickerServer ..> StubGroup\n@enduml",
        "structure": [
            {
                "file": "tests/test_mimicker_get.py",
                "functions": [
                    {
                        "name": "test_get_404",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_get_default_200_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_get_picked_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_get_body_as_text",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_get_body_as_json",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_get_path_param",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_get_empty_response",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_get_headers",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_get_with_delay",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server",
                            "delay_in_seconds"
                        ]
                    },
                    {
                        "name": "test_get_with_timedout_delay",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "_call_route_and_measure_duration",
                        "docstring": "Performs the call and return its duration in seconds.",
                        "comments": null,
                        "args": [
                            "client",
                            "route"
                        ]
                    },
                    {
                        "name": "test_get_delays_should_affect_only_their_respective_call",
                        "docstring": "Ensures that the delay of a stub group affects only the given route.",
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/conftest.py",
                "functions": [
                    {
                        "name": "mimicker_server",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_route.py",
                "functions": [
                    {
                        "name": "test_route_initialization",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_route_body",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_route_status",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_route_with_path_parameter",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_route_build",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_route_response_func",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_mimicker_put.py",
                "functions": [
                    {
                        "name": "test_put_404",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_put_default_200_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_put_picked_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_put_body_as_text",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_put_body_as_json",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_put_empty_response",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_put_text_content",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_put_json_content",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_mimicker_patch.py",
                "functions": [
                    {
                        "name": "test_patch_404",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_patch_default_200_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_patch_picked_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_patch_body_as_text",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_patch_body_as_json",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_patch_empty_response",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_patch_text_content",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_patch_json_content",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_stub_group.py",
                "functions": [
                    {
                        "name": "test_none_given_no_stubs",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_no_match",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_match",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_none_on_partial_match",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_match_w_path_param",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_match_w_delay",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_match_stub_with_response_func",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_match_given_unexpected_header",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_match_given_partial_expected_headers",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_mimicker_post.py",
                "functions": [
                    {
                        "name": "test_post_404",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_default_200_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_picked_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_body_as_text",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_body_as_json",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_reuse_request_body",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_empty_response",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_body_with_text_content",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_body_with_json_content",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_post_file_upload",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_mimicker_delete.py",
                "functions": [
                    {
                        "name": "test_delete_404",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_delete_default_200_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_delete_picked_status_code",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    },
                    {
                        "name": "test_delete_empty_response",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "mimicker_server"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/support/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/support/client.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Client",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "root"
                                ]
                            },
                            {
                                "name": "get",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "headers"
                                ]
                            },
                            {
                                "name": "post_as_json",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body",
                                    "headers"
                                ]
                            },
                            {
                                "name": "post_as_text",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body",
                                    "headers"
                                ]
                            },
                            {
                                "name": "post_as_image",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body",
                                    "headers"
                                ]
                            },
                            {
                                "name": "post_as_file",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body",
                                    "headers"
                                ]
                            },
                            {
                                "name": "put_as_json",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body",
                                    "headers"
                                ]
                            },
                            {
                                "name": "put_as_text",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body",
                                    "headers"
                                ]
                            },
                            {
                                "name": "put_as_file",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body",
                                    "headers"
                                ]
                            },
                            {
                                "name": "put",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body",
                                    "headers"
                                ]
                            },
                            {
                                "name": "delete",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "headers"
                                ]
                            },
                            {
                                "name": "patch",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path"
                                ]
                            },
                            {
                                "name": "patch_as_json",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body"
                                ]
                            },
                            {
                                "name": "patch_as_text",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "path",
                                    "body"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "mimicker/handler.py",
                "functions": [],
                "classes": [
                    {
                        "name": "MimickerHandler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "stub_matcher"
                                ]
                            },
                            {
                                "name": "do_GET",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "do_POST",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "do_PUT",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "do_DELETE",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "do_PATCH",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "_handle_request",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "method"
                                ]
                            },
                            {
                                "name": "_send_response",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "matched_stub",
                                    "path_params",
                                    "request_body",
                                    "request_headers"
                                ]
                            },
                            {
                                "name": "_set_headers",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "headers"
                                ]
                            },
                            {
                                "name": "_write_response",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "response",
                                    "path_params"
                                ]
                            },
                            {
                                "name": "_send_404_response",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "method"
                                ]
                            },
                            {
                                "name": "_format_response",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "response",
                                    "path_params"
                                ]
                            },
                            {
                                "name": "_get_request_body",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "mimicker/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "mimicker/route.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Route",
                        "docstring": "Represents an HTTP route with configurable response properties.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes a new Route.\n\nArgs:\n    method (str): The HTTP method (GET, POST, PUT, DELETE, etc.).\n    path (str): The URL path for the route, supporting parameterized paths.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "method",
                                    "path"
                                ]
                            },
                            {
                                "name": "delay",
                                "docstring": "Sets the delay (in seconds) before returning the response.\n\nArgs:\n    delay (float): The delay time in seconds.\n\nReturns:\n    Route: The current Route instance (for method chaining).",
                                "comments": null,
                                "args": [
                                    "self",
                                    "delay"
                                ]
                            },
                            {
                                "name": "body",
                                "docstring": "Sets the response body for the route.\n\nArgs:\n    response (Union[Dict[str, Any], str], optional): The response body (JSON or string).\n    Defaults to an empty string.\n\nReturns:\n    Route: The current Route instance (for method chaining).",
                                "comments": null,
                                "args": [
                                    "self",
                                    "response"
                                ]
                            },
                            {
                                "name": "status",
                                "docstring": "Sets the HTTP status code for the response.\n\nArgs:\n    status_code (int): The HTTP status code (e.g., 200, 404, 500).\n\nReturns:\n    Route: The current Route instance (for method chaining).",
                                "comments": null,
                                "args": [
                                    "self",
                                    "status_code"
                                ]
                            },
                            {
                                "name": "headers",
                                "docstring": "Sets the HTTP headers for the response.\n\nArgs:\n    headers (List[Tuple[str, str]]): A list of key-value pairs representing headers.\n\nReturns:\n    Route: The current Route instance (for method chaining).",
                                "comments": null,
                                "args": [
                                    "self",
                                    "headers"
                                ]
                            },
                            {
                                "name": "response_func",
                                "docstring": "Sets a custom response function for dynamic responses.\n\nArgs:\n    func (Callable[..., Tuple[int, Any]]): A function that returns (status_code, response_body).\n\nReturns:\n    Route: The current Route instance (for method chaining).",
                                "comments": null,
                                "args": [
                                    "self",
                                    "func"
                                ]
                            },
                            {
                                "name": "build",
                                "docstring": "Builds the route configuration dictionary.\n\nReturns:\n    Dict[str, Any]: The route configuration containing method, path, response settings, and handlers.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "mimicker/stub_group.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Stub",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "StubGroup",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "add",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "method",
                                    "pattern",
                                    "status_code",
                                    "response",
                                    "delay",
                                    "response_func",
                                    "headers"
                                ]
                            },
                            {
                                "name": "match",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "method",
                                    "path",
                                    "request_headers"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "mimicker/mimicker.py",
                "functions": [
                    {
                        "name": "get",
                        "docstring": "Creates a GET route for the specified path.\n\nArgs:\n    path (str): The URL path for the route.\n\nReturns:\n    Route: A GET route instance.",
                        "comments": null,
                        "args": [
                            "path"
                        ]
                    },
                    {
                        "name": "post",
                        "docstring": "Creates a POST route for the specified path.\n\nArgs:\n    path (str): The URL path for the route.\n\nReturns:\n    Route: A POST route instance.",
                        "comments": null,
                        "args": [
                            "path"
                        ]
                    },
                    {
                        "name": "put",
                        "docstring": "Creates a PUT route for the specified path.\n\nArgs:\n    path (str): The URL path for the route.\n\nReturns:\n    Route: A PUT route instance.",
                        "comments": null,
                        "args": [
                            "path"
                        ]
                    },
                    {
                        "name": "delete",
                        "docstring": "Creates a DELETE route for the specified path.\n\nArgs:\n    path (str): The URL path for the route.\n\nReturns:\n    Route: A DELETE route instance.",
                        "comments": null,
                        "args": [
                            "path"
                        ]
                    },
                    {
                        "name": "patch",
                        "docstring": "Creates a PATCH route for the specified path.\n\nArgs:\n    path (str): The URL path for the route.\n\nReturns:\n    Route: A PATCH route instance.",
                        "comments": null,
                        "args": [
                            "path"
                        ]
                    },
                    {
                        "name": "mimicker",
                        "docstring": "Starts a Mimicker server on the specified port.\n\nArgs:\n    port (int, optional): The port to run the server on. Defaults to 8080.\n\nReturns:\n    MimickerServer: An instance of the running Mimicker server.",
                        "comments": null,
                        "args": [
                            "port"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "mimicker/server.py",
                "functions": [],
                "classes": [
                    {
                        "name": "ReusableAddressThreadingTCPServer",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "allow_reuse_address"
                        ]
                    },
                    {
                        "name": "MimickerServer",
                        "docstring": "A lightweight HTTP mocking server.\n\nThis server allows defining request-response routes for testing or simulation purposes.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes the Mimicker server.\n\nArgs:\n    port (int, optional): The port to run the server on. Defaults to 8080.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "port"
                                ]
                            },
                            {
                                "name": "_handler_factory",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "routes",
                                "docstring": "Adds multiple routes to the server.\n\nArgs:\n    *routes (Route): One or more Route instances to be added.\n\nReturns:\n    MimickerServer: The current server instance (for method chaining).",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "start",
                                "docstring": "Starts the Mimicker server in a background thread.\n\nReturns:\n    MimickerServer: The current server instance (for method chaining).",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "shutdown",
                                "docstring": "Shuts down the Mimicker server gracefully.\n\nEnsures that the server is stopped and the thread is joined if still running.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            }
        ],
        "test_cases": {
            "tests/test_mimicker_delete.py::test_delete_404": {
                "testid": "tests/test_mimicker_delete.py::test_delete_404",
                "result": "passed",
                "test_implementation": "def test_delete_404(mimicker_server):\n    mimicker_server.routes(\n        delete(\"/remove\")\n    )\n    resp = Client().delete('/not-found')\n    assert_that(resp.status_code, is_(404))"
            },
            "tests/test_mimicker_delete.py::test_delete_default_200_status_code": {
                "testid": "tests/test_mimicker_delete.py::test_delete_default_200_status_code",
                "result": "passed",
                "test_implementation": "def test_delete_default_200_status_code(mimicker_server):\n    mimicker_server.routes(\n        delete(\"/remove\")\n    )\n    resp = Client().delete('/remove')\n    assert_that(resp.status_code, is_(200))"
            },
            "tests/test_mimicker_delete.py::test_delete_picked_status_code": {
                "testid": "tests/test_mimicker_delete.py::test_delete_picked_status_code",
                "result": "passed",
                "test_implementation": "def test_delete_picked_status_code(mimicker_server):\n    mimicker_server.routes(\n        delete(\"/remove\").\n        status(204)\n    )\n    resp = Client().delete('/remove')\n    assert_that(resp.status_code, is_(204))"
            },
            "tests/test_mimicker_delete.py::test_delete_empty_response": {
                "testid": "tests/test_mimicker_delete.py::test_delete_empty_response",
                "result": "passed",
                "test_implementation": "def test_delete_empty_response(mimicker_server):\n    mimicker_server.routes(\n        delete(\"/remove\").\n        body(None).\n        status(204)\n    )\n    resp = Client().delete('/remove')\n    assert_that(resp.status_code, is_(204))\n    assert_that(resp.text, is_(\"\"))"
            },
            "tests/test_mimicker_get.py::test_get_404": {
                "testid": "tests/test_mimicker_get.py::test_get_404",
                "result": "passed",
                "test_implementation": "def test_get_404(mimicker_server):\n    mimicker_server.routes(\n        get(\"/hello\")\n    )\n    resp = Client().get('/not-found')\n    assert_that(resp.status_code, is_(404))"
            },
            "tests/test_mimicker_get.py::test_get_default_200_status_code": {
                "testid": "tests/test_mimicker_get.py::test_get_default_200_status_code",
                "result": "passed",
                "test_implementation": "def test_get_default_200_status_code(mimicker_server):\n    mimicker_server.routes(\n        get(\"/hello\")\n    )\n    resp = Client().get('/hello')\n    assert_that(resp.status_code, is_(200))"
            },
            "tests/test_mimicker_get.py::test_get_picked_status_code": {
                "testid": "tests/test_mimicker_get.py::test_get_picked_status_code",
                "result": "passed",
                "test_implementation": "def test_get_picked_status_code(mimicker_server):\n    mimicker_server.routes(\n        get(\"/hello\").\n        status(201)\n    )\n    resp = Client().get('/hello')\n    assert_that(resp.status_code, is_(201))"
            },
            "tests/test_mimicker_get.py::test_get_body_as_text": {
                "testid": "tests/test_mimicker_get.py::test_get_body_as_text",
                "result": "passed",
                "test_implementation": "def test_get_body_as_text(mimicker_server):\n    mimicker_server.routes(\n        get('/hello').\n        body(\"hello world\")\n    )\n    resp = Client().get('/hello')\n    assert_that(resp.text, is_(\"hello world\"))"
            },
            "tests/test_mimicker_get.py::test_get_body_as_json": {
                "testid": "tests/test_mimicker_get.py::test_get_body_as_json",
                "result": "passed",
                "test_implementation": "def test_get_body_as_json(mimicker_server):\n    body = {\"message\": \"Hello, World!\"}\n    mimicker_server.routes(\n        get(\"/hello\").\n        body(body).\n        status(200)\n    )\n    resp = Client().get('/hello')\n    assert_that(resp.headers, has_entry(\"Content-Type\", \"application/json\"))\n    assert_that(resp.json(), equal_to(body))"
            },
            "tests/test_mimicker_get.py::test_get_path_param": {
                "testid": "tests/test_mimicker_get.py::test_get_path_param",
                "result": "passed",
                "test_implementation": "def test_get_path_param(mimicker_server):\n    mimicker_server.routes(\n        get(\"/hello/{greet}\").\n        body({\"message\": \"Hello, {greet}!\"}).\n        status(200)\n    )\n    resp = Client().get('/hello/world')\n    assert_that(resp.json(), equal_to({\"message\": \"Hello, world!\"}))"
            },
            "tests/test_mimicker_get.py::test_get_empty_response": {
                "testid": "tests/test_mimicker_get.py::test_get_empty_response",
                "result": "passed",
                "test_implementation": "def test_get_empty_response(mimicker_server):\n    mimicker_server.routes(\n        get(\"/empty\").\n        body(None).\n        status(204)\n    )\n    resp = Client().get('/empty')\n    assert_that(resp.status_code, is_(204))\n    assert_that(resp.text, is_(\"\"))"
            },
            "tests/test_mimicker_get.py::test_get_headers": {
                "testid": "tests/test_mimicker_get.py::test_get_headers",
                "result": "passed",
                "test_implementation": "def test_get_headers(mimicker_server):\n    mimicker_server.routes(\n        get(\"/hello\").\n        body(\"hi there\").\n        headers([(\"Content-Type\", \"text/plain\")])\n    )\n    resp = Client().get('/hello', {\"Content-Type\": \"application/json\"})\n    assert_that(resp.status_code, is_(200))"
            },
            "tests/test_mimicker_get.py::test_get_with_delay[0.1]": {
                "testid": "tests/test_mimicker_get.py::test_get_with_delay[0.1]",
                "result": "passed",
                "test_implementation": "def test_get_with_delay(mimicker_server, delay_in_seconds: float):\n    mimicker_server.routes(\n        get(\"/wait\").\n        delay(delay_in_seconds).\n        body(\"finally here\")\n    )\n\n    start = perf_counter()\n    resp = Client().get('/wait')\n    duration_seconds = perf_counter() - start\n\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.text, is_(\"finally here\"))\n    assert_that(duration_seconds, is_(greater_than_or_equal_to(delay_in_seconds)))\n    assert_that(duration_seconds, is_(close_to(delay_in_seconds, 0.05)))"
            },
            "tests/test_mimicker_get.py::test_get_with_delay[0.2]": {
                "testid": "tests/test_mimicker_get.py::test_get_with_delay[0.2]",
                "result": "passed",
                "test_implementation": "def test_get_with_delay(mimicker_server, delay_in_seconds: float):\n    mimicker_server.routes(\n        get(\"/wait\").\n        delay(delay_in_seconds).\n        body(\"finally here\")\n    )\n\n    start = perf_counter()\n    resp = Client().get('/wait')\n    duration_seconds = perf_counter() - start\n\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.text, is_(\"finally here\"))\n    assert_that(duration_seconds, is_(greater_than_or_equal_to(delay_in_seconds)))\n    assert_that(duration_seconds, is_(close_to(delay_in_seconds, 0.05)))"
            },
            "tests/test_mimicker_get.py::test_get_with_delay[0.5]": {
                "testid": "tests/test_mimicker_get.py::test_get_with_delay[0.5]",
                "result": "passed",
                "test_implementation": "def test_get_with_delay(mimicker_server, delay_in_seconds: float):\n    mimicker_server.routes(\n        get(\"/wait\").\n        delay(delay_in_seconds).\n        body(\"finally here\")\n    )\n\n    start = perf_counter()\n    resp = Client().get('/wait')\n    duration_seconds = perf_counter() - start\n\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.text, is_(\"finally here\"))\n    assert_that(duration_seconds, is_(greater_than_or_equal_to(delay_in_seconds)))\n    assert_that(duration_seconds, is_(close_to(delay_in_seconds, 0.05)))"
            },
            "tests/test_mimicker_get.py::test_get_with_timedout_delay": {
                "testid": "tests/test_mimicker_get.py::test_get_with_timedout_delay",
                "result": "passed",
                "test_implementation": "def test_get_with_timedout_delay(mimicker_server):\n    mimicker_server.routes(\n        get(\"/wait\").\n        delay(0.1).\n        body(\"finally here\")\n    )\n\n    with raises(ReadTimeout) as error:\n        Client().get('/wait', timeout=0.05)\n\n    assert_that(str(error.value),\n                is_(\"HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=0.05)\"))"
            },
            "tests/test_mimicker_get.py::test_get_delays_should_affect_only_their_respective_call": {
                "testid": "tests/test_mimicker_get.py::test_get_delays_should_affect_only_their_respective_call",
                "result": "passed",
                "test_implementation": "def test_get_delays_should_affect_only_their_respective_call(mimicker_server):\n    \"\"\"\n    Ensures that the delay of a stub group affects only the given route.\n    \"\"\"\n\n    slow_delay = 0.1\n    medium_delay = 0.01\n    mimicker_server.routes(\n        get(\"/slow\").delay(slow_delay).body(\"slowly here\"),\n        get(\"/medium\").delay(medium_delay).body(\"here\"),\n        get(\"/rapid\").body(\"rapidly here\"),\n    )\n\n    client = Client()\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        slow_call_future = executor.submit(_call_route_and_measure_duration, client, \"/slow\")\n        medium_call_future = executor.submit(_call_route_and_measure_duration, client, \"/medium\")\n        rapid_call_future = executor.submit(_call_route_and_measure_duration, client, \"/rapid\")\n        slow_call_duration = slow_call_future.result()\n        medium_call_duration = medium_call_future.result()\n        rapid_call_duration = rapid_call_future.result()\n\n    assert_that(slow_call_duration, is_(greater_than_or_equal_to(slow_delay)))\n    assert_that(slow_call_duration, is_(close_to(slow_delay, 0.05)))\n\n    assert_that(medium_call_duration, is_(greater_than_or_equal_to(medium_delay)))\n    assert_that(medium_call_duration, is_(close_to(medium_delay, 0.05)))\n\n    assert_that(rapid_call_duration, is_(greater_than_or_equal_to(0)))\n    assert_that(rapid_call_duration, is_(close_to(0, 0.05)))"
            },
            "tests/test_mimicker_patch.py::test_patch_404": {
                "testid": "tests/test_mimicker_patch.py::test_patch_404",
                "result": "passed",
                "test_implementation": "def test_patch_404(mimicker_server):\n    mimicker_server.routes(\n        patch(\"/modify\")\n    )\n    resp = Client().patch('/not-found')\n    assert_that(resp.status_code, is_(404))"
            },
            "tests/test_mimicker_patch.py::test_patch_default_200_status_code": {
                "testid": "tests/test_mimicker_patch.py::test_patch_default_200_status_code",
                "result": "passed",
                "test_implementation": "def test_patch_default_200_status_code(mimicker_server):\n    mimicker_server.routes(\n        patch(\"/modify\")\n    )\n    resp = Client().patch('/modify')\n    assert_that(resp.status_code, is_(200))"
            },
            "tests/test_mimicker_patch.py::test_patch_picked_status_code": {
                "testid": "tests/test_mimicker_patch.py::test_patch_picked_status_code",
                "result": "passed",
                "test_implementation": "def test_patch_picked_status_code(mimicker_server):\n    mimicker_server.routes(\n        patch(\"/modify\").\n        status(202)\n    )\n    resp = Client().patch('/modify')\n    assert_that(resp.status_code, is_(202))"
            },
            "tests/test_mimicker_patch.py::test_patch_body_as_text": {
                "testid": "tests/test_mimicker_patch.py::test_patch_body_as_text",
                "result": "passed",
                "test_implementation": "def test_patch_body_as_text(mimicker_server):\n    mimicker_server.routes(\n        patch('/modify').\n        body(\"modification successful\")\n    )\n    resp = Client().patch_as_text('/modify', body=\"modification successful\")\n    assert_that(resp.text, is_(\"modification successful\"))"
            },
            "tests/test_mimicker_patch.py::test_patch_body_as_json": {
                "testid": "tests/test_mimicker_patch.py::test_patch_body_as_json",
                "result": "passed",
                "test_implementation": "def test_patch_body_as_json(mimicker_server):\n    body = {\"result\": \"modified\"}\n    mimicker_server.routes(\n        patch(\"/modify\").\n        body(body).\n        status(200)\n    )\n    resp = Client().patch_as_json('/modify', body=body)\n    assert_that(resp.headers, has_entry(\"Content-Type\", \"application/json\"))\n    assert_that(resp.json(), equal_to(body))"
            },
            "tests/test_mimicker_patch.py::test_patch_empty_response": {
                "testid": "tests/test_mimicker_patch.py::test_patch_empty_response",
                "result": "passed",
                "test_implementation": "def test_patch_empty_response(mimicker_server):\n    mimicker_server.routes(\n        patch(\"/clear\").\n        body(None).\n        status(204)\n    )\n    resp = Client().patch('/clear')\n    assert_that(resp.status_code, is_(204))\n    assert_that(resp.text, is_(\"\"))"
            },
            "tests/test_mimicker_patch.py::test_patch_text_content": {
                "testid": "tests/test_mimicker_patch.py::test_patch_text_content",
                "result": "passed",
                "test_implementation": "def test_patch_text_content(mimicker_server):\n    body = \"This is a modification with plain text.\"\n    mimicker_server.routes(\n        patch(\"/modify-text\").\n        body(body).\n        status(200)\n    )\n    resp = Client().patch_as_text('/modify-text', body=body)\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.text, is_(body))"
            },
            "tests/test_mimicker_patch.py::test_patch_json_content": {
                "testid": "tests/test_mimicker_patch.py::test_patch_json_content",
                "result": "passed",
                "test_implementation": "def test_patch_json_content(mimicker_server):\n    body = {\"message\": \"Modified successfully\"}\n    mimicker_server.routes(\n        patch(\"/modify-json\").\n        body(body).\n        status(200)\n    )\n    resp = Client().patch_as_json('/modify-json', body=body)\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.json(), equal_to(body))"
            },
            "tests/test_mimicker_post.py::test_post_404": {
                "testid": "tests/test_mimicker_post.py::test_post_404",
                "result": "passed",
                "test_implementation": "def test_post_404(mimicker_server):\n    mimicker_server.routes(\n        post(\"/submit\")\n    )\n    resp = Client().post_as_json('/not-found')\n    assert_that(resp.status_code, is_(404))"
            },
            "tests/test_mimicker_post.py::test_post_default_200_status_code": {
                "testid": "tests/test_mimicker_post.py::test_post_default_200_status_code",
                "result": "passed",
                "test_implementation": "def test_post_default_200_status_code(mimicker_server):\n    mimicker_server.routes(\n        post(\"/submit\")\n    )\n    resp = Client().post_as_json('/submit')\n    assert_that(resp.status_code, is_(200))"
            },
            "tests/test_mimicker_post.py::test_post_picked_status_code": {
                "testid": "tests/test_mimicker_post.py::test_post_picked_status_code",
                "result": "passed",
                "test_implementation": "def test_post_picked_status_code(mimicker_server):\n    mimicker_server.routes(\n        post(\"/submit\").\n        status(201)\n    )\n    resp = Client().post_as_json('/submit')\n    assert_that(resp.status_code, is_(201))"
            },
            "tests/test_mimicker_post.py::test_post_body_as_text": {
                "testid": "tests/test_mimicker_post.py::test_post_body_as_text",
                "result": "passed",
                "test_implementation": "def test_post_body_as_text(mimicker_server):\n    mimicker_server.routes(\n        post('/submit').\n        body(\"submission successful\")\n    )\n    resp = Client().post_as_text('/submit', body=\"submission successful\")\n    assert_that(resp.text, is_(\"submission successful\"))"
            },
            "tests/test_mimicker_post.py::test_post_body_as_json": {
                "testid": "tests/test_mimicker_post.py::test_post_body_as_json",
                "result": "passed",
                "test_implementation": "def test_post_body_as_json(mimicker_server):\n    body = {\"result\": \"created\"}\n    mimicker_server.routes(\n        post(\"/submit\").\n        body(body).\n        status(201)\n    )\n    resp = Client().post_as_json('/submit', body=body)\n    assert_that(resp.headers, has_entry(\"Content-Type\", \"application/json\"))\n    assert_that(resp.json(), equal_to(body))"
            },
            "tests/test_mimicker_post.py::test_post_reuse_request_body": {
                "testid": "tests/test_mimicker_post.py::test_post_reuse_request_body",
                "result": "passed",
                "test_implementation": "def test_post_reuse_request_body(mimicker_server):\n    def response_func(**kwargs):\n        payload = kwargs.get(\"payload\")\n        return 202, {\"my_counter\": 1 + payload.get('counter')}\n\n    body = {\"counter\": 1}\n    mimicker_server.routes(\n        post(\"/bump_counter\").response_func(response_func)\n    )\n    resp = Client().post_as_json('/bump_counter', body=body)\n    assert_that(resp.status_code, equal_to(202))\n    assert_that(resp.json(), equal_to({\"my_counter\": 2}))"
            },
            "tests/test_mimicker_post.py::test_post_empty_response": {
                "testid": "tests/test_mimicker_post.py::test_post_empty_response",
                "result": "passed",
                "test_implementation": "def test_post_empty_response(mimicker_server):\n    mimicker_server.routes(\n        post(\"/clear\").\n        body(None).\n        status(204)\n    )\n    resp = Client().post_as_json('/clear')\n    assert_that(resp.status_code, is_(204))\n    assert_that(resp.text, is_(\"\"))"
            },
            "tests/test_mimicker_post.py::test_post_body_with_text_content": {
                "testid": "tests/test_mimicker_post.py::test_post_body_with_text_content",
                "result": "passed",
                "test_implementation": "def test_post_body_with_text_content(mimicker_server):\n    body = \"This is a plain text submission.\"\n    mimicker_server.routes(\n        post(\"/submit-text\").\n        body(body).\n        status(200)\n    )\n    resp = Client().post_as_text('/submit-text', body=body)\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.text, is_(body))"
            },
            "tests/test_mimicker_post.py::test_post_body_with_json_content": {
                "testid": "tests/test_mimicker_post.py::test_post_body_with_json_content",
                "result": "passed",
                "test_implementation": "def test_post_body_with_json_content(mimicker_server):\n    body = {\"message\": \"Data submitted\"}\n    mimicker_server.routes(\n        post(\"/submit-json\").\n        body(body).\n        status(201)\n    )\n    resp = Client().post_as_json('/submit-json', body=body)\n    assert_that(resp.status_code, is_(201))\n    assert_that(resp.json(), equal_to(body))"
            },
            "tests/test_mimicker_post.py::test_post_file_upload": {
                "testid": "tests/test_mimicker_post.py::test_post_file_upload",
                "result": "passed",
                "test_implementation": "def test_post_file_upload(mimicker_server):\n    file_content = b\"Test file content\"\n    mimicker_server.routes(\n        post(\"/upload\").\n        body(\"File uploaded successfully\").\n        status(200)\n    )\n    with open('/tmp/test_file.txt', 'wb') as f:\n        f.write(file_content)\n\n    with open('/tmp/test_file.txt', 'rb') as file:\n        resp = Client().post_as_file('/upload', body={'file': file})\n\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.text, is_(\"File uploaded successfully\"))"
            },
            "tests/test_mimicker_put.py::test_put_404": {
                "testid": "tests/test_mimicker_put.py::test_put_404",
                "result": "passed",
                "test_implementation": "def test_put_404(mimicker_server):\n    mimicker_server.routes(\n        put(\"/update\")\n    )\n    resp = Client().put('/not-found')\n    assert_that(resp.status_code, is_(404))"
            },
            "tests/test_mimicker_put.py::test_put_default_200_status_code": {
                "testid": "tests/test_mimicker_put.py::test_put_default_200_status_code",
                "result": "passed",
                "test_implementation": "def test_put_default_200_status_code(mimicker_server):\n    mimicker_server.routes(\n        put(\"/update\")\n    )\n    resp = Client().put('/update')\n    assert_that(resp.status_code, is_(200))"
            },
            "tests/test_mimicker_put.py::test_put_picked_status_code": {
                "testid": "tests/test_mimicker_put.py::test_put_picked_status_code",
                "result": "passed",
                "test_implementation": "def test_put_picked_status_code(mimicker_server):\n    mimicker_server.routes(\n        put(\"/update\").\n        status(202)\n    )\n    resp = Client().put('/update')\n    assert_that(resp.status_code, is_(202))"
            },
            "tests/test_mimicker_put.py::test_put_body_as_text": {
                "testid": "tests/test_mimicker_put.py::test_put_body_as_text",
                "result": "passed",
                "test_implementation": "def test_put_body_as_text(mimicker_server):\n    mimicker_server.routes(\n        put('/update').\n        body(\"update successful\")\n    )\n    resp = Client().put_as_text('/update', body=\"update successful\")\n    assert_that(resp.text, is_(\"update successful\"))"
            },
            "tests/test_mimicker_put.py::test_put_body_as_json": {
                "testid": "tests/test_mimicker_put.py::test_put_body_as_json",
                "result": "passed",
                "test_implementation": "def test_put_body_as_json(mimicker_server):\n    body = {\"result\": \"updated\"}\n    mimicker_server.routes(\n        put(\"/update\").\n        body(body).\n        status(200)\n    )\n    resp = Client().put_as_json('/update', body=body)\n    assert_that(resp.headers, has_entry(\"Content-Type\", \"application/json\"))\n    assert_that(resp.json(), equal_to(body))"
            },
            "tests/test_mimicker_put.py::test_put_empty_response": {
                "testid": "tests/test_mimicker_put.py::test_put_empty_response",
                "result": "passed",
                "test_implementation": "def test_put_empty_response(mimicker_server):\n    mimicker_server.routes(\n        put(\"/clear\").\n        body(None).\n        status(204)\n    )\n    resp = Client().put('/clear')\n    assert_that(resp.status_code, is_(204))\n    assert_that(resp.text, is_(\"\"))"
            },
            "tests/test_mimicker_put.py::test_put_text_content": {
                "testid": "tests/test_mimicker_put.py::test_put_text_content",
                "result": "passed",
                "test_implementation": "def test_put_text_content(mimicker_server):\n    body = \"This is an update with plain text.\"\n    mimicker_server.routes(\n        put(\"/update-text\").\n        body(body).\n        status(200)\n    )\n    resp = Client().put_as_text('/update-text', body=body)\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.text, is_(body))"
            },
            "tests/test_mimicker_put.py::test_put_json_content": {
                "testid": "tests/test_mimicker_put.py::test_put_json_content",
                "result": "passed",
                "test_implementation": "def test_put_json_content(mimicker_server):\n    body = {\"message\": \"Updated successfully\"}\n    mimicker_server.routes(\n        put(\"/update-json\").\n        body(body).\n        status(200)\n    )\n    resp = Client().put_as_json('/update-json', body=body)\n    assert_that(resp.status_code, is_(200))\n    assert_that(resp.json(), equal_to(body))"
            },
            "tests/test_route.py::test_route_initialization": {
                "testid": "tests/test_route.py::test_route_initialization",
                "result": "passed",
                "test_implementation": "def test_route_initialization():\n    route = Route(\"GET\", \"/hello\")\n    assert_that(route.method, equal_to(\"GET\"))\n    assert_that(route.path, equal_to(\"/hello\"))\n    assert_that(route._delay, equal_to(0))\n    assert_that(route._status, equal_to(200))\n    assert_that(route._body, equal_to({}))"
            },
            "tests/test_route.py::test_route_body": {
                "testid": "tests/test_route.py::test_route_body",
                "result": "passed",
                "test_implementation": "def test_route_body():\n    route = Route(\"GET\", \"/hello\").body({\"message\": \"Hello, World!\"})\n    assert_that(route._body, has_entry(\"message\", \"Hello, World!\"))"
            },
            "tests/test_route.py::test_route_status": {
                "testid": "tests/test_route.py::test_route_status",
                "result": "passed",
                "test_implementation": "def test_route_status():\n    route = Route(\"GET\", \"/hello\").status(404)\n    assert_that(route._status, equal_to(404))"
            },
            "tests/test_route.py::test_route_with_path_parameter": {
                "testid": "tests/test_route.py::test_route_with_path_parameter",
                "result": "passed",
                "test_implementation": "def test_route_with_path_parameter():\n    route = Route(\"GET\", \"/hello/{greet}\")\n    compiled_path = route._compiled_path\n    match = compiled_path.match(\"/hello/world\")\n    assert_that(match.group(\"greet\"), equal_to(\"world\"))"
            },
            "tests/test_route.py::test_route_build": {
                "testid": "tests/test_route.py::test_route_build",
                "result": "passed",
                "test_implementation": "def test_route_build():\n    route = (Route(\"GET\", \"/hello/{greet}\")\n             .delay(2)\n             .body({\"message\": \"Hello, {greet}!\"})\n             .status(201))\n    route_config = route.build()\n\n    assert_that(route_config[\"method\"], equal_to(\"GET\"))\n    assert_that(route_config[\"path\"], equal_to(\"/hello/{greet}\"))\n    assert_that(route_config[\"delay\"], equal_to(2))\n    assert_that(route_config[\"status\"], equal_to(201))\n    assert_that(route_config[\"body\"], equal_to({\"message\": \"Hello, {greet}!\"}))\n    assert_that(route_config[\"compiled_path\"], instance_of(re.Pattern))"
            },
            "tests/test_route.py::test_route_response_func": {
                "testid": "tests/test_route.py::test_route_response_func",
                "result": "passed",
                "test_implementation": "def test_route_response_func():\n    def mock_response_func():\n        return 202, {\"message\": \"Generated dynamically\"}\n\n    route = Route(\"GET\", \"/dynamic\").response_func(mock_response_func)\n    route_config = route.build()\n\n    assert_that(route_config[\"response_func\"], equal_to(mock_response_func))\n\n    status_code, response = route_config[\"response_func\"]()\n    assert_that(status_code, equal_to(202))\n    assert_that(response, equal_to({\"message\": \"Generated dynamically\"}))"
            },
            "tests/test_stub_group.py::test_none_given_no_stubs": {
                "testid": "tests/test_stub_group.py::test_none_given_no_stubs",
                "result": "passed",
                "test_implementation": "def test_none_given_no_stubs():\n    stub_group = StubGroup()\n    matched, _ = stub_group.match(\"GET\", \"/hello/kuku\")\n    assert_that(matched, none())"
            },
            "tests/test_stub_group.py::test_no_match": {
                "testid": "tests/test_stub_group.py::test_no_match",
                "result": "passed",
                "test_implementation": "def test_no_match():\n    stub_group = StubGroup()\n    stub_group.add(\"GET\", \"/hi\", 200, {\"message\": \"hello\"})\n    matched, _ = stub_group.match(\"GET\", \"/hello/kuku\")\n    assert_that(matched, none())"
            },
            "tests/test_stub_group.py::test_match": {
                "testid": "tests/test_stub_group.py::test_match",
                "result": "passed",
                "test_implementation": "def test_match():\n    stub_group = StubGroup()\n    stub_group.add(\"GET\", \"/hi\", 200, {\"message\": \"hello\"})\n    matched, _ = stub_group.match(\"GET\", \"/hi\")\n    assert_that(matched, is_((200, 0., {\"message\": \"hello\"}, None, None)))"
            },
            "tests/test_stub_group.py::test_none_on_partial_match": {
                "testid": "tests/test_stub_group.py::test_none_on_partial_match",
                "result": "passed",
                "test_implementation": "def test_none_on_partial_match():\n    stub_group = StubGroup()\n    stub_group.add(\"POST\", \"/hi\", 200, {\"message\": \"hello\"})\n    matched, _ = stub_group.match(\"POST\", \"/hi/hello\")\n    assert_that(matched, none())"
            },
            "tests/test_stub_group.py::test_match_w_path_param": {
                "testid": "tests/test_stub_group.py::test_match_w_path_param",
                "result": "passed",
                "test_implementation": "def test_match_w_path_param():\n    stub_group = StubGroup()\n    stub_group.add(\"GET\",\n                   r\"^/hello/(?P<name>\\w+)$\",\n                   200,\n                   {\"message\": \"Hello, {name}!\"})\n\n    matched, path_param = stub_group.match(\"GET\", \"/hello/mimicker\")\n\n    assert_that(matched, is_((200, 0., {\"message\": \"Hello, {name}!\"}, None, None)))\n    assert_that(path_param, is_({\"name\": \"mimicker\"}))"
            },
            "tests/test_stub_group.py::test_match_w_delay": {
                "testid": "tests/test_stub_group.py::test_match_w_delay",
                "result": "passed",
                "test_implementation": "def test_match_w_delay():\n    stub_group = StubGroup()\n    stub_group.add(\"GET\", \"/hi\", 200, {\"message\": \"hello\"}, delay=3.)\n\n    matched, _ = stub_group.match(\"GET\", \"/hi\")\n\n    assert_that(matched, is_((200, 3., {\"message\": \"hello\"}, None, None)))"
            },
            "tests/test_stub_group.py::test_match_stub_with_response_func": {
                "testid": "tests/test_stub_group.py::test_match_stub_with_response_func",
                "result": "passed",
                "test_implementation": "def test_match_stub_with_response_func():\n    stub_group = StubGroup()\n\n    def dynamic_response():\n        return 202, {\"status\": \"dynamic\"}\n\n    stub_group.add(\"GET\", r\"^/dynamic$\",\n                   200, {}, response_func=dynamic_response)\n    matched, _ = stub_group.match(\"GET\", \"/dynamic\")\n    assert_that(matched, is_((200, 0., {}, dynamic_response, None)))"
            },
            "tests/test_stub_group.py::test_match_given_unexpected_header": {
                "testid": "tests/test_stub_group.py::test_match_given_unexpected_header",
                "result": "passed",
                "test_implementation": "def test_match_given_unexpected_header():\n    stub_group = StubGroup()\n    stub_group.add(\n        \"GET\", \"/hi\", 200,\n        {\"message\": \"hello\"}, headers=[(\"Content-Type\", \"text/plain\")])\n    matched, _ = stub_group.match(\n        \"GET\", \"/hi\",\n        request_headers={\"Content-Type\": \"application/json\"})\n    assert_that(matched, is_((200, 0., {'message': 'hello'}, None,\n                              [('Content-Type', 'text/plain')])))"
            },
            "tests/test_stub_group.py::test_match_given_partial_expected_headers": {
                "testid": "tests/test_stub_group.py::test_match_given_partial_expected_headers",
                "result": "passed",
                "test_implementation": "def test_match_given_partial_expected_headers():\n    stub_group = StubGroup()\n    stub_group.add(\"GET\", \"/hi\", 200,\n                   {\"message\": \"hello\"},\n                   headers=[\n                       (\"Content-Type\", \"application/json\"),\n                       (\"Authorization\", \"Bearer YOUR_TOKEN\"),\n                       (\"Custom-Header\", \"CustomValue\")\n                   ])\n    matched, _ = stub_group.match(\n        \"GET\", \"/hi\",\n        request_headers={\"Content-Type\": \"application/json\"})\n    assert_that(matched, is_((200, 0., {'message': 'hello'},\n                              None, [\n                                  ('Content-Type', 'application/json'),\n                                  ('Authorization', 'Bearer YOUR_TOKEN'),\n                                  ('Custom-Header', 'CustomValue')])))"
            }
        },
        "SRS_document": "# Software Requirements Specification: Mimicker HTTP Mocking Server\n\n## Table of Contents\n1.  Introduction\n    1.1 Purpose\n    1.2 Scope\n    1.3 Definitions, Acronyms, and Abbreviations\n    1.4 References\n    1.5 Overview\n2.  Overall Description\n    2.1 Product Perspective\n    2.2 Product Functions\n    2.3 User Characteristics\n    2.4 Constraints\n    2.5 Assumptions and Dependencies\n3.  Specific Requirements\n    3.1 Functional Requirements\n        3.1.1 Server Management\n        3.1.2 Route Definition\n        3.1.3 HTTP Method Handling\n        3.1.4 Response Configuration\n        3.1.5 Path Parameter Handling\n        3.1.6 Response Delay\n        3.1.7 Dynamic Response Generation\n        3.1.8 Request Processing and Error Handling\n    3.2 Non-Functional Requirements\n    3.3 External Interface Requirements\n\n## 1. Introduction\n\n### 1.1 Purpose\nThis Software Requirements Specification (SRS) document defines the functional and non-functional requirements for Mimicker, a Python-native HTTP mocking server. The primary goal of this SRS is to provide a clear, unambiguous, and comprehensive specification that will be used to assess software developers. Developers will use this SRS and a subset of original test cases to build the Mimicker application. Their success will be measured by their implementation's ability to pass all original test cases, including private ones.\n\n### 1.2 Scope\nThe software to be developed is a lightweight HTTP server that allows users to define mock responses for specific HTTP endpoints. Key functionalities include:\n*   Starting and stopping the mock server on a configurable port.\n*   Defining routes based on HTTP method and path.\n*   Configuring static responses, including status code, headers, and body (text, JSON, or raw bytes).\n*   Supporting path parameters in route definitions and their use in response bodies.\n*   Introducing configurable delays for responses.\n*   Allowing dynamic response generation based on request details (e.g., payload, headers, path parameters).\n\nThe system is intended for use in integration testing, local development, and CI environments to simulate HTTP dependencies.\n\n### 1.3 Definitions, Acronyms, and Abbreviations\n*   **SRS:** Software Requirements Specification\n*   **HTTP:** Hypertext Transfer Protocol\n*   **API:** Application Programming Interface\n*   **JSON:** JavaScript Object Notation\n*   **CI:** Continuous Integration\n*   **Route:** A specific URL path pattern combined with an HTTP method for which a mock response is defined.\n*   **Stub:** A defined mock response for a given route.\n*   **Path Parameter:** A variable part of a URL path, e.g., `{id}` in `/users/{id}`.\n\n### 1.4 References\n*   Mimicker README.md (provided as input)\n*   Mimicker Original Source Code (contextual understanding for LLM)\n*   Mimicker Original Test Cases (contextual understanding for LLM and basis for test traceability)\n\n### 1.5 Overview\nThis SRS is organized into three main sections:\n*   **Section 1 (Introduction):** Provides an overview of the SRS, including its purpose, scope, definitions, and references.\n*   **Section 2 (Overall Description):** Describes the product perspective, its functions, user characteristics, constraints, and assumptions.\n*   **Section 3 (Specific Requirements):** Details the functional, non-functional, and any external interface requirements for the system. Functional requirements are organized by major capability.\n\n## 2. Overall Description\n\n### 2.1 Product Perspective\nMimicker is a standalone, Python-native HTTP server designed for creating HTTP stubs. It acts as a lightweight alternative to more complex mocking solutions, intended to be easy to integrate into Python-based testing workflows without requiring external dependencies for its core mocking functionality. It allows developers to simulate HTTP services by defining expected requests and their corresponding mock responses.\n\n### 2.2 Product Functions\nThe Mimicker system shall provide the following core functions:\n*   **Server Initialization and Control:** Allow users to start the mock server on a specified network port and shut it down gracefully.\n*   **Route Definition:** Enable users to define multiple routes, each associated with an HTTP method (GET, POST, PUT, DELETE, PATCH) and a URL path.\n*   **Response Configuration:** For each route, allow users to specify the HTTP status code, response headers, and response body.\n*   **Path Parameter Support:** Allow URL paths to include named parameters, and use these parameters to customize response bodies.\n*   **Response Delay Simulation:** Enable configuration of a delay before a response is sent, to simulate network latency or slow services.\n*   **Dynamic Response Generation:** Provide a mechanism for users to define custom logic (a function) that generates a response dynamically based on the details of the incoming request (e.g., body, headers, path parameters).\n*   **Error Handling:** Respond with appropriate HTTP error codes (e.g., 404 Not Found) for requests that do not match any defined route.\n\n### 2.3 User Characteristics\nThe primary users of Mimicker are:\n*   Software Developers: Utilizing the server during unit, component, or integration testing to mock external HTTP services.\n*   QA Engineers: Employing the server to create stable test environments with predictable HTTP responses for automated or manual testing.\nUsers are expected to have a working knowledge of Python programming and HTTP concepts.\n\n### 2.4 Constraints\n*   **CSTR-1:** The system shall be implementable using Python 3.7 or higher.\n*   **CSTR-2 (Design Goal):** The core mocking server functionality should aim to be self-contained, minimizing reliance on third-party libraries beyond the Python standard library where feasible, to maintain its lightweight nature. (Derived from README: \"Mimicker requires no third-party libraries and is lightweight\"). This is a design goal for the assessed developer, not a strict prohibition if functionality dictates, but the spirit of \"lightweight\" should be preserved.\n\n### 2.5 Assumptions and Dependencies\n*   **ASMP-1:** Users will have a Python 3.7+ environment installed and operational.\n*   **ASMP-2:** The system will operate on a network interface (e.g., localhost) allowing HTTP communication on the configured port.\n*   **ASMP-3:** Users are responsible for ensuring the configured port is available.\n\n## 3. Specific Requirements\n\n### 3.1 Functional Requirements\n\n#### 3.1.1 Server Management\n\n*   **FR-SM-001:** The system shall provide a mechanism to initialize and start the HTTP mock server on a user-specified network port.\n*   **FR-SM-002:** The system shall default to port 8080 if no port is specified during server initialization.\n*   **FR-SM-003:** The system shall provide a mechanism to shut down the running HTTP mock server, releasing the network port.\n\n#### 3.1.2 Route Definition\n\n*   **FR-RD-001:** The system shall allow users to define multiple routes for the mock server.\n*   **FR-RD-002:** Each route definition shall consist of an HTTP method and a URL path pattern.\n*   **FR-RD-003:** The system shall provide distinct functions or mechanisms for defining routes for HTTP GET, POST, PUT, DELETE, and PATCH methods.\n\n#### 3.1.3 HTTP Method Handling\n(Specific method tests verify common functionalities like status codes, bodies, etc., which are detailed in 3.1.4. These FRs ensure the methods themselves are supported.)\n\n*   **FR-HTTP-001:** The system shall handle incoming HTTP GET requests that match a defined GET route.\n*   **FR-HTTP-002:** The system shall handle incoming HTTP POST requests that match a defined POST route.\n*   **FR-HTTP-003:** The system shall handle incoming HTTP PUT requests that match a defined PUT route.\n*   **FR-HTTP-004:** The system shall handle incoming HTTP DELETE requests that match a defined DELETE route.\n*   **FR-HTTP-005:** The system shall handle incoming HTTP PATCH requests that match a defined PATCH route.\n\n#### 3.1.4 Response Configuration\n\n*   **FR-RCFG-001:** The system shall allow users to specify an HTTP status code for the response of a defined route.\n*   **FR-RCFG-002:** If no status code is explicitly configured for a route, the system shall default to HTTP status code 200 (OK).\n*   **FR-RCFG-003:** The system shall allow users to specify a body for the response of a defined route.\n*   **FR-RCFG-004:** If the configured response body is a dictionary, the system shall serialize it as a JSON string and send it as the HTTP response body.\n*   **FR-RCFG-005:** If the configured response body is a string, the system shall send it as a plain text HTTP response body, encoded in UTF-8.\n*   **FR-RCFG-006:** The system shall allow the response body to be specified as raw bytes (e.g., for file content). These bytes should be transmitted directly as the HTTP response entity.\n*   **FR-RCFG-007:** If the configured response body is `None` or an empty string, the system shall send an empty HTTP response body.\n*   **FR-RCFG-008:** The system shall allow users to specify a list of (header_name, header_value) tuples as HTTP headers for the response of a defined route.\n*   **FR-RCFG-009:** If custom response headers are configured, they shall be included in the HTTP response.\n*   **FR-RCFG-010:** For responses with dictionary-based bodies (intended as JSON), the system shall automatically set the `Content-Type` header to `application/json` if no `Content-Type` header is explicitly specified in the route's header configuration.\n*   **FR-RCFG-011:** If no custom headers are configured for a route with a dictionary-based body, the `Content-Type` header shall default to `application/json`.\n\n#### 3.1.5 Path Parameter Handling\n\n*   **FR-PP-001:** The system shall support defining URL path patterns that include named placeholders for dynamic segments (path parameters), e.g., `/resource/{id}`.\n*   **FR-PP-002:** The system shall correctly match incoming request paths against defined path patterns containing path parameters.\n*   **FR-PP-003:** If a route with path parameters is matched and its response body is a dictionary containing string values with corresponding placeholders (e.g., `{\"message\": \"Hello, {name}!\"}`), the system shall substitute these placeholders with the actual values extracted from the request path.\n*   **FR-PP-004:** Path parameter placeholders should match segments in the path that do not contain `/`.\n\n#### 3.1.6 Response Delay\n\n*   **FR-RDLY-001:** The system shall allow users to configure a response delay in seconds (float) for a defined route.\n*   **FR-RDLY-002:** If a response delay is configured for a route, the system shall wait for the specified duration before sending the response.\n*   **FR-RDLY-003:** If no delay is configured, the default delay shall be 0 seconds (no artificial delay).\n\n#### 3.1.7 Dynamic Response Generation\n\n*   **FR-DYN-001:** The system shall allow users to associate a custom Python function with a route to generate responses dynamically.\n*   **FR-DYN-002:** The custom response function, when invoked, shall determine the HTTP status code and the response body (or content).\n*   **FR-DYN-003:** The system shall provide the dynamic response function with access to the following request details:\n    *   Parsed JSON request body (as a Python dictionary, or `None` if not present/valid JSON).\n    *   Request headers (as a Python dictionary with lowercase keys).\n    *   Path parameters extracted from the URL (as a Python dictionary).\n*   **FR-DYN-004:** If a dynamic response function is configured for a route, any statically configured body, status, or headers for that route are superseded by the function's output for that request.\n\n#### 3.1.8 Request Processing and Error Handling\n\n*   **FR-PROC-001:** The system shall parse incoming HTTP request bodies with `Content-Type` indicating JSON as JSON objects.\n    *   This functionality is primarily to support FR-DYN-003.\n*   **FR-PROC-002:** If an incoming request path and method do not match any defined route, the system shall respond with an HTTP 404 (Not Found) status code.\n*   **FR-PROC-003:** The system should be capable of handling multiple concurrent requests, with each request being processed according to its matched route configuration independently.\n\n### 3.2 Non-Functional Requirements\n\n*   **NFR-PERF-001: Response Delay Accuracy:** The system shall ensure that the actual delay introduced before sending a response is close to the configured delay for a route (e.g., within +/- 0.05 seconds of the configured delay).\n*   **NFR-PERF-002: Independent Route Delays:** Delays configured for one route shall not impact the response time (beyond its own configured delay) of other routes when multiple routes are accessed concurrently. Each concurrent request's delay should be honored independently.\n\n### 3.3 External Interface Requirements\n*   **EIR-1: HTTP Interface:** The system shall expose its functionality over HTTP/1.1.\n    *   Interaction with the system (sending requests to mock endpoints) is exclusively via HTTP.\n*   **EIR-2: Programmatic Configuration Interface (Python API):** The system shall provide a Python API for configuration. This includes:\n    *   Functions to initialize and start/stop the server (e.g., `mimicker(port)`).\n    *   Functions to define routes for HTTP methods (e.g., `get(path)`, `post(path)`).\n    *   A fluent interface or chainable methods on route objects to configure response status, body, headers, delay, and dynamic response functions.",
        "structured_requirements": [
            {
                "requirement_id": "CSTR-1",
                "requirement_description": "The system shall be implementable using Python 3.7 or higher.",
                "test_traceability": [
                    {
                        "id": "README",
                        "description": "Requirements section"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "Overall project setup.",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "CSTR-2",
                "requirement_description": "(Design Goal): The core mocking server functionality should aim to be self-contained, minimizing reliance on third-party libraries beyond the Python standard library where feasible, to maintain its lightweight nature. (Derived from README: \"Mimicker requires no third-party libraries and is lightweight\"). This is a design goal for the assessed developer, not a strict prohibition if functionality dictates, but the spirit of \"lightweight\" should be preserved.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "ASMP-1",
                "requirement_description": "Users will have a Python 3.7+ environment installed and operational.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "ASMP-2",
                "requirement_description": "The system will operate on a network interface (e.g., localhost) allowing HTTP communication on the configured port.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "ASMP-3",
                "requirement_description": "Users are responsible for ensuring the configured port is available.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "FR-SM-001",
                "requirement_description": "The system shall provide a mechanism to initialize and start the HTTP mock server on a user-specified network port.",
                "test_traceability": [
                    {
                        "id": "`tests/conftest.py::mimicker_server`",
                        "description": "implicitly tests server startup"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/mimicker.py::mimicker`",
                        "description": ""
                    },
                    {
                        "id": "`mimicker/server.py::MimickerServer::__init__`",
                        "description": ""
                    },
                    {
                        "id": "`mimicker/server.py::MimickerServer::start`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SM-002",
                "requirement_description": "The system shall default to port 8080 if no port is specified during server initialization.",
                "test_traceability": [
                    {
                        "id": "`tests/conftest.py::mimicker_server`",
                        "description": "uses port 8080 by default in its internal call to `mimicker()`"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/mimicker.py::mimicker`",
                        "description": ""
                    },
                    {
                        "id": "`mimicker/server.py::MimickerServer::__init__`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SM-003",
                "requirement_description": "The system shall provide a mechanism to shut down the running HTTP mock server, releasing the network port.",
                "test_traceability": [
                    {
                        "id": "`tests/conftest.py::mimicker_server`",
                        "description": "calls `server.shutdown()`"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/server.py::MimickerServer::shutdown`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RD-001",
                "requirement_description": "The system shall allow users to define multiple routes for the mock server.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_delays_should_affect_only_their_respective_call`",
                        "description": "defines multiple routes"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/server.py::MimickerServer::routes`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RD-002",
                "requirement_description": "Each route definition shall consist of an HTTP method and a URL path pattern.",
                "test_traceability": [
                    {
                        "id": "`tests/test_route.py::test_route_initialization`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::__init__`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RD-003",
                "requirement_description": "The system shall provide distinct functions or mechanisms for defining routes for HTTP GET, POST, PUT, DELETE, and PATCH methods.",
                "test_traceability": [
                    {
                        "id": "Implicitly through usage in all `test_mimicker_*.py` files",
                        "description": "e.g., `get()`, `post()`"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/mimicker.py::get`",
                        "description": ""
                    },
                    {
                        "id": "::post",
                        "description": ""
                    },
                    {
                        "id": "::put",
                        "description": ""
                    },
                    {
                        "id": "::delete",
                        "description": ""
                    },
                    {
                        "id": "::patch",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HTTP-001",
                "requirement_description": "The system shall handle incoming HTTP GET requests that match a defined GET route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py`",
                        "description": "entire file"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::do_GET`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HTTP-002",
                "requirement_description": "The system shall handle incoming HTTP POST requests that match a defined POST route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_post.py`",
                        "description": "entire file"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::do_POST`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HTTP-003",
                "requirement_description": "The system shall handle incoming HTTP PUT requests that match a defined PUT route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_put.py`",
                        "description": "entire file"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::do_PUT`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HTTP-004",
                "requirement_description": "The system shall handle incoming HTTP DELETE requests that match a defined DELETE route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_delete.py`",
                        "description": "entire file"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::do_DELETE`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HTTP-005",
                "requirement_description": "The system shall handle incoming HTTP PATCH requests that match a defined PATCH route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_patch.py`",
                        "description": "entire file"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::do_PATCH`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-001",
                "requirement_description": "The system shall allow users to specify an HTTP status code for the response of a defined route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_picked_status_code`",
                        "description": ""
                    },
                    {
                        "id": "`tests/test_mimicker_post.py::test_post_picked_status_code`",
                        "description": ""
                    },
                    {
                        "id": "etc.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::status`",
                        "description": ""
                    },
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_send_response`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-002",
                "requirement_description": "If no status code is explicitly configured for a route, the system shall default to HTTP status code 200 (OK).",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_default_200_status_code`",
                        "description": ""
                    },
                    {
                        "id": "`tests/test_mimicker_post.py::test_post_default_200_status_code`",
                        "description": ""
                    },
                    {
                        "id": "etc.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::__init__`",
                        "description": "default `_status = 200`"
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-003",
                "requirement_description": "The system shall allow users to specify a body for the response of a defined route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_body_as_text`",
                        "description": ""
                    },
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_body_as_json`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::body`",
                        "description": ""
                    },
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_write_response`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-004",
                "requirement_description": "If the configured response body is a dictionary, the system shall serialize it as a JSON string and send it as the HTTP response body.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_body_as_json`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_write_response`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-005",
                "requirement_description": "If the configured response body is a string, the system shall send it as a plain text HTTP response body, encoded in UTF-8.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_body_as_text`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_write_response`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-006",
                "requirement_description": "The system shall allow the response body to be specified as raw bytes (e.g., for file content). These bytes should be transmitted directly as the HTTP response entity.",
                "test_traceability": [
                    {
                        "id": "Derived from README example: `get(\"/file\").body(open(\"example.txt\", \"rb\").read())`. No direct original test case for *serving* bytes, but this is a clearly stated capability in documentation.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::body`",
                        "description": "can accept bytes"
                    },
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_write_response`",
                        "description": "would need to handle bytes correctly"
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-007",
                "requirement_description": "If the configured response body is `None` or an empty string, the system shall send an empty HTTP response body.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_empty_response`",
                        "description": "tests with `body(None)`"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::body`",
                        "description": "sets `_body` to `\"\"` if `None`"
                    },
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_write_response`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-008",
                "requirement_description": "The system shall allow users to specify a list of (header_name, header_value) tuples as HTTP headers for the response of a defined route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_headers`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::headers`",
                        "description": ""
                    },
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_set_headers`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-009",
                "requirement_description": "If custom response headers are configured, they shall be included in the HTTP response.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_headers`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_set_headers`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-010",
                "requirement_description": "For responses with dictionary-based bodies (intended as JSON), the system shall automatically set the `Content-Type` header to `application/json` if no `Content-Type` header is explicitly specified in the route's header configuration.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_body_as_json`",
                        "description": "verifies `Content-Type` is `application/json`"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_set_headers`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RCFG-011",
                "requirement_description": "If no custom headers are configured for a route with a dictionary-based body, the `Content-Type` header shall default to `application/json`.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_body_as_json`",
                        "description": "no explicit headers set on route, `Content-Type` is asserted"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_set_headers`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PP-001",
                "requirement_description": "The system shall support defining URL path patterns that include named placeholders for dynamic segments (path parameters), e.g., `/resource/{id}`.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_path_param`",
                        "description": ""
                    },
                    {
                        "id": "`tests/test_route.py::test_route_with_path_parameter`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::__init__`",
                        "description": "path compilation logic"
                    }
                ]
            },
            {
                "requirement_id": "FR-PP-002",
                "requirement_description": "The system shall correctly match incoming request paths against defined path patterns containing path parameters.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_path_param`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/stub_group.py::StubGroup::match`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PP-003",
                "requirement_description": "If a route with path parameters is matched and its response body is a dictionary containing string values with corresponding placeholders (e.g., `{\"message\": \"Hello, {name}!\"}`), the system shall substitute these placeholders with the actual values extracted from the request path.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_path_param`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_format_response`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PP-004",
                "requirement_description": "Path parameter placeholders should match segments in the path that do not contain `/`.",
                "test_traceability": [
                    {
                        "id": "`tests/test_route.py::test_route_with_path_parameter`",
                        "description": "implicitly, `[^/]+` in regex"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::__init__`",
                        "description": "regex `(?P<\\1>[^/]+)`"
                    }
                ]
            },
            {
                "requirement_id": "FR-RDLY-001",
                "requirement_description": "The system shall allow users to configure a response delay in seconds (float) for a defined route.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_with_delay`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::delay`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RDLY-002",
                "requirement_description": "If a response delay is configured for a route, the system shall wait for the specified duration before sending the response.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_with_delay`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_send_response`",
                        "description": "calls `sleep(delay)`"
                    }
                ]
            },
            {
                "requirement_id": "FR-RDLY-003",
                "requirement_description": "If no delay is configured, the default delay shall be 0 seconds (no artificial delay).",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_delays_should_affect_only_their_respective_call`",
                        "description": "the `/rapid` route has no delay"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::__init__`",
                        "description": "default `_delay = 0.`"
                    }
                ]
            },
            {
                "requirement_id": "FR-DYN-001",
                "requirement_description": "The system shall allow users to associate a custom Python function with a route to generate responses dynamically.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_post.py::test_post_reuse_request_body`",
                        "description": ""
                    },
                    {
                        "id": "`tests/test_route.py::test_route_response_func`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/route.py::Route::response_func`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-DYN-002",
                "requirement_description": "The custom response function, when invoked, shall determine the HTTP status code and the response body (or content).",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_post.py::test_post_reuse_request_body`",
                        "description": "function returns status and body"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_send_response`",
                        "description": "uses result of `response_func`"
                    }
                ]
            },
            {
                "requirement_id": "FR-DYN-003",
                "requirement_description": "The system shall provide the dynamic response function with access to the following request details:\n    *   Parsed JSON request body (as a Python dictionary, or `None` if not present/valid JSON).\n    *   Request headers (as a Python dictionary with lowercase keys).\n    *   Path parameters extracted from the URL (as a Python dictionary).",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_post.py::test_post_reuse_request_body` (uses `kwargs.get(\"payload\")`). README example for `response_func` shows `payload`, `headers`, `params`.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_send_response`",
                        "description": "calls `response_func(payload=request_body, headers=request_headers, params=path_params)`"
                    }
                ]
            },
            {
                "requirement_id": "FR-DYN-004",
                "requirement_description": "If a dynamic response function is configured for a route, any statically configured body, status, or headers for that route are superseded by the function's output for that request.",
                "test_traceability": [
                    {
                        "id": "`tests/test_route.py::test_route_response_func`",
                        "description": "static status/body are set on route but function provides different ones"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_send_response`",
                        "description": "if `response_func` exists, it's called and its results are used for status and response content"
                    }
                ]
            },
            {
                "requirement_id": "FR-PROC-001",
                "requirement_description": "The system shall parse incoming HTTP request bodies with `Content-Type` indicating JSON as JSON objects.\n    *   This functionality is primarily to support FR-DYN-003.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_post.py::test_post_reuse_request_body`",
                        "description": "client sends JSON, `response_func` receives parsed payload"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_get_request_body`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PROC-002",
                "requirement_description": "If an incoming request path and method do not match any defined route, the system shall respond with an HTTP 404 (Not Found) status code.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_404`",
                        "description": ""
                    },
                    {
                        "id": "`tests/test_mimicker_post.py::test_post_404`",
                        "description": ""
                    },
                    {
                        "id": "etc.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_handle_request`",
                        "description": ""
                    },
                    {
                        "id": "`mimicker/handler.py::MimickerHandler::_send_404_response`",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PROC-003",
                "requirement_description": "The system should be capable of handling multiple concurrent requests, with each request being processed according to its matched route configuration independently.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_delays_should_affect_only_their_respective_call`",
                        "description": "client sends concurrent requests, server handles them with their respective delays"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/server.py::ReusableAddressThreadingTCPServer`",
                        "description": "uses `socketserver.ThreadingTCPServer`"
                    }
                ]
            },
            {
                "requirement_id": "NFR-PERF-001",
                "requirement_description": "Response Delay Accuracy: The system shall ensure that the actual delay introduced before sending a response is close to the configured delay for a route (e.g., within +/- 0.05 seconds of the configured delay).",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_with_delay`",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "NFR-PERF-002",
                "requirement_description": "Independent Route Delays: Delays configured for one route shall not impact the response time (beyond its own configured delay) of other routes when multiple routes are accessed concurrently. Each concurrent request's delay should be honored independently.",
                "test_traceability": [
                    {
                        "id": "`tests/test_mimicker_get.py::test_get_delays_should_affect_only_their_respective_call`",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-1",
                "requirement_description": "HTTP Interface: The system shall expose its functionality over HTTP/1.1.\n    *   Interaction with the system (sending requests to mock endpoints) is exclusively via HTTP.",
                "test_traceability": [
                    {
                        "id": "All functional tests in `tests/test_mimicker_*.py` use an HTTP client.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/handler.py::MimickerHandler`",
                        "description": "subclasses `http.server.SimpleHTTPRequestHandler`"
                    }
                ]
            },
            {
                "requirement_id": "EIR-2",
                "requirement_description": "Programmatic Configuration Interface (Python API): The system shall provide a Python API for configuration. This includes:\n    *   Functions to initialize and start/stop the server (e.g., `mimicker(port)`).\n    *   Functions to define routes for HTTP methods (e.g., `get(path)`, `post(path)`).\n    *   A fluent interface or chainable methods on route objects to configure response status, body, headers, delay, and dynamic response functions.",
                "test_traceability": [
                    {
                        "id": "All test files demonstrate usage of this Python API for setting up mocks. README examples also showcase this API.",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "`mimicker/mimicker.py`",
                        "description": ""
                    },
                    {
                        "id": "`mimicker/route.py`",
                        "description": "defines the fluent interface"
                    }
                ]
            }
        ],
        "commit_sha": "25311b9caf84e9ef9cebc73ec885e9f07313afc4",
        "full_code_skeleton": "--- File: mimicker/handler.py ---\n```python\nclass MimickerHandler(http.server.SimpleHTTPRequestHandler):\n    def __init__(self, stub_matcher: StubGroup, *args, **kwargs):\n        pass\n\n    def do_GET(self):\n        pass\n\n    def do_POST(self):\n        pass\n\n    def do_PUT(self):\n        pass\n\n    def do_DELETE(self):\n        pass\n\n    def do_PATCH(self):\n        pass\n\n    def _handle_request(self, method: str):\n        pass\n\n    def _send_response(self, matched_stub: Stub, path_params: Dict[str, str], request_body: Any, request_headers: Dict[str, str]):\n        pass\n\n    def _set_headers(self, headers: Optional[List[Tuple[str, str]]]):\n        pass\n\n    def _write_response(self, response: Any, path_params: Dict[str, str]):\n        pass\n\n    def _send_404_response(self, method: str):\n        pass\n\n    @staticmethod\n    def _format_response(response: dict, path_params: dict):\n        pass\n\n    def _get_request_body(self) -> Optional[dict]:\n        pass\n```\n--- File: mimicker/__init__.py ---\n```python\n```\n--- File: mimicker/route.py ---\n```python\nclass Route:\n    \"\"\"\n    Represents an HTTP route with configurable response properties.\n    \"\"\"\n\n    def __init__(self, method: str, path: str):\n        \"\"\"\n        Initializes a new Route.\n\n        Args:\n            method (str): The HTTP method (GET, POST, PUT, DELETE, etc.).\n            path (str): The URL path for the route, supporting parameterized paths.\n        \"\"\"\n        pass\n\n    def delay(self, delay: float):\n        \"\"\"\n        Sets the delay (in seconds) before returning the response.\n\n        Args:\n            delay (float): The delay time in seconds.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def body(self, response: Union[Dict[str, Any], str] = None):\n        \"\"\"\n        Sets the response body for the route.\n\n        Args:\n            response (Union[Dict[str, Any], str], optional): The response body (JSON or string).\n            Defaults to an empty string.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def status(self, status_code: int):\n        \"\"\"\n        Sets the HTTP status code for the response.\n\n        Args:\n            status_code (int): The HTTP status code (e.g., 200, 404, 500).\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def headers(self, headers: List[Tuple[str, str]]):\n        \"\"\"\n        Sets the HTTP headers for the response.\n\n        Args:\n            headers (List[Tuple[str, str]]): A list of key-value pairs representing headers.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def response_func(self, func: Callable[..., Tuple[int, Any]]):\n        \"\"\"\n        Sets a custom response function for dynamic responses.\n\n        Args:\n            func (Callable[..., Tuple[int, Any]]): A function that returns (status_code, response_body).\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def build(self):\n        \"\"\"\n        Builds the route configuration dictionary.\n\n        Returns:\n            Dict[str, Any]: The route configuration containing method, path, response settings, and handlers.\n        \"\"\"\n        pass\n```\n--- File: mimicker/stub_group.py ---\n```python\nclass Stub(NamedTuple):\n    status_code: int\n    delay: float\n    response: Any\n    response_func: Optional[Callable]\n    headers: Optional[List[Tuple[str, str]]]\n\n\nclass StubGroup:\n    def __init__(self):\n        pass\n\n    def add(self, method: str, pattern: Union[str, Pattern],\n            status_code: int, response: Any, delay: Optional[float] = 0,\n            response_func: Optional[Callable[[], Tuple[int, Any]]] = None,\n            headers: Optional[List[Tuple[str, str]]] = None):\n        pass\n\n    def match(self, method: str, path: str,\n              request_headers: Optional[Dict[str, str]] = None) -> Tuple[Stub, Dict[str, str]]:\n        pass\n```\n--- File: mimicker/mimicker.py ---\n```python\ndef get(path: str) -> Route:\n    \"\"\"\n    Creates a GET route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A GET route instance.\n    \"\"\"\n    pass\n\n\ndef post(path: str) -> Route:\n    \"\"\"\n    Creates a POST route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A POST route instance.\n    \"\"\"\n    pass\n\n\ndef put(path: str) -> Route:\n    \"\"\"\n    Creates a PUT route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A PUT route instance.\n    \"\"\"\n    pass\n\n\ndef delete(path: str) -> Route:\n    \"\"\"\n    Creates a DELETE route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A DELETE route instance.\n    \"\"\"\n    pass\n\n\ndef patch(path: str) -> Route:\n    \"\"\"\n    Creates a PATCH route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A PATCH route instance.\n    \"\"\"\n    pass\n\n\ndef mimicker(port: int = 8080) -> MimickerServer:\n    \"\"\"\n    Starts a Mimicker server on the specified port.\n\n    Args:\n        port (int, optional): The port to run the server on. Defaults to 8080.\n\n    Returns:\n        MimickerServer: An instance of the running Mimicker server.\n    \"\"\"\n    pass\n```\n--- File: mimicker/server.py ---\n```python\nclass ReusableAddressThreadingTCPServer(socketserver.ThreadingTCPServer):\n    allow_reuse_address = True\n\n\nclass MimickerServer:\n    \"\"\"\n    A lightweight HTTP mocking server.\n\n    This server allows defining request-response routes for testing or simulation purposes.\n    \"\"\"\n    def __init__(self, port: int = 8080):\n        \"\"\"\n        Initializes the Mimicker server.\n\n        Args:\n            port (int, optional): The port to run the server on. Defaults to 8080.\n        \"\"\"\n        pass\n\n    def _handler_factory(self, *args):\n        pass\n\n    def routes(self, *routes: Route):\n        \"\"\"\n        Adds multiple routes to the server.\n\n        Args:\n            *routes (Route): One or more Route instances to be added.\n\n        Returns:\n            MimickerServer: The current server instance (for method chaining).\n        \"\"\"\n        pass\n\n    def start(self):\n        \"\"\"\n        Starts the Mimicker server in a background thread.\n\n        Returns:\n            MimickerServer: The current server instance (for method chaining).\n        \"\"\"\n        pass\n\n    def shutdown(self):\n        \"\"\"\n        Shuts down the Mimicker server gracefully.\n\n        Ensures that the server is stopped and the thread is joined if still running.\n        \"\"\"\n        pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "mimicker/handler.py",
                "code": "class MimickerHandler(http.server.SimpleHTTPRequestHandler):\n    def __init__(self, stub_matcher: StubGroup, *args, **kwargs):\n        pass\n\n    def do_GET(self):\n        pass\n\n    def do_POST(self):\n        pass\n\n    def do_PUT(self):\n        pass\n\n    def do_DELETE(self):\n        pass\n\n    def do_PATCH(self):\n        pass\n\n    def _handle_request(self, method: str):\n        pass\n\n    def _send_response(self, matched_stub: Stub, path_params: Dict[str, str], request_body: Any, request_headers: Dict[str, str]):\n        pass\n\n    def _set_headers(self, headers: Optional[List[Tuple[str, str]]]):\n        pass\n\n    def _write_response(self, response: Any, path_params: Dict[str, str]):\n        pass\n\n    def _send_404_response(self, method: str):\n        pass\n\n    @staticmethod\n    def _format_response(response: dict, path_params: dict):\n        pass\n\n    def _get_request_body(self) -> Optional[dict]:\n        pass\n"
            },
            {
                "file_path": "mimicker/__init__.py",
                "code": ""
            },
            {
                "file_path": "mimicker/route.py",
                "code": "class Route:\n    \"\"\"\n    Represents an HTTP route with configurable response properties.\n    \"\"\"\n\n    def __init__(self, method: str, path: str):\n        \"\"\"\n        Initializes a new Route.\n\n        Args:\n            method (str): The HTTP method (GET, POST, PUT, DELETE, etc.).\n            path (str): The URL path for the route, supporting parameterized paths.\n        \"\"\"\n        pass\n\n    def delay(self, delay: float):\n        \"\"\"\n        Sets the delay (in seconds) before returning the response.\n\n        Args:\n            delay (float): The delay time in seconds.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def body(self, response: Union[Dict[str, Any], str] = None):\n        \"\"\"\n        Sets the response body for the route.\n\n        Args:\n            response (Union[Dict[str, Any], str], optional): The response body (JSON or string).\n            Defaults to an empty string.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def status(self, status_code: int):\n        \"\"\"\n        Sets the HTTP status code for the response.\n\n        Args:\n            status_code (int): The HTTP status code (e.g., 200, 404, 500).\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def headers(self, headers: List[Tuple[str, str]]):\n        \"\"\"\n        Sets the HTTP headers for the response.\n\n        Args:\n            headers (List[Tuple[str, str]]): A list of key-value pairs representing headers.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def response_func(self, func: Callable[..., Tuple[int, Any]]):\n        \"\"\"\n        Sets a custom response function for dynamic responses.\n\n        Args:\n            func (Callable[..., Tuple[int, Any]]): A function that returns (status_code, response_body).\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def build(self):\n        \"\"\"\n        Builds the route configuration dictionary.\n\n        Returns:\n            Dict[str, Any]: The route configuration containing method, path, response settings, and handlers.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "mimicker/stub_group.py",
                "code": "class Stub(NamedTuple):\n    status_code: int\n    delay: float\n    response: Any\n    response_func: Optional[Callable]\n    headers: Optional[List[Tuple[str, str]]]\n\n\nclass StubGroup:\n    def __init__(self):\n        pass\n\n    def add(self, method: str, pattern: Union[str, Pattern],\n            status_code: int, response: Any, delay: Optional[float] = 0,\n            response_func: Optional[Callable[[], Tuple[int, Any]]] = None,\n            headers: Optional[List[Tuple[str, str]]] = None):\n        pass\n\n    def match(self, method: str, path: str,\n              request_headers: Optional[Dict[str, str]] = None) -> Tuple[Stub, Dict[str, str]]:\n        pass\n"
            },
            {
                "file_path": "mimicker/mimicker.py",
                "code": "def get(path: str) -> Route:\n    \"\"\"\n    Creates a GET route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A GET route instance.\n    \"\"\"\n    pass\n\n\ndef post(path: str) -> Route:\n    \"\"\"\n    Creates a POST route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A POST route instance.\n    \"\"\"\n    pass\n\n\ndef put(path: str) -> Route:\n    \"\"\"\n    Creates a PUT route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A PUT route instance.\n    \"\"\"\n    pass\n\n\ndef delete(path: str) -> Route:\n    \"\"\"\n    Creates a DELETE route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A DELETE route instance.\n    \"\"\"\n    pass\n\n\ndef patch(path: str) -> Route:\n    \"\"\"\n    Creates a PATCH route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A PATCH route instance.\n    \"\"\"\n    pass\n\n\ndef mimicker(port: int = 8080) -> MimickerServer:\n    \"\"\"\n    Starts a Mimicker server on the specified port.\n\n    Args:\n        port (int, optional): The port to run the server on. Defaults to 8080.\n\n    Returns:\n        MimickerServer: An instance of the running Mimicker server.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "mimicker/server.py",
                "code": "class ReusableAddressThreadingTCPServer(socketserver.ThreadingTCPServer):\n    allow_reuse_address = True\n\n\nclass MimickerServer:\n    \"\"\"\n    A lightweight HTTP mocking server.\n\n    This server allows defining request-response routes for testing or simulation purposes.\n    \"\"\"\n    def __init__(self, port: int = 8080):\n        \"\"\"\n        Initializes the Mimicker server.\n\n        Args:\n            port (int, optional): The port to run the server on. Defaults to 8080.\n        \"\"\"\n        pass\n\n    def _handler_factory(self, *args):\n        pass\n\n    def routes(self, *routes: Route):\n        \"\"\"\n        Adds multiple routes to the server.\n\n        Args:\n            *routes (Route): One or more Route instances to be added.\n\n        Returns:\n            MimickerServer: The current server instance (for method chaining).\n        \"\"\"\n        pass\n\n    def start(self):\n        \"\"\"\n        Starts the Mimicker server in a background thread.\n\n        Returns:\n            MimickerServer: The current server instance (for method chaining).\n        \"\"\"\n        pass\n\n    def shutdown(self):\n        \"\"\"\n        Shuts down the Mimicker server gracefully.\n\n        Ensures that the server is stopped and the thread is joined if still running.\n        \"\"\"\n        pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: mimicker/mimicker.py ---\n```python\ndef get(path: str) -> Route:\n    \"\"\"\n    Creates a GET route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A GET route instance.\n    \"\"\"\n    pass\n\ndef post(path: str) -> Route:\n    \"\"\"\n    Creates a POST route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A POST route instance.\n    \"\"\"\n    pass\n\ndef put(path: str) -> Route:\n    \"\"\"\n    Creates a PUT route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A PUT route instance.\n    \"\"\"\n    pass\n\ndef delete(path: str) -> Route:\n    \"\"\"\n    Creates a DELETE route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A DELETE route instance.\n    \"\"\"\n    pass\n\ndef patch(path: str) -> Route:\n    \"\"\"\n    Creates a PATCH route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A PATCH route instance.\n    \"\"\"\n    pass\n\ndef mimicker(port: int = 8080) -> MimickerServer:\n    \"\"\"\n    Starts a Mimicker server on the specified port.\n\n    Args:\n        port (int, optional): The port to run the server on. Defaults to 8080.\n\n    Returns:\n        MimickerServer: An instance of the running Mimicker server.\n    \"\"\"\n    pass\n```\n--- File: mimicker/route.py ---\n```python\nclass Route:\n    \"\"\"\n    Represents an HTTP route with configurable response properties.\n    \"\"\"\n\n    def __init__(self, method: str, path: str):\n        \"\"\"\n        Initializes a new Route.\n\n        Args:\n            method (str): The HTTP method (GET, POST, PUT, DELETE, etc.).\n            path (str): The URL path for the route, supporting parameterized paths.\n        \"\"\"\n        pass\n\n    def delay(self, delay: float):\n        \"\"\"\n        Sets the delay (in seconds) before returning the response.\n\n        Args:\n            delay (float): The delay time in seconds.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def body(self, response: Union[Dict[str, Any], str] = None):\n        \"\"\"\n        Sets the response body for the route.\n\n        Args:\n            response (Union[Dict[str, Any], str], optional): The response body (JSON or string).\n            Defaults to an empty string.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def status(self, status_code: int):\n        \"\"\"\n        Sets the HTTP status code for the response.\n\n        Args:\n            status_code (int): The HTTP status code (e.g., 200, 404, 500).\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def headers(self, headers: List[Tuple[str, str]]):\n        \"\"\"\n        Sets the HTTP headers for the response.\n\n        Args:\n            headers (List[Tuple[str, str]]): A list of key-value pairs representing headers.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def response_func(self, func: Callable[..., Tuple[int, Any]]):\n        \"\"\"\n        Sets a custom response function for dynamic responses.\n\n        Args:\n            func (Callable[..., Tuple[int, Any]]): A function that returns (status_code, response_body).\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def build(self):\n        \"\"\"\n        Builds the route configuration dictionary.\n\n        Returns:\n            Dict[str, Any]: The route configuration containing method, path, response settings, and handlers.\n        \"\"\"\n        pass\n```\n--- File: mimicker/server.py ---\n```python\nclass MimickerServer:\n    \"\"\"\n    A lightweight HTTP mocking server.\n\n    This server allows defining request-response routes for testing or simulation purposes.\n    \"\"\"\n    def __init__(self, port: int = 8080):\n        \"\"\"\n        Initializes the Mimicker server.\n\n        Args:\n            port (int, optional): The port to run the server on. Defaults to 8080.\n        \"\"\"\n        pass\n\n    def routes(self, *routes: Route):\n        \"\"\"\n        Adds multiple routes to the server.\n\n        Args:\n            *routes (Route): One or more Route instances to be added.\n\n        Returns:\n            MimickerServer: The current server instance (for method chaining).\n        \"\"\"\n        pass\n\n    def start(self):\n        \"\"\"\n        Starts the Mimicker server in a background thread.\n\n        Returns:\n            MimickerServer: The current server instance (for method chaining).\n        \"\"\"\n        pass\n\n    def shutdown(self):\n        \"\"\"\n        Shuts down the Mimicker server gracefully.\n\n        Ensures that the server is stopped and the thread is joined if still running.\n        \"\"\"\n        pass\n```\n--- File: mimicker/stub_group.py ---\n```python\nclass Stub(NamedTuple):\n    status_code: int\n    delay: float\n    response: Any\n    response_func: Optional[Callable]\n    headers: Optional[List[Tuple[str, str]]]\n\nclass StubGroup:\n    def __init__(self):\n        pass\n\n    def add(self, method: str, pattern: Union[str, Pattern],\n            status_code: int, response: Any, delay: Optional[float] = 0,\n            response_func: Optional[Callable[[], Tuple[int, Any]]] = None,\n            headers: Optional[List[Tuple[str, str]]] = None):\n        pass\n\n    def match(self, method: str, path: str,\n              request_headers: Optional[Dict[str, str]] = None) -> Tuple[Stub, Dict[str, str]]:\n        pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "mimicker/mimicker.py",
                "code": "def get(path: str) -> Route:\n    \"\"\"\n    Creates a GET route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A GET route instance.\n    \"\"\"\n    pass\n\ndef post(path: str) -> Route:\n    \"\"\"\n    Creates a POST route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A POST route instance.\n    \"\"\"\n    pass\n\ndef put(path: str) -> Route:\n    \"\"\"\n    Creates a PUT route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A PUT route instance.\n    \"\"\"\n    pass\n\ndef delete(path: str) -> Route:\n    \"\"\"\n    Creates a DELETE route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A DELETE route instance.\n    \"\"\"\n    pass\n\ndef patch(path: str) -> Route:\n    \"\"\"\n    Creates a PATCH route for the specified path.\n\n    Args:\n        path (str): The URL path for the route.\n\n    Returns:\n        Route: A PATCH route instance.\n    \"\"\"\n    pass\n\ndef mimicker(port: int = 8080) -> MimickerServer:\n    \"\"\"\n    Starts a Mimicker server on the specified port.\n\n    Args:\n        port (int, optional): The port to run the server on. Defaults to 8080.\n\n    Returns:\n        MimickerServer: An instance of the running Mimicker server.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "mimicker/route.py",
                "code": "class Route:\n    \"\"\"\n    Represents an HTTP route with configurable response properties.\n    \"\"\"\n\n    def __init__(self, method: str, path: str):\n        \"\"\"\n        Initializes a new Route.\n\n        Args:\n            method (str): The HTTP method (GET, POST, PUT, DELETE, etc.).\n            path (str): The URL path for the route, supporting parameterized paths.\n        \"\"\"\n        pass\n\n    def delay(self, delay: float):\n        \"\"\"\n        Sets the delay (in seconds) before returning the response.\n\n        Args:\n            delay (float): The delay time in seconds.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def body(self, response: Union[Dict[str, Any], str] = None):\n        \"\"\"\n        Sets the response body for the route.\n\n        Args:\n            response (Union[Dict[str, Any], str], optional): The response body (JSON or string).\n            Defaults to an empty string.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def status(self, status_code: int):\n        \"\"\"\n        Sets the HTTP status code for the response.\n\n        Args:\n            status_code (int): The HTTP status code (e.g., 200, 404, 500).\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def headers(self, headers: List[Tuple[str, str]]):\n        \"\"\"\n        Sets the HTTP headers for the response.\n\n        Args:\n            headers (List[Tuple[str, str]]): A list of key-value pairs representing headers.\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def response_func(self, func: Callable[..., Tuple[int, Any]]):\n        \"\"\"\n        Sets a custom response function for dynamic responses.\n\n        Args:\n            func (Callable[..., Tuple[int, Any]]): A function that returns (status_code, response_body).\n\n        Returns:\n            Route: The current Route instance (for method chaining).\n        \"\"\"\n        pass\n\n    def build(self):\n        \"\"\"\n        Builds the route configuration dictionary.\n\n        Returns:\n            Dict[str, Any]: The route configuration containing method, path, response settings, and handlers.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "mimicker/server.py",
                "code": "class MimickerServer:\n    \"\"\"\n    A lightweight HTTP mocking server.\n\n    This server allows defining request-response routes for testing or simulation purposes.\n    \"\"\"\n    def __init__(self, port: int = 8080):\n        \"\"\"\n        Initializes the Mimicker server.\n\n        Args:\n            port (int, optional): The port to run the server on. Defaults to 8080.\n        \"\"\"\n        pass\n\n    def routes(self, *routes: Route):\n        \"\"\"\n        Adds multiple routes to the server.\n\n        Args:\n            *routes (Route): One or more Route instances to be added.\n\n        Returns:\n            MimickerServer: The current server instance (for method chaining).\n        \"\"\"\n        pass\n\n    def start(self):\n        \"\"\"\n        Starts the Mimicker server in a background thread.\n\n        Returns:\n            MimickerServer: The current server instance (for method chaining).\n        \"\"\"\n        pass\n\n    def shutdown(self):\n        \"\"\"\n        Shuts down the Mimicker server gracefully.\n\n        Ensures that the server is stopped and the thread is joined if still running.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "mimicker/stub_group.py",
                "code": "class Stub(NamedTuple):\n    status_code: int\n    delay: float\n    response: Any\n    response_func: Optional[Callable]\n    headers: Optional[List[Tuple[str, str]]]\n\nclass StubGroup:\n    def __init__(self):\n        pass\n\n    def add(self, method: str, pattern: Union[str, Pattern],\n            status_code: int, response: Any, delay: Optional[float] = 0,\n            response_func: Optional[Callable[[], Tuple[int, Any]]] = None,\n            headers: Optional[List[Tuple[str, str]]] = None):\n        pass\n\n    def match(self, method: str, path: str,\n              request_headers: Optional[Dict[str, str]] = None) -> Tuple[Stub, Dict[str, str]]:\n        pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "tests/test_mimicker_get.py::test_get_default_200_status_code",
                "covers": [
                    "mimicker.mimicker.mimicker - basic server instantiation and start (via fixture)",
                    "mimicker.server.MimickerServer.__init__ - basic server instantiation (via mimicker.mimicker in fixture)",
                    "mimicker.server.MimickerServer.start - server start (via mimicker.mimicker in fixture)",
                    "mimicker.server.MimickerServer.shutdown - server shutdown (via fixture)",
                    "mimicker.mimicker.get - happy path with default status code",
                    "mimicker.server.MimickerServer.routes - basic usage for a GET route",
                    "mimicker.route.Route.__init__ - implicit instantiation for GET route"
                ]
            },
            {
                "test_id": "tests/test_mimicker_get.py::test_get_picked_status_code",
                "covers": [
                    "mimicker.route.Route.status - setting a custom status code"
                ]
            },
            {
                "test_id": "tests/test_mimicker_get.py::test_get_path_param",
                "covers": [
                    "mimicker.route.Route.body - setting JSON response body with path parameter substitution",
                    "mimicker.mimicker.get - happy path with a parameterized URL",
                    "mimicker.route.Route.__init__ - processing of parameterized path",
                    "mimicker.route.Route.status - explicit use with specific status code (200)"
                ]
            },
            {
                "test_id": "tests/test_mimicker_get.py::test_get_with_delay[0.1]",
                "covers": [
                    "mimicker.route.Route.delay - setting a response delay"
                ]
            },
            {
                "test_id": "tests/test_mimicker_get.py::test_get_headers",
                "covers": [
                    "mimicker.route.Route.headers - setting custom response headers"
                ]
            },
            {
                "test_id": "tests/test_mimicker_post.py::test_post_default_200_status_code",
                "covers": [
                    "mimicker.mimicker.post - happy path with default status code"
                ]
            },
            {
                "test_id": "tests/test_mimicker_post.py::test_post_reuse_request_body",
                "covers": [
                    "mimicker.route.Route.response_func - setting a dynamic response function"
                ]
            },
            {
                "test_id": "tests/test_mimicker_put.py::test_put_default_200_status_code",
                "covers": [
                    "mimicker.mimicker.put - happy path with default status code"
                ]
            },
            {
                "test_id": "tests/test_mimicker_delete.py::test_delete_default_200_status_code",
                "covers": [
                    "mimicker.mimicker.delete - happy path with default status code"
                ]
            },
            {
                "test_id": "tests/test_mimicker_patch.py::test_patch_default_200_status_code",
                "covers": [
                    "mimicker.mimicker.patch - happy path with default status code"
                ]
            },
            {
                "test_id": "tests/test_route.py::test_route_initialization",
                "covers": [
                    "mimicker.route.Route.__init__ - direct instantiation and verification of default properties"
                ]
            },
            {
                "test_id": "tests/test_route.py::test_route_build",
                "covers": [
                    "mimicker.route.Route.build - building the route configuration dictionary"
                ]
            },
            {
                "test_id": "tests/test_stub_group.py::test_match",
                "covers": [
                    "mimicker.stub_group.StubGroup.__init__ - direct instantiation",
                    "mimicker.stub_group.StubGroup.add - adding a basic stub directly",
                    "mimicker.stub_group.StubGroup.match - matching a path against added stubs directly"
                ]
            }
        ]
    },
    {
        "idx": 96919,
        "repo_name": "RyderCRD_sagkit",
        "url": "https://github.com/RyderCRD/sagkit",
        "description": "A toolkit for constructing schedule-abstraction graph in Python",
        "stars": 11,
        "forks": 0,
        "language": "python",
        "size": 957,
        "created_at": "2024-11-05T08:58:22+00:00",
        "updated_at": "2025-01-12T06:52:43+00:00",
        "pypi_info": {
            "name": "sagkit",
            "version": "0.0.9",
            "url": "https://files.pythonhosted.org/packages/ac/aa/acdd0847e50bfd1690142f28b75fad0b0db706e7bcd742daf6049c57eeb3/sagkit-0.0.9.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 26,
            "comment_ratio": 0.18986486486486487,
            "pyfile_content_length": 57706,
            "pyfile_code_lines": 1480,
            "test_file_exist": true,
            "test_file_content_length": 25012,
            "pytest_framework": true,
            "test_case_num": 23,
            "metadata_path": [
                "pyproject.toml"
            ],
            "readme_content_length": 6452,
            "llm_reason": "The project, SAGkit, is a Python toolkit for response-time analysis using Schedule-Abstraction Graphs. \n\nPositive aspects:\n*   **Self-Contained & Independent:** The core functionalities (jobset generation, SAG construction) operate on local files (reading job parameters, writing .dot graph files and .csv statistics) and do not require internet access or external APIs for their primary operation or testing. Dependencies are expected to be pip-installable Python packages.\n*   **Clear & Well-Defined Functionality:** The project offers two main CLI tools (`jobset_generator` and `sag_constructor`) with clearly defined parameters and input/output formats. The input job format is specified, and output formats (.dot, .csv) are standard.\n*   **Testable & Verifiable Output:** The project includes a comprehensive suite of unit tests that verify file generation, output content (including exact .dot file content in some cases), and internal logic. This is a significant advantage for benchmarking an AI's reconstructed version.\n*   **No Graphical User Interface:** The project is CLI-based, suitable for programmatic interaction and testing.\n*   **Appropriate Scope:** The codebase, inferred from the structure, appears to be of a manageable size for an AI to replicate. It's more complex than a toy example but not excessively large.\n*   **Predominantly Code-Based Solution:** The task is to generate Python code implementing the toolkit's logic.\n*   **Well-Structured:** The code is organized into modules for constructors, schedulers, and utilities, which can aid in understanding and replication.\n\nNegative aspects or concerns:\n*   **Complexity of Core Algorithms:** The primary challenge lies in replicating the SAG construction algorithms (original, extended, hybrid). These are domain-specific (real-time scheduling theory) and likely derived from academic research (the author's paper is mentioned). While the existing project serves as the specification, an AI would need to correctly implement this potentially intricate logic. This is what makes the difficulty 'Medium'.\n*   **Domain-Specific Knowledge:** Replicating the tool requires understanding concepts like job parameters (BCAT, WCAT, etc.), scheduling policies, and graph states, which is more involved than generic programming tasks.\n*   **Exact Replication of Outputs:** Some tests check for exact floating-point numbers or exact .dot file string matches. Achieving this level of precision might be challenging for an AI, though structural correctness and logical equivalence would still be valuable.\n\nOverall, SAGkit is a good candidate. Its self-contained nature, clear CLI, file-based I/O, and extensive test suite make it highly suitable for the benchmark's requirements. The main challenge is the algorithmic complexity, which places it at a 'Medium' difficulty, providing a meaningful test for an AI's capability to rebuild a specialized, non-trivial software tool from scratch.",
            "llm_project_type": "CLI toolkit for scheduling analysis and graph generation",
            "llm_rating": 75,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "RyderCRD_sagkit",
            "finish_test": true,
            "test_case_result": {
                "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_execution_scenarios": "passed",
                "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_idle_time": "passed",
                "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_read_jobs": "passed",
                "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_construct_SAG": "passed",
                "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_execution_scenarios": "passed",
                "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_idle_time": "passed",
                "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG": "passed",
                "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_execution_scenarios": "passed",
                "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_idle_time": "passed",
                "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_read_jobs": "passed",
                "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG": "passed",
                "tests/schedulers/test_edf_scheduler.py::TestEDFScheduler::test_compare": "passed",
                "tests/schedulers/test_fp_scheduler.py::TestFPScheduler::test_compare": "passed",
                "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator": "passed",
                "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor": "passed",
                "tests/utils/test_job.py::TestJob::test___str__": "passed",
                "tests/utils/test_job.py::TestJob::test_is_potentially_next": "passed",
                "tests/utils/test_job.py::TestJob::test_is_priority_eligible": "passed",
                "tests/utils/test_job.py::TestJob::test_set_to_non_triggered": "passed",
                "tests/utils/test_job.py::TestJob::test_set_to_triggered": "passed",
                "tests/utils/test_state.py::TestState::test_init": "passed",
                "tests/utils/test_state.py::TestState::test_is_leaf": "passed",
                "tests/utils/test_state.py::TestState::test_str": "passed"
            },
            "success_count": 23,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "unknown_count": 0,
            "total_count": 23,
            "success_rate": 1.0,
            "coverage_report": {
                "covered_lines": 266,
                "num_statements": 375,
                "percent_covered": 69.81519507186859,
                "percent_covered_display": "70",
                "missing_lines": 109,
                "excluded_lines": 0,
                "num_branches": 112,
                "num_partial_branches": 8,
                "covered_branches": 74,
                "missing_branches": 38
            },
            "coverage_result": {}
        },
        "codelines_count": 1480,
        "codefiles_count": 26,
        "code_length": 57706,
        "test_files_count": 9,
        "test_code_length": 25012,
        "class_diagram": "@startuml\nclass TestJobsetGenerator {\n    setUpClass(cls): void\n    tearDownClass(cls): void\n    test_jobset_generator(): void\n}\nclass TestSAGConstructor {\n    test_constructor(): void\n}\nclass TestHybridConstructor {\n    setUpClass(cls): void\n    tearDownClass(cls): void\n    setUp(): void\n    test_construct_SAG(): void\n    test_count_execution_scenarios(): void\n    test_count_idle_time(): void\n}\nclass TestOriginalConstructor {\n    setUpClass(cls): void\n    tearDownClass(cls): void\n    setUp(): void\n    test_read_jobs(): void\n    test_construct_SAG(): void\n    test_count_execution_scenarios(): void\n    test_count_idle_time(): void\n    test_save_SAG(): void\n}\nclass TestExtendedConstructor {\n    setUpClass(cls): void\n    tearDownClass(cls): void\n    setUp(): void\n    test_read_jobs(): void\n    test_count_execution_scenarios(): void\n    test_count_idle_time(): void\n}\nclass TestFPScheduler {\n    setUp(): void\n    test_compare(): void\n}\nclass TestEDFScheduler {\n    setUp(): void\n    test_compare(): void\n}\nclass TestJob {\n    setUp(): void\n    test_set_to_non_triggered(): void\n    test_set_to_triggered(): void\n    test_is_priority_eligible(): void\n    test_is_potentially_next(): void\n    test___str__(): void\n}\nclass TestState {\n    test_is_leaf(): void\n    test_str(): void\n    test_init(): void\n}\nclass Jobset_generator {\n    __init__(num_ins, ET_ratio, utilization, num_job): void\n    generate_jobs(num_job, utilization, ET_ratio): void\n    generate(jobset_folder): void\n}\nclass SAG_constructor {\n    __init__(jobset_folder, constructor_type, save_dot, save_statistics): void\n    construct(): void\n}\nclass Hybrid_constructor {\n    construct_SAG(): void\n    count_execution_scenarios(): void\n    count_idle_time(): void\n}\nclass Extended_constructor {\n    read_jobs(file_path): Unknown\n    count_execution_scenarios(): void\n    count_idle_time(): void\n}\nclass Constructor {\n    __init__(header, to_merge): Unknown\n    read_jobs(file_path): Unknown\n    find_shortest_leaf(): State\n    match(a, b): bool\n    expand(leaf, job, to_merge): Unknown\n    construct_SAG(): Unknown\n    count_execution_scenarios(): void\n    count_idle_time(): void\n    do_statistics(): void\n    save_SAG(save_folder, jobset_path): Unknown\n}\nclass EDF_Scheduler {\n    compare(job1, job2): bool\n}\nclass FP_Scheduler {\n    compare(job1, job2): bool\n}\nclass State {\n    __init__(id, EFT, LFT, job_path): Unknown\n    is_leaf(): bool\n    __str__(): str\n}\nclass Job {\n    __init__(id, BCAT, WCAT, BCET, WCET, DDL, priority, is_ET): Unknown\n    set_to_non_triggered(): Unknown\n    set_to_triggered(): Unknown\n    is_priority_eligible(future_jobs, time): bool\n    is_potentially_next(future_jobs, time, state_LFT): bool\n    __str__(): str\n}\nTestExtendedConstructor --> Extended_constructor\nConstructor --> State\nConstructor --> Job\nTestSAGConstructor --> SAG_constructor\nTestFPScheduler --> Job\nExtended_constructor --> Job\nSAG_constructor --> Extended_constructor\nConstructor <|-- Hybrid_constructor\nTestJob --> Job\nTestOriginalConstructor --> Constructor\nHybrid_constructor --> State\nTestState --> State\nTestHybridConstructor --> Hybrid_constructor\nTestEDFScheduler --> Job\nConstructor <|-- Extended_constructor\nSAG_constructor --> Hybrid_constructor\nTestJobsetGenerator --> Jobset_generator\nTestSAGConstructor ..> SAG_constructor\nTestOriginalConstructor ..> Constructor\nTestState ..> State\nHybrid_constructor ..> State\nTestEDFScheduler ..> Job\nTestExtendedConstructor ..> Extended_constructor\nTestJobsetGenerator ..> Jobset_generator\nSAG_constructor ..> Hybrid_constructor\nSAG_constructor ..> Extended_constructor\nTestFPScheduler ..> Job\nTestHybridConstructor ..> Hybrid_constructor\nConstructor ..> Job\nConstructor ..> State\nExtended_constructor ..> Job\nTestJob ..> Job\n@enduml",
        "structure": [
            {
                "file": "tests/test_jobset_generator.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestJobsetGenerator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUpClass",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls"
                                ]
                            },
                            {
                                "name": "tearDownClass",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls"
                                ]
                            },
                            {
                                "name": "test_jobset_generator",
                                "docstring": null,
                                "comments": "Test the jobset generator",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/test_sag_constructor.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestSAGConstructor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_constructor",
                                "docstring": null,
                                "comments": "Test the constructor method",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/constructors/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/constructors/test_hybrid_constructor.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestHybridConstructor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUpClass",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls"
                                ]
                            },
                            {
                                "name": "tearDownClass",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls"
                                ]
                            },
                            {
                                "name": "setUp",
                                "docstring": null,
                                "comments": "Initialize the constructor object",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_construct_SAG",
                                "docstring": null,
                                "comments": "Test the read_jobs method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_count_execution_scenarios",
                                "docstring": null,
                                "comments": "Test the count_execution_scenarios method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_count_idle_time",
                                "docstring": null,
                                "comments": "Test the count_idle_time method",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/constructors/test_original_constructor.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestOriginalConstructor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUpClass",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls"
                                ]
                            },
                            {
                                "name": "tearDownClass",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls"
                                ]
                            },
                            {
                                "name": "setUp",
                                "docstring": null,
                                "comments": "Set up constructor",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_read_jobs",
                                "docstring": null,
                                "comments": "Test read_jobs method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_construct_SAG",
                                "docstring": null,
                                "comments": "Test find_shortest_leaf method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_count_execution_scenarios",
                                "docstring": null,
                                "comments": "Test match method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_count_idle_time",
                                "docstring": null,
                                "comments": "Test count_idle_time method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_save_SAG",
                                "docstring": null,
                                "comments": "Test save_SAG method",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/constructors/test_extended_constructor.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestExtendedConstructor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUpClass",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls"
                                ]
                            },
                            {
                                "name": "tearDownClass",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "cls"
                                ]
                            },
                            {
                                "name": "setUp",
                                "docstring": null,
                                "comments": "Set up constructor",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_read_jobs",
                                "docstring": null,
                                "comments": "Test read_jobs method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_count_execution_scenarios",
                                "docstring": null,
                                "comments": "Test count_execution_scenarios method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_count_idle_time",
                                "docstring": null,
                                "comments": "Test count_idle_time method",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/schedulers/test_fp_scheduler.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestFPScheduler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": null,
                                "comments": "Initialize the job objects",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_compare",
                                "docstring": null,
                                "comments": "Test the compare method",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/schedulers/test_edf_scheduler.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestEDFScheduler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": null,
                                "comments": "Initialize the job objects",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_compare",
                                "docstring": null,
                                "comments": "Test the compare method",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/schedulers/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/utils/test_job.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestJob",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "setUp",
                                "docstring": null,
                                "comments": "Initialize the job object",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_set_to_non_triggered",
                                "docstring": null,
                                "comments": "Test the set_to_non_triggered method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_set_to_triggered",
                                "docstring": null,
                                "comments": "Test the set_to_triggered method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_is_priority_eligible",
                                "docstring": null,
                                "comments": "Test the is_priority_eligible method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_is_potentially_next",
                                "docstring": null,
                                "comments": "Test the is_potentially_next method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test___str__",
                                "docstring": null,
                                "comments": "Test the __str__ method",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/utils/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/utils/test_state.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestState",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_is_leaf",
                                "docstring": null,
                                "comments": "Test the is_leaf method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_str",
                                "docstring": null,
                                "comments": "Test the str method",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_init",
                                "docstring": null,
                                "comments": "Test the init method",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sagkit/jobset_generator.py",
                "functions": [
                    {
                        "name": "int_or_int_list",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "value"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "Jobset_generator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "num_ins",
                                    "ET_ratio",
                                    "utilization",
                                    "num_job"
                                ]
                            },
                            {
                                "name": "generate_jobs",
                                "docstring": null,
                                "comments": "Generate jobs",
                                "args": [
                                    "self",
                                    "num_job",
                                    "utilization",
                                    "ET_ratio"
                                ]
                            },
                            {
                                "name": "generate",
                                "docstring": null,
                                "comments": "Generate jobsets",
                                "args": [
                                    "self",
                                    "jobset_folder"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/sag_constructor.py",
                "functions": [
                    {
                        "name": "str_list",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "value"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "SAG_constructor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "jobset_folder",
                                    "constructor_type",
                                    "save_dot",
                                    "save_statistics"
                                ]
                            },
                            {
                                "name": "construct",
                                "docstring": null,
                                "comments": "Construct SAGs with specified construction algorithms",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/constructors/hybrid_constructor.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Hybrid_constructor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "construct_SAG",
                                "docstring": null,
                                "comments": "Override read_jobs method of the Constructor class",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "count_execution_scenarios",
                                "docstring": null,
                                "comments": "Override count_execution_scenarios method of the Constructor class",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "count_idle_time",
                                "docstring": null,
                                "comments": "Override count_idle_time method of the Constructor class",
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/constructors/extended_constructor.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Extended_constructor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "read_jobs",
                                "docstring": null,
                                "comments": "Read jobs from file",
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "count_execution_scenarios",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "count_idle_time",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/constructors/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sagkit/constructors/original_constructor.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Constructor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "header",
                                    "to_merge"
                                ]
                            },
                            {
                                "name": "read_jobs",
                                "docstring": null,
                                "comments": "Read jobs from file",
                                "args": [
                                    "self",
                                    "file_path"
                                ]
                            },
                            {
                                "name": "find_shortest_leaf",
                                "docstring": null,
                                "comments": "Find the shortest leaf",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "match",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "a",
                                    "b"
                                ]
                            },
                            {
                                "name": "expand",
                                "docstring": null,
                                "comments": "Expansion phase with or without merging",
                                "args": [
                                    "self",
                                    "leaf",
                                    "job",
                                    "to_merge"
                                ]
                            },
                            {
                                "name": "construct_SAG",
                                "docstring": null,
                                "comments": "Construct SAG",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "count_execution_scenarios",
                                "docstring": null,
                                "comments": "Count the number of execution scenarios",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "count_idle_time",
                                "docstring": null,
                                "comments": "Count the maximum idle time",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "do_statistics",
                                "docstring": null,
                                "comments": "Do some statistics",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "save_SAG",
                                "docstring": null,
                                "comments": "Output the SAG in .dot format\nhttps://dreampuf.github.io/GraphvizOnline to visualize the SAG\nIf that doesn't work, try viewing the site in incognito mode",
                                "args": [
                                    "self",
                                    "save_folder",
                                    "jobset_path"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/schedulers/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sagkit/schedulers/edf_scheduler.py",
                "functions": [],
                "classes": [
                    {
                        "name": "EDF_Scheduler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "compare",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "job1",
                                    "job2"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/schedulers/fp_scheduler.py",
                "functions": [],
                "classes": [
                    {
                        "name": "FP_Scheduler",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "compare",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "job1",
                                    "job2"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/utils/state.py",
                "functions": [],
                "classes": [
                    {
                        "name": "State",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "id",
                                    "EFT",
                                    "LFT",
                                    "job_path"
                                ]
                            },
                            {
                                "name": "is_leaf",
                                "docstring": null,
                                "comments": "Determine if the state is a leaf node",
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sagkit/utils/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sagkit/utils/job.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Job",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "id",
                                    "BCAT",
                                    "WCAT",
                                    "BCET",
                                    "WCET",
                                    "DDL",
                                    "priority",
                                    "is_ET"
                                ]
                            },
                            {
                                "name": "set_to_non_triggered",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "set_to_triggered",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "is_priority_eligible",
                                "docstring": null,
                                "comments": "Determine if the job is priority-eligible at a given time",
                                "args": [
                                    "self",
                                    "future_jobs",
                                    "time"
                                ]
                            },
                            {
                                "name": "is_potentially_next",
                                "docstring": null,
                                "comments": "Determine if the job is potentially the next job to be scheduled",
                                "args": [
                                    "self",
                                    "future_jobs",
                                    "time",
                                    "state_LFT"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            }
        ],
        "test_cases": {
            "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_execution_scenarios": {
                "testid": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_execution_scenarios",
                "result": "passed",
                "test_implementation": "    def test_count_execution_scenarios(self):\n        self.assertEqual(self.constructor.count_execution_scenarios(), (0, 0))\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.assertEqual(\n            self.constructor.count_execution_scenarios(),\n            (1.380211241711606, 1.6812412373755872),\n        )"
            },
            "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_idle_time": {
                "testid": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_idle_time",
                "result": "passed",
                "test_implementation": "    def test_count_idle_time(self):\n        self.assertEqual(self.constructor.count_idle_time(), 0)\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.assertEqual(\n            self.constructor.count_execution_scenarios(),\n            (1.380211241711606, 1.6812412373755872),\n        )\n        self.assertEqual(self.constructor.count_idle_time(), 0)"
            },
            "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_read_jobs": {
                "testid": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_read_jobs",
                "result": "passed",
                "test_implementation": "    def test_read_jobs(self):\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.assertEqual(len(self.constructor.job_list), 2)\n        self.assertEqual(self.constructor.job_list[0].id, 0)\n        self.assertEqual(self.constructor.job_list[0].BCAT, 1)\n        self.assertEqual(self.constructor.job_list[0].WCAT, 2)\n        self.assertEqual(self.constructor.job_list[0].BCET, 3)\n        self.assertEqual(self.constructor.job_list[0].WCET, 4)\n        self.assertEqual(self.constructor.job_list[0].BCET_REC, 3)\n        self.assertEqual(self.constructor.job_list[0].WCET_REC, 4)\n        self.assertEqual(self.constructor.job_list[0].DDL, 5)\n        self.assertEqual(self.constructor.job_list[0].priority, 6)\n        self.assertFalse(self.constructor.job_list[0].is_ET)\n\n        self.assertEqual(self.constructor.job_list[1].id, 1)\n        self.assertEqual(self.constructor.job_list[1].BCAT, 2)\n        self.assertEqual(self.constructor.job_list[1].WCAT, 3)\n        self.assertEqual(self.constructor.job_list[1].BCET, 0)\n        self.assertEqual(self.constructor.job_list[1].WCET, 5)\n        self.assertEqual(self.constructor.job_list[1].BCET_REC, 4)\n        self.assertEqual(self.constructor.job_list[1].WCET_REC, 5)\n        self.assertEqual(self.constructor.job_list[1].DDL, 6)\n        self.assertEqual(self.constructor.job_list[1].priority, 7)\n        self.assertTrue(self.constructor.job_list[1].is_ET)"
            },
            "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_construct_SAG": {
                "testid": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_construct_SAG",
                "result": "passed",
                "test_implementation": "    def test_construct_SAG(self):\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.constructor.construct_SAG()\n        self.assertEqual(len(self.constructor.state_list), 4)\n        self.assertEqual(self.constructor.state_list[0].id, 0)\n        self.assertEqual(self.constructor.state_list[0].depth, 0)\n        self.assertEqual(self.constructor.state_list[0].EFT, 0)\n        self.assertEqual(self.constructor.state_list[0].LFT, 0)\n        self.assertEqual(self.constructor.state_list[1].id, 1)\n        self.assertEqual(self.constructor.state_list[1].depth, 1)\n        self.assertEqual(self.constructor.state_list[1].EFT, 4)\n        self.assertEqual(self.constructor.state_list[1].LFT, 6)\n        self.assertEqual(self.constructor.state_list[2].id, 2)\n        self.assertEqual(self.constructor.state_list[2].depth, 2)\n        self.assertEqual(self.constructor.state_list[2].EFT, 8)\n        self.assertEqual(self.constructor.state_list[2].LFT, 11)\n        self.assertEqual(self.constructor.state_list[3].id, 3)\n        self.assertEqual(self.constructor.state_list[3].depth, 2)\n        self.assertEqual(self.constructor.state_list[3].EFT, 4)\n        self.assertEqual(self.constructor.state_list[3].LFT, 6)"
            },
            "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_execution_scenarios": {
                "testid": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_execution_scenarios",
                "result": "passed",
                "test_implementation": "    def test_count_execution_scenarios(self):\n        self.assertEqual(self.constructor.count_execution_scenarios(), (0, 0))\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.assertEqual(\n            self.constructor.count_execution_scenarios(),\n            (1.380211241711606, 1.380211241711606),\n        )"
            },
            "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_idle_time": {
                "testid": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_idle_time",
                "result": "passed",
                "test_implementation": "    def test_count_idle_time(self):\n        self.assertEqual(self.constructor.count_idle_time(), 0)\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.constructor.construct_SAG()\n        self.assertEqual(self.constructor.count_idle_time(), 0)"
            },
            "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG": {
                "testid": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                "result": "passed",
                "test_implementation": "    def test_construct_SAG(self):\n        self.constructor.construct_SAG()\n        self.assertEqual(len(self.constructor.state_list), 1)\n        self.assertEqual(self.constructor.state_list[0].id, 0)\n        self.assertEqual(self.constructor.state_list[0].EFT, 0)\n        self.assertEqual(self.constructor.state_list[0].LFT, 0)\n        self.assertEqual(self.constructor.state_list[0].job_path, [])\n        self.assertEqual(self.constructor.state_list[0].depth, 0)\n        self.assertEqual(self.constructor.state_list[0].next_jobs, [])\n        self.assertEqual(self.constructor.state_list[0].next_states, [])\n\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.constructor.construct_SAG()\n\n        self.assertEqual(len(self.constructor.state_list), 3)\n        self.assertEqual(self.constructor.state_list[0].id, 0)\n        self.assertEqual(self.constructor.state_list[0].EFT, 0)\n        self.assertEqual(self.constructor.state_list[0].LFT, 0)\n        self.assertEqual(self.constructor.state_list[0].job_path, [])\n        self.assertEqual(self.constructor.state_list[0].depth, 0)\n        self.assertEqual(\n            self.constructor.state_list[0].next_jobs, [self.constructor.job_list[0]]\n        )\n        self.assertEqual(\n            self.constructor.state_list[0].next_states, [self.constructor.state_list[1]]\n        )\n        self.assertEqual(self.constructor.state_list[1].id, 1)\n        self.assertEqual(self.constructor.state_list[1].EFT, 4)\n        self.assertEqual(self.constructor.state_list[1].LFT, 6)\n        self.assertEqual(self.constructor.state_list[2].id, 2)\n        self.assertEqual(self.constructor.state_list[2].EFT, 8)\n        self.assertEqual(self.constructor.state_list[2].LFT, 11)"
            },
            "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_execution_scenarios": {
                "testid": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_execution_scenarios",
                "result": "passed",
                "test_implementation": "    def test_count_execution_scenarios(self):\n        self.assertEqual(self.constructor.count_execution_scenarios(), (0, 0))\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.assertEqual(\n            self.constructor.count_execution_scenarios(),\n            (1.380211241711606, 1.2041199826559248),\n        )"
            },
            "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_idle_time": {
                "testid": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_idle_time",
                "result": "passed",
                "test_implementation": "    def test_count_idle_time(self):\n        self.assertEqual(self.constructor.count_idle_time(), 0)\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.assertEqual(self.constructor.count_idle_time(), 4)"
            },
            "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_read_jobs": {
                "testid": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_read_jobs",
                "result": "passed",
                "test_implementation": "    def test_read_jobs(self):\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.assertEqual(len(self.constructor.job_list), 2)\n        self.assertEqual(self.constructor.job_list[0].id, 0)\n        self.assertEqual(self.constructor.job_list[0].BCAT, 1)\n        self.assertEqual(self.constructor.job_list[0].WCAT, 2)\n        self.assertEqual(self.constructor.job_list[0].BCET, 3)\n        self.assertEqual(self.constructor.job_list[0].WCET, 4)\n        self.assertEqual(self.constructor.job_list[0].BCET_REC, 3)\n        self.assertEqual(self.constructor.job_list[0].WCET_REC, 4)\n        self.assertEqual(self.constructor.job_list[0].DDL, 5)\n        self.assertEqual(self.constructor.job_list[0].priority, 6)\n        self.assertFalse(self.constructor.job_list[0].is_ET)\n\n        self.assertEqual(self.constructor.job_list[1].id, 1)\n        self.assertEqual(self.constructor.job_list[1].BCAT, 2)\n        self.assertEqual(self.constructor.job_list[1].WCAT, 3)\n        self.assertEqual(self.constructor.job_list[1].BCET, 4)\n        self.assertEqual(self.constructor.job_list[1].WCET, 5)\n        self.assertEqual(self.constructor.job_list[1].BCET_REC, 4)\n        self.assertEqual(self.constructor.job_list[1].WCET_REC, 5)\n        self.assertEqual(self.constructor.job_list[1].DDL, 6)\n        self.assertEqual(self.constructor.job_list[1].priority, 7)\n        self.assertTrue(self.constructor.job_list[1].is_ET)"
            },
            "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG": {
                "testid": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG",
                "result": "passed",
                "test_implementation": "    def test_save_SAG(self):\n        test_file_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"../../test_jobs.txt\")\n        )\n        self.constructor.read_jobs(test_file_path)\n        self.constructor.construct_SAG()\n        self.constructor.save_SAG(\"test_\", \"SAG.txt\")\n        self.assertTrue(os.path.exists(\"test_original_SAG.txt\"))\n        # No idea how to keep the indentation, please help\n        with open(\"test_original_SAG.txt\") as f:\n            self.assertEqual(\n                f.read(),\n                \"\"\"digraph finite_state_machine {\nrankdir = LR;\nsize = \"8,5\";\nnode [shape = doublecircle, fontsize = 20, fixedsize = true, width = 1.1, height = 1.1];\n\"S1\\\\n[0, 0]\";\nnode [shape = circle, fontsize = 20, fixedsize = true, width = 1.1, height = 1.1];\n\"S1\\\\n[0, 0]\" -> \"S2\\\\n[4, 6]\" [label=\"J1\", fontsize=20];\n\"S2\\\\n[4, 6]\" -> \"S3\\\\n[8, 11]\" [label=\"J2\", fontsize=20];\n}\"\"\",\n            )\n        os.remove(\"test_original_SAG.txt\")"
            },
            "tests/schedulers/test_edf_scheduler.py::TestEDFScheduler::test_compare": {
                "testid": "tests/schedulers/test_edf_scheduler.py::TestEDFScheduler::test_compare",
                "result": "passed",
                "test_implementation": "    def test_compare(self):\n        self.assertTrue(EDF_Scheduler.compare(self.job_1, self.job_2))\n        self.assertTrue(EDF_Scheduler.compare(self.job_1, self.job_3))\n        self.assertTrue(EDF_Scheduler.compare(self.job_1, self.job_4))\n        self.assertTrue(EDF_Scheduler.compare(self.job_2, self.job_3))\n        self.assertTrue(EDF_Scheduler.compare(self.job_2, self.job_4))\n        self.assertTrue(EDF_Scheduler.compare(self.job_3, self.job_4))"
            },
            "tests/schedulers/test_fp_scheduler.py::TestFPScheduler::test_compare": {
                "testid": "tests/schedulers/test_fp_scheduler.py::TestFPScheduler::test_compare",
                "result": "passed",
                "test_implementation": "    def test_compare(self):\n        self.assertTrue(FP_Scheduler.compare(self.job_1, self.job_2))\n        self.assertFalse(FP_Scheduler.compare(self.job_1, self.job_3))\n        self.assertTrue(FP_Scheduler.compare(self.job_1, self.job_4))\n        self.assertFalse(FP_Scheduler.compare(self.job_2, self.job_3))\n        self.assertFalse(FP_Scheduler.compare(self.job_2, self.job_4))\n        self.assertTrue(FP_Scheduler.compare(self.job_3, self.job_4))"
            },
            "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator": {
                "testid": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                "result": "passed",
                "test_implementation": "    def test_jobset_generator(self):\n        num_ins = 1\n        ET_ratio = [0, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n        utilization = [45, 50, 55, 60, 65, 70, 75]\n        num_job = 1000\n        output_folder = \"./temp_jobsets/\"\n        jobset_generator = Jobset_generator(num_ins, ET_ratio, utilization, num_job)\n        jobset_generator.generate(output_folder)\n        self.assertTrue(os.path.exists(output_folder))\n\n        jobset_folder = output_folder\n        jobset_paths = os.listdir(jobset_folder)\n        jobset_paths.sort(\n            key=lambda x: (\n                int(x.split(\"-\")[1]),\n                int(x.split(\"-\")[2]),\n                int(x.split(\"-\")[3]),\n                int(x.split(\"-\")[4][:-4]),\n            )\n        )\n\n        self.assertEqual(len(jobset_paths), 84)\n        self.assertEqual(jobset_paths[0], \"jobset-45-0-1000-1.txt\")\n        self.assertEqual(jobset_paths[1], \"jobset-45-10-1000-1.txt\")\n        self.assertEqual(jobset_paths[2], \"jobset-45-15-1000-1.txt\")\n        self.assertEqual(jobset_paths[3], \"jobset-45-20-1000-1.txt\")\n        self.assertEqual(jobset_paths[4], \"jobset-45-30-1000-1.txt\")\n        self.assertEqual(jobset_paths[5], \"jobset-45-40-1000-1.txt\")\n        self.assertEqual(jobset_paths[6], \"jobset-45-50-1000-1.txt\")\n        self.assertEqual(jobset_paths[7], \"jobset-45-60-1000-1.txt\")\n        self.assertEqual(jobset_paths[8], \"jobset-45-70-1000-1.txt\")\n        self.assertEqual(jobset_paths[9], \"jobset-45-80-1000-1.txt\")\n        self.assertEqual(jobset_paths[10], \"jobset-45-90-1000-1.txt\")\n        self.assertEqual(jobset_paths[11], \"jobset-45-100-1000-1.txt\")\n        self.assertEqual(jobset_paths[12], \"jobset-50-0-1000-1.txt\")\n        self.assertEqual(jobset_paths[-1], \"jobset-75-100-1000-1.txt\")"
            },
            "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor": {
                "testid": "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor",
                "result": "passed",
                "test_implementation": "    def test_constructor(self):\n        sag_constructor = SAG_constructor()\n        self.assertEqual(sag_constructor.jobset_folder, \"./jobsets/\")\n        self.assertEqual(\n            sag_constructor.constructor_type, [\"original\", \"extended\", \"hybrid\"]\n        )\n        self.assertIsNotNone(sag_constructor.save_dot)"
            },
            "tests/utils/test_job.py::TestJob::test___str__": {
                "testid": "tests/utils/test_job.py::TestJob::test___str__",
                "result": "passed",
                "test_implementation": "    def test___str__(self):\n        self.assertEqual(str(self.job), \"1 1 1 1 1 1 1\")"
            },
            "tests/utils/test_job.py::TestJob::test_is_potentially_next": {
                "testid": "tests/utils/test_job.py::TestJob::test_is_potentially_next",
                "result": "passed",
                "test_implementation": "    def test_is_potentially_next(self):\n        future_jobs = [Job(2, 2, 2, 2, 2, 2, 2, 2)]\n        self.assertTrue(self.job.is_potentially_next(future_jobs, 1, 1))"
            },
            "tests/utils/test_job.py::TestJob::test_is_priority_eligible": {
                "testid": "tests/utils/test_job.py::TestJob::test_is_priority_eligible",
                "result": "passed",
                "test_implementation": "    def test_is_priority_eligible(self):\n        future_jobs = [Job(2, 2, 2, 2, 2, 2, 2, 2)]\n        self.assertTrue(self.job.is_priority_eligible(future_jobs, 1))"
            },
            "tests/utils/test_job.py::TestJob::test_set_to_non_triggered": {
                "testid": "tests/utils/test_job.py::TestJob::test_set_to_non_triggered",
                "result": "passed",
                "test_implementation": "    def test_set_to_non_triggered(self):\n        self.assertEqual(self.job.BCET, 1)\n        self.assertEqual(self.job.WCET, 1)\n        self.job.set_to_non_triggered()\n        self.assertEqual(self.job.BCET, 0)\n        self.assertEqual(self.job.WCET, 0)"
            },
            "tests/utils/test_job.py::TestJob::test_set_to_triggered": {
                "testid": "tests/utils/test_job.py::TestJob::test_set_to_triggered",
                "result": "passed",
                "test_implementation": "    def test_set_to_triggered(self):\n        self.job.set_to_non_triggered()\n        self.job.set_to_triggered()\n        self.assertEqual(self.job.BCET, 1)\n        self.assertEqual(self.job.WCET, 1)"
            },
            "tests/utils/test_state.py::TestState::test_init": {
                "testid": "tests/utils/test_state.py::TestState::test_init",
                "result": "passed",
                "test_implementation": "    def test_init(self):\n        state = State(1, 1, 1, [])\n        self.assertEqual(state.id, 1)\n        self.assertEqual(state.EFT, 1)\n        self.assertEqual(state.LFT, 1)\n        self.assertEqual(state.depth, 0)\n        self.assertEqual(state.job_path, [])\n        self.assertEqual(state.next_jobs, [])\n        self.assertEqual(state.next_states, [])"
            },
            "tests/utils/test_state.py::TestState::test_is_leaf": {
                "testid": "tests/utils/test_state.py::TestState::test_is_leaf",
                "result": "passed",
                "test_implementation": "    def test_is_leaf(self):\n        state = State(1, 1, 1, [])\n        self.assertTrue(state.is_leaf())"
            },
            "tests/utils/test_state.py::TestState::test_str": {
                "testid": "tests/utils/test_state.py::TestState::test_str",
                "result": "passed",
                "test_implementation": "    def test_str(self):\n        state = State(1, 1, 1, [])\n        self.assertEqual(str(state), \"State 1 [1, 1]\")"
            }
        },
        "SRS_document": "**Software Requirements Specification: SAGkit**\n\n**1. Introduction**\n\n**1.1 Purpose**\nThis Software Requirements Specification (SRS) document defines the functional and external interface requirements for SAGkit, a Python toolkit for response-time analysis based on Schedule-Abstraction Graphs (SAGs). The primary purpose of this SRS is to serve as the sole specification for software developers who will implement the SAGkit system. Their implementation will be assessed based on its adherence to this SRS and its ability to pass a comprehensive suite of test cases (both public and private).\n\n**1.2 Scope**\nThe SAGkit system, as defined in this document, encompasses the following core functionalities:\n*   Generation of jobsets based on specified parameters.\n*   Construction of Schedule Abstraction Graphs (SAGs) from jobsets using three distinct algorithms: Original, Extended, and Hybrid.\n*   Calculation and reporting of various statistics related to the constructed SAGs.\n*   Output of SAGs in a .dot file format for visualization.\n*   Output of statistics in a .csv file format.\n\nThe system is intended to be used as a command-line tool. This document specifies WHAT the system must do, not HOW it should be implemented internally, allowing developers to make their own design choices.\n\n**1.3 Definitions, Acronyms, and Abbreviations**\n*   **SAG:** Schedule Abstraction Graph. A graph representation used for response-time analysis.\n*   **Job:** A computational task with defined timing properties.\n*   **Jobset:** A collection of jobs.\n*   **BCAT:** Best-Case Arrival Time.\n*   **WCAT:** Worst-Case Arrival Time.\n*   **BCET:** Best-Case Execution Time.\n*   **WCET:** Worst-Case Execution Time.\n*   **DDL:** Deadline (absolute time).\n*   **ET:** Event-Triggered. Refers to a job that may be absent.\n*   **EFT:** Earliest Finish Time of a state in the SAG.\n*   **LFT:** Latest Finish Time of a state in the SAG.\n*   **CLI:** Command Line Interface.\n*   **SRS:** Software Requirements Specification.\n*   **FP:** Fixed-Priority scheduling.\n*   **EDF:** Earliest Deadline First scheduling.\n\n**1.4 References**\n*   Provided README.md file for SAGkit.\n*   Original source code of the SAGkit Python project (for contextual understanding by the SRS author).\n*   Original test case implementations for SAGkit (for deriving and tracing requirements).\n\n**1.5 Overview**\nThis SRS is organized as follows:\n*   Section 1: Introduction - Provides an overview of the SRS, its purpose, scope, definitions, references, and document overview.\n*   Section 2: Overall Description - Describes the product perspective, summarizes its functions, user characteristics, constraints, assumptions, and dependencies.\n*   Section 3: Specific Requirements - Details all requirements, including functional requirements, external interface requirements, and a statement on non-functional requirements. This section is critical for implementation.\n\n**2. Overall Description**\n\n**2.1 Product Perspective**\nSAGkit is a Python toolkit designed for researchers and developers working on real-time systems. It aids in response-time analysis by constructing and analyzing Schedule-Abstraction Graphs. It operates as a standalone set of command-line utilities.\n\n**2.2 Product Functions (Summary)**\nThe SAGkit system provides the following key functions:\n*   **Jobset Generation:** Creates files containing sets of jobs with specified characteristics (e.g., utilization, percentage of event-triggered jobs).\n*   **SAG Construction:** Reads jobset files and builds SAGs using one of three selectable methods: Original, Extended, or Hybrid.\n*   **Statistics Computation:** Calculates metrics from the constructed SAGs, such as the number of states, execution scenarios, idle time, and construction time.\n*   **Output Generation:** Saves the constructed SAGs in .dot format for external visualization and saves computed statistics in .csv format.\n\n**2.3 User Characteristics**\nThe typical users of SAGkit are:\n*   Researchers in real-time scheduling and schedulability analysis.\n*   Software developers working with real-time systems who need to perform response-time analysis.\nUsers are expected to be familiar with concepts of real-time scheduling and command-line tools.\n\n**2.4 Constraints**\n*   The system shall be implemented in Python.\n*   The system must adhere to the specified input and output file formats.\n*   The system must be invokable via a command-line interface as specified.\n\n**2.5 Assumptions and Dependencies**\n*   The system will run in a standard Python environment where necessary libraries for basic operations (e.g., file I/O, math) are available.\n*   Input jobset files provided to the SAG constructor will conform to the format specified in this document.\n\n**3. Specific Requirements**\n\n**3.1 Functional Requirements**\n\n**3.1.1 General System Operation**\n*   **FR-GEN-001:** The system shall provide two main command-line utilities: one for jobset generation and one for SAG construction.\n\n**3.1.2 Job Entity**\n*   **FR-JOB-001: Job Attributes:** A Job entity shall be characterized by the following attributes:\n    1.  Identifier (ID, integer, unique within a jobset, typically 0-indexed).\n    2.  Best-Case Arrival Time (BCAT, integer).\n    3.  Worst-Case Arrival Time (WCAT, integer, WCAT >= BCAT).\n    4.  Best-Case Execution Time (BCET, integer).\n    5.  Worst-Case Execution Time (WCET, integer, WCET >= BCET).\n    6.  Absolute Deadline (DDL, integer).\n    7.  Priority (integer, a smaller value implies higher priority).\n    8.  Event-Triggered status (is_ET, integer: 0 for false/time-triggered, 1 for true/event-triggered).\n*   **FR-JOB-002: Job String Representation:** A Job entity shall have a string representation consisting of its BCAT, WCAT, BCET, WCET, DDL, priority, and is_ET status, space-separated in that order.\n*   **FR-JOB-003: ET Job Execution Time Backup:** For an Event-Triggered (ET) job, the system shall internally maintain a record of its original BCET and WCET values (referred to as BCET_REC and WCET_REC) if its active BCET/WCET are modified (e.g., for non-triggered scenarios).\n*   **FR-JOB-004: Non-Triggered State for ET Job:** An ET job shall be modifiable to a \"non-triggered\" state where its active BCET and WCET become 0.\n*   **FR-JOB-005: Triggered State Restoration for ET Job:** An ET job in a \"non-triggered\" state shall be restorable to its \"triggered\" state, where its active BCET and WCET revert to their original recorded values (BCET_REC, WCET_REC).\n\n**3.1.3 State Entity (for SAG)**\n*   **FR-STATE-001: State Attributes:** A State entity in an SAG shall be characterized by:\n    1.  Identifier (ID, integer, unique within a constructed SAG, typically 0-indexed).\n    2.  Earliest Finish Time (EFT, integer).\n    3.  Latest Finish Time (LFT, integer).\n    4.  Depth (integer, number of jobs in the execution path leading to this state).\n    5.  Job Path (ordered list of Job entities executed to reach this state).\n    6.  Next Jobs (list of Job entities that can extend the path from this state).\n    7.  Next States (list of State entities reachable from this state by executing corresponding next jobs).\n*   **FR-STATE-002: Leaf State Identification:** A State entity shall be identifiable as a leaf state if it has no next states.\n*   **FR-STATE-003: State String Representation:** A State entity shall have a string representation in the format \"State [ID] [[EFT], [LFT]]\".\n\n**3.1.4 Scheduling Logic Primitives**\n*   **FR-SCHED-001: Fixed-Priority (FP) Comparison:** The system shall provide a job comparison mechanism based on the Fixed-Priority (FP) policy: job1 has higher priority than job2 if job1's priority value is less than job2's priority value.\n*   **FR-SCHED-002: Earliest Deadline First (EDF) Comparison:** The system shall provide a job comparison mechanism based on the Earliest Deadline First (EDF) policy: job1 has higher priority than job2 if job1's DDL is less than job2's DDL. If DDLs are equal, priority is determined by the job's priority attribute (lower value is higher priority).\n*   **FR-SCHED-003: Job Priority Eligibility:** A job shall be determined as priority-eligible at a given `time` with respect to a list of `future_jobs` (unexecuted jobs) if no other job in `future_jobs` that could have arrived by or at `time` (i.e., `future_job.WCAT <= time`) has higher priority according to the FP scheduling policy (see FR-SCHED-001).\n*   **FR-SCHED-004: Job Potential Next:** A job shall be determined as potentially the next job to be scheduled from a current `state_LFT` at a given evaluation `time` with respect to a list of `future_jobs` if:\n    1.  Its BCAT is less than or equal to `state_LFT`, OR\n    2.  No other distinct job in `future_jobs` satisfies all of the following:\n        a.  Its WCAT is less than the evaluation `time`.\n        b.  It is priority-eligible (see FR-SCHED-003) at an effective time `max(other_job.WCAT, state_LFT)`.\n\n**3.1.5 Jobset Generation**\n*   **FR-JGEN-001: Parameterized Jobset Generation:** The system shall generate jobsets based on input parameters: number of instances, ET ratio(s), utilization percentage(s), and number of jobs per jobset.\n*   **FR-JGEN-002: Job Generation Logic:** For each job in a jobset, attributes shall be generated as follows (based on the \"setup in our paper\" mentioned in README):\n    1.  BCAT: Random integer between 1 and 9990 (inclusive).\n    2.  WCAT: BCAT + random integer between 0 and 9 (inclusive).\n    3.  BCET: Random integer between 2 (inclusive) and `floor(utilization / 5) - 7` (inclusive). `utilization` is an input parameter.\n    4.  WCET: BCET + random integer between 1 and 4 (inclusive).\n    5.  DDL: Fixed value of 10000.\n    6.  Priority: Random integer between 1 and 10 (inclusive).\n    7.  ET status: Determined based on `ET_ratio`. A job is ET (value 1) if `random_integer(0-99) >= (100 - ET_ratio)`. Otherwise, it's not ET (value 0).\n*   **FR-JGEN-003: Output Folder Creation:** The jobset generator shall create the specified output folder for jobsets if it does not already exist.\n*   **FR-JGEN-004: Jobset File Naming Convention:** Generated jobset files shall be named according to the pattern: `jobset-{utilization}-{ET_ratio}-{num_job}-{instance_num}.txt`.\n*   **FR-JGEN-005: Jobset File Content:** Each generated jobset file shall contain one job per line, with job attributes formatted according to FR-JOB-002.\n*   **FR-JGEN-006: Iteration over Parameter Combinations:** The jobset generator shall iterate through all combinations of provided ET_ratio(s), utilization(s), and num_job(s) for each specified number of instances.\n\n**3.1.6 SAG Construction - Common Logic**\n*   **FR-SAGC-001: Jobset Reading:** The SAG constructor shall read job definitions from specified jobset files. Each line in a jobset file represents one job (see FR-EIF-JFS-001). Job IDs are assigned sequentially starting from 0.\n*   **FR-SAGC-002: Root State Initialization:** SAG construction shall begin with a root state having ID 0, EFT 0, LFT 0, depth 0, and an empty job path.\n*   **FR-SAGC-003: Iterative SAG Expansion:** The SAG shall be constructed by iteratively expanding from the leaf state with the shallowest depth until all jobs in the jobset have been included in all complete paths (i.e., paths of length equal to the total number of jobs).\n*   **FR-SAGC-004: Shortest Leaf Selection:** In each iteration of SAG expansion, the system shall select a leaf state with the minimum depth among all current leaf states to expand next.\n*   **FR-SAGC-005: Eligible Successor Identification:** For a selected leaf state, the system shall identify all eligible successor jobs from the set of unexecuted jobs. A job is an eligible successor if:\n    1.  It is priority-eligible at an effective start time `t_E = max(leaf_state.EFT, job.BCAT)` (see FR-SCHED-003).\n    2.  It is potentially the next job to be scheduled from `leaf_state.LFT` at `t_E` (see FR-SCHED-004).\n*   **FR-SAGC-006: Expansion Error Handling:** If, during SAG construction, no eligible successor job can be found to expand a leaf state before all jobs are included in the path, the system shall terminate processing for that jobset and indicate an error (e.g., \"No eligible successor found\").\n*   **FR-SAGC-007: State Expansion Logic (EFT):** When expanding from a `leaf_state` with a `job`, the Earliest Finish Time (EFT) of the `successor_state` shall be `max(leaf_state.EFT, job.BCAT) + job.BCET`.\n*   **FR-SAGC-008: State Expansion Logic (LFT):** When expanding from a `leaf_state` with a `job`, the Latest Finish Time (LFT) of the `successor_state` shall be calculated as: `min(max(leaf_state.LFT, min(j.WCAT for j in unexecuted_jobs_including_current_job)), t_H) + job.WCET`.\n    *   `unexecuted_jobs_including_current_job` refers to all jobs not yet in `leaf_state.job_path`.\n    *   `t_H` is a preemption time, calculated as the minimum of `(future_job.WCAT - 1)` for all `future_job`s in `unexecuted_jobs_including_current_job` that have a higher priority (lower priority value) than the current `job`. If no such higher priority job exists, `t_H` is effectively infinite (or a very large number like `sys.maxsize`).\n*   **FR-SAGC-009: State Merging Criteria:** If state merging is enabled, a newly created `successor_state` shall be merged with an existing `state` in the `state_list` if:\n    1.  `state.depth == successor_state.depth`.\n    2.  Their time windows overlap: `max(state.EFT, successor_state.EFT) <= min(state.LFT, successor_state.LFT)`.\n    3.  They have been reached by the same set of jobs, irrespective of execution order (i.e., `sorted(state.job_path, key=by_job_id) == sorted(successor_state.job_path, key=by_job_id)`).\n*   **FR-SAGC-010: State Merging Action:** If a `successor_state` is merged with an existing `state`:\n    1.  The `leaf_state`'s `next_states` list shall point to the existing `state`.\n    2.  The existing `state.EFT` shall be updated to `min(existing_state.EFT, successor_state.EFT)`.\n    3.  The existing `state.LFT` shall be updated to `max(existing_state.LFT, successor_state.LFT)`.\n    4.  The `successor_state` is not added as a new distinct state to the `state_list`.\n    If no merge occurs, the `successor_state` is added to the `state_list` and linked from `leaf_state`.\n*   **FR-SAGC-011: Jobset Path Sorting for Processing:** The SAG constructor, when processing a folder of jobsets, shall sort the jobset file paths based on utilization, ET_ratio, number of jobs (runnable_number in code), and instance number extracted from filenames matching the pattern `jobset-{utilization}-{ET_ratio}-{num_job}-{instance_num}.txt`, if filenames follow this pattern. Otherwise, no specific sort order is guaranteed.\n\n**3.1.7 SAG Construction - Original Method Specifics (`constructor_type=\"original\"`)**\n*   **FR-ORIG-001: Standard Job Reading:** The Original constructor shall read jobs as defined in the input file without modification to their BCET/WCET based on ET status.\n*   **FR-ORIG-002: Standard SAG Expansion:** The Original constructor shall use the common expansion logic (FR-SAGC-003 to FR-SAGC-010) with state merging enabled by default.\n*   **FR-ORIG-003: Execution Scenarios Calculation (Original):**\n    The \"actual execution scenarios\" (log10) shall be calculated as `log10(product over all jobs J of: N_J)`, where `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET - J.BCET + X)`, with `X=2` if `J.is_ET` is true, and `X=1` if `J.is_ET` is false.\n    The \"analyzed execution scenarios\" (log10) shall be calculated identically but with `X=1` for all jobs, regardless of ET status.\n*   **FR-ORIG-004: Idle Time Calculation (Original):** The maximum idle time shall be calculated as the sum of `job.BCET` for all jobs where `job.is_ET` is true.\n\n**3.1.8 SAG Construction - Extended Method Specifics (`constructor_type=\"extended\"`)**\n*   **FR-EXT-001: Modified Job Reading (Extended):** The Extended constructor shall read jobs and if a job `is_ET` true, its active `BCET` shall be set to 0. The original BCET must be stored as `BCET_REC`.\n*   **FR-EXT-002: Standard SAG Expansion (Extended):** The Extended constructor shall use the common expansion logic (FR-SAGC-003 to FR-SAGC-010) with state merging enabled by default. Job execution times (BCET/WCET) used in expansion are those active after potential modification by FR-EXT-001.\n*   **FR-EXT-003: Execution Scenarios Calculation (Extended):**\n    The \"actual execution scenarios\" (log10) shall be calculated as `log10(product over all jobs J of: N_J)`, where `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET - J.BCET_REC + X)`, with `X=2` if `J.is_ET` is true, and `X=1` if `J.is_ET` is false. (Uses `BCET_REC` for ET jobs).\n    The \"analyzed execution scenarios\" (log10) shall be calculated as `log10(product over all jobs J of: N_J)`, where `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET - J.BCET + 1)` if `J.is_ET` is false. If `J.is_ET` is true, `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET + 1)` (since active `J.BCET` is 0 for ET jobs).\n*   **FR-EXT-004: Idle Time Calculation (Extended):** The maximum idle time shall be 0.\n\n**3.1.9 SAG Construction - Hybrid Method Specifics (`constructor_type=\"hybrid\"`)**\n*   **FR-HYB-001: Standard Job Reading (Hybrid):** The Hybrid constructor shall read jobs as defined in the input file without initial modification to their BCET/WCET based on ET status.\n*   **FR-HYB-002: Dual Expansion for ET Jobs (Hybrid):** During SAG expansion, if an `eligible_successor` job `is_ET` true, the Hybrid constructor shall perform two expansions from the `shortest_leaf`:\n    1.  First expansion: The ET job is temporarily set to a \"non-triggered\" state (BCET=0, WCET=0 using FR-JOB-004), and expansion (FR-SAGC-007 to FR-SAGC-010) is performed.\n    2.  Second expansion: The ET job is restored to its \"triggered\" state (original BCET_REC, WCET_REC using FR-JOB-005), and expansion is performed again.\n    For non-ET jobs, standard single expansion occurs. State merging is enabled by default.\n*   **FR-HYB-003: Execution Scenarios Calculation (Hybrid):**\n    Both \"actual execution scenarios\" (log10) and \"analyzed execution scenarios\" (log10) shall be calculated as `log10(product over all jobs J of: N_J)`, where `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET - J.BCET + X)`, with `X=2` if `J.is_ET` is true, and `X=1` if `J.is_ET` is false. (BCET here refers to the original BCET of the job).\n*   **FR-HYB-004: Idle Time Calculation (Hybrid):** The maximum idle time shall be 0.\n\n**3.1.10 Statistics Calculation**\n*   **FR-STAT-001: Statistics Collection:** After SAG construction, the system shall calculate the following statistics:\n    1.  Number of states in the SAG.\n    2.  Number of actual execution scenarios (log10, specific calculation depends on constructor type, see FR-ORIG-003, FR-EXT-003, FR-HYB-003).\n    3.  Number of analyzed execution scenarios (log10, specific calculation depends on constructor type).\n    4.  Valid ratio of analyzed SAG (log10): `analyzed_es_counter - actual_es_counter`.\n    5.  Maximum width of the SAG (maximum number of states at any single depth level).\n    6.  Maximum idle time (specific calculation depends on constructor type, see FR-ORIG-004, FR-EXT-004, FR-HYB-004).\n    7.  Construction time (e.g., in nanoseconds).\n\n**3.1.11 Output Generation**\n*   **FR-OUT-001: SAG .dot File Output:** If requested, the system shall save the constructed SAG for each jobset into a .dot file. The output folder for .dot files shall be created if it doesn't exist.\n*   **FR-OUT-002: .dot File Naming:** The .dot file for a given jobset shall be named `{constructor_type}_{original_jobset_filename}` (e.g., `original_jobset-45-0-1000-1.txt`).\n*   **FR-OUT-003: Statistics .csv File Output:** If a path is provided, the system shall save the calculated statistics for all processed jobsets and constructor types into a single .csv file. If the file exists, it should be overwritten or removed before writing new data.\n\n**3.2 External Interface Requirements**\n\n**3.2.1 Command Line Interface (CLI)**\n\n**3.2.1.1 Jobset Generator CLI (`python -m sagkit.jobset_generator ...`)**\n*   **FR-EIF-JGCLI-001: `--ET_ratio` Argument:** Accepts an integer or a comma-separated list of integers specifying the percentage(s) of jobs that are Event-Triggered. Default: 15.\n*   **FR-EIF-JGCLI-002: `--utilization` Argument:** Accepts an integer or a comma-separated list of integers specifying the target utilization percentage(s). Default: 45. (Note: `generate_jobs` implementation implies specific valid utilization steps, e.g., 45, 50,...75 for BCET calculation).\n*   **FR-EIF-JGCLI-003: `--jobset_folder` Argument:** Accepts a string path for the folder where generated jobsets will be saved. Default: \"./jobsets/\".\n*   **FR-EIF-JGCLI-004: `--num_job` Argument:** Accepts an integer or a comma-separated list of integers for the number of jobs in each jobset. Default: 1000.\n*   **FR-EIF-JGCLI-005: `--num_instance` Argument:** Accepts an integer for how many jobsets to generate for each combination of parameters. Default: 1.\n\n**3.2.1.2 SAG Constructor CLI (`python -m sagkit.sag_constructor ...`)**\n*   **FR-EIF-SCCLI-001: `--jobset_folder` Argument:** Accepts a string path to the folder containing jobset files to be processed. Default: \"./jobsets/\".\n*   **FR-EIF-SCCLI-002: `--constructor_type` Argument:** Accepts a string or a comma-separated list of strings specifying which SAG constructor(s) to use (e.g., \"original\", \"extended\", \"hybrid\"). Default: \"original,extended,hybrid\".\n*   **FR-EIF-SCCLI-003: `--save_dot` Argument:** Accepts a string path to a folder where .dot files representing the SAGs will be saved. If an empty string or not provided, interpreted based on default. Default folder: \"./dotfiles/\". Default behavior: save if path given.\n*   **FR-EIF-SCCLI-004: `--save_statistics` Argument:** Accepts a string path for the .csv file where statistics will be saved. Default: \"\" (empty string, implies no statistics saving unless specified, or specific default like \"./statistics.csv\" as in README). The code default is `\"\"` for the argument but `SAG_constructor` class initializes it to `./statistics.csv`. Let's take class default. The test confirms `save_statistics` exists as an attribute. Behavior: Save if path is non-empty.\n\n**3.2.2 File Interfaces**\n\n**3.2.2.1 Jobset Input File Format**\n*   **FR-EIF-JFS-001: Jobset File Structure:** Each jobset file shall be a plain text file. Each line shall represent a single job.\n*   **FR-EIF-JFS-002: Job Record Format:** Each line (job record) in a jobset file shall consist of seven space-separated integer values in the following order: BCAT, WCAT, BCET, WCET, DDL, priority, ET (0 or 1).\n\n**3.2.2.2 SAG Output File Format (.dot)**\n*   **FR-EIF-DOT-001: DOT File Header:** Each .dot file shall begin with:\n    ```dot\n    digraph finite_state_machine {\n    rankdir = LR;\n    size = \"8,5\";\n    node [shape = doublecircle, fontsize = 20, fixedsize = true, width = 1.1, height = 1.1];\n    \"S1\\\\n[0, 0]\";\n    node [shape = circle, fontsize = 20, fixedsize = true, width = 1.1, height = 1.1];\n    ```\n    (Note: S1 refers to state with ID 0. The first node \"S1\\\\n[0,0]\" is specified as `doublecircle`, then the default node shape is set to `circle`.)\n*   **FR-EIF-DOT-002: DOT File Node Representation:** States in the SAG shall be represented as nodes. The root node (ID 0) shall be ` \"S1\\\\n[0, 0]\" ` (using 1-based indexing for \"S\" prefix, but 0,0 for EFT/LFT). Other nodes shall be labeled ` \"S{state.id + 1}\\\\n[{state.EFT}, {state.LFT}]\" `.\n*   **FR-EIF-DOT-003: DOT File Edge Representation:** Transitions between states shall be represented as directed edges:\n    `\"S{from_state.id + 1}\\\\n[{from_state.EFT}, {from_state.LFT}]\" -> \"S{to_state.id + 1}\\\\n[{to_state.EFT}, {to_state.LFT}]\" [label=\"J{job.id + 1}\", fontsize=20];`\n*   **FR-EIF-DOT-004: DOT File Footer:** Each .dot file shall end with a closing brace `}`.\n\n**3.2.2.3 Statistics Output File Format (.csv)**\n*   **FR-EIF-CSV-001: CSV File Structure:** The statistics output shall be a CSV (Comma Separated Values) file.\n*   **FR-EIF-CSV-002: CSV Header:** For each constructor type processed, the CSV file shall contain a line with the constructor type name (e.g., \"original\"), followed by a header line:\n    `Utilization,ET_Ratio,Number of States,Number of actual execution scenarios (log10),Number of analyzed execution scenarios (log10),Valid ratio of analyzed SAG (log10),Maximum width,Maximum idle time,Construction time (ns)`\n*   **FR-EIF-CSV-003: CSV Data Rows:** Each data row following the header shall correspond to one processed jobset and contain values for the statistics defined in FR-STAT-001, in the order specified by the header FR-EIF-CSV-002. Specifically:\n    `utilization, ET_ratio, num_states, actual_es_log10, analyzed_es_log10, (analyzed_es_log10 - actual_es_log10), max_width, idle_time, construction_time_ns`\n    Jobset parameters (utilization, ET_ratio) are derived from the jobset filename if it matches the standard pattern.\n\n**3.3 Non-Functional Requirements**\n*   **NFR-001: Test Case Validation:** No specific non-functional requirements (e.g., performance benchmarks, security levels, specific usability metrics) have been identified that are directly and explicitly validated by dedicated original test cases beyond functional correctness and adherence to specified output formats. The system's primary NFR is to correctly implement the functionalities to pass all (public and private) tests.",
        "structured_requirements": [
            {
                "requirement_id": "FR-GEN-001",
                "requirement_description": "The system shall provide two main command-line utilities: one for jobset generation and one for SAG construction.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py",
                        "description": "(for generator invocation)"
                    },
                    {
                        "id": "tests/test_sag_constructor.py",
                        "description": "(for constructor invocation)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py",
                        "description": "(main block)"
                    },
                    {
                        "id": "src/sagkit/sag_constructor.py",
                        "description": "(main block)"
                    }
                ]
            },
            {
                "requirement_id": "FR-JOB-001",
                "requirement_description": "Job Attributes:** A Job entity shall be characterized by the following attributes:\n    1.  Identifier (ID, integer, unique within a jobset, typically 0-indexed).\n    2.  Best-Case Arrival Time (BCAT, integer).\n    3.  Worst-Case Arrival Time (WCAT, integer, WCAT >= BCAT).\n    4.  Best-Case Execution Time (BCET, integer).\n    5.  Worst-Case Execution Time (WCET, integer, WCET >= BCET).\n    6.  Absolute Deadline (DDL, integer).\n    7.  Priority (integer, a smaller value implies higher priority).\n    8.  Event-Triggered status (is_ET, integer: 0 for false/time-triggered, 1 for true/event-triggered).",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_job.py::TestJob::setUp",
                        "description": ""
                    },
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_read_jobs",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/job.py::Job::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JOB-002",
                "requirement_description": "Job String Representation:** A Job entity shall have a string representation consisting of its BCAT, WCAT, BCET, WCET, DDL, priority, and is_ET status, space-separated in that order.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_job.py::TestJob::test___str__",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/job.py::Job::__str__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JOB-003",
                "requirement_description": "ET Job Execution Time Backup:** For an Event-Triggered (ET) job, the system shall internally maintain a record of its original BCET and WCET values (referred to as BCET_REC and WCET_REC) if its active BCET/WCET are modified (e.g., for non-triggered scenarios).",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_job.py::TestJob::test_set_to_non_triggered",
                        "description": ""
                    },
                    {
                        "id": "tests/utils/test_job.py::TestJob::test_set_to_triggered",
                        "description": ""
                    },
                    {
                        "id": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_read_jobs",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/job.py::Job::__init__",
                        "description": ""
                    },
                    {
                        "id": "src/sagkit/utils/job.py::Job::set_to_non_triggered",
                        "description": ""
                    },
                    {
                        "id": "src/sagkit/utils/job.py::Job::set_to_triggered",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JOB-004",
                "requirement_description": "Non-Triggered State for ET Job:** An ET job shall be modifiable to a \"non-triggered\" state where its active BCET and WCET become 0.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_job.py::TestJob::test_set_to_non_triggered",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/job.py::Job::set_to_non_triggered",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JOB-005",
                "requirement_description": "Triggered State Restoration for ET Job:** An ET job in a \"non-triggered\" state shall be restorable to its \"triggered\" state, where its active BCET and WCET revert to their original recorded values (BCET_REC, WCET_REC).",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_job.py::TestJob::test_set_to_triggered",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/job.py::Job::set_to_triggered",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-STATE-001",
                "requirement_description": "State Attributes:** A State entity in an SAG shall be characterized by:\n    1.  Identifier (ID, integer, unique within a constructed SAG, typically 0-indexed).\n    2.  Earliest Finish Time (EFT, integer).\n    3.  Latest Finish Time (LFT, integer).\n    4.  Depth (integer, number of jobs in the execution path leading to this state).\n    5.  Job Path (ordered list of Job entities executed to reach this state).\n    6.  Next Jobs (list of Job entities that can extend the path from this state).\n    7.  Next States (list of State entities reachable from this state by executing corresponding next jobs).",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_state.py::TestState::test_init",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/state.py::State::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-STATE-002",
                "requirement_description": "Leaf State Identification:** A State entity shall be identifiable as a leaf state if it has no next states.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_state.py::TestState::test_is_leaf",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/state.py::State::is_leaf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-STATE-003",
                "requirement_description": "State String Representation:** A State entity shall have a string representation in the format \"State [ID] [[EFT], [LFT]]\".",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_state.py::TestState::test_str",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/state.py::State::__str__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SCHED-001",
                "requirement_description": "Fixed-Priority (FP) Comparison:** The system shall provide a job comparison mechanism based on the Fixed-Priority (FP) policy: job1 has higher priority than job2 if job1's priority value is less than job2's priority value.",
                "test_traceability": [
                    {
                        "id": "tests/schedulers/test_fp_scheduler.py::TestFPScheduler::test_compare",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/schedulers/fp_scheduler.py::FP_Scheduler::compare",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SCHED-002",
                "requirement_description": "Earliest Deadline First (EDF) Comparison:** The system shall provide a job comparison mechanism based on the Earliest Deadline First (EDF) policy: job1 has higher priority than job2 if job1's DDL is less than job2's DDL. If DDLs are equal, priority is determined by the job's priority attribute (lower value is higher priority).",
                "test_traceability": [
                    {
                        "id": "tests/schedulers/test_edf_scheduler.py::TestEDFScheduler::test_compare",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/schedulers/edf_scheduler.py::EDF_Scheduler::compare",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SCHED-003",
                "requirement_description": "Job Priority Eligibility:** A job shall be determined as priority-eligible at a given `time` with respect to a list of `future_jobs` (unexecuted jobs) if no other job in `future_jobs` that could have arrived by or at `time` (i.e., `future_job.WCAT <= time`) has higher priority according to the FP scheduling policy (see FR-SCHED-001).",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_job.py::TestJob::test_is_priority_eligible",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/job.py::Job::is_priority_eligible",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SCHED-004",
                "requirement_description": "Job Potential Next:** A job shall be determined as potentially the next job to be scheduled from a current `state_LFT` at a given evaluation `time` with respect to a list of `future_jobs` if:\n    1.  Its BCAT is less than or equal to `state_LFT`, OR\n    2.  No other distinct job in `future_jobs` satisfies all of the following:\n        a.  Its WCAT is less than the evaluation `time`.\n        b.  It is priority-eligible (see FR-SCHED-003) at an effective time `max(other_job.WCAT, state_LFT)`.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_job.py::TestJob::test_is_potentially_next",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/utils/job.py::Job::is_potentially_next",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JGEN-001",
                "requirement_description": "Parameterized Jobset Generation:** The system shall generate jobsets based on input parameters: number of instances, ET ratio(s), utilization percentage(s), and number of jobs per jobset.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py::Jobset_generator::__init__",
                        "description": ""
                    },
                    {
                        "id": "src/sagkit/jobset_generator.py::Jobset_generator::generate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JGEN-002",
                "requirement_description": "Job Generation Logic:** For each job in a jobset, attributes shall be generated as follows (based on the \"setup in our paper\" mentioned in README):\n    1.  BCAT: Random integer between 1 and 9990 (inclusive).\n    2.  WCAT: BCAT + random integer between 0 and 9 (inclusive).\n    3.  BCET: Random integer between 2 (inclusive) and `floor(utilization / 5) - 7` (inclusive). `utilization` is an input parameter.\n    4.  WCET: BCET + random integer between 1 and 4 (inclusive).\n    5.  DDL: Fixed value of 10000.\n    6.  Priority: Random integer between 1 and 10 (inclusive).\n    7.  ET status: Determined based on `ET_ratio`. A job is ET (value 1) if `random_integer(0-99) >= (100 - ET_ratio)`. Otherwise, it's not ET (value 0).",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": "verifies file creation; content relies on this logic."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py::Jobset_generator::generate_jobs",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JGEN-003",
                "requirement_description": "Output Folder Creation:** The jobset generator shall create the specified output folder for jobsets if it does not already exist.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py::Jobset_generator::generate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JGEN-004",
                "requirement_description": "Jobset File Naming Convention:** Generated jobset files shall be named according to the pattern: `jobset-{utilization}-{ET_ratio}-{num_job}-{instance_num}.txt`.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py::Jobset_generator::generate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JGEN-005",
                "requirement_description": "Jobset File Content:** Each generated jobset file shall contain one job per line, with job attributes formatted according to FR-JOB-002.",
                "test_traceability": [
                    {
                        "id": "",
                        "description": "Indirectly covered by tests that consume these files, e.g., constructor tests."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py::Jobset_generator::generate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-JGEN-006",
                "requirement_description": "Iteration over Parameter Combinations:** The jobset generator shall iterate through all combinations of provided ET_ratio(s), utilization(s), and num_job(s) for each specified number of instances.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": "(verifies total number of files generated)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py::Jobset_generator::generate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-001",
                "requirement_description": "Jobset Reading:** The SAG constructor shall read job definitions from specified jobset files. Each line in a jobset file represents one job (see FR-EIF-JFS-001). Job IDs are assigned sequentially starting from 0.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_read_jobs",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::read_jobs",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-002",
                "requirement_description": "Root State Initialization:** SAG construction shall begin with a root state having ID 0, EFT 0, LFT 0, depth 0, and an empty job path.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": "(initial state verification)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::construct_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-003",
                "requirement_description": "Iterative SAG Expansion:** The SAG shall be constructed by iteratively expanding from the leaf state with the shallowest depth until all jobs in the jobset have been included in all complete paths (i.e., paths of length equal to the total number of jobs).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": "(verifies final graph structure)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::construct_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-004",
                "requirement_description": "Shortest Leaf Selection:** In each iteration of SAG expansion, the system shall select a leaf state with the minimum depth among all current leaf states to expand next.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": "(implicit in the construction logic tested)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::find_shortest_leaf",
                        "description": ""
                    },
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::construct_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-005",
                "requirement_description": "Eligible Successor Identification:** For a selected leaf state, the system shall identify all eligible successor jobs from the set of unexecuted jobs. A job is an eligible successor if:\n    1.  It is priority-eligible at an effective start time `t_E = max(leaf_state.EFT, job.BCAT)` (see FR-SCHED-003).\n    2.  It is potentially the next job to be scheduled from `leaf_state.LFT` at `t_E` (see FR-SCHED-004).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": "(implicit in the construction logic)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::construct_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-006",
                "requirement_description": "Expansion Error Handling:** If, during SAG construction, no eligible successor job can be found to expand a leaf state before all jobs are included in the path, the system shall terminate processing for that jobset and indicate an error (e.g., \"No eligible successor found\").",
                "test_traceability": [
                    {
                        "id": "None",
                        "description": "This is an error path, specific test might not exist for sys.exit"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::construct_SAG",
                        "description": "(sys.exit call)"
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-007",
                "requirement_description": "State Expansion Logic (EFT):** When expanding from a `leaf_state` with a `job`, the Earliest Finish Time (EFT) of the `successor_state` shall be `max(leaf_state.EFT, job.BCAT) + job.BCET`.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": "(verifies EFT values)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::expand",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-008",
                "requirement_description": "State Expansion Logic (LFT):** When expanding from a `leaf_state` with a `job`, the Latest Finish Time (LFT) of the `successor_state` shall be calculated as: `min(max(leaf_state.LFT, min(j.WCAT for j in unexecuted_jobs_including_current_job)), t_H) + job.WCET`.\n    *   `unexecuted_jobs_including_current_job` refers to all jobs not yet in `leaf_state.job_path`.\n    *   `t_H` is a preemption time, calculated as the minimum of `(future_job.WCAT - 1)` for all `future_job`s in `unexecuted_jobs_including_current_job` that have a higher priority (lower priority value) than the current `job`. If no such higher priority job exists, `t_H` is effectively infinite (or a very large number like `sys.maxsize`).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": "(verifies LFT values)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::expand",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-009",
                "requirement_description": "State Merging Criteria:** If state merging is enabled, a newly created `successor_state` shall be merged with an existing `state` in the `state_list` if:\n    1.  `state.depth == successor_state.depth`.\n    2.  Their time windows overlap: `max(state.EFT, successor_state.EFT) <= min(state.LFT, successor_state.LFT)`.\n    3.  They have been reached by the same set of jobs, irrespective of execution order (i.e., `sorted(state.job_path, key=by_job_id) == sorted(successor_state.job_path, key=by_job_id)`).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": "(implicit, as merging affects the final graph structure and state count)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::match",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-010",
                "requirement_description": "State Merging Action:** If a `successor_state` is merged with an existing `state`:\n    1.  The `leaf_state`'s `next_states` list shall point to the existing `state`.\n    2.  The existing `state.EFT` shall be updated to `min(existing_state.EFT, successor_state.EFT)`.\n    3.  The existing `state.LFT` shall be updated to `max(existing_state.LFT, successor_state.LFT)`.\n    4.  The `successor_state` is not added as a new distinct state to the `state_list`.\n    If no merge occurs, the `successor_state` is added to the `state_list` and linked from `leaf_state`.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::expand",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SAGC-011",
                "requirement_description": "Jobset Path Sorting for Processing:** The SAG constructor, when processing a folder of jobsets, shall sort the jobset file paths based on utilization, ET_ratio, number of jobs (runnable_number in code), and instance number extracted from filenames matching the pattern `jobset-{utilization}-{ET_ratio}-{num_job}-{instance_num}.txt`, if filenames follow this pattern. Otherwise, no specific sort order is guaranteed.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": "(sorts paths for assertion), behavior is implicit in `sag_constructor.py` if files are named accordingly."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py::SAG_constructor::construct",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ORIG-001",
                "requirement_description": "Standard Job Reading:** The Original constructor shall read jobs as defined in the input file without modification to their BCET/WCET based on ET status.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_read_jobs",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::read_jobs",
                        "description": "(as it's the base)"
                    }
                ]
            },
            {
                "requirement_id": "FR-ORIG-002",
                "requirement_description": "Standard SAG Expansion:** The Original constructor shall use the common expansion logic (FR-SAGC-003 to FR-SAGC-010) with state merging enabled by default.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::construct_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ORIG-003",
                "requirement_description": "Execution Scenarios Calculation (Original):\n    The \"actual execution scenarios\" (log10) shall be calculated as `log10(product over all jobs J of: N_J)`, where `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET - J.BCET + X)`, with `X=2` if `J.is_ET` is true, and `X=1` if `J.is_ET` is false.\n    The \"analyzed execution scenarios\" (log10) shall be calculated identically but with `X=1` for all jobs, regardless of ET status.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_execution_scenarios",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::count_execution_scenarios",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ORIG-004",
                "requirement_description": "Idle Time Calculation (Original):** The maximum idle time shall be calculated as the sum of `job.BCET` for all jobs where `job.is_ET` is true.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_idle_time",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::count_idle_time",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EXT-001",
                "requirement_description": "Modified Job Reading (Extended):** The Extended constructor shall read jobs and if a job `is_ET` true, its active `BCET` shall be set to 0. The original BCET must be stored as `BCET_REC`.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_read_jobs",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/extended_constructor.py::Extended_constructor::read_jobs",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EXT-002",
                "requirement_description": "Standard SAG Expansion (Extended):** The Extended constructor shall use the common expansion logic (FR-SAGC-003 to FR-SAGC-010) with state merging enabled by default. Job execution times (BCET/WCET) used in expansion are those active after potential modification by FR-EXT-001.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_construct_SAG",
                        "description": "(Extended constructor tests are missing `test_construct_SAG`, Hybrid inherits from Original, Extended inherits from Original. This test is for Hybrid, but implies base construction is used. Let's assume general construction logic applies, using the modified job properties.) A more direct test is needed here. However, based on code structure, it uses base `construct_SAG`."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::construct_SAG",
                        "description": "(inherited and used by Extended_constructor)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EXT-003",
                "requirement_description": "Execution Scenarios Calculation (Extended):\n    The \"actual execution scenarios\" (log10) shall be calculated as `log10(product over all jobs J of: N_J)`, where `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET - J.BCET_REC + X)`, with `X=2` if `J.is_ET` is true, and `X=1` if `J.is_ET` is false. (Uses `BCET_REC` for ET jobs).\n    The \"analyzed execution scenarios\" (log10) shall be calculated as `log10(product over all jobs J of: N_J)`, where `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET - J.BCET + 1)` if `J.is_ET` is false. If `J.is_ET` is true, `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET + 1)` (since active `J.BCET` is 0 for ET jobs).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_execution_scenarios",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/extended_constructor.py::Extended_constructor::count_execution_scenarios",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EXT-004",
                "requirement_description": "Idle Time Calculation (Extended):** The maximum idle time shall be 0.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_idle_time",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/extended_constructor.py::Extended_constructor::count_idle_time",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HYB-001",
                "requirement_description": "Standard Job Reading (Hybrid):** The Hybrid constructor shall read jobs as defined in the input file without initial modification to their BCET/WCET based on ET status.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_construct_SAG",
                        "description": "(setUp calls read_jobs from base class)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::read_jobs",
                        "description": "(inherited and used by Hybrid_constructor)"
                    }
                ]
            },
            {
                "requirement_id": "FR-HYB-002",
                "requirement_description": "Dual Expansion for ET Jobs (Hybrid):** During SAG expansion, if an `eligible_successor` job `is_ET` true, the Hybrid constructor shall perform two expansions from the `shortest_leaf`:\n    1.  First expansion: The ET job is temporarily set to a \"non-triggered\" state (BCET=0, WCET=0 using FR-JOB-004), and expansion (FR-SAGC-007 to FR-SAGC-010) is performed.\n    2.  Second expansion: The ET job is restored to its \"triggered\" state (original BCET_REC, WCET_REC using FR-JOB-005), and expansion is performed again.\n    For non-ET jobs, standard single expansion occurs. State merging is enabled by default.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_construct_SAG",
                        "description": "(verifies state list based on this logic)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/hybrid_constructor.py::Hybrid_constructor::construct_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HYB-003",
                "requirement_description": "Execution Scenarios Calculation (Hybrid):\n    Both \"actual execution scenarios\" (log10) and \"analyzed execution scenarios\" (log10) shall be calculated as `log10(product over all jobs J of: N_J)`, where `N_J = (J.WCAT - J.BCAT + 1) * (J.WCET - J.BCET + X)`, with `X=2` if `J.is_ET` is true, and `X=1` if `J.is_ET` is false. (BCET here refers to the original BCET of the job).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_execution_scenarios",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/hybrid_constructor.py::Hybrid_constructor::count_execution_scenarios",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HYB-004",
                "requirement_description": "Idle Time Calculation (Hybrid):** The maximum idle time shall be 0.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_idle_time",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/hybrid_constructor.py::Hybrid_constructor::count_idle_time",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-STAT-001",
                "requirement_description": "Statistics Collection:** After SAG construction, the system shall calculate the following statistics:\n    1.  Number of states in the SAG.\n    2.  Number of actual execution scenarios (log10, specific calculation depends on constructor type, see FR-ORIG-003, FR-EXT-003, FR-HYB-003).\n    3.  Number of analyzed execution scenarios (log10, specific calculation depends on constructor type).\n    4.  Valid ratio of analyzed SAG (log10): `analyzed_es_counter - actual_es_counter`.\n    5.  Maximum width of the SAG (maximum number of states at any single depth level).\n    6.  Maximum idle time (specific calculation depends on constructor type, see FR-ORIG-004, FR-EXT-004, FR-HYB-004).\n    7.  Construction time (e.g., in nanoseconds).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                        "description": "(checks num states)"
                    },
                    {
                        "id": "::test_count_execution_scenarios",
                        "description": ""
                    },
                    {
                        "id": "::test_count_idle_time",
                        "description": "Other statistics are implicitly part of `do_statistics` which is tested by checking CSV output."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::do_statistics",
                        "description": ""
                    },
                    {
                        "id": "src/sagkit/sag_constructor.py::SAG_constructor::construct",
                        "description": "(for construction time)"
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-001",
                "requirement_description": "SAG .dot File Output:** If requested, the system shall save the constructed SAG for each jobset into a .dot file. The output folder for .dot files shall be created if it doesn't exist.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::save_SAG",
                        "description": ""
                    },
                    {
                        "id": "src/sagkit/sag_constructor.py::SAG_constructor::construct",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-002",
                "requirement_description": ".dot File Naming:** The .dot file for a given jobset shall be named `{constructor_type}_{original_jobset_filename}` (e.g., `original_jobset-45-0-1000-1.txt`).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG",
                        "description": "(uses \"test_\" prefix and \"SAG.txt\" suffix in test, actual code uses `header` (constructor type) and `jobset_path.split(\"/\")[-1]`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::save_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-OUT-003",
                "requirement_description": "Statistics .csv File Output:** If a path is provided, the system shall save the calculated statistics for all processed jobsets and constructor types into a single .csv file. If the file exists, it should be overwritten or removed before writing new data.",
                "test_traceability": [
                    {
                        "id": "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor",
                        "description": "(checks default path), actual CSV writing is implicitly tested by example usages and expected if `save_statistics` path is given."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py::SAG_constructor::construct",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-JGCLI-001",
                "requirement_description": "`--ET_ratio` Argument:** Accepts an integer or a comma-separated list of integers specifying the percentage(s) of jobs that are Event-Triggered. Default: 15.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": "(uses multiple values)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-JGCLI-002",
                "requirement_description": "`--utilization` Argument:** Accepts an integer or a comma-separated list of integers specifying the target utilization percentage(s). Default: 45. (Note: `generate_jobs` implementation implies specific valid utilization steps, e.g., 45, 50,...75 for BCET calculation).",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": "(uses multiple values)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-JGCLI-003",
                "requirement_description": "`--jobset_folder` Argument:** Accepts a string path for the folder where generated jobsets will be saved. Default: \"./jobsets/\".",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-JGCLI-004",
                "requirement_description": "`--num_job` Argument:** Accepts an integer or a comma-separated list of integers for the number of jobs in each jobset. Default: 1000.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-JGCLI-005",
                "requirement_description": "`--num_instance` Argument:** Accepts an integer for how many jobsets to generate for each combination of parameters. Default: 1.",
                "test_traceability": [
                    {
                        "id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/jobset_generator.py",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-SCCLI-001",
                "requirement_description": "`--jobset_folder` Argument:** Accepts a string path to the folder containing jobset files to be processed. Default: \"./jobsets/\".",
                "test_traceability": [
                    {
                        "id": "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-SCCLI-002",
                "requirement_description": "`--constructor_type` Argument:** Accepts a string or a comma-separated list of strings specifying which SAG constructor(s) to use (e.g., \"original\", \"extended\", \"hybrid\"). Default: \"original,extended,hybrid\".",
                "test_traceability": [
                    {
                        "id": "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-SCCLI-003",
                "requirement_description": "`--save_dot` Argument:** Accepts a string path to a folder where .dot files representing the SAGs will be saved. If an empty string or not provided, interpreted based on default. Default folder: \"./dotfiles/\". Default behavior: save if path given.",
                "test_traceability": [
                    {
                        "id": "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py",
                        "description": "(argparse setup)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-SCCLI-004",
                "requirement_description": "`--save_statistics` Argument:** Accepts a string path for the .csv file where statistics will be saved. Default: \"\" (empty string, implies no statistics saving unless specified, or specific default like \"./statistics.csv\" as in README). The code default is `\"\"` for the argument but `SAG_constructor` class initializes it to `./statistics.csv`. Let's take class default. The test confirms `save_statistics` exists as an attribute. Behavior: Save if path is non-empty.",
                "test_traceability": [
                    {
                        "id": "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py",
                        "description": "(argparse setup, `SAG_constructor` class default)"
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-JFS-001",
                "requirement_description": "Jobset File Structure:** Each jobset file shall be a plain text file. Each line shall represent a single job.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::setUpClass",
                        "description": "(creates such a file)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::read_jobs",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-JFS-002",
                "requirement_description": "Job Record Format:** Each line (job record) in a jobset file shall consist of seven space-separated integer values in the following order: BCAT, WCAT, BCET, WCET, DDL, priority, ET (0 or 1).",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::setUpClass",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::read_jobs",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-DOT-001",
                "requirement_description": "DOT File Header:** Each .dot file shall begin with:\n    ```dot\n    digraph finite_state_machine {\n    rankdir = LR;\n    size = \"8,5\";\n    node [shape = doublecircle, fontsize = 20, fixedsize = true, width = 1.1, height = 1.1];\n    \"S1\\\\n[0, 0]\";\n    node [shape = circle, fontsize = 20, fixedsize = true, width = 1.1, height = 1.1];\n    ```\n    (Note: S1 refers to state with ID 0. The first node \"S1\\\\n[0,0]\" is specified as `doublecircle`, then the default node shape is set to `circle`.)",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::save_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-DOT-002",
                "requirement_description": "DOT File Node Representation:** States in the SAG shall be represented as nodes. The root node (ID 0) shall be ` \"S1\\\\n[0, 0]\" ` (using 1-based indexing for \"S\" prefix, but 0,0 for EFT/LFT). Other nodes shall be labeled ` \"S{state.id + 1}\\\\n[{state.EFT}, {state.LFT}]\" `.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::save_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-DOT-003",
                "requirement_description": "DOT File Edge Representation:** Transitions between states shall be represented as directed edges:\n    `\"S{from_state.id + 1}\\\\n[{from_state.EFT}, {from_state.LFT}]\" -> \"S{to_state.id + 1}\\\\n[{to_state.EFT}, {to_state.LFT}]\" [label=\"J{job.id + 1}\", fontsize=20];`",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::save_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-DOT-004",
                "requirement_description": "DOT File Footer:** Each .dot file shall end with a closing brace `}`.",
                "test_traceability": [
                    {
                        "id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/constructors/original_constructor.py::Constructor::save_SAG",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-CSV-001",
                "requirement_description": "CSV File Structure:** The statistics output shall be a CSV (Comma Separated Values) file.",
                "test_traceability": [
                    {
                        "id": "",
                        "description": "Inferred from `csv.writer` usage and file extension."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py::SAG_constructor::construct",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-CSV-002",
                "requirement_description": "CSV Header:** For each constructor type processed, the CSV file shall contain a line with the constructor type name (e.g., \"original\"), followed by a header line:\n    `Utilization,ET_Ratio,Number of States,Number of actual execution scenarios (log10),Number of analyzed execution scenarios (log10),Valid ratio of analyzed SAG (log10),Maximum width,Maximum idle time,Construction time (ns)`",
                "test_traceability": [
                    {
                        "id": "",
                        "description": "Inferred from code."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py::SAG_constructor::construct",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EIF-CSV-003",
                "requirement_description": "CSV Data Rows:** Each data row following the header shall correspond to one processed jobset and contain values for the statistics defined in FR-STAT-001, in the order specified by the header FR-EIF-CSV-002. Specifically:\n    `utilization, ET_ratio, num_states, actual_es_log10, analyzed_es_log10, (analyzed_es_log10 - actual_es_log10), max_width, idle_time, construction_time_ns`\n    Jobset parameters (utilization, ET_ratio) are derived from the jobset filename if it matches the standard pattern.",
                "test_traceability": [
                    {
                        "id": "",
                        "description": "Inferred from code."
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sagkit/sag_constructor.py::SAG_constructor::construct",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "NFR-001",
                "requirement_description": "Test Case Validation:** No specific non-functional requirements (e.g., performance benchmarks, security levels, specific usability metrics) have been identified that are directly and explicitly validated by dedicated original test cases beyond functional correctness and adherence to specified output formats. The system's primary NFR is to correctly implement the functionalities to pass all (public and private) tests.",
                "test_traceability": [],
                "code_traceability": []
            }
        ],
        "commit_sha": "c5da42eafa18b19c5728919acee4ac98408d6f81",
        "full_code_skeleton": "--- File: src/sagkit/__init__.py ---\n```python\n# This file is empty or only contains elements to be excluded.\n```\n--- File: src/sagkit/jobset_generator.py ---\n```python\nclass Jobset_generator:\n    def __init__(self, num_ins, ET_ratio, utilization, num_job):\n        pass\n\n    # Generate jobs\n    def generate_jobs(self, num_job, utilization, ET_ratio):\n        pass\n\n    # Generate jobsets\n    def generate(self, jobset_folder):\n        pass\n\n\ndef int_or_int_list(value):\n    pass\n```\n--- File: src/sagkit/sag_constructor.py ---\n```python\nclass SAG_constructor:\n    def __init__(\n        self,\n        jobset_folder=\"./jobsets/\",\n        constructor_type=[\"original\", \"extended\", \"hybrid\"],\n        save_dot=\"./dotfiles/\",\n        save_statistics=\"./statistics.csv\",\n    ):\n        pass\n\n    # Construct SAGs with specified construction algorithms\n    def construct(self):\n        pass\n\n\ndef str_list(value):\n    pass\n```\n--- File: src/sagkit/constructors/hybrid_constructor.py ---\n```python\nclass Hybrid_constructor(Constructor):\n\n    # Override read_jobs method of the Constructor class\n    def construct_SAG(self):\n        pass\n\n    # Override count_execution_scenarios method of the Constructor class\n    def count_execution_scenarios(self):\n        pass\n\n    # Override count_idle_time method of the Constructor class\n    def count_idle_time(self):\n        pass\n```\n--- File: src/sagkit/constructors/extended_constructor.py ---\n```python\nclass Extended_constructor(Constructor):\n\n    # Read jobs from file\n    def read_jobs(self, file_path: str) -> None:\n        pass\n\n    def count_execution_scenarios(self):\n        pass\n\n    def count_idle_time(self):\n        pass\n```\n--- File: src/sagkit/constructors/__init__.py ---\n```python\n# This file is empty or only contains elements to be excluded.\n```\n--- File: src/sagkit/constructors/original_constructor.py ---\n```python\nclass Constructor:\n    def __init__(self, header, to_merge=True) -> None:\n        pass\n\n    # Read jobs from file\n    def read_jobs(self, file_path: str) -> None:\n        pass\n\n    # Find the shortest leaf\n    def find_shortest_leaf(self) -> State:\n        pass\n\n    # Match two states\n    @staticmethod\n    def match(a: State, b: State) -> bool:\n        pass\n\n    # Expansion phase with or without merging\n    def expand(self, leaf: State, job: Job, to_merge: bool) -> None:\n        pass\n\n    # Construct SAG\n    def construct_SAG(self) -> None:\n        pass\n\n    # Count the number of execution scenarios\n    def count_execution_scenarios(self):\n        pass\n\n    # Count the maximum idle time\n    def count_idle_time(self):\n        pass\n\n    # Do some statistics\n    def do_statistics(self):\n        pass\n\n    # Output the SAG in .dot format\n    # https://dreampuf.github.io/GraphvizOnline to visualize the SAG\n    # If that doesn't work, try viewing the site in incognito mode\n    def save_SAG(self, save_folder: str, jobset_path: str) -> None:\n        pass\n```\n--- File: src/sagkit/schedulers/__init__.py ---\n```python\n# This file is empty or only contains elements to be excluded.\n```\n--- File: src/sagkit/schedulers/edf_scheduler.py ---\n```python\nclass EDF_Scheduler:\n\n    # Earliest Deadline First (EDF) algorithm\n    # Return True if job1 is prior to job2\n    @staticmethod\n    def compare(job1, job2) -> bool:\n        pass\n```\n--- File: src/sagkit/schedulers/fp_scheduler.py ---\n```python\nclass FP_Scheduler:\n\n    # Fixed-Priority (FP) algorithm\n    # Return True if job1 is prior to job2\n    @staticmethod\n    def compare(job1, job2) -> bool:\n        pass\n```\n--- File: src/sagkit/utils/state.py ---\n```python\nclass State:\n    def __init__(self, id: int, EFT: int, LFT: int, job_path: List) -> None:\n        pass\n\n    # Determine if the state is a leaf node\n    def is_leaf(self) -> bool:\n        pass\n\n    def __str__(self) -> str:\n        pass\n```\n--- File: src/sagkit/utils/__init__.py ---\n```python\n# This file is empty or only contains elements to be excluded.\n```\n--- File: src/sagkit/utils/job.py ---\n```python\nclass Job:\n    def __init__(\n        self,\n        id: int,\n        BCAT: int,\n        WCAT: int,\n        BCET: int,\n        WCET: int,\n        DDL: int,\n        priority: int,\n        is_ET: int,\n    ) -> None:\n        pass\n\n    def set_to_non_triggered(self) -> None:\n        pass\n\n    def set_to_triggered(self) -> None:\n        pass\n\n    # Determine if the job is priority-eligible at a given time\n    def is_priority_eligible(self, future_jobs: List, time: int) -> bool:\n        pass\n\n    # Determine if the job is potentially the next job to be scheduled\n    def is_potentially_next(self, future_jobs: List, time: int, state_LFT: int) -> bool:\n        pass\n\n    def __str__(self) -> str:\n        pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "src/sagkit/__init__.py",
                "code": "# This file is empty or only contains elements to be excluded.\n"
            },
            {
                "file_path": "src/sagkit/jobset_generator.py",
                "code": "class Jobset_generator:\n    def __init__(self, num_ins, ET_ratio, utilization, num_job):\n        pass\n\n    # Generate jobs\n    def generate_jobs(self, num_job, utilization, ET_ratio):\n        pass\n\n    # Generate jobsets\n    def generate(self, jobset_folder):\n        pass\n\n\ndef int_or_int_list(value):\n    pass\n"
            },
            {
                "file_path": "src/sagkit/sag_constructor.py",
                "code": "class SAG_constructor:\n    def __init__(\n        self,\n        jobset_folder=\"./jobsets/\",\n        constructor_type=[\"original\", \"extended\", \"hybrid\"],\n        save_dot=\"./dotfiles/\",\n        save_statistics=\"./statistics.csv\",\n    ):\n        pass\n\n    # Construct SAGs with specified construction algorithms\n    def construct(self):\n        pass\n\n\ndef str_list(value):\n    pass\n"
            },
            {
                "file_path": "src/sagkit/constructors/hybrid_constructor.py",
                "code": "class Hybrid_constructor(Constructor):\n\n    # Override read_jobs method of the Constructor class\n    def construct_SAG(self):\n        pass\n\n    # Override count_execution_scenarios method of the Constructor class\n    def count_execution_scenarios(self):\n        pass\n\n    # Override count_idle_time method of the Constructor class\n    def count_idle_time(self):\n        pass\n"
            },
            {
                "file_path": "src/sagkit/constructors/extended_constructor.py",
                "code": "class Extended_constructor(Constructor):\n\n    # Read jobs from file\n    def read_jobs(self, file_path: str) -> None:\n        pass\n\n    def count_execution_scenarios(self):\n        pass\n\n    def count_idle_time(self):\n        pass\n"
            },
            {
                "file_path": "src/sagkit/constructors/__init__.py",
                "code": "# This file is empty or only contains elements to be excluded.\n"
            },
            {
                "file_path": "src/sagkit/constructors/original_constructor.py",
                "code": "class Constructor:\n    def __init__(self, header, to_merge=True) -> None:\n        pass\n\n    # Read jobs from file\n    def read_jobs(self, file_path: str) -> None:\n        pass\n\n    # Find the shortest leaf\n    def find_shortest_leaf(self) -> State:\n        pass\n\n    # Match two states\n    @staticmethod\n    def match(a: State, b: State) -> bool:\n        pass\n\n    # Expansion phase with or without merging\n    def expand(self, leaf: State, job: Job, to_merge: bool) -> None:\n        pass\n\n    # Construct SAG\n    def construct_SAG(self) -> None:\n        pass\n\n    # Count the number of execution scenarios\n    def count_execution_scenarios(self):\n        pass\n\n    # Count the maximum idle time\n    def count_idle_time(self):\n        pass\n\n    # Do some statistics\n    def do_statistics(self):\n        pass\n\n    # Output the SAG in .dot format\n    # https://dreampuf.github.io/GraphvizOnline to visualize the SAG\n    # If that doesn't work, try viewing the site in incognito mode\n    def save_SAG(self, save_folder: str, jobset_path: str) -> None:\n        pass\n"
            },
            {
                "file_path": "src/sagkit/schedulers/__init__.py",
                "code": "# This file is empty or only contains elements to be excluded.\n"
            },
            {
                "file_path": "src/sagkit/schedulers/edf_scheduler.py",
                "code": "class EDF_Scheduler:\n\n    # Earliest Deadline First (EDF) algorithm\n    # Return True if job1 is prior to job2\n    @staticmethod\n    def compare(job1, job2) -> bool:\n        pass\n"
            },
            {
                "file_path": "src/sagkit/schedulers/fp_scheduler.py",
                "code": "class FP_Scheduler:\n\n    # Fixed-Priority (FP) algorithm\n    # Return True if job1 is prior to job2\n    @staticmethod\n    def compare(job1, job2) -> bool:\n        pass\n"
            },
            {
                "file_path": "src/sagkit/utils/state.py",
                "code": "class State:\n    def __init__(self, id: int, EFT: int, LFT: int, job_path: List) -> None:\n        pass\n\n    # Determine if the state is a leaf node\n    def is_leaf(self) -> bool:\n        pass\n\n    def __str__(self) -> str:\n        pass\n"
            },
            {
                "file_path": "src/sagkit/utils/__init__.py",
                "code": "# This file is empty or only contains elements to be excluded.\n"
            },
            {
                "file_path": "src/sagkit/utils/job.py",
                "code": "class Job:\n    def __init__(\n        self,\n        id: int,\n        BCAT: int,\n        WCAT: int,\n        BCET: int,\n        WCET: int,\n        DDL: int,\n        priority: int,\n        is_ET: int,\n    ) -> None:\n        pass\n\n    def set_to_non_triggered(self) -> None:\n        pass\n\n    def set_to_triggered(self) -> None:\n        pass\n\n    # Determine if the job is priority-eligible at a given time\n    def is_priority_eligible(self, future_jobs: List, time: int) -> bool:\n        pass\n\n    # Determine if the job is potentially the next job to be scheduled\n    def is_potentially_next(self, future_jobs: List, time: int, state_LFT: int) -> bool:\n        pass\n\n    def __str__(self) -> str:\n        pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: src/sagkit/jobset_generator.py ---\n```python\nclass Jobset_generator:\n    def __init__(self, num_ins, ET_ratio, utilization, num_job):\n        pass\n\n    def generate(self, jobset_folder):\n        pass\n```\n--- File: src/sagkit/sag_constructor.py ---\n```python\nclass SAG_constructor:\n    def __init__(\n        self,\n        jobset_folder=\"./jobsets/\",\n        constructor_type=[\"original\", \"extended\", \"hybrid\"],\n        save_dot=\"./dotfiles/\",\n        save_statistics=\"./statistics.csv\",\n    ):\n        pass\n```\n--- File: src/sagkit/constructors/original_constructor.py ---\n```python\nclass Constructor:\n    def __init__(self, header, to_merge=True) -> None:\n        pass\n\n    def read_jobs(self, file_path: str) -> None:\n        pass\n\n    def construct_SAG(self) -> None:\n        pass\n\n    def count_execution_scenarios(self):\n        pass\n\n    def count_idle_time(self):\n        pass\n\n    def save_SAG(self, save_folder: str, jobset_path: str) -> None:\n        pass\n```\n--- File: src/sagkit/constructors/extended_constructor.py ---\n```python\nclass Extended_constructor(Constructor):\n    def read_jobs(self, file_path: str) -> None:\n        pass\n\n    def count_execution_scenarios(self):\n        pass\n\n    def count_idle_time(self):\n        pass\n```\n--- File: src/sagkit/constructors/hybrid_constructor.py ---\n```python\nclass Hybrid_constructor(Constructor):\n    def construct_SAG(self):\n        pass\n\n    def count_execution_scenarios(self):\n        pass\n\n    def count_idle_time(self):\n        pass\n```\n--- File: src/sagkit/schedulers/edf_scheduler.py ---\n```python\nclass EDF_Scheduler:\n    @staticmethod\n    def compare(job1, job2) -> bool:\n        pass\n```\n--- File: src/sagkit/schedulers/fp_scheduler.py ---\n```python\nclass FP_Scheduler:\n    @staticmethod\n    def compare(job1, job2) -> bool:\n        pass\n```\n--- File: src/sagkit/utils/job.py ---\n```python\nclass Job:\n    def __init__(\n        self,\n        id: int,\n        BCAT: int,\n        WCAT: int,\n        BCET: int,\n        WCET: int,\n        DDL: int,\n        priority: int,\n        is_ET: int,\n    ) -> None:\n        pass\n\n    def set_to_non_triggered(self) -> None:\n        pass\n\n    def set_to_triggered(self) -> None:\n        pass\n\n    def is_priority_eligible(self, future_jobs: List, time: int) -> bool:\n        pass\n\n    def is_potentially_next(self, future_jobs: List, time: int, state_LFT: int) -> bool:\n        pass\n\n    def __str__(self) -> str:\n        pass\n```\n--- File: src/sagkit/utils/state.py ---\n```python\nclass State:\n    def __init__(self, id: int, EFT: int, LFT: int, job_path: List) -> None:\n        pass\n\n    def is_leaf(self) -> bool:\n        pass\n\n    def __str__(self) -> str:\n        pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "src/sagkit/jobset_generator.py",
                "code": "class Jobset_generator:\n    def __init__(self, num_ins, ET_ratio, utilization, num_job):\n        pass\n\n    def generate(self, jobset_folder):\n        pass\n"
            },
            {
                "file_path": "src/sagkit/sag_constructor.py",
                "code": "class SAG_constructor:\n    def __init__(\n        self,\n        jobset_folder=\"./jobsets/\",\n        constructor_type=[\"original\", \"extended\", \"hybrid\"],\n        save_dot=\"./dotfiles/\",\n        save_statistics=\"./statistics.csv\",\n    ):\n        pass\n"
            },
            {
                "file_path": "src/sagkit/constructors/original_constructor.py",
                "code": "class Constructor:\n    def __init__(self, header, to_merge=True) -> None:\n        pass\n\n    def read_jobs(self, file_path: str) -> None:\n        pass\n\n    def construct_SAG(self) -> None:\n        pass\n\n    def count_execution_scenarios(self):\n        pass\n\n    def count_idle_time(self):\n        pass\n\n    def save_SAG(self, save_folder: str, jobset_path: str) -> None:\n        pass\n"
            },
            {
                "file_path": "src/sagkit/constructors/extended_constructor.py",
                "code": "class Extended_constructor(Constructor):\n    def read_jobs(self, file_path: str) -> None:\n        pass\n\n    def count_execution_scenarios(self):\n        pass\n\n    def count_idle_time(self):\n        pass\n"
            },
            {
                "file_path": "src/sagkit/constructors/hybrid_constructor.py",
                "code": "class Hybrid_constructor(Constructor):\n    def construct_SAG(self):\n        pass\n\n    def count_execution_scenarios(self):\n        pass\n\n    def count_idle_time(self):\n        pass\n"
            },
            {
                "file_path": "src/sagkit/schedulers/edf_scheduler.py",
                "code": "class EDF_Scheduler:\n    @staticmethod\n    def compare(job1, job2) -> bool:\n        pass\n"
            },
            {
                "file_path": "src/sagkit/schedulers/fp_scheduler.py",
                "code": "class FP_Scheduler:\n    @staticmethod\n    def compare(job1, job2) -> bool:\n        pass\n"
            },
            {
                "file_path": "src/sagkit/utils/job.py",
                "code": "class Job:\n    def __init__(\n        self,\n        id: int,\n        BCAT: int,\n        WCAT: int,\n        BCET: int,\n        WCET: int,\n        DDL: int,\n        priority: int,\n        is_ET: int,\n    ) -> None:\n        pass\n\n    def set_to_non_triggered(self) -> None:\n        pass\n\n    def set_to_triggered(self) -> None:\n        pass\n\n    def is_priority_eligible(self, future_jobs: List, time: int) -> bool:\n        pass\n\n    def is_potentially_next(self, future_jobs: List, time: int, state_LFT: int) -> bool:\n        pass\n\n    def __str__(self) -> str:\n        pass\n"
            },
            {
                "file_path": "src/sagkit/utils/state.py",
                "code": "class State:\n    def __init__(self, id: int, EFT: int, LFT: int, job_path: List) -> None:\n        pass\n\n    def is_leaf(self) -> bool:\n        pass\n\n    def __str__(self) -> str:\n        pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_execution_scenarios",
                "covers": [
                    "sagkit.constructors.extended_constructor.Extended_constructor.count_execution_scenarios - happy path, overridden calculation"
                ]
            },
            {
                "test_id": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_count_idle_time",
                "covers": [
                    "sagkit.constructors.extended_constructor.Extended_constructor.count_idle_time - happy path, overridden calculation"
                ]
            },
            {
                "test_id": "tests/constructors/test_extended_constructor.py::TestExtendedConstructor::test_read_jobs",
                "covers": [
                    "sagkit.constructors.extended_constructor.Extended_constructor.__init__ - basic instantiation for extended constructor (via parent sagkit.constructors.original_constructor.Constructor)",
                    "sagkit.constructors.extended_constructor.Extended_constructor.read_jobs - happy path, overridden job reading logic"
                ]
            },
            {
                "test_id": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_construct_SAG",
                "covers": [
                    "sagkit.constructors.hybrid_constructor.Hybrid_constructor.__init__ - basic instantiation for hybrid constructor (via parent sagkit.constructors.original_constructor.Constructor)",
                    "sagkit.constructors.hybrid_constructor.Hybrid_constructor.construct_SAG - happy path, overridden SAG construction logic (also covers Job.set_to_non_triggered and Job.set_to_triggered usage)"
                ]
            },
            {
                "test_id": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_execution_scenarios",
                "covers": [
                    "sagkit.constructors.hybrid_constructor.Hybrid_constructor.count_execution_scenarios - happy path, overridden calculation"
                ]
            },
            {
                "test_id": "tests/constructors/test_hybrid_constructor.py::TestHybridConstructor::test_count_idle_time",
                "covers": [
                    "sagkit.constructors.hybrid_constructor.Hybrid_constructor.count_idle_time - happy path, overridden calculation"
                ]
            },
            {
                "test_id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_construct_SAG",
                "covers": [
                    "sagkit.constructors.original_constructor.Constructor.construct_SAG - happy path SAG construction"
                ]
            },
            {
                "test_id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_execution_scenarios",
                "covers": [
                    "sagkit.constructors.original_constructor.Constructor.count_execution_scenarios - happy path calculation"
                ]
            },
            {
                "test_id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_count_idle_time",
                "covers": [
                    "sagkit.constructors.original_constructor.Constructor.count_idle_time - happy path calculation"
                ]
            },
            {
                "test_id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_read_jobs",
                "covers": [
                    "sagkit.constructors.original_constructor.Constructor.__init__ - basic instantiation for original constructor",
                    "sagkit.constructors.original_constructor.Constructor.read_jobs - happy path reading jobs from file"
                ]
            },
            {
                "test_id": "tests/constructors/test_original_constructor.py::TestOriginalConstructor::test_save_SAG",
                "covers": [
                    "sagkit.constructors.original_constructor.Constructor.save_SAG - happy path, file saving"
                ]
            },
            {
                "test_id": "tests/schedulers/test_edf_scheduler.py::TestEDFScheduler::test_compare",
                "covers": [
                    "sagkit.schedulers.edf_scheduler.EDF_Scheduler.compare - static method, comparing jobs by EDF policy"
                ]
            },
            {
                "test_id": "tests/schedulers/test_fp_scheduler.py::TestFPScheduler::test_compare",
                "covers": [
                    "sagkit.schedulers.fp_scheduler.FP_Scheduler.compare - static method, comparing jobs by FP policy"
                ]
            },
            {
                "test_id": "tests/test_jobset_generator.py::TestJobsetGenerator::test_jobset_generator",
                "covers": [
                    "sagkit.jobset_generator.Jobset_generator.__init__ - happy path instantiation",
                    "sagkit.jobset_generator.Jobset_generator.generate - happy path, file generation"
                ]
            },
            {
                "test_id": "tests/test_sag_constructor.py::TestSAGConstructor::test_constructor",
                "covers": [
                    "sagkit.sag_constructor.SAG_constructor.__init__ - happy path instantiation, default attributes"
                ]
            },
            {
                "test_id": "tests/utils/test_job.py::TestJob::test___str__",
                "covers": [
                    "sagkit.utils.job.Job.__init__ - basic instantiation for Job object",
                    "sagkit.utils.job.Job.__str__ - testing string representation"
                ]
            },
            {
                "test_id": "tests/utils/test_job.py::TestJob::test_is_potentially_next",
                "covers": [
                    "sagkit.utils.job.Job.is_potentially_next - core behavior test"
                ]
            },
            {
                "test_id": "tests/utils/test_job.py::TestJob::test_is_priority_eligible",
                "covers": [
                    "sagkit.utils.job.Job.is_priority_eligible - core behavior test"
                ]
            },
            {
                "test_id": "tests/utils/test_job.py::TestJob::test_set_to_non_triggered",
                "covers": [
                    "sagkit.utils.job.Job.set_to_non_triggered - core behavior test"
                ]
            },
            {
                "test_id": "tests/utils/test_job.py::TestJob::test_set_to_triggered",
                "covers": [
                    "sagkit.utils.job.Job.set_to_triggered - core behavior test"
                ]
            },
            {
                "test_id": "tests/utils/test_state.py::TestState::test_init",
                "covers": [
                    "sagkit.utils.state.State.__init__ - happy path instantiation and attribute check"
                ]
            },
            {
                "test_id": "tests/utils/test_state.py::TestState::test_is_leaf",
                "covers": [
                    "sagkit.utils.state.State.is_leaf - happy path for a new (leaf) state"
                ]
            },
            {
                "test_id": "tests/utils/test_state.py::TestState::test_str",
                "covers": [
                    "sagkit.utils.state.State.__str__ - testing string representation"
                ]
            }
        ]
    },
    {
        "idx": 66428,
        "repo_name": "Zozi96_hash-forge",
        "url": "https://github.com/Zozi96/hash-forge",
        "description": "Hash Forge is a lightweight Python library designed to simplify the process of hashing and verifying data using a variety of secure hashing algorithms.",
        "stars": 10,
        "forks": 0,
        "language": "python",
        "size": 130,
        "created_at": "2024-10-16T23:41:06+00:00",
        "updated_at": "2025-02-27T19:39:53+00:00",
        "pypi_info": {
            "name": "hash-forge",
            "version": "2.0",
            "url": "https://files.pythonhosted.org/packages/26/7f/1edb1b47b04c5f176d1248cd88e0091470562cb603b496a47934b66c7049/hash_forge-2.0.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 25,
            "comment_ratio": 0.5692438402718777,
            "pyfile_content_length": 88632,
            "pyfile_code_lines": 2354,
            "test_file_exist": true,
            "test_file_content_length": 53833,
            "pytest_framework": true,
            "test_case_num": 66,
            "metadata_path": [
                "pyproject.toml"
            ],
            "readme_content_length": 4339,
            "llm_reason": "The 'Hash Forge' project is a very strong candidate for an AI 'Build from Scratch' benchmark.\n\nPositive Aspects:\n*   **Self-Contained & Independent:** The project's core functionality (cryptographic hashing) is entirely local. It relies on standard Python libraries (like `hashlib`) and pip-installable third-party packages (`bcrypt`, `argon2-cffi`, `scrypt`, `pycryptodome`, `blake3`) that perform local computations and do not require internet access or external services for their core operation. This aligns perfectly with the self-containment requirement for the AI-rebuilt solution.\n*   **Clear & Well-Defined Functionality:** The project provides a clear service: a unified interface (`HashManager`) for hashing, verifying, and checking rehash needs for various cryptographic hashing algorithms. The supported algorithms and the API are well-defined by the README and code structure.\n*   **Testable & Verifiable Output:** The project includes an extensive suite of unit tests for each hasher and the `HashManager`. These tests cover hash formats, verification logic, and rehash conditions, providing an excellent basis for verifying the AI's reconstructed project.\n*   **No GUI:** It is a library, intended for programmatic use, fitting the no-GUI requirement.\n*   **Appropriate Complexity (Medium):** Rebuilding this library involves implementing an interface (`PHasher`), several concrete hasher classes (for bcrypt, Scrypt, Argon2, Blake2, PBKDF2, Whirlpool, RIPEMD-160, Blake3), and a manager class (`HashManager`). This requires understanding of OOP, string manipulation for custom hash formats, interaction with multiple underlying cryptographic libraries, and implementing logic for parameter-dependent rehashing. This is non-trivial but not excessively complex for an AI to replicate.\n*   **Well-Understood Problem Domain:** Password hashing and data integrity are common software development concepts.\n*   **Predominantly Code-Based Solution:** The task is entirely about Python code generation.\n*   **Modular Design:** The separation of concerns between the `HashManager` and individual `Hasher` implementations (following a protocol) is a good architectural pattern for the AI to replicate.\n\nNegative Aspects / Considerations for Specification:\n*   **Dependency Management & Usage:** The AI must correctly use the appropriate underlying Python libraries for each of the ~8 hashing algorithms and manage their specific parameters (e.g., iteration counts, salt lengths, cost factors). The specification for the AI must be clear about which libraries to use and what default parameters are expected for each hasher to match the original project's behavior and tests.\n*   **Custom Hash String Formats:** Each hasher produces a specific string format (e.g., `algorithm$params$salt$hash`). The AI must replicate these formats precisely for parsing and verification to work correctly.\n*   **Rehash Logic Nuances:** The `needs_rehash` logic varies per algorithm and depends on its parameters. Replicating this accurately for all hashers is a key part of the challenge.\n*   **Optional Dependency Handling:** The original project appears to handle optional dependencies gracefully (e.g., via the `load_library` method in `PHasher`). Replicating this dynamic loading and error handling would add to the realism and complexity but is a fair part of rebuilding *this specific* project.\n\nOverall, the project's clear scope, self-contained nature, excellent testability, and moderate complexity make it highly suitable. The challenges it presents are appropriate for evaluating an AI's capability to build a realistic, small-to-medium-sized utility library from scratch.",
            "llm_project_type": "Hashing utility library",
            "llm_rating": 85,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "Zozi96_hash-forge",
            "finish_test": true,
            "test_case_result": {
                "src/tests/test_argon2.py::test_argon2_verify_correct_data": "passed",
                "src/tests/test_argon2.py::test_argon2_verify_incorrect_data": "passed",
                "src/tests/test_argon2.py::test_argon2_needs_rehash_false": "passed",
                "src/tests/test_argon2.py::test_argon2_needs_rehash_true": "passed",
                "src/tests/test_argon2.py::test_argon2_invalid_hash_format": "passed",
                "src/tests/test_bcrypt.py::test_bcrypt_hash_format": "passed",
                "src/tests/test_bcrypt.py::test_bcrypt_verify_correct_data": "passed",
                "src/tests/test_bcrypt.py::test_bcrypt_verify_incorrect_data": "passed",
                "src/tests/test_bcrypt.py::test_bcrypt_needs_rehash_false": "passed",
                "src/tests/test_bcrypt.py::test_bcrypt_invalid_hash_format": "passed",
                "src/tests/test_bcrypt.py::test_bcrypt_unknown_algorithm": "passed",
                "src/tests/test_bcrypt_sha256.py::test_bcrypt_hash_format": "passed",
                "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_correct_data": "passed",
                "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_incorrect_data": "passed",
                "src/tests/test_bcrypt_sha256.py::test_bcrypt_needs_rehash_false": "passed",
                "src/tests/test_bcrypt_sha256.py::test_bcrypt_invalid_hash_format": "passed",
                "src/tests/test_bcrypt_sha256.py::test_bcrypt_unknown_algorithm": "passed",
                "src/tests/test_blake2.py::test_hash_creation": "passed",
                "src/tests/test_blake2.py::test_verify_hash_correct": "passed",
                "src/tests/test_blake2.py::test_verify_hash_incorrect": "passed",
                "src/tests/test_blake2.py::test_needs_rehash_false": "passed",
                "src/tests/test_blake2.py::test_needs_rehash_true": "passed",
                "src/tests/test_blake2.py::test_hash_with_key": "passed",
                "src/tests/test_blake2.py::test_verify_fails_on_modified_hash": "passed",
                "src/tests/test_blake3.py::test_hash_creation": "passed",
                "src/tests/test_blake3.py::test_verify_hash_correct": "passed",
                "src/tests/test_blake3.py::test_verify_hash_incorrect": "passed",
                "src/tests/test_blake3.py::test_hash_with_key": "passed",
                "src/tests/test_blake3.py::test_verify_fails_on_modified_hash": "passed",
                "src/tests/test_hash_manager.py::test_hash_with_preferred_hasher": "passed",
                "src/tests/test_hash_manager.py::test_verify_with_preferred_hasher": "passed",
                "src/tests/test_hash_manager.py::test_verify_incorrect_data": "passed",
                "src/tests/test_hash_manager.py::test_needs_rehash_false": "passed",
                "src/tests/test_hash_manager.py::test_needs_rehash_true_due_to_iterations": "passed",
                "src/tests/test_hash_manager.py::test_invalid_hashed_data_format": "passed",
                "src/tests/test_hash_manager.py::test_unknown_algorithm": "passed",
                "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_hash_format": "passed",
                "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_correct_data": "passed",
                "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_incorrect_data": "passed",
                "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_false": "passed",
                "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_true": "passed",
                "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_invalid_hash_format": "passed",
                "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_hash_format": "passed",
                "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_correct_data": "passed",
                "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_incorrect_data": "passed",
                "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_false": "passed",
                "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_true": "passed",
                "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_invalid_hash_format": "passed",
                "src/tests/test_ripemd160.py::test_ripemd160_hash_format": "passed",
                "src/tests/test_ripemd160.py::test_ripemd160_verify_correct_data": "passed",
                "src/tests/test_ripemd160.py::test_ripemd160_verify_incorrect_data": "passed",
                "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_false": "passed",
                "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_true": "passed",
                "src/tests/test_ripemd160.py::test_ripemd160_invalid_hash_format": "passed",
                "src/tests/test_scrypt.py::test_scrypt_hash_format": "passed",
                "src/tests/test_scrypt.py::test_scrypt_verify_correct_data": "passed",
                "src/tests/test_scrypt.py::test_scrypt_verify_incorrect_data": "passed",
                "src/tests/test_scrypt.py::test_scrypt_needs_rehash_false": "passed",
                "src/tests/test_scrypt.py::test_scrypt_needs_rehash_true": "passed",
                "src/tests/test_scrypt.py::test_scrypt_invalid_hash_format": "passed",
                "src/tests/test_whirlpool.py::test_whirlpool_hash_format": "passed",
                "src/tests/test_whirlpool.py::test_whirlpool_verify_correct_data": "passed",
                "src/tests/test_whirlpool.py::test_whirlpool_verify_incorrect_data": "passed",
                "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_false": "passed",
                "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_true": "passed",
                "src/tests/test_whirlpool.py::test_whirlpool_invalid_hash_format": "passed"
            },
            "success_count": 66,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "unknown_count": 0,
            "total_count": 66,
            "success_rate": 1.0,
            "coverage_report": {
                "covered_lines": 636,
                "num_statements": 663,
                "percent_covered": 95.13590844062946,
                "percent_covered_display": "95",
                "missing_lines": 27,
                "excluded_lines": 0,
                "num_branches": 36,
                "num_partial_branches": 7,
                "covered_branches": 29,
                "missing_branches": 7
            },
            "coverage_result": {}
        },
        "codelines_count": 2354,
        "codefiles_count": 25,
        "code_length": 88632,
        "test_files_count": 11,
        "test_code_length": 53833,
        "class_diagram": "@startuml\nclass PHasher {\n    algorithm: ClassVar[str]\n    library_module: ClassVar[Unknown]\n    hash(): str\n    verify(): bool\n    needs_rehash(): bool\n    load_library(name): ModuleType\n}\nclass PBKDF2Sha256Hasher {\n    algorithm: ClassVar[str]\n    digest: ClassVar[Callable[Unknown, Any]]\n    __slots__: Unknown\n    __init__(iterations, salt_length): Unknown\n    hash(): str\n    verify(): bool\n    needs_rehash(): bool\n}\nclass PBKDF2Sha1Hasher {\n    algorithm: Unknown\n    digest: Unknown\n}\nclass Argon2Hasher {\n    algorithm: ClassVar[str]\n    library_module: ClassVar[str]\n    __slots__: Unknown\n    __init__(time_cost, salt_len, memory_cost, parallelism, hash_len): Unknown\n    hash(): str\n    verify(): bool\n    needs_rehash(): bool\n    _get_hasher(): Any\n}\nclass Ripemd160Hasher {\n    algorithm: ClassVar[str]\n    library_module: ClassVar[str]\n    __init__(): Unknown\n    hash(): str\n    verify(): bool\n    needs_rehash(): bool\n}\nclass WhirlpoolHasher {\n    algorithm: ClassVar[str]\n    library_module: ClassVar[str]\n    __init__(): Unknown\n    hash(): str\n    verify(): bool\n    needs_rehash(): bool\n}\nclass Blake2Hasher {\n    algorithm: ClassVar[str]\n    __slots__: Unknown\n    __init__(key, digest_size): Unknown\n    hash(): str\n    verify(): bool\n    needs_rehash(): bool\n}\nclass BCryptSha256Hasher {\n    algorithm: ClassVar[str]\n    library_module: ClassVar[str]\n    digest: Unknown\n    __slots__: Unknown\n    __init__(rounds): Unknown\n    hash(): str\n    verify(): bool\n    needs_rehash(): bool\n    _get_hexdigest(_string, digest): bytes\n}\nclass BCryptHasher {\n    algorithm: Unknown\n    digest: Unknown\n}\nclass ScryptHasher {\n    algorithm: ClassVar[str]\n    __slots__: Unknown\n    __init__(work_factor, block_size, parallelism, maxmem, dklen, salt_length): Unknown\n    hash(_string): str\n    verify(_string, _hashed_string): bool\n    needs_rehash(_hashed_string): bool\n    generate_salt(): str\n}\nclass Blake3Hasher {\n    algorithm: ClassVar[str]\n    library_module: ClassVar[str]\n    __init__(): Unknown\n    hash(): str\n    verify(): bool\n    needs_rehash(): bool\n}\nPHasher <|-- Argon2Hasher\nPHasher <|-- WhirlpoolHasher\nPHasher <|-- PBKDF2Sha256Hasher\nPHasher <|-- Blake2Hasher\nBCryptSha256Hasher <|-- BCryptHasher\nPHasher <|-- ScryptHasher\nPHasher <|-- BCryptSha256Hasher\nPHasher <|-- Blake3Hasher\nPBKDF2Sha256Hasher <|-- PBKDF2Sha1Hasher\nPHasher <|-- Ripemd160Hasher\n@enduml",
        "structure": [
            {
                "file": "src/tests/test_pbkdf2_sha1.py",
                "functions": [
                    {
                        "name": "pbkdf2_hasher",
                        "docstring": "Fixture for creating a PBKDF2Sha1Hasher instance with a specified number of iterations.\n\nReturns:\n    PBKDF2Sha1Hasher: An instance of PBKDF2Sha1Hasher configured with 100,000 iterations.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_pbkdf2_hash_format",
                        "docstring": "Test the format of the PBKDF2 hashed string.\n\nThis test ensures that the hashed string generated by the PBKDF2Sha1Hasher\nfollows the expected format. The format is expected to be:\n'pbkdf2_sha256$iterations$salt$hash'.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\nAssertions:\n    - The hashed string should be split into 4 parts using the '$' delimiter.\n    - The first part should be 'pbkdf2_sha256'.\n    - The second part should be the number of iterations, which should be 100,000.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_verify_correct_data",
                        "docstring": "Test the PBKDF2 hasher's ability to verify correct data.\n\nThis test ensures that the PBKDF2Sha1Hasher can correctly hash a given\ndata string and subsequently verify that the hashed value matches\nthe original data.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\nAsserts:\n    The verification of the hashed data against the original data\n    returns True.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_verify_incorrect_data",
                        "docstring": "Test the PBKDF2Sha1Hasher's verify method with incorrect data.\n\nThis test ensures that the verify method returns False when provided\nwith data that does not match the original hashed data.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\nAsserts:\n    The verify method should return False when the provided data does not\n    match the hashed data.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_needs_rehash_false",
                        "docstring": "Test that the PBKDF2Sha1Hasher does not require rehashing for a freshly hashed password.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\nAsserts:\n    The hashed password does not need rehashing.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_needs_rehash_true",
                        "docstring": "Test if the PBKDF2 hasher correctly identifies when a hashed password needs rehashing.\n\nThis test creates an instance of PBKDF2Sha1Hasher with a lower iteration count (50,000)\nto simulate an outdated hash. It then hashes a sample password and checks if the\ncurrent PBKDF2Sha1Hasher instance (with presumably higher iteration count) correctly\nidentifies that the old hash needs rehashing.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha1Hasher): The PBKDF2 hasher instance to test against.\n\nAsserts:\n    True if the `needs_rehash` method correctly identifies that the old hash\n    needs rehashing.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_invalid_hash_format",
                        "docstring": "Test the PBKDF2 hasher's verify method with an invalid hash format.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\nAsserts:\n    The verify method should return False when provided with an invalid hash format.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_bcrypt_sha256.py",
                "functions": [
                    {
                        "name": "bcrypt_hasher",
                        "docstring": "Creates and returns an instance of BCryptSha256Hasher with a specified number of rounds.\n\nReturns:\n    BCryptSha256Hasher: An instance of BCryptSha256Hasher configured with 12 rounds.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_bcrypt_hash_format",
                        "docstring": "Test the format of the hash generated by the BCryptSha256Hasher.\n\nThis test ensures that the hash generated by the BCryptSha256Hasher\nfollows the expected format. The hash should start with 'bcrypt_sha256$'\nand match the regular expression pattern for a valid bcrypt_sha256 hash.\n\nArgs:\n    bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\nRaises:\n    AssertionError: If the hash does not start with 'bcrypt_sha256$' or\n                    does not match the expected format.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_verify_correct_data",
                        "docstring": "Test the BCryptSha256Hasher's verify method with correct data.\n\nThis test ensures that the verify method returns True when provided with\nthe correct data that was previously hashed using the BCryptSha256Hasher.\n\nArgs:\n    bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\nAsserts:\n    The verify method should return True when the correct data is provided.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_verify_incorrect_data",
                        "docstring": "Test the verification of incorrect data using the BCryptSha256Hasher.\n\nThis test ensures that the `verify` method of the `BCryptSha256Hasher` class\nreturns `False` when provided with data that does not match the original hashed data.\n\nArgs:\n    bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher class.\n\nAsserts:\n    The `verify` method should return `False` when the wrong data is provided.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_needs_rehash_false",
                        "docstring": "Test that the `needs_rehash` method of `BCryptSha256Hasher` returns False.\n\nThis test verifies that when a password is hashed using the `BCryptSha256Hasher`\nand the number of rounds matches the expected configuration, the `needs_rehash`\nmethod correctly identifies that the hash does not need to be rehashed.\n\nArgs:\n    bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\nRaises:\n    AssertionError: If the `needs_rehash` method returns True, indicating that\n                    the hash needs rehashing when it should not.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_invalid_hash_format",
                        "docstring": "Test the behavior of the BCryptSha256Hasher when provided with an invalid hash format.\n\nArgs:\n    bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\nAsserts:\n    - The `verify` method should return False when the hash format is invalid.\n    - The `needs_rehash` method should return False when the hash format is invalid.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_unknown_algorithm",
                        "docstring": "Test the behavior of BCryptSha256Hasher when an unknown algorithm is used in the hashed value.\n\nArgs:\n    bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\nAsserts:\n    - The verification should fail when the hashed value uses an unknown algorithm.\n    - The needs_rehash method should return False when the hashed value uses an unknown algorithm.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_scrypt.py",
                "functions": [
                    {
                        "name": "scrypt_hasher",
                        "docstring": "Fixture for creating a ScryptHasher instance with default settings.\n\nReturns:\n    ScryptHasher: An instance of ScryptHasher.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_scrypt_hash_format",
                        "docstring": "Test the format of the Scrypt hashed string.\n\nThis test ensures that the hashed string generated by the ScryptHasher\nfollows the expected format. The format is expected to be:\n'scrypt$work_factor$salt$block_size$parallelism$hash'.\n\nArgs:\n    scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\nAssertions:\n    - The hashed string should be split into 6 parts using the '$' delimiter.\n    - The first part should be 'scrypt'.",
                        "comments": null,
                        "args": [
                            "scrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_scrypt_verify_correct_data",
                        "docstring": "Test the Scrypt hasher's ability to verify correct data.\n\nThis test ensures that the ScryptHasher can correctly hash a given\ndata string and subsequently verify that the hashed value matches\nthe original data.\n\nArgs:\n    scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\nAsserts:\n    The verification of the hashed data against the original data\n    returns True.",
                        "comments": null,
                        "args": [
                            "scrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_scrypt_verify_incorrect_data",
                        "docstring": "Test the ScryptHasher's verify method with incorrect data.\n\nThis test ensures that the verify method returns False when provided\nwith data that does not match the original hashed data.\n\nArgs:\n    scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\nAsserts:\n    The verify method should return False when the provided data does not\n    match the hashed data.",
                        "comments": null,
                        "args": [
                            "scrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_scrypt_needs_rehash_false",
                        "docstring": "Test that the ScryptHasher does not require rehashing for a freshly hashed password.\n\nArgs:\n    scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\nAsserts:\n    The hashed password does not need rehashing.",
                        "comments": null,
                        "args": [
                            "scrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_scrypt_needs_rehash_true",
                        "docstring": "Test if the Scrypt hasher correctly identifies when a hashed password needs rehashing.\n\nThis test creates an instance of ScryptHasher with different work factor or block size\nto simulate an outdated hash. It then hashes a sample password and checks if the\ncurrent ScryptHasher instance correctly identifies that the old hash needs rehashing.\n\nArgs:\n    scrypt_hasher (ScryptHasher): The Scrypt hasher instance to test against.\n\nAsserts:\n    True if the `needs_rehash` method correctly identifies that the old hash\n    needs rehashing.",
                        "comments": null,
                        "args": [
                            "scrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_scrypt_invalid_hash_format",
                        "docstring": "Test the Scrypt hasher's verify method with an invalid hash format.\n\nArgs:\n    scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\nAsserts:\n    The verify method should return False when provided with an invalid hash format.",
                        "comments": null,
                        "args": [
                            "scrypt_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/tests/test_whirlpool.py",
                "functions": [
                    {
                        "name": "whirlpool_hasher",
                        "docstring": "Fixture for creating a WhirlpoolHasher instance.\n\nReturns:\n    WhirlpoolHasher: An instance of WhirlpoolHasher.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_whirlpool_hash_format",
                        "docstring": "Test the format of the Whirlpool hashed string.\n\nThis test ensures that the hashed string generated by the WhirlpoolHasher\nfollows the expected format. The format is expected to be:\n'whirlpool$hash'.\n\nArgs:\n    whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\nAssertions:\n    - The hashed string should be split into 2 parts using the '$' delimiter.\n    - The first part should be 'whirlpool'.",
                        "comments": null,
                        "args": [
                            "whirlpool_hasher"
                        ]
                    },
                    {
                        "name": "test_whirlpool_verify_correct_data",
                        "docstring": "Test the Whirlpool hasher's ability to verify correct data.\n\nThis test ensures that the WhirlpoolHasher can correctly hash a given\ndata string and subsequently verify that the hashed value matches\nthe original data.\n\nArgs:\n    whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\nAsserts:\n    The verification of the hashed data against the original data\n    returns True.",
                        "comments": null,
                        "args": [
                            "whirlpool_hasher"
                        ]
                    },
                    {
                        "name": "test_whirlpool_verify_incorrect_data",
                        "docstring": "Test the WhirlpoolHasher's verify method with incorrect data.\n\nThis test ensures that the verify method returns False when provided\nwith data that does not match the original hashed data.\n\nArgs:\n    whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\nAsserts:\n    The verify method should return False when the provided data does not\n    match the hashed data.",
                        "comments": null,
                        "args": [
                            "whirlpool_hasher"
                        ]
                    },
                    {
                        "name": "test_whirlpool_needs_rehash_false",
                        "docstring": "Test that the WhirlpoolHasher does not require rehashing for a freshly hashed password.\n\nArgs:\n    whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\nAsserts:\n    The hashed password does not need rehashing.",
                        "comments": null,
                        "args": [
                            "whirlpool_hasher"
                        ]
                    },
                    {
                        "name": "test_whirlpool_needs_rehash_true",
                        "docstring": "Test if the Whirlpool hasher correctly identifies when a hashed password needs rehashing.\n\nThis test simulates an outdated hash by modifying the algorithm name in the hashed string.\nIt then checks if the current WhirlpoolHasher instance correctly identifies that the old hash needs rehashing.\n\nArgs:\n    whirlpool_hasher (WhirlpoolHasher): The Whirlpool hasher instance to test against.\n\nAsserts:\n    True if the `needs_rehash` method correctly identifies that the old hash\n    needs rehashing.",
                        "comments": null,
                        "args": [
                            "whirlpool_hasher"
                        ]
                    },
                    {
                        "name": "test_whirlpool_invalid_hash_format",
                        "docstring": "Test the Whirlpool hasher's verify method with an invalid hash format.\n\nArgs:\n    whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\nAsserts:\n    The verify method should return False when provided with an invalid hash format.",
                        "comments": null,
                        "args": [
                            "whirlpool_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_bcrypt.py",
                "functions": [
                    {
                        "name": "bcrypt_hasher",
                        "docstring": "Creates and returns an instance of BCryptHasher with a specified number of rounds.\n\nReturns:\n    BCryptHasher: An instance of BCryptHasher configured with 12 rounds.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_bcrypt_hash_format",
                        "docstring": "Test the format of the hash generated by the BCryptHasher.\n\nThis test ensures that the hash generated by the BCryptHasher\nfollows the expected format. The hash should start with 'bcrypt$'\nand match the regular expression pattern for a valid bcrypt_sha256 hash.\n\nArgs:\n    bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\nRaises:\n    AssertionError: If the hash does not start with 'bcrypt$' or\n                    does not match the expected format.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_verify_correct_data",
                        "docstring": "Test the BCryptHasher's verify method with correct data.\n\nThis test ensures that the verify method returns True when provided with\nthe correct data that was previously hashed using the BCryptHasher.\n\nArgs:\n    bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\nAsserts:\n    The verify method should return True when the correct data is provided.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_verify_incorrect_data",
                        "docstring": "Test the verification of incorrect data using the BCryptHasher.\n\nThis test ensures that the `verify` method of the `BCryptHasher` class\nreturns `False` when provided with data that does not match the original hashed data.\n\nArgs:\n    bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher class.\n\nAsserts:\n    The `verify` method should return `False` when the wrong data is provided.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_needs_rehash_false",
                        "docstring": "Test that the `needs_rehash` method of `BCryptHasher` returns False.\n\nThis test verifies that when a password is hashed using the `BCryptHasher`\nand the number of rounds matches the expected configuration, the `needs_rehash`\nmethod correctly identifies that the hash does not need to be rehashed.\n\nArgs:\n    bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\nRaises:\n    AssertionError: If the `needs_rehash` method returns True, indicating that\n                    the hash needs rehashing when it should not.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_invalid_hash_format",
                        "docstring": "Test the behavior of the BCryptHasher when provided with an invalid hash format.\n\nArgs:\n    bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\nAsserts:\n    - The `verify` method should return False when the hash format is invalid.\n    - The `needs_rehash` method should return False when the hash format is invalid.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    },
                    {
                        "name": "test_bcrypt_unknown_algorithm",
                        "docstring": "Test the behavior of BCryptHasher when an unknown algorithm is used in the hashed value.\n\nArgs:\n    bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\nAsserts:\n    - The verification should fail when the hashed value uses an unknown algorithm.\n    - The needs_rehash method should return False when the hashed value uses an unknown algorithm.",
                        "comments": null,
                        "args": [
                            "bcrypt_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_blake3.py",
                "functions": [
                    {
                        "name": "blake3_hasher",
                        "docstring": "Fixture to create an instance of Blake3Hasher.\n\nReturns:\n    Blake3Hasher: An instance of Blake3Hasher initialized with a random key.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_hash_creation",
                        "docstring": "Test the creation of a hash using the Blake3Hasher.\n\nThis test verifies that the hashed value of a given string starts with the\nexpected prefix \"blake2b$\" and that the hashed value is correctly formatted\nwith three parts separated by the '$' character.\n\nArgs:\n    blake3_hasher (Blake3Hasher): An instance of the Blake3Hasher class.\n\nRaises:\n    AssertionError: If the hashed value does not start with \"blake2b$\" or\n                    if the hashed value does not contain exactly three parts\n                    separated by the '$' character.",
                        "comments": null,
                        "args": [
                            "blake3_hasher"
                        ]
                    },
                    {
                        "name": "test_verify_hash_correct",
                        "docstring": "Tests the `verify` method of the `Blake3Hasher` class to ensure that it correctly verifies a hashed value.\n\nArgs:\n    blake3_hasher (Blake3Hasher): An instance of the `Blake3Hasher` class.\n\nTest Steps:\n1. Hashes a sample string using the `hash` method of `Blake3Hasher`.\n2. Verifies that the original string matches the hashed value using the `verify` method.\n\nAsserts:\n    The `verify` method returns True when the original string matches the hashed value.",
                        "comments": null,
                        "args": [
                            "blake3_hasher"
                        ]
                    },
                    {
                        "name": "test_verify_hash_incorrect",
                        "docstring": "Test the `verify` method of the `Blake3Hasher` class with an incorrect string.\n\nThis test ensures that the `verify` method returns `False` when provided with\na string that does not match the original hashed value.\n\nArgs:\n    blake3_hasher (Blake3Hasher): An instance of the Blake3Hasher class.\n\nAsserts:\n    The `verify` method returns `False` when the incorrect string is provided.",
                        "comments": null,
                        "args": [
                            "blake3_hasher"
                        ]
                    },
                    {
                        "name": "test_hash_with_key",
                        "docstring": "Test the Blake3Hasher class with a key.\n\nThis test generates a random key using `secrets.token_urlsafe()`,\ncreates an instance of `Blake3Hasher` with the generated key,\nand hashes a sample string. It then verifies that the hashed\nvalue matches the original string and does not match an incorrect string.\n\nAssertions:\n    - The hashed value should be verified as True for the correct string.\n    - The hashed value should be verified as False for an incorrect string.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_verify_fails_on_modified_hash",
                        "docstring": "Test that the `verify` method of `Blake3Hasher` fails when the hash has been modified.\nThis test ensures that the `verify` method returns `False` when the hash of a given\nstring is altered. It first hashes a sample string, then modifies the resulting hash\nby replacing the first occurrence of the character 'a' with 'b'. Finally, it verifies\nthat the `verify` method correctly identifies the modified hash as invalid.\nArgs:\n    blake3_hasher (Blake3Hasher): An instance of the Blake3Hasher class.\nReturns:\n    None",
                        "comments": null,
                        "args": [
                            "blake3_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_ripemd160.py",
                "functions": [
                    {
                        "name": "ripemd160_hasher",
                        "docstring": "Fixture for creating a Ripemd160Hasher instance.\n\nReturns:\n    Ripemd160Hasher: An instance of Ripemd160Hasher.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_ripemd160_hash_format",
                        "docstring": "Test the format of the RIPEMD-160 hashed string.\n\nThis test ensures that the hashed string generated by the Ripemd160Hasher\nfollows the expected format. The format is expected to be:\n'RIPEMD-160$hashed_value'.\n\nArgs:\n    ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\nAssertions:\n    - The hashed string should be split into 2 parts using the '$' delimiter.\n    - The first part should be 'RIPEMD-160'.",
                        "comments": null,
                        "args": [
                            "ripemd160_hasher"
                        ]
                    },
                    {
                        "name": "test_ripemd160_verify_correct_data",
                        "docstring": "Test the Ripemd160Hasher's ability to verify correct data.\n\nThis test ensures that the Ripemd160Hasher can correctly hash a given\ndata string and subsequently verify that the hashed value matches\nthe original data.\n\nArgs:\n    ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\nAsserts:\n    The verification of the hashed data against the original data\n    returns True.",
                        "comments": null,
                        "args": [
                            "ripemd160_hasher"
                        ]
                    },
                    {
                        "name": "test_ripemd160_verify_incorrect_data",
                        "docstring": "Test the Ripemd160Hasher's verify method with incorrect data.\n\nThis test ensures that the verify method returns False when provided\nwith data that does not match the original hashed data.\n\nArgs:\n    ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\nAsserts:\n    The verify method should return False when the provided data does not\n    match the hashed data.",
                        "comments": null,
                        "args": [
                            "ripemd160_hasher"
                        ]
                    },
                    {
                        "name": "test_ripemd160_needs_rehash_false",
                        "docstring": "Test that the Ripemd160Hasher does not require rehashing for a freshly hashed password.\n\nArgs:\n    ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\nAsserts:\n    The hashed password does not need rehashing.",
                        "comments": null,
                        "args": [
                            "ripemd160_hasher"
                        ]
                    },
                    {
                        "name": "test_ripemd160_needs_rehash_true",
                        "docstring": "Test if the Ripemd160Hasher correctly identifies when a hashed password needs rehashing.\n\nThis test simulates an outdated hash by modifying the algorithm name in the hashed string.\nIt then checks if the current Ripemd160Hasher instance correctly identifies that the old hash needs rehashing.\n\nArgs:\n    ripemd160_hasher (Ripemd160Hasher): The Ripemd160Hasher instance to test against.\n\nAsserts:\n    True if the `needs_rehash` method correctly identifies that the old hash\n    needs rehashing.",
                        "comments": null,
                        "args": [
                            "ripemd160_hasher"
                        ]
                    },
                    {
                        "name": "test_ripemd160_invalid_hash_format",
                        "docstring": "Test the Ripemd160Hasher's verify method with an invalid hash format.\n\nArgs:\n    ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\nAsserts:\n    The verify method should return False when provided with an invalid hash format.",
                        "comments": null,
                        "args": [
                            "ripemd160_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_argon2.py",
                "functions": [
                    {
                        "name": "argon2_hasher",
                        "docstring": "Creates and returns an instance of Argon2Hasher.\n\nReturns:\n    Argon2Hasher: An instance of the Argon2Hasher class.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_argon2_verify_correct_data",
                        "docstring": "Test the Argon2Hasher's verify method with correct data.\n\nThis test ensures that the `verify` method of the `Argon2Hasher` class\nreturns `True` when provided with the correct data and its corresponding hash.\n\nArgs:\n    argon2_hasher (Argon2Hasher): An instance of the Argon2Hasher class.\n\nAsserts:\n    The `verify` method should return `True` when the correct data and its hash are provided.",
                        "comments": null,
                        "args": [
                            "argon2_hasher"
                        ]
                    },
                    {
                        "name": "test_argon2_verify_incorrect_data",
                        "docstring": "Test the Argon2Hasher's verify method with incorrect data.\n\nThis test ensures that the verify method returns False when provided with\ndata that does not match the original hashed data.\n\nArgs:\n    argon2_hasher (Argon2Hasher): An instance of the Argon2Hasher class.\n\nAsserts:\n    The verify method should return False when the wrong data is provided.",
                        "comments": null,
                        "args": [
                            "argon2_hasher"
                        ]
                    },
                    {
                        "name": "test_argon2_needs_rehash_false",
                        "docstring": "Test that the `needs_rehash` method of the `Argon2Hasher` class returns False.\n\nThis test verifies that when data is hashed using the `Argon2Hasher` class,\nthe resulting hash does not require rehashing with the current parameters.\n\nArgs:\n    argon2_hasher (Argon2Hasher): An instance of the Argon2Hasher class.\n\nRaises:\n    AssertionError: If the `needs_rehash` method returns True, indicating that\n                    the hashed data needs rehashing.",
                        "comments": null,
                        "args": [
                            "argon2_hasher"
                        ]
                    },
                    {
                        "name": "test_argon2_needs_rehash_true",
                        "docstring": "Test if the Argon2Hasher correctly identifies that a hash needs rehashing.\n\nThis test checks whether the `needs_rehash` method of the `Argon2Hasher` class\nreturns `True` when the hash was generated with a lower time cost than the current\nhasher's configuration.\n\nArgs:\n    argon2_hasher (Argon2Hasher): An instance of Argon2Hasher with the current configuration.\n\nRaises:\n    AssertionError: If the `needs_rehash` method does not return `True` for a hash\n                    generated with a lower time cost.",
                        "comments": null,
                        "args": [
                            "argon2_hasher"
                        ]
                    },
                    {
                        "name": "test_argon2_invalid_hash_format",
                        "docstring": "Test the Argon2Hasher's behavior with an invalid hash format.\n\nThis test ensures that the Argon2Hasher correctly identifies and handles\nan invalid hash format. Specifically, it verifies that:\n1. The `verify` method returns False when provided with an invalid hash format.\n2. The `needs_rehash` method returns False when provided with an invalid hash format.\n\nArgs:\n    argon2_hasher (Argon2Hasher): An instance of the Argon2Hasher class.\n\nRaises:\n    AssertionError: If the `verify` method does not return False for an invalid hash format.\n    AssertionError: If the `needs_rehash` method does not return False for an invalid hash format.",
                        "comments": null,
                        "args": [
                            "argon2_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_pbkdf2_sha256.py",
                "functions": [
                    {
                        "name": "pbkdf2_hasher",
                        "docstring": "Fixture for creating a PBKDF2Sha256Hasher instance with a specified number of iterations.\n\nReturns:\n    PBKDF2Sha256Hasher: An instance of PBKDF2Sha256Hasher configured with 100,000 iterations.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_pbkdf2_hash_format",
                        "docstring": "Test the format of the PBKDF2 hashed string.\n\nThis test ensures that the hashed string generated by the PBKDF2Sha256Hasher\nfollows the expected format. The format is expected to be:\n'pbkdf2_sha256$iterations$salt$hash'.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\nAssertions:\n    - The hashed string should be split into 4 parts using the '$' delimiter.\n    - The first part should be 'pbkdf2_sha256'.\n    - The second part should be the number of iterations, which should be 100,000.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_verify_correct_data",
                        "docstring": "Test the PBKDF2 hasher's ability to verify correct data.\n\nThis test ensures that the PBKDF2Sha256Hasher can correctly hash a given\ndata string and subsequently verify that the hashed value matches\nthe original data.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\nAsserts:\n    The verification of the hashed data against the original data\n    returns True.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_verify_incorrect_data",
                        "docstring": "Test the PBKDF2Sha256Hasher's verify method with incorrect data.\n\nThis test ensures that the verify method returns False when provided\nwith data that does not match the original hashed data.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\nAsserts:\n    The verify method should return False when the provided data does not\n    match the hashed data.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_needs_rehash_false",
                        "docstring": "Test that the PBKDF2Sha256Hasher does not require rehashing for a freshly hashed password.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\nAsserts:\n    The hashed password does not need rehashing.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_needs_rehash_true",
                        "docstring": "Test if the PBKDF2 hasher correctly identifies when a hashed password needs rehashing.\n\nThis test creates an instance of PBKDF2Sha256Hasher with a lower iteration count (50,000)\nto simulate an outdated hash. It then hashes a sample password and checks if the\ncurrent PBKDF2Sha256Hasher instance (with presumably higher iteration count) correctly\nidentifies that the old hash needs rehashing.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha256Hasher): The PBKDF2 hasher instance to test against.\n\nAsserts:\n    True if the `needs_rehash` method correctly identifies that the old hash\n    needs rehashing.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    },
                    {
                        "name": "test_pbkdf2_invalid_hash_format",
                        "docstring": "Test the PBKDF2 hasher's verify method with an invalid hash format.\n\nArgs:\n    pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\nAsserts:\n    The verify method should return False when provided with an invalid hash format.",
                        "comments": null,
                        "args": [
                            "pbkdf2_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_blake2.py",
                "functions": [
                    {
                        "name": "blake2_hasher",
                        "docstring": "Fixture to create an instance of Blake2Hasher.\n\nReturns:\n    Blake2Hasher: An instance of Blake2Hasher initialized with a random key.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_hash_creation",
                        "docstring": "Test the creation of a hash using the Blake2Hasher.\n\nThis test verifies that the hashed value of a given string starts with the\nexpected prefix \"blake2b$\" and that the hashed value is correctly formatted\nwith three parts separated by the '$' character.\n\nArgs:\n    blake2_hasher (Blake2Hasher): An instance of the Blake2Hasher class.\n\nRaises:\n    AssertionError: If the hashed value does not start with \"blake2b$\" or\n                    if the hashed value does not contain exactly three parts\n                    separated by the '$' character.",
                        "comments": null,
                        "args": [
                            "blake2_hasher"
                        ]
                    },
                    {
                        "name": "test_verify_hash_correct",
                        "docstring": "Tests the `verify` method of the `Blake2Hasher` class to ensure that it correctly verifies a hashed value.\n\nArgs:\n    blake2_hasher (Blake2Hasher): An instance of the `Blake2Hasher` class.\n\nTest Steps:\n1. Hashes a sample string using the `hash` method of `Blake2Hasher`.\n2. Verifies that the original string matches the hashed value using the `verify` method.\n\nAsserts:\n    The `verify` method returns True when the original string matches the hashed value.",
                        "comments": null,
                        "args": [
                            "blake2_hasher"
                        ]
                    },
                    {
                        "name": "test_verify_hash_incorrect",
                        "docstring": "Test the `verify` method of the `Blake2Hasher` class with an incorrect string.\n\nThis test ensures that the `verify` method returns `False` when provided with\na string that does not match the original hashed value.\n\nArgs:\n    blake2_hasher (Blake2Hasher): An instance of the Blake2Hasher class.\n\nAsserts:\n    The `verify` method returns `False` when the incorrect string is provided.",
                        "comments": null,
                        "args": [
                            "blake2_hasher"
                        ]
                    },
                    {
                        "name": "test_needs_rehash_false",
                        "docstring": "Test that the `needs_rehash` method of the `Blake2Hasher` class returns False\nfor a freshly hashed value.\n\nArgs:\n    blake2_hasher (Blake2Hasher): An instance of the Blake2Hasher class.\n\nAsserts:\n    The `needs_rehash` method should return False for the given hashed value.",
                        "comments": null,
                        "args": [
                            "blake2_hasher"
                        ]
                    },
                    {
                        "name": "test_needs_rehash_true",
                        "docstring": "Test the `needs_rehash` method of the `Blake2Hasher` class.\n\nThis test verifies that the `needs_rehash` method correctly identifies\nwhen a hashed value needs to be rehashed due to a different digest size.\n\nSteps:\n1. Generate a random key using `secrets.token_urlsafe()`.\n2. Create two instances of `Blake2Hasher` with the same key but different digest sizes (32 and 64).\n3. Hash a sample string using the 32-byte hasher.\n4. Assert that the 64-byte hasher's `needs_rehash` method returns `True` when passed the 32-byte hashed value.\n\nExpected Result:\nThe `needs_rehash` method should return `True` indicating that the hashed value needs to be rehashed to match the \n64-byte digest size.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_hash_with_key",
                        "docstring": "Test the Blake2Hasher class with a key.\n\nThis test generates a random key using `secrets.token_urlsafe()`,\ncreates an instance of `Blake2Hasher` with the generated key,\nand hashes a sample string. It then verifies that the hashed\nvalue matches the original string and does not match an incorrect string.\n\nAssertions:\n    - The hashed value should be verified as True for the correct string.\n    - The hashed value should be verified as False for an incorrect string.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_verify_fails_on_modified_hash",
                        "docstring": "Test that the `verify` method of `Blake2Hasher` fails when the hash has been modified.\nThis test ensures that the `verify` method returns `False` when the hash of a given\nstring is altered. It first hashes a sample string, then modifies the resulting hash\nby replacing the first occurrence of the character 'a' with 'b'. Finally, it verifies\nthat the `verify` method correctly identifies the modified hash as invalid.\nArgs:\n    blake2_hasher (Blake2Hasher): An instance of the Blake2Hasher class.\nReturns:\n    None",
                        "comments": null,
                        "args": [
                            "blake2_hasher"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/tests/test_hash_manager.py",
                "functions": [
                    {
                        "name": "hash_manager_instance",
                        "docstring": "Fixture to create an instance of HashManager.\n\nArgs:\n    available_hashers: A list of available hashers to be used by the HashManager.\n\nReturns:\n    HashManager: An instance of HashManager initialized with the provided hashers.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_hash_with_preferred_hasher",
                        "docstring": "Test the hashing functionality using the preferred hasher.\n\nThis test verifies that the `hash` method of the `hash_manager_instance`\nproduces a hash that starts with the expected prefix for the preferred\nhasher (`pbkdf2_sha256$150000$`). It also checks that the `verify` method\ncorrectly validates the original data against the generated hash.\n\nArgs:\n    hash_manager_instance: An instance of the HashManager class configured\n                           with the preferred hasher.\n\nAsserts:\n    - The generated hash starts with 'pbkdf2_sha256$150000$'.\n    - The `verify` method returns True when verifying the original data\n      against the generated hash.",
                        "comments": null,
                        "args": [
                            "hash_manager_instance"
                        ]
                    },
                    {
                        "name": "test_verify_with_preferred_hasher",
                        "docstring": "Test the `verify` method of `HashManager` using the preferred hasher.\n\nThis test ensures that the `verify` method correctly verifies a hashed value\ngenerated by the `hash` method when using the preferred hasher.\n\nArgs:\n    hash_manager_instance (HashManager): An instance of the `HashManager` class.\n\nAsserts:\n    The `verify` method returns True when verifying a hashed value generated\n    from the same input data.",
                        "comments": null,
                        "args": [
                            "hash_manager_instance"
                        ]
                    },
                    {
                        "name": "test_verify_incorrect_data",
                        "docstring": "Test the `verify` method of `HashManager` with incorrect data.\n\nThis test ensures that the `verify` method returns `False` when provided\nwith data that does not match the original hashed data.\n\nArgs:\n    hash_manager_instance (HashManager): An instance of the `HashManager` class.\n\nAsserts:\n    The `verify` method returns `False` when the input data does not match\n    the hashed data.",
                        "comments": null,
                        "args": [
                            "hash_manager_instance"
                        ]
                    },
                    {
                        "name": "test_needs_rehash_false",
                        "docstring": "Test that the `needs_rehash` method of `HashManager` returns False for a freshly hashed password.\n\nArgs:\n    hash_manager_instance (HashManager): An instance of the `HashManager` class.\n\nAsserts:\n    The `needs_rehash` method returns False for a newly hashed password.",
                        "comments": null,
                        "args": [
                            "hash_manager_instance"
                        ]
                    },
                    {
                        "name": "test_needs_rehash_true_due_to_iterations",
                        "docstring": "Test if the `needs_rehash` method returns True when the number of iterations\nused in the hash is lower than the current standard.\n\nThis test creates a hash using an old PBKDF2 hasher with fewer iterations\nand checks if the `needs_rehash` method correctly identifies that the hash\nneeds to be rehashed due to the insufficient number of iterations.\n\nArgs:\n    hash_manager_instance (HashManager): An instance of the HashManager class.",
                        "comments": null,
                        "args": [
                            "hash_manager_instance"
                        ]
                    },
                    {
                        "name": "test_invalid_hashed_data_format",
                        "docstring": "Test the behavior of HashManager when provided with an invalid hashed data format.\n\nThis test ensures that:\n1. The `verify` method returns False when given an invalid hash format.\n2. The `needs_rehash` method returns True when given an invalid hash format.\n\nArgs:\n    hash_manager_instance (HashManager): An instance of the HashManager class.",
                        "comments": null,
                        "args": [
                            "hash_manager_instance"
                        ]
                    },
                    {
                        "name": "test_unknown_algorithm",
                        "docstring": "Test the behavior of HashManager when an unknown algorithm is encountered.\n\nArgs:\n    hash_manager_instance (HashManager): An instance of the HashManager class.\n\nAsserts:\n    - The `verify` method should return False when provided with a hash string\n      that uses an unknown algorithm.\n    - The `needs_rehash` method should return True for the hash string with an\n      unknown algorithm.",
                        "comments": null,
                        "args": [
                            "hash_manager_instance"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/hash_forge/__init__.py",
                "functions": [],
                "classes": [
                    {
                        "name": "HashManager",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initialize the HashForge instance with one or more hashers.\n\nArgs:\n    *hashers (PHasher): One or more hasher instances to be used by the HashForge.\n\nRaises:\n    ValueError: If no hashers are provided.\n\nAttributes:\n    hashers (Set[Tuple[str, PHasher]]): A set of tuples containing the algorithm name and the hasher instance.\n    preferred_hasher (PHasher): The first hasher provided, used as the preferred hasher.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Hashes the given string using the preferred hasher.\n\nArgs:\n    string (str): The string to be hashed.\n\nReturns:\n    str: The hashed string.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "string"
                                ]
                            },
                            {
                                "name": "verify",
                                "docstring": "Verifies if a given string matches a hashed string using the appropriate hashing algorithm.\n\nArgs:\n    string (str): The plain text string to verify.\n    hashed_string (str): The hashed string to compare against.\n\nReturns:\n    bool: True if the string matches the hashed string, False otherwise.\n\nRaises:\n    IndexError: If the hashed string does not contain a valid algorithm identifier.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "string",
                                    "hashed_string"
                                ]
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Determines if a given hashed string needs to be rehashed.\n\nThis method checks if the hashing algorithm used for the given hashed string\nis the preferred algorithm or if the hashed string needs to be rehashed\naccording to the hasher's criteria.\n\nArgs:\n    hashed_string (str): The hashed string to check.\n\nReturns:\n    bool: True if the hashed string needs to be rehashed, False otherwise.\n\nRaises:\n    IndexError: If the hashed string format is invalid.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "hashed_string"
                                ]
                            },
                            {
                                "name": "_get_hasher_by_hash",
                                "docstring": "Retrieve the hasher instance that matches the given hashed string.\n\nThis method iterates through the available hashers and returns the first\nhasher whose algorithm matches the beginning of the provided hashed string.\n\nArgs:\n    hashed_string (str): The hashed string to match against available hashers.\n\nReturns:\n    PHasher | None: The hasher instance that matches the hashed string, or\n    None if no match is found.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "hashed_string"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/hash_forge/protocols.py",
                "functions": [],
                "classes": [
                    {
                        "name": "PHasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "hash",
                                "docstring": "Computes the hash of the given string.\n\nArgs:\n    _string (str): The input string to be hashed.\n\nReturns:\n    str: The resulting hash as a string.\n\nRaises:\n    NotImplementedError: This method should be implemented by subclasses.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "verify",
                                "docstring": "Verify if the provided string matches the hashed string.\n\nArgs:\n    _string (str): The original string to verify.\n    _hashed_string (str): The hashed string to compare against.\n\nReturns:\n    bool: True if the original string matches the hashed string, False otherwise.\n\nRaises:\n    NotImplementedError: This method should be implemented by subclasses.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Determine if a hashed string needs to be rehashed.\n\nArgs:\n    _hashed_string (str): The hashed string to check.\n\nReturns:\n    bool: True if the hashed string needs to be rehashed, False otherwise.\n\nRaises:\n    NotImplementedError: This method should be implemented by subclasses.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "load_library",
                                "docstring": "Loads a library module by its name.\n\nThis function attempts to import a module specified by the `name` parameter.\nIf the module is not found, it raises an ImportError with a message indicating\nthe required third-party library to install.\n\nArgs:\n    name (str): The name of the module to import.\n\nReturns:\n    ModuleType: The imported module.\n\nRaises:\n    ImportError: If the module cannot be imported, with a message suggesting\n                the required third-party library to install.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "name"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/hash_forge/utils.py",
                "functions": [
                    {
                        "name": "get_random_string",
                        "docstring": "Return a securely generated random string.\n\nThe bit length of the returned value can be calculated with the formula:\n    log_2(len(allowed_chars)^length)\n\nFor example, with default `allowed_chars` (26+26+10), this gives:\n  * length: 12, bit length =~ 71 bits\n  * length: 22, bit length =~ 131 bits",
                        "comments": null,
                        "args": [
                            "length",
                            "allowed_chars"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "src/hash_forge/constants.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/hash_forge/hashers/pbkdf2_hasher.py",
                "functions": [],
                "classes": [
                    {
                        "name": "PBKDF2Sha256Hasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "iterations",
                                    "salt_length"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Hashes a given string using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm.\n\nArgs:\n    _string (str): The input string to be hashed.\n\nReturns:\n    str: The hashed string in the format 'algorithm$iterations$salt$hashed'.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "verify",
                                "docstring": "Verifies if a given string matches the hashed string using PBKDF2 algorithm.\n\nArgs:\n    _string (str): The plain text string to verify.\n    _hashed_string (str): The hashed string to compare against, formatted as 'algorithm$iterations$salt$hashed'.\n\nReturns:\n    bool: True if the string matches the hashed string, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Determines if a hashed string needs to be rehashed based on the number of iterations.\n\nArgs:\n    _hashed_string (str): The hashed string to check.\n\nReturns:\n    bool: True if rehash is needed, False otherwise.",
                                "comments": null,
                                "args": []
                            }
                        ],
                        "attributes": [
                            "__slots__"
                        ]
                    },
                    {
                        "name": "PBKDF2Sha1Hasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "algorithm",
                            "digest"
                        ]
                    }
                ]
            },
            {
                "file": "src/hash_forge/hashers/argon2_hasher.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Argon2Hasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initialize the Argon2Hasher with optional parameters for hashing configuration.\n\nArgs:\n    time_cost (int | None): The time cost parameter for Argon2. Defaults to None.\n    salt_len (int | None): The length of the salt. Defaults to None.\n    memory_cost (int | None): The memory cost parameter for Argon2. Defaults to None.\n    parallelism (int | None): The degree of parallelism for Argon2. Defaults to None.\n    hash_len (int | None): The length of the resulting hash. Defaults to None.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "time_cost",
                                    "salt_len",
                                    "memory_cost",
                                    "parallelism",
                                    "hash_len"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Hashes the given string using Argon2 algorithm.\n\nArgs:\n    _string (str): The string to be hashed.\n\nReturns:\n    str: The formatted hash string containing the algorithm, time cost, memory cost, parallelism, salt, and\n    hashed value.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "verify",
                                "docstring": "Verifies if a given string matches the provided hashed string using Argon2.\n\nArgs:\n    _string (str): The plain string to verify.\n    _hashed_string (str): The hashed string to compare against.\n\nReturns:\n    bool: True if the string matches the hashed string, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Determines if the given hashed string needs to be rehashed based on the current time cost.\n\nArgs:\n    _hashed_string (str): The hashed string to check.\n\nReturns:\n    bool: True if the hashed string needs to be rehashed, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "_get_hasher",
                                "docstring": "Creates and returns a configured instance of argon2.PasswordHasher.\n\nThis method uses the provided configuration parameters to set up the\nPasswordHasher instance. The parameters that can be configured are:\n- time_cost: The time cost parameter for the Argon2 algorithm.\n- memory_cost: The memory cost parameter for the Argon2 algorithm.\n- parallelism: The parallelism parameter for the Argon2 algorithm.\n- hash_len: The length of the generated hash.\n- salt_len: The length of the salt.\n\nReturns:\n    argon2.PasswordHasher: A configured instance of the PasswordHasher.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": [
                            "__slots__"
                        ]
                    }
                ]
            },
            {
                "file": "src/hash_forge/hashers/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/hash_forge/hashers/ripemd160_hasher.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Ripemd160Hasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes the RIPEMD-160 hasher instance.\n\nThis method loads the RIPEMD-160 hashing library and assigns it to the\ninstance variable `self.ripemd160`.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Hashes the given string using the RIPEMD-160 algorithm.\n\nArgs:\n    _string (str): The input string to be hashed.\n\nReturns:\n    str: The hashed string in the format 'algorithm$hashed_value'.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "verify",
                                "docstring": "Verify if the provided string matches the given hashed value.\n\nArgs:\n    _string (str): The original string to verify.\n    _hashed (str): The hashed value to compare against, in the format 'algorithm$hash_value'.\n\nReturns:\n    bool: True if the string matches the hashed value, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Determines if the given hashed string needs to be rehashed.\n\nArgs:\n    _hashed_string (str): The hashed string to check.\n\nReturns:\n    bool: True if the algorithm used in the hashed string does not match\n          the current algorithm, indicating that a rehash is needed.",
                                "comments": null,
                                "args": []
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/hash_forge/hashers/whirlpool_hasher.py",
                "functions": [],
                "classes": [
                    {
                        "name": "WhirlpoolHasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes the WhirlpoolHasher instance.\n\nThis constructor initializes the WhirlpoolHasher by loading the SHA-512\nhashing library module.\n\nAttributes:\n    sha512: The loaded SHA-512 hashing library module.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Computes the hash of the given string using the SHA-512 algorithm.\n\nArgs:\n    _string (str): The input string to be hashed.\n\nReturns:\n    str: The resulting hash as a hexadecimal string prefixed with the algorithm name.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "verify",
                                "docstring": "Verifies if the given string matches the given hash.\n\nArgs:\n    _string (str): The input string to verify.\n    _hashed_string (str): The hash to compare against.\n\nReturns:\n    bool: True if the hash matches the input string, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Determines if the given hash needs to be rehashed.\n\nArgs:\n    _hashed_string (str): The hash to check.\n\nReturns:\n    bool: True if the hash needs to be rehashed, False otherwise.",
                                "comments": null,
                                "args": []
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/hash_forge/hashers/blake2_hasher.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Blake2Hasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes the Blake2Hasher with a key and an optional digest size.\n\nArgs:\n    key (str): The key to use for the hash function.\n    digest_size (int, optional): The size of the digest in bytes. Defaults to 64.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "key",
                                    "digest_size"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Hashes the given string using the BLAKE2b algorithm.\n\nArgs:\n    _string (str): The string to be hashed.\n\nReturns:\n    str: The formatted hash string containing the algorithm, digest size, and hashed value.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "verify",
                                "docstring": "Verifies if a given string matches the hashed string using BLAKE2b.\n\nArgs:\n    _string (str): The plain text string to verify.\n    _hashed_string (str): The hashed string to compare against.\n\nReturns:\n    bool: True if the plain text string matches the hashed string, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Checks if the hashed string needs to be rehashed based on the digest size.\n\nArgs:\n    _hashed_string (str): The hashed string to check.\n\nReturns:\n    bool: True if the hashed string needs to be rehashed, False otherwise.",
                                "comments": null,
                                "args": []
                            }
                        ],
                        "attributes": [
                            "__slots__"
                        ]
                    }
                ]
            },
            {
                "file": "src/hash_forge/hashers/bcrypt_hasher.py",
                "functions": [],
                "classes": [
                    {
                        "name": "BCryptSha256Hasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes the BcryptHasher with the specified number of rounds.\n\nArgs:\n    rounds (int, optional): The number of rounds to use for hashing. Defaults to 12.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "rounds"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Hashes the given string using bcrypt algorithm.\n\nArgs:\n    _string (str): The string to be hashed.\n\nReturns:\n    str: The formatted hash string containing the algorithm, rounds, salt, and hashed value.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "verify",
                                "docstring": "Verify if a given string matches the hashed string using bcrypt.\n\nArgs:\n    _string (str): The plain text string to verify.\n    _hashed_string (str): The hashed string to compare against.\n\nReturns:\n    bool: True if the plain text string matches the hashed string, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Check if the hashed string needs to be rehashed.\n\nThis method determines whether the provided hashed string needs to be rehashed\nbased on the algorithm and the number of rounds used during hashing.\n\nArgs:\n    _hashed_string (str): The hashed string to check.\n\nReturns:\n    bool: True if the hashed string needs to be rehashed, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "_get_hexdigest",
                                "docstring": "Generate a hexadecimal digest for a given string using the specified digest function.\n\nArgs:\n    _string (str): The input string to be hashed.\n    digest (Callable): A callable digest function (e.g., hashlib.sha256).\n\nReturns:\n    bytes: The hexadecimal representation of the digest.",
                                "comments": null,
                                "args": [
                                    "_string",
                                    "digest"
                                ]
                            }
                        ],
                        "attributes": [
                            "__slots__"
                        ]
                    },
                    {
                        "name": "BCryptHasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": [
                            "algorithm",
                            "digest"
                        ]
                    }
                ]
            },
            {
                "file": "src/hash_forge/hashers/scrypt_hasher.py",
                "functions": [
                    {
                        "name": "_generate_salt",
                        "docstring": "Generates a base64-encoded salt string.\n\nArgs:\n    salt (int): The number of bytes to generate for the salt.\n\nReturns:\n    str: A base64-encoded string representation of the generated salt.",
                        "comments": null,
                        "args": [
                            "salt"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "ScryptHasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initialize the ScryptHasher with the given parameters.\n\nArgs:\n    work_factor (int): The CPU/memory cost parameter. Default is 2**14.\n    block_size (int): The block size parameter. Default is 8.\n    parallelism (int): The parallelization parameter. Default is 5.\n    maxmem (int): The maximum memory to use in bytes. Default is 0 (no limit).\n    dklen (int): The length of the derived key. Default is 64.\n    salt_length (int): The length of the salt. Default is 16.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "work_factor",
                                    "block_size",
                                    "parallelism",
                                    "maxmem",
                                    "dklen",
                                    "salt_length"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Hashes the given string using the scrypt algorithm.\n\nArgs:\n    _string (str): The input string to be hashed.\n\nReturns:\n    str: The hashed string in the format 'algorithm$work_factor$salt$block_size$parallelism$hashed_value'.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "_string"
                                ]
                            },
                            {
                                "name": "verify",
                                "docstring": "Verify if a given string matches the hashed string.\n\nArgs:\n    _string (str): The original string to verify.\n    _hashed_string (str): The hashed string to compare against.\n\nReturns:\n    bool: True if the original string matches the hashed string, False otherwise.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "_string",
                                    "_hashed_string"
                                ]
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Determines if the given hashed string needs to be rehashed based on the current\nwork factor, block size, and parallelism parameters.\n\nArgs:\n    _hashed_string (str): The hashed string to check, expected to be in the format\n                          \"$<prefix>$<n>$<r>$<p>$<hash>\".\n\nReturns:\n    bool: True if the hashed string needs to be rehashed, False otherwise.",
                                "comments": null,
                                "args": [
                                    "self",
                                    "_hashed_string"
                                ]
                            },
                            {
                                "name": "generate_salt",
                                "docstring": "Generates a cryptographic salt.\n\nReturns:\n    str: A string representing the generated salt.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": [
                            "__slots__"
                        ]
                    }
                ]
            },
            {
                "file": "src/hash_forge/hashers/blake3_hasher.py",
                "functions": [],
                "classes": [
                    {
                        "name": "Blake3Hasher",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": "Initializes the Blake3Hasher with a key and an optional digest size.\n\nArgs:\n    key (str): The key to use for the hash function.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "hash",
                                "docstring": "Hashes the given string using the BLAKE3 algorithm and returns the result in a specific format.\n\nArgs:\n    _string (str): The input string to be hashed.\n\nReturns:\n    str: The hashed string in the format \"algorithm$hashed_hex\".",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "verify",
                                "docstring": "Verify if a given string matches a hashed string using the BLAKE3 algorithm.\n\nArgs:\n    _string (str): The input string to verify.\n    _hashed_string (str): The hashed string to compare against, in the format 'algorithm$hashed_value'.\n\nReturns:\n    bool: True if the input string matches the hashed string, False otherwise.",
                                "comments": null,
                                "args": []
                            },
                            {
                                "name": "needs_rehash",
                                "docstring": "Checks if the hashed string needs to be rehashed based on the digest size.\n\nArgs:\n    _hashed_string (str): The hashed string to check.\n\nReturns:\n    bool: True if the hashed string needs to be rehashed, False otherwise.",
                                "comments": null,
                                "args": []
                            }
                        ],
                        "attributes": []
                    }
                ]
            }
        ],
        "test_cases": {
            "src/tests/test_argon2.py::test_argon2_verify_correct_data": {
                "testid": "src/tests/test_argon2.py::test_argon2_verify_correct_data",
                "result": "passed",
                "test_implementation": "def test_argon2_verify_correct_data(argon2_hasher: Argon2Hasher) -> None:\n    \"\"\"\n    Test the Argon2Hasher's verify method with correct data.\n\n    This test ensures that the `verify` method of the `Argon2Hasher` class\n    returns `True` when provided with the correct data and its corresponding hash.\n\n    Args:\n        argon2_hasher (Argon2Hasher): An instance of the Argon2Hasher class.\n\n    Asserts:\n        The `verify` method should return `True` when the correct data and its hash are provided.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = argon2_hasher.hash(data)\n    assert argon2_hasher.verify(data, hashed) is True, \"Verification should succeed for correct data\""
            },
            "src/tests/test_argon2.py::test_argon2_verify_incorrect_data": {
                "testid": "src/tests/test_argon2.py::test_argon2_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_argon2_verify_incorrect_data(argon2_hasher: Argon2Hasher) -> None:\n    \"\"\"\n    Test the Argon2Hasher's verify method with incorrect data.\n\n    This test ensures that the verify method returns False when provided with\n    data that does not match the original hashed data.\n\n    Args:\n        argon2_hasher (Argon2Hasher): An instance of the Argon2Hasher class.\n\n    Asserts:\n        The verify method should return False when the wrong data is provided.\n    \"\"\"\n    data = \"TestData123!\"\n    wrong_data = \"WrongData456!\"\n    hashed: str = argon2_hasher.hash(data)\n    assert argon2_hasher.verify(wrong_data, hashed) is False, \"Verification should fail for incorrect data\""
            },
            "src/tests/test_argon2.py::test_argon2_needs_rehash_false": {
                "testid": "src/tests/test_argon2.py::test_argon2_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_argon2_needs_rehash_false(argon2_hasher: Argon2Hasher) -> None:\n    \"\"\"\n    Test that the `needs_rehash` method of the `Argon2Hasher` class returns False.\n\n    This test verifies that when data is hashed using the `Argon2Hasher` class,\n    the resulting hash does not require rehashing with the current parameters.\n\n    Args:\n        argon2_hasher (Argon2Hasher): An instance of the Argon2Hasher class.\n\n    Raises:\n        AssertionError: If the `needs_rehash` method returns True, indicating that\n                        the hashed data needs rehashing.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed = argon2_hasher.hash(data)\n    assert argon2_hasher.needs_rehash(hashed) is False, \"Hashed data should not need rehashing with current parameters\""
            },
            "src/tests/test_argon2.py::test_argon2_needs_rehash_true": {
                "testid": "src/tests/test_argon2.py::test_argon2_needs_rehash_true",
                "result": "passed",
                "test_implementation": "def test_argon2_needs_rehash_true(argon2_hasher: Argon2Hasher) -> None:\n    \"\"\"\n    Test if the Argon2Hasher correctly identifies that a hash needs rehashing.\n\n    This test checks whether the `needs_rehash` method of the `Argon2Hasher` class\n    returns `True` when the hash was generated with a lower time cost than the current\n    hasher's configuration.\n\n    Args:\n        argon2_hasher (Argon2Hasher): An instance of Argon2Hasher with the current configuration.\n\n    Raises:\n        AssertionError: If the `needs_rehash` method does not return `True` for a hash\n                        generated with a lower time cost.\n    \"\"\"\n    data = \"TestData123!\"\n    old_hasher = Argon2Hasher(time_cost=1)\n    old_hashed = old_hasher.hash(data)\n    assert (\n        argon2_hasher.needs_rehash(old_hashed) is True\n    ), \"Hashed data should need rehashing due to increased time_cost\""
            },
            "src/tests/test_argon2.py::test_argon2_invalid_hash_format": {
                "testid": "src/tests/test_argon2.py::test_argon2_invalid_hash_format",
                "result": "passed",
                "test_implementation": "def test_argon2_invalid_hash_format(argon2_hasher: Argon2Hasher) -> None:\n    \"\"\"\n    Test the Argon2Hasher's behavior with an invalid hash format.\n\n    This test ensures that the Argon2Hasher correctly identifies and handles\n    an invalid hash format. Specifically, it verifies that:\n    1. The `verify` method returns False when provided with an invalid hash format.\n    2. The `needs_rehash` method returns False when provided with an invalid hash format.\n\n    Args:\n        argon2_hasher (Argon2Hasher): An instance of the Argon2Hasher class.\n\n    Raises:\n        AssertionError: If the `verify` method does not return False for an invalid hash format.\n        AssertionError: If the `needs_rehash` method does not return False for an invalid hash format.\n    \"\"\"\n    data = \"TestData123!\"\n    invalid_hashed = \"invalid$hash$format\"\n    assert argon2_hasher.verify(data, invalid_hashed) is False, \"Verification should fail for invalid hash format\"\n    assert (\n        argon2_hasher.needs_rehash(invalid_hashed) is False\n    ), \"needs_rehash should return False for invalid hash format\""
            },
            "src/tests/test_bcrypt.py::test_bcrypt_hash_format": {
                "testid": "src/tests/test_bcrypt.py::test_bcrypt_hash_format",
                "result": "passed",
                "test_implementation": "def test_bcrypt_hash_format(bcrypt_hasher: BCryptHasher) -> None:\n    \"\"\"\n    Test the format of the hash generated by the BCryptHasher.\n\n    This test ensures that the hash generated by the BCryptHasher\n    follows the expected format. The hash should start with 'bcrypt$'\n    and match the regular expression pattern for a valid bcrypt_sha256 hash.\n\n    Args:\n        bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\n    Raises:\n        AssertionError: If the hash does not start with 'bcrypt$' or\n                        does not match the expected format.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed = bcrypt_hasher.hash(data)\n    pattern = r'^bcrypt\\$2[abxy]\\$\\d{2}\\$[./A-Za-z0-9]{53}$'\n    assert hashed.startswith('bcrypt$'), \"Hash should start with 'bcrypt$'\"\n    assert re.match(pattern, hashed) is not None, \"Hash should match the expected format\""
            },
            "src/tests/test_bcrypt.py::test_bcrypt_verify_correct_data": {
                "testid": "src/tests/test_bcrypt.py::test_bcrypt_verify_correct_data",
                "result": "passed",
                "test_implementation": "def test_bcrypt_verify_correct_data(bcrypt_hasher: BCryptHasher) -> None:\n    \"\"\"\n    Test the BCryptHasher's verify method with correct data.\n\n    This test ensures that the verify method returns True when provided with\n    the correct data that was previously hashed using the BCryptHasher.\n\n    Args:\n        bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\n    Asserts:\n        The verify method should return True when the correct data is provided.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed = bcrypt_hasher.hash(data)\n    assert bcrypt_hasher.verify(data, hashed) is True, \"Verification should succeed for correct data\""
            },
            "src/tests/test_bcrypt.py::test_bcrypt_verify_incorrect_data": {
                "testid": "src/tests/test_bcrypt.py::test_bcrypt_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_bcrypt_verify_incorrect_data(bcrypt_hasher: BCryptHasher) -> None:\n    \"\"\"\n    Test the verification of incorrect data using the BCryptHasher.\n\n    This test ensures that the `verify` method of the `BCryptHasher` class\n    returns `False` when provided with data that does not match the original hashed data.\n\n    Args:\n        bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher class.\n\n    Asserts:\n        The `verify` method should return `False` when the wrong data is provided.\n    \"\"\"\n    data = \"TestData123!\"\n    wrong_data = \"WrongData456!\"\n    hashed = bcrypt_hasher.hash(data)\n    assert bcrypt_hasher.verify(wrong_data, hashed) is False, \"Verification should fail for incorrect data\""
            },
            "src/tests/test_bcrypt.py::test_bcrypt_needs_rehash_false": {
                "testid": "src/tests/test_bcrypt.py::test_bcrypt_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_bcrypt_needs_rehash_false(bcrypt_hasher: BCryptHasher) -> None:\n    \"\"\"\n    Test that the `needs_rehash` method of `BCryptHasher` returns False.\n\n    This test verifies that when a password is hashed using the `BCryptHasher`\n    and the number of rounds matches the expected configuration, the `needs_rehash`\n    method correctly identifies that the hash does not need to be rehashed.\n\n    Args:\n        bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\n    Raises:\n        AssertionError: If the `needs_rehash` method returns True, indicating that\n                        the hash needs rehashing when it should not.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed = bcrypt_hasher.hash(data)\n    assert bcrypt_hasher.needs_rehash(hashed) is False, \"Hash should not need rehashing if rounds match\""
            },
            "src/tests/test_bcrypt.py::test_bcrypt_invalid_hash_format": {
                "testid": "src/tests/test_bcrypt.py::test_bcrypt_invalid_hash_format",
                "result": "passed",
                "test_implementation": "def test_bcrypt_invalid_hash_format(bcrypt_hasher: BCryptHasher) -> None:\n    \"\"\"\n    Test the behavior of the BCryptHasher when provided with an invalid hash format.\n\n    Args:\n        bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\n    Asserts:\n        - The `verify` method should return False when the hash format is invalid.\n        - The `needs_rehash` method should return False when the hash format is invalid.\n    \"\"\"\n    data = \"TestData123!\"\n    invalid_hashed = \"invalid$hash$format\"\n    assert bcrypt_hasher.verify(data, invalid_hashed) is False, \"Verification should fail for invalid hash format\"\n    assert (\n        bcrypt_hasher.needs_rehash(invalid_hashed) is False\n    ), \"needs_rehash should return False for invalid hash format\""
            },
            "src/tests/test_bcrypt.py::test_bcrypt_unknown_algorithm": {
                "testid": "src/tests/test_bcrypt.py::test_bcrypt_unknown_algorithm",
                "result": "passed",
                "test_implementation": "def test_bcrypt_unknown_algorithm(bcrypt_hasher: BCryptHasher) -> None:\n    \"\"\"\n    Test the behavior of BCryptHasher when an unknown algorithm is used in the hashed value.\n\n    Args:\n        bcrypt_hasher (BCryptHasher): An instance of the BCryptHasher.\n\n    Asserts:\n        - The verification should fail when the hashed value uses an unknown algorithm.\n        - The needs_rehash method should return False when the hashed value uses an unknown algorithm.\n    \"\"\"\n    data = \"TestData123!\"\n    unknown_hashed = \"unknown_algo$2b$12$abcdefghijklmnopqrstuv$hashvalue1234567\"\n    assert bcrypt_hasher.verify(data, unknown_hashed) is False, \"Verification should fail for unknown algorithm\"\n    assert bcrypt_hasher.needs_rehash(unknown_hashed) is False, \"needs_rehash should return False for unknown algorithm\""
            },
            "src/tests/test_bcrypt_sha256.py::test_bcrypt_hash_format": {
                "testid": "src/tests/test_bcrypt_sha256.py::test_bcrypt_hash_format",
                "result": "passed",
                "test_implementation": "def test_bcrypt_hash_format(bcrypt_hasher: BCryptSha256Hasher) -> None:\n    \"\"\"\n    Test the format of the hash generated by the BCryptSha256Hasher.\n\n    This test ensures that the hash generated by the BCryptSha256Hasher\n    follows the expected format. The hash should start with 'bcrypt_sha256$'\n    and match the regular expression pattern for a valid bcrypt_sha256 hash.\n\n    Args:\n        bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\n    Raises:\n        AssertionError: If the hash does not start with 'bcrypt_sha256$' or\n                        does not match the expected format.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed = bcrypt_hasher.hash(data)\n    pattern = r'^bcrypt_sha256\\$2[abxy]\\$\\d{2}\\$[./A-Za-z0-9]{53}$'\n    assert hashed.startswith('bcrypt_sha256$'), \"Hash should start with 'bcrypt_sha256$'\"\n    assert re.match(pattern, hashed) is not None, \"Hash should match the expected format\""
            },
            "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_correct_data": {
                "testid": "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_correct_data",
                "result": "passed",
                "test_implementation": "def test_bcrypt_verify_correct_data(bcrypt_hasher: BCryptSha256Hasher) -> None:\n    \"\"\"\n    Test the BCryptSha256Hasher's verify method with correct data.\n\n    This test ensures that the verify method returns True when provided with\n    the correct data that was previously hashed using the BCryptSha256Hasher.\n\n    Args:\n        bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\n    Asserts:\n        The verify method should return True when the correct data is provided.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed = bcrypt_hasher.hash(data)\n    assert bcrypt_hasher.verify(data, hashed) is True, \"Verification should succeed for correct data\""
            },
            "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_incorrect_data": {
                "testid": "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_bcrypt_verify_incorrect_data(bcrypt_hasher: BCryptSha256Hasher) -> None:\n    \"\"\"\n    Test the verification of incorrect data using the BCryptSha256Hasher.\n\n    This test ensures that the `verify` method of the `BCryptSha256Hasher` class\n    returns `False` when provided with data that does not match the original hashed data.\n\n    Args:\n        bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher class.\n\n    Asserts:\n        The `verify` method should return `False` when the wrong data is provided.\n    \"\"\"\n    data = \"TestData123!\"\n    wrong_data = \"WrongData456!\"\n    hashed = bcrypt_hasher.hash(data)\n    assert bcrypt_hasher.verify(wrong_data, hashed) is False, \"Verification should fail for incorrect data\""
            },
            "src/tests/test_bcrypt_sha256.py::test_bcrypt_needs_rehash_false": {
                "testid": "src/tests/test_bcrypt_sha256.py::test_bcrypt_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_bcrypt_needs_rehash_false(bcrypt_hasher: BCryptSha256Hasher) -> None:\n    \"\"\"\n    Test that the `needs_rehash` method of `BCryptSha256Hasher` returns False.\n\n    This test verifies that when a password is hashed using the `BCryptSha256Hasher`\n    and the number of rounds matches the expected configuration, the `needs_rehash`\n    method correctly identifies that the hash does not need to be rehashed.\n\n    Args:\n        bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\n    Raises:\n        AssertionError: If the `needs_rehash` method returns True, indicating that\n                        the hash needs rehashing when it should not.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed = bcrypt_hasher.hash(data)\n    assert bcrypt_hasher.needs_rehash(hashed) is False, \"Hash should not need rehashing if rounds match\""
            },
            "src/tests/test_bcrypt_sha256.py::test_bcrypt_invalid_hash_format": {
                "testid": "src/tests/test_bcrypt_sha256.py::test_bcrypt_invalid_hash_format",
                "result": "passed",
                "test_implementation": "def test_bcrypt_invalid_hash_format(bcrypt_hasher: BCryptSha256Hasher) -> None:\n    \"\"\"\n    Test the behavior of the BCryptSha256Hasher when provided with an invalid hash format.\n\n    Args:\n        bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\n    Asserts:\n        - The `verify` method should return False when the hash format is invalid.\n        - The `needs_rehash` method should return False when the hash format is invalid.\n    \"\"\"\n    data = \"TestData123!\"\n    invalid_hashed = \"invalid$hash$format\"\n    assert bcrypt_hasher.verify(data, invalid_hashed) is False, \"Verification should fail for invalid hash format\"\n    assert (\n        bcrypt_hasher.needs_rehash(invalid_hashed) is False\n    ), \"needs_rehash should return False for invalid hash format\""
            },
            "src/tests/test_bcrypt_sha256.py::test_bcrypt_unknown_algorithm": {
                "testid": "src/tests/test_bcrypt_sha256.py::test_bcrypt_unknown_algorithm",
                "result": "passed",
                "test_implementation": "def test_bcrypt_unknown_algorithm(bcrypt_hasher: BCryptSha256Hasher) -> None:\n    \"\"\"\n    Test the behavior of BCryptSha256Hasher when an unknown algorithm is used in the hashed value.\n\n    Args:\n        bcrypt_hasher (BCryptSha256Hasher): An instance of the BCryptSha256Hasher.\n\n    Asserts:\n        - The verification should fail when the hashed value uses an unknown algorithm.\n        - The needs_rehash method should return False when the hashed value uses an unknown algorithm.\n    \"\"\"\n    data = \"TestData123!\"\n    unknown_hashed = \"unknown_algo$2b$12$abcdefghijklmnopqrstuv$hashvalue1234567\"\n    assert bcrypt_hasher.verify(data, unknown_hashed) is False, \"Verification should fail for unknown algorithm\"\n    assert bcrypt_hasher.needs_rehash(unknown_hashed) is False, \"needs_rehash should return False for unknown algorithm\""
            },
            "src/tests/test_blake2.py::test_hash_creation": {
                "testid": "src/tests/test_blake2.py::test_hash_creation",
                "result": "passed",
                "test_implementation": "def test_hash_creation(blake2_hasher: Blake2Hasher) -> None:\n    \"\"\"\n    Test the creation of a hash using the Blake2Hasher.\n\n    This test verifies that the hashed value of a given string starts with the\n    expected prefix \"blake2b$\" and that the hashed value is correctly formatted\n    with three parts separated by the '$' character.\n\n    Args:\n        blake2_hasher (Blake2Hasher): An instance of the Blake2Hasher class.\n\n    Raises:\n        AssertionError: If the hashed value does not start with \"blake2b$\" or\n                        if the hashed value does not contain exactly three parts\n                        separated by the '$' character.\n    \"\"\"\n    _string = \"example_password\"\n    hashed_value = blake2_hasher.hash(_string)\n\n    assert hashed_value.startswith(\"blake2b$\")\n    assert len(hashed_value.split(\"$\")) == 3"
            },
            "src/tests/test_blake2.py::test_verify_hash_correct": {
                "testid": "src/tests/test_blake2.py::test_verify_hash_correct",
                "result": "passed",
                "test_implementation": "def test_verify_hash_correct(blake2_hasher: Blake2Hasher) -> None:\n    \"\"\"\n    Tests the `verify` method of the `Blake2Hasher` class to ensure that it correctly verifies a hashed value.\n\n    Args:\n        blake2_hasher (Blake2Hasher): An instance of the `Blake2Hasher` class.\n\n    Test Steps:\n    1. Hashes a sample string using the `hash` method of `Blake2Hasher`.\n    2. Verifies that the original string matches the hashed value using the `verify` method.\n\n    Asserts:\n        The `verify` method returns True when the original string matches the hashed value.\n    \"\"\"\n    _string = \"example_password\"\n    hashed_value = blake2_hasher.hash(_string)\n\n    assert blake2_hasher.verify(_string, hashed_value) is True"
            },
            "src/tests/test_blake2.py::test_verify_hash_incorrect": {
                "testid": "src/tests/test_blake2.py::test_verify_hash_incorrect",
                "result": "passed",
                "test_implementation": "def test_verify_hash_incorrect(blake2_hasher: Blake2Hasher) -> None:\n    \"\"\"\n    Test the `verify` method of the `Blake2Hasher` class with an incorrect string.\n\n    This test ensures that the `verify` method returns `False` when provided with\n    a string that does not match the original hashed value.\n\n    Args:\n        blake2_hasher (Blake2Hasher): An instance of the Blake2Hasher class.\n\n    Asserts:\n        The `verify` method returns `False` when the incorrect string is provided.\n    \"\"\"\n    _string = \"example_password\"\n    incorrect_string = \"wrong_password\"\n    hashed_value = blake2_hasher.hash(_string)\n\n    assert blake2_hasher.verify(incorrect_string, hashed_value) is False"
            },
            "src/tests/test_blake2.py::test_needs_rehash_false": {
                "testid": "src/tests/test_blake2.py::test_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_needs_rehash_false(blake2_hasher: Blake2Hasher) -> None:\n    \"\"\"\n    Test that the `needs_rehash` method of the `Blake2Hasher` class returns False\n    for a freshly hashed value.\n\n    Args:\n        blake2_hasher (Blake2Hasher): An instance of the Blake2Hasher class.\n\n    Asserts:\n        The `needs_rehash` method should return False for the given hashed value.\n    \"\"\"\n    _string = \"example_password\"\n    hashed_value = blake2_hasher.hash(_string)\n\n    assert blake2_hasher.needs_rehash(hashed_value) is False"
            },
            "src/tests/test_blake2.py::test_needs_rehash_true": {
                "testid": "src/tests/test_blake2.py::test_needs_rehash_true",
                "result": "passed",
                "test_implementation": "def test_needs_rehash_true() -> None:\n    \"\"\"\n    Test the `needs_rehash` method of the `Blake2Hasher` class.\n\n    This test verifies that the `needs_rehash` method correctly identifies\n    when a hashed value needs to be rehashed due to a different digest size.\n\n    Steps:\n    1. Generate a random key using `secrets.token_urlsafe()`.\n    2. Create two instances of `Blake2Hasher` with the same key but different digest sizes (32 and 64).\n    3. Hash a sample string using the 32-byte hasher.\n    4. Assert that the 64-byte hasher's `needs_rehash` method returns `True` when passed the 32-byte hashed value.\n\n    Expected Result:\n    The `needs_rehash` method should return `True` indicating that the hashed value needs to be rehashed to match the \n    64-byte digest size.\n    \"\"\"\n    key: str = secrets.token_urlsafe()\n    blake2_hasher_32 = Blake2Hasher(key, digest_size=32)\n    blake2_hasher_64 = Blake2Hasher(key, digest_size=64)\n\n    _string = \"example_password\"\n    hashed_value_32 = blake2_hasher_32.hash(_string)\n\n    assert blake2_hasher_64.needs_rehash(hashed_value_32) is True"
            },
            "src/tests/test_blake2.py::test_hash_with_key": {
                "testid": "src/tests/test_blake2.py::test_hash_with_key",
                "result": "passed",
                "test_implementation": "def test_hash_with_key() -> None:\n    \"\"\"\n    Test the Blake2Hasher class with a key.\n\n    This test generates a random key using `secrets.token_urlsafe()`,\n    creates an instance of `Blake2Hasher` with the generated key,\n    and hashes a sample string. It then verifies that the hashed\n    value matches the original string and does not match an incorrect string.\n\n    Assertions:\n        - The hashed value should be verified as True for the correct string.\n        - The hashed value should be verified as False for an incorrect string.\n    \"\"\"\n    key: str = secrets.token_urlsafe()\n    blake2_hasher_with_key = Blake2Hasher(key=key)\n\n    _string = \"example_password\"\n    hashed_value = blake2_hasher_with_key.hash(_string)\n\n    assert blake2_hasher_with_key.verify(_string, hashed_value) is True\n    assert blake2_hasher_with_key.verify(\"wrong_password\", hashed_value) is False"
            },
            "src/tests/test_blake2.py::test_verify_fails_on_modified_hash": {
                "testid": "src/tests/test_blake2.py::test_verify_fails_on_modified_hash",
                "result": "passed",
                "test_implementation": "def test_verify_fails_on_modified_hash(blake2_hasher: Blake2Hasher) -> None:\n    \"\"\"\n    Test that the `verify` method of `Blake2Hasher` fails when the hash has been modified.\n    This test ensures that the `verify` method returns `False` when the hash of a given\n    string is altered. It first hashes a sample string, then modifies the resulting hash\n    by replacing the first occurrence of the character 'a' with 'b'. Finally, it verifies\n    that the `verify` method correctly identifies the modified hash as invalid.\n    Args:\n        blake2_hasher (Blake2Hasher): An instance of the Blake2Hasher class.\n    Returns:\n        None\n    \"\"\"\n    _string = \"example_password\"\n    hashed_value = blake2_hasher.hash(_string)\n    modified_hash = hashed_value.replace(\"a\", \"b\", 1)\n\n    assert blake2_hasher.verify(_string, modified_hash) is False"
            },
            "src/tests/test_blake3.py::test_hash_creation": {
                "testid": "src/tests/test_blake3.py::test_hash_creation",
                "result": "passed",
                "test_implementation": "def test_hash_creation(blake3_hasher: Blake3Hasher) -> None:\n    \"\"\"\n    Test the creation of a hash using the Blake3Hasher.\n\n    This test verifies that the hashed value of a given string starts with the\n    expected prefix \"blake2b$\" and that the hashed value is correctly formatted\n    with three parts separated by the '$' character.\n\n    Args:\n        blake3_hasher (Blake3Hasher): An instance of the Blake3Hasher class.\n\n    Raises:\n        AssertionError: If the hashed value does not start with \"blake2b$\" or\n                        if the hashed value does not contain exactly three parts\n                        separated by the '$' character.\n    \"\"\"\n    _string = \"example_password\"\n    hashed_value = blake3_hasher.hash(_string)\n\n    assert hashed_value.startswith(\"blake3$\")\n    assert len(hashed_value.split(\"$\")) == 2"
            },
            "src/tests/test_blake3.py::test_verify_hash_correct": {
                "testid": "src/tests/test_blake3.py::test_verify_hash_correct",
                "result": "passed",
                "test_implementation": "def test_verify_hash_correct(blake3_hasher: Blake3Hasher) -> None:\n    \"\"\"\n    Tests the `verify` method of the `Blake3Hasher` class to ensure that it correctly verifies a hashed value.\n\n    Args:\n        blake3_hasher (Blake3Hasher): An instance of the `Blake3Hasher` class.\n\n    Test Steps:\n    1. Hashes a sample string using the `hash` method of `Blake3Hasher`.\n    2. Verifies that the original string matches the hashed value using the `verify` method.\n\n    Asserts:\n        The `verify` method returns True when the original string matches the hashed value.\n    \"\"\"\n    _string = \"example_password\"\n    hashed_value = blake3_hasher.hash(_string)\n\n    assert blake3_hasher.verify(_string, hashed_value) is True"
            },
            "src/tests/test_blake3.py::test_verify_hash_incorrect": {
                "testid": "src/tests/test_blake3.py::test_verify_hash_incorrect",
                "result": "passed",
                "test_implementation": "def test_verify_hash_incorrect(blake3_hasher: Blake3Hasher) -> None:\n    \"\"\"\n    Test the `verify` method of the `Blake3Hasher` class with an incorrect string.\n\n    This test ensures that the `verify` method returns `False` when provided with\n    a string that does not match the original hashed value.\n\n    Args:\n        blake3_hasher (Blake3Hasher): An instance of the Blake3Hasher class.\n\n    Asserts:\n        The `verify` method returns `False` when the incorrect string is provided.\n    \"\"\"\n    _string = \"example_password\"\n    incorrect_string = \"wrong_password\"\n    hashed_value = blake3_hasher.hash(_string)\n\n    assert blake3_hasher.verify(incorrect_string, hashed_value) is False"
            },
            "src/tests/test_blake3.py::test_hash_with_key": {
                "testid": "src/tests/test_blake3.py::test_hash_with_key",
                "result": "passed",
                "test_implementation": "def test_hash_with_key() -> None:\n    \"\"\"\n    Test the Blake3Hasher class with a key.\n\n    This test generates a random key using `secrets.token_urlsafe()`,\n    creates an instance of `Blake3Hasher` with the generated key,\n    and hashes a sample string. It then verifies that the hashed\n    value matches the original string and does not match an incorrect string.\n\n    Assertions:\n        - The hashed value should be verified as True for the correct string.\n        - The hashed value should be verified as False for an incorrect string.\n    \"\"\"\n    blake3_hasher_with_key = Blake3Hasher()\n\n    _string = \"example_password\"\n    hashed_value = blake3_hasher_with_key.hash(_string)\n\n    assert blake3_hasher_with_key.verify(_string, hashed_value) is True\n    assert blake3_hasher_with_key.verify(\"wrong_password\", hashed_value) is False"
            },
            "src/tests/test_blake3.py::test_verify_fails_on_modified_hash": {
                "testid": "src/tests/test_blake3.py::test_verify_fails_on_modified_hash",
                "result": "passed",
                "test_implementation": "def test_verify_fails_on_modified_hash(blake3_hasher: Blake3Hasher) -> None:\n    \"\"\"\n    Test that the `verify` method of `Blake3Hasher` fails when the hash has been modified.\n    This test ensures that the `verify` method returns `False` when the hash of a given\n    string is altered. It first hashes a sample string, then modifies the resulting hash\n    by replacing the first occurrence of the character 'a' with 'b'. Finally, it verifies\n    that the `verify` method correctly identifies the modified hash as invalid.\n    Args:\n        blake3_hasher (Blake3Hasher): An instance of the Blake3Hasher class.\n    Returns:\n        None\n    \"\"\"\n    _string = \"example_password\"\n    hashed_value = blake3_hasher.hash(_string)\n    modified_hash = hashed_value.replace(\"a\", \"b\", 1)\n\n    assert blake3_hasher.verify(_string, modified_hash) is False"
            },
            "src/tests/test_hash_manager.py::test_hash_with_preferred_hasher": {
                "testid": "src/tests/test_hash_manager.py::test_hash_with_preferred_hasher",
                "result": "passed",
                "test_implementation": "def test_hash_with_preferred_hasher(hash_manager_instance: HashManager) -> None:\n    \"\"\"\n    Test the hashing functionality using the preferred hasher.\n\n    This test verifies that the `hash` method of the `hash_manager_instance`\n    produces a hash that starts with the expected prefix for the preferred\n    hasher (`pbkdf2_sha256$150000$`). It also checks that the `verify` method\n    correctly validates the original data against the generated hash.\n\n    Args:\n        hash_manager_instance: An instance of the HashManager class configured\n                               with the preferred hasher.\n\n    Asserts:\n        - The generated hash starts with 'pbkdf2_sha256$150000$'.\n        - The `verify` method returns True when verifying the original data\n          against the generated hash.\n    \"\"\"\n    hashed: str = hash_manager_instance.hash(\"HashManagerTestData!\")\n    assert hashed.startswith(\"pbkdf2_sha256$150000$\")\n    assert hash_manager_instance.verify(\"HashManagerTestData!\", hashed) is True"
            },
            "src/tests/test_hash_manager.py::test_verify_with_preferred_hasher": {
                "testid": "src/tests/test_hash_manager.py::test_verify_with_preferred_hasher",
                "result": "passed",
                "test_implementation": "def test_verify_with_preferred_hasher(hash_manager_instance: HashManager) -> None:\n    \"\"\"\n    Test the `verify` method of `HashManager` using the preferred hasher.\n\n    This test ensures that the `verify` method correctly verifies a hashed value\n    generated by the `hash` method when using the preferred hasher.\n\n    Args:\n        hash_manager_instance (HashManager): An instance of the `HashManager` class.\n\n    Asserts:\n        The `verify` method returns True when verifying a hashed value generated\n        from the same input data.\n    \"\"\"\n    hashed: str = hash_manager_instance.hash(\"HashManagerTestData!\")\n    assert hash_manager_instance.verify(\"HashManagerTestData!\", hashed) is True"
            },
            "src/tests/test_hash_manager.py::test_verify_incorrect_data": {
                "testid": "src/tests/test_hash_manager.py::test_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_verify_incorrect_data(hash_manager_instance: HashManager) -> None:\n    \"\"\"\n    Test the `verify` method of `HashManager` with incorrect data.\n\n    This test ensures that the `verify` method returns `False` when provided\n    with data that does not match the original hashed data.\n\n    Args:\n        hash_manager_instance (HashManager): An instance of the `HashManager` class.\n\n    Asserts:\n        The `verify` method returns `False` when the input data does not match\n        the hashed data.\n    \"\"\"\n    hashed: str = hash_manager_instance.hash(\"HashManagerTestData!\")\n    assert hash_manager_instance.verify(\"IncorrectData\", hashed) is False"
            },
            "src/tests/test_hash_manager.py::test_needs_rehash_false": {
                "testid": "src/tests/test_hash_manager.py::test_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_needs_rehash_false(hash_manager_instance: HashManager) -> None:\n    \"\"\"\n    Test that the `needs_rehash` method of `HashManager` returns False for a freshly hashed password.\n\n    Args:\n        hash_manager_instance (HashManager): An instance of the `HashManager` class.\n\n    Asserts:\n        The `needs_rehash` method returns False for a newly hashed password.\n    \"\"\"\n    hashed: str = hash_manager_instance.hash(\"HashManagerTestData!\")\n    assert hash_manager_instance.needs_rehash(hashed) is False"
            },
            "src/tests/test_hash_manager.py::test_needs_rehash_true_due_to_iterations": {
                "testid": "src/tests/test_hash_manager.py::test_needs_rehash_true_due_to_iterations",
                "result": "passed",
                "test_implementation": "def test_needs_rehash_true_due_to_iterations(hash_manager_instance: HashManager) -> None:\n    \"\"\"\n    Test if the `needs_rehash` method returns True when the number of iterations\n    used in the hash is lower than the current standard.\n\n    This test creates a hash using an old PBKDF2 hasher with fewer iterations\n    and checks if the `needs_rehash` method correctly identifies that the hash\n    needs to be rehashed due to the insufficient number of iterations.\n\n    Args:\n        hash_manager_instance (HashManager): An instance of the HashManager class.\n    \"\"\"\n    old_pbkdf2_hasher = PBKDF2Sha256Hasher(iterations=100_000)\n    old_hashed = old_pbkdf2_hasher.hash(\"HashManagerTestData!\")\n    assert hash_manager_instance.needs_rehash(old_hashed) is True"
            },
            "src/tests/test_hash_manager.py::test_invalid_hashed_data_format": {
                "testid": "src/tests/test_hash_manager.py::test_invalid_hashed_data_format",
                "result": "passed",
                "test_implementation": "def test_invalid_hashed_data_format(hash_manager_instance: HashManager) -> None:\n    \"\"\"\n    Test the behavior of HashManager when provided with an invalid hashed data format.\n\n    This test ensures that:\n    1. The `verify` method returns False when given an invalid hash format.\n    2. The `needs_rehash` method returns True when given an invalid hash format.\n\n    Args:\n        hash_manager_instance (HashManager): An instance of the HashManager class.\n    \"\"\"\n    assert hash_manager_instance.verify(\"HashManagerTestData!\", \"invalid$hash$format\") is False\n    assert hash_manager_instance.needs_rehash(\"invalid$hash$format\") is True"
            },
            "src/tests/test_hash_manager.py::test_unknown_algorithm": {
                "testid": "src/tests/test_hash_manager.py::test_unknown_algorithm",
                "result": "passed",
                "test_implementation": "def test_unknown_algorithm(hash_manager_instance: HashManager) -> None:\n    \"\"\"\n    Test the behavior of HashManager when an unknown algorithm is encountered.\n\n    Args:\n        hash_manager_instance (HashManager): An instance of the HashManager class.\n\n    Asserts:\n        - The `verify` method should return False when provided with a hash string\n          that uses an unknown algorithm.\n        - The `needs_rehash` method should return True for the hash string with an\n          unknown algorithm.\n    \"\"\"\n    unknown_hashed = \"unknown_algo$12345$salt$hash\"\n    assert hash_manager_instance.verify(\"HashManagerTestData!\", unknown_hashed) is False\n    assert hash_manager_instance.needs_rehash(unknown_hashed) is True"
            },
            "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_hash_format": {
                "testid": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_hash_format",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_hash_format(pbkdf2_hasher: PBKDF2Sha1Hasher) -> None:\n    \"\"\"\n    Test the format of the PBKDF2 hashed string.\n\n    This test ensures that the hashed string generated by the PBKDF2Sha1Hasher\n    follows the expected format. The format is expected to be:\n    'pbkdf2_sha256$iterations$salt$hash'.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\n    Assertions:\n        - The hashed string should be split into 4 parts using the '$' delimiter.\n        - The first part should be 'pbkdf2_sha256'.\n        - The second part should be the number of iterations, which should be 100,000.\n    \"\"\"\n    hashed: str = pbkdf2_hasher.hash(\"TestData123!\")\n    parts: list[str] = hashed.split('$')\n    assert len(parts) == 4\n    assert parts[0] == 'pbkdf2_sha1'\n    assert int(parts[1]) == 100_000"
            },
            "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_correct_data": {
                "testid": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_correct_data",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_verify_correct_data(pbkdf2_hasher: PBKDF2Sha1Hasher) -> None:\n    \"\"\"\n    Test the PBKDF2 hasher's ability to verify correct data.\n\n    This test ensures that the PBKDF2Sha1Hasher can correctly hash a given\n    data string and subsequently verify that the hashed value matches\n    the original data.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\n    Asserts:\n        The verification of the hashed data against the original data\n        returns True.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = pbkdf2_hasher.hash(data)\n    assert pbkdf2_hasher.verify(data, hashed) is True"
            },
            "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_incorrect_data": {
                "testid": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_verify_incorrect_data(pbkdf2_hasher: PBKDF2Sha1Hasher) -> None:\n    \"\"\"\n    Test the PBKDF2Sha1Hasher's verify method with incorrect data.\n\n    This test ensures that the verify method returns False when provided\n    with data that does not match the original hashed data.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\n    Asserts:\n        The verify method should return False when the provided data does not\n        match the hashed data.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = pbkdf2_hasher.hash(data)\n    assert pbkdf2_hasher.verify(\"WrongData\", hashed) is False"
            },
            "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_false": {
                "testid": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_needs_rehash_false(pbkdf2_hasher: PBKDF2Sha1Hasher) -> None:\n    \"\"\"\n    Test that the PBKDF2Sha1Hasher does not require rehashing for a freshly hashed password.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\n    Asserts:\n        The hashed password does not need rehashing.\n    \"\"\"\n    hashed: str = pbkdf2_hasher.hash(\"TestData123!\")\n    assert pbkdf2_hasher.needs_rehash(hashed) is False"
            },
            "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_true": {
                "testid": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_true",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_needs_rehash_true(pbkdf2_hasher: PBKDF2Sha1Hasher) -> None:\n    \"\"\"\n    Test if the PBKDF2 hasher correctly identifies when a hashed password needs rehashing.\n\n    This test creates an instance of PBKDF2Sha1Hasher with a lower iteration count (50,000)\n    to simulate an outdated hash. It then hashes a sample password and checks if the\n    current PBKDF2Sha1Hasher instance (with presumably higher iteration count) correctly\n    identifies that the old hash needs rehashing.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha1Hasher): The PBKDF2 hasher instance to test against.\n\n    Asserts:\n        True if the `needs_rehash` method correctly identifies that the old hash\n        needs rehashing.\n    \"\"\"\n    old_hasher = PBKDF2Sha1Hasher(iterations=50_000)\n    old_hashed: str = old_hasher.hash(\"TestData123!\")\n    assert pbkdf2_hasher.needs_rehash(old_hashed) is True"
            },
            "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_invalid_hash_format": {
                "testid": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_invalid_hash_format",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_invalid_hash_format(pbkdf2_hasher: PBKDF2Sha1Hasher) -> None:\n    \"\"\"\n    Test the PBKDF2 hasher's verify method with an invalid hash format.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha1Hasher): An instance of the PBKDF2Sha1Hasher class.\n\n    Asserts:\n        The verify method should return False when provided with an invalid hash format.\n    \"\"\"\n    assert pbkdf2_hasher.verify(\"TestData123!\", \"invalid$hash$format\") is False"
            },
            "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_hash_format": {
                "testid": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_hash_format",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_hash_format(pbkdf2_hasher: PBKDF2Sha256Hasher) -> None:\n    \"\"\"\n    Test the format of the PBKDF2 hashed string.\n\n    This test ensures that the hashed string generated by the PBKDF2Sha256Hasher\n    follows the expected format. The format is expected to be:\n    'pbkdf2_sha256$iterations$salt$hash'.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\n    Assertions:\n        - The hashed string should be split into 4 parts using the '$' delimiter.\n        - The first part should be 'pbkdf2_sha256'.\n        - The second part should be the number of iterations, which should be 100,000.\n    \"\"\"\n    hashed: str = pbkdf2_hasher.hash(\"TestData123!\")\n    parts: list[str] = hashed.split('$')\n    assert len(parts) == 4\n    assert parts[0] == 'pbkdf2_sha256'\n    assert int(parts[1]) == 100_000"
            },
            "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_correct_data": {
                "testid": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_correct_data",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_verify_correct_data(pbkdf2_hasher: PBKDF2Sha256Hasher) -> None:\n    \"\"\"\n    Test the PBKDF2 hasher's ability to verify correct data.\n\n    This test ensures that the PBKDF2Sha256Hasher can correctly hash a given\n    data string and subsequently verify that the hashed value matches\n    the original data.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\n    Asserts:\n        The verification of the hashed data against the original data\n        returns True.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = pbkdf2_hasher.hash(data)\n    assert pbkdf2_hasher.verify(data, hashed) is True"
            },
            "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_incorrect_data": {
                "testid": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_verify_incorrect_data(pbkdf2_hasher: PBKDF2Sha256Hasher) -> None:\n    \"\"\"\n    Test the PBKDF2Sha256Hasher's verify method with incorrect data.\n\n    This test ensures that the verify method returns False when provided\n    with data that does not match the original hashed data.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\n    Asserts:\n        The verify method should return False when the provided data does not\n        match the hashed data.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = pbkdf2_hasher.hash(data)\n    assert pbkdf2_hasher.verify(\"WrongData\", hashed) is False"
            },
            "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_false": {
                "testid": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_needs_rehash_false(pbkdf2_hasher: PBKDF2Sha256Hasher) -> None:\n    \"\"\"\n    Test that the PBKDF2Sha256Hasher does not require rehashing for a freshly hashed password.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\n    Asserts:\n        The hashed password does not need rehashing.\n    \"\"\"\n    hashed: str = pbkdf2_hasher.hash(\"TestData123!\")\n    assert pbkdf2_hasher.needs_rehash(hashed) is False"
            },
            "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_true": {
                "testid": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_true",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_needs_rehash_true(pbkdf2_hasher: PBKDF2Sha256Hasher) -> None:\n    \"\"\"\n    Test if the PBKDF2 hasher correctly identifies when a hashed password needs rehashing.\n\n    This test creates an instance of PBKDF2Sha256Hasher with a lower iteration count (50,000)\n    to simulate an outdated hash. It then hashes a sample password and checks if the\n    current PBKDF2Sha256Hasher instance (with presumably higher iteration count) correctly\n    identifies that the old hash needs rehashing.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha256Hasher): The PBKDF2 hasher instance to test against.\n\n    Asserts:\n        True if the `needs_rehash` method correctly identifies that the old hash\n        needs rehashing.\n    \"\"\"\n    old_hasher = PBKDF2Sha256Hasher(iterations=50_000)\n    old_hashed: str = old_hasher.hash(\"TestData123!\")\n    assert pbkdf2_hasher.needs_rehash(old_hashed) is True"
            },
            "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_invalid_hash_format": {
                "testid": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_invalid_hash_format",
                "result": "passed",
                "test_implementation": "def test_pbkdf2_invalid_hash_format(pbkdf2_hasher: PBKDF2Sha256Hasher) -> None:\n    \"\"\"\n    Test the PBKDF2 hasher's verify method with an invalid hash format.\n\n    Args:\n        pbkdf2_hasher (PBKDF2Sha256Hasher): An instance of the PBKDF2Sha256Hasher class.\n\n    Asserts:\n        The verify method should return False when provided with an invalid hash format.\n    \"\"\"\n    assert pbkdf2_hasher.verify(\"TestData123!\", \"invalid$hash$format\") is False"
            },
            "src/tests/test_ripemd160.py::test_ripemd160_hash_format": {
                "testid": "src/tests/test_ripemd160.py::test_ripemd160_hash_format",
                "result": "passed",
                "test_implementation": "def test_ripemd160_hash_format(ripemd160_hasher: Ripemd160Hasher) -> None:\n    \"\"\"\n    Test the format of the RIPEMD-160 hashed string.\n\n    This test ensures that the hashed string generated by the Ripemd160Hasher\n    follows the expected format. The format is expected to be:\n    'RIPEMD-160$hashed_value'.\n\n    Args:\n        ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\n    Assertions:\n        - The hashed string should be split into 2 parts using the '$' delimiter.\n        - The first part should be 'RIPEMD-160'.\n    \"\"\"\n    hashed: str = ripemd160_hasher.hash(\"TestData123!\")\n    parts: list[str] = hashed.split('$')\n    assert len(parts) == 2, \"Hash format is incorrect; expected 2 parts separated by '$'.\"\n    assert parts[0] == 'RIPEMD-160', \"Algorithm name in hash does not match 'RIPEMD-160'.\""
            },
            "src/tests/test_ripemd160.py::test_ripemd160_verify_correct_data": {
                "testid": "src/tests/test_ripemd160.py::test_ripemd160_verify_correct_data",
                "result": "passed",
                "test_implementation": "def test_ripemd160_verify_correct_data(ripemd160_hasher: Ripemd160Hasher) -> None:\n    \"\"\"\n    Test the Ripemd160Hasher's ability to verify correct data.\n\n    This test ensures that the Ripemd160Hasher can correctly hash a given\n    data string and subsequently verify that the hashed value matches\n    the original data.\n\n    Args:\n        ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\n    Asserts:\n        The verification of the hashed data against the original data\n        returns True.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = ripemd160_hasher.hash(data)\n    assert ripemd160_hasher.verify(data, hashed) is True, \"Verification failed for correct data.\""
            },
            "src/tests/test_ripemd160.py::test_ripemd160_verify_incorrect_data": {
                "testid": "src/tests/test_ripemd160.py::test_ripemd160_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_ripemd160_verify_incorrect_data(ripemd160_hasher: Ripemd160Hasher) -> None:\n    \"\"\"\n    Test the Ripemd160Hasher's verify method with incorrect data.\n\n    This test ensures that the verify method returns False when provided\n    with data that does not match the original hashed data.\n\n    Args:\n        ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\n    Asserts:\n        The verify method should return False when the provided data does not\n        match the hashed data.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = ripemd160_hasher.hash(data)\n    assert ripemd160_hasher.verify(\"WrongData\", hashed) is False, \"Verification incorrectly succeeded for wrong data.\""
            },
            "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_false": {
                "testid": "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_ripemd160_needs_rehash_false(ripemd160_hasher: Ripemd160Hasher) -> None:\n    \"\"\"\n    Test that the Ripemd160Hasher does not require rehashing for a freshly hashed password.\n\n    Args:\n        ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\n    Asserts:\n        The hashed password does not need rehashing.\n    \"\"\"\n    hashed: str = ripemd160_hasher.hash(\"TestData123!\")\n    assert ripemd160_hasher.needs_rehash(hashed) is False, \"needs_rehash incorrectly returned True for fresh hash.\""
            },
            "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_true": {
                "testid": "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_true",
                "result": "passed",
                "test_implementation": "def test_ripemd160_needs_rehash_true(ripemd160_hasher: Ripemd160Hasher) -> None:\n    \"\"\"\n    Test if the Ripemd160Hasher correctly identifies when a hashed password needs rehashing.\n\n    This test simulates an outdated hash by modifying the algorithm name in the hashed string.\n    It then checks if the current Ripemd160Hasher instance correctly identifies that the old hash needs rehashing.\n\n    Args:\n        ripemd160_hasher (Ripemd160Hasher): The Ripemd160Hasher instance to test against.\n\n    Asserts:\n        True if the `needs_rehash` method correctly identifies that the old hash\n        needs rehashing.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = ripemd160_hasher.hash(data)\n    # Simulate an outdated hash by changing the algorithm name\n    old_hashed = hashed.replace('RIPEMD-160$', 'oldalgo$', 1)\n    assert ripemd160_hasher.needs_rehash(old_hashed) is True, \"needs_rehash failed to identify outdated hash.\""
            },
            "src/tests/test_ripemd160.py::test_ripemd160_invalid_hash_format": {
                "testid": "src/tests/test_ripemd160.py::test_ripemd160_invalid_hash_format",
                "result": "passed",
                "test_implementation": "def test_ripemd160_invalid_hash_format(ripemd160_hasher: Ripemd160Hasher) -> None:\n    \"\"\"\n    Test the Ripemd160Hasher's verify method with an invalid hash format.\n\n    Args:\n        ripemd160_hasher (Ripemd160Hasher): An instance of the Ripemd160Hasher class.\n\n    Asserts:\n        The verify method should return False when provided with an invalid hash format.\n    \"\"\"\n    assert (\n        ripemd160_hasher.verify(\"TestData123!\", \"invalid$hash$format\") is False\n    ), \"Verification did not fail for invalid hash format.\""
            },
            "src/tests/test_scrypt.py::test_scrypt_hash_format": {
                "testid": "src/tests/test_scrypt.py::test_scrypt_hash_format",
                "result": "passed",
                "test_implementation": "def test_scrypt_hash_format(scrypt_hasher: ScryptHasher) -> None:\n    \"\"\"\n    Test the format of the Scrypt hashed string.\n\n    This test ensures that the hashed string generated by the ScryptHasher\n    follows the expected format. The format is expected to be:\n    'scrypt$work_factor$salt$block_size$parallelism$hash'.\n\n    Args:\n        scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\n    Assertions:\n        - The hashed string should be split into 6 parts using the '$' delimiter.\n        - The first part should be 'scrypt'.\n    \"\"\"\n    hashed: str = scrypt_hasher.hash(\"TestData123!\")\n    parts: list[str] = hashed.split('$')\n    assert len(parts) == 6\n    assert parts[0] == 'scrypt'"
            },
            "src/tests/test_scrypt.py::test_scrypt_verify_correct_data": {
                "testid": "src/tests/test_scrypt.py::test_scrypt_verify_correct_data",
                "result": "passed",
                "test_implementation": "def test_scrypt_verify_correct_data(scrypt_hasher: ScryptHasher) -> None:\n    \"\"\"\n    Test the Scrypt hasher's ability to verify correct data.\n\n    This test ensures that the ScryptHasher can correctly hash a given\n    data string and subsequently verify that the hashed value matches\n    the original data.\n\n    Args:\n        scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\n    Asserts:\n        The verification of the hashed data against the original data\n        returns True.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = scrypt_hasher.hash(data)\n    assert scrypt_hasher.verify(data, hashed) is True"
            },
            "src/tests/test_scrypt.py::test_scrypt_verify_incorrect_data": {
                "testid": "src/tests/test_scrypt.py::test_scrypt_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_scrypt_verify_incorrect_data(scrypt_hasher: ScryptHasher) -> None:\n    \"\"\"\n    Test the ScryptHasher's verify method with incorrect data.\n\n    This test ensures that the verify method returns False when provided\n    with data that does not match the original hashed data.\n\n    Args:\n        scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\n    Asserts:\n        The verify method should return False when the provided data does not\n        match the hashed data.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = scrypt_hasher.hash(data)\n    assert scrypt_hasher.verify(\"WrongData\", hashed) is False"
            },
            "src/tests/test_scrypt.py::test_scrypt_needs_rehash_false": {
                "testid": "src/tests/test_scrypt.py::test_scrypt_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_scrypt_needs_rehash_false(scrypt_hasher: ScryptHasher) -> None:\n    \"\"\"\n    Test that the ScryptHasher does not require rehashing for a freshly hashed password.\n\n    Args:\n        scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\n    Asserts:\n        The hashed password does not need rehashing.\n    \"\"\"\n    hashed: str = scrypt_hasher.hash(\"TestData123!\")\n    assert scrypt_hasher.needs_rehash(hashed) is False"
            },
            "src/tests/test_scrypt.py::test_scrypt_needs_rehash_true": {
                "testid": "src/tests/test_scrypt.py::test_scrypt_needs_rehash_true",
                "result": "passed",
                "test_implementation": "def test_scrypt_needs_rehash_true(scrypt_hasher: ScryptHasher) -> None:\n    \"\"\"\n    Test if the Scrypt hasher correctly identifies when a hashed password needs rehashing.\n\n    This test creates an instance of ScryptHasher with different work factor or block size\n    to simulate an outdated hash. It then hashes a sample password and checks if the\n    current ScryptHasher instance correctly identifies that the old hash needs rehashing.\n\n    Args:\n        scrypt_hasher (ScryptHasher): The Scrypt hasher instance to test against.\n\n    Asserts:\n        True if the `needs_rehash` method correctly identifies that the old hash\n        needs rehashing.\n    \"\"\"\n    old_hasher = ScryptHasher(work_factor=2**10)\n    old_hashed: str = old_hasher.hash(\"TestData123!\")\n    assert scrypt_hasher.needs_rehash(old_hashed) is True"
            },
            "src/tests/test_scrypt.py::test_scrypt_invalid_hash_format": {
                "testid": "src/tests/test_scrypt.py::test_scrypt_invalid_hash_format",
                "result": "passed",
                "test_implementation": "def test_scrypt_invalid_hash_format(scrypt_hasher: ScryptHasher) -> None:\n    \"\"\"\n    Test the Scrypt hasher's verify method with an invalid hash format.\n\n    Args:\n        scrypt_hasher (ScryptHasher): An instance of the ScryptHasher class.\n\n    Asserts:\n        The verify method should return False when provided with an invalid hash format.\n    \"\"\"\n    assert scrypt_hasher.verify(\"TestData123!\", \"invalid$hash$format\") is False"
            },
            "src/tests/test_whirlpool.py::test_whirlpool_hash_format": {
                "testid": "src/tests/test_whirlpool.py::test_whirlpool_hash_format",
                "result": "passed",
                "test_implementation": "def test_whirlpool_hash_format(whirlpool_hasher: WhirlpoolHasher) -> None:\n    \"\"\"\n    Test the format of the Whirlpool hashed string.\n\n    This test ensures that the hashed string generated by the WhirlpoolHasher\n    follows the expected format. The format is expected to be:\n    'whirlpool$hash'.\n\n    Args:\n        whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\n    Assertions:\n        - The hashed string should be split into 2 parts using the '$' delimiter.\n        - The first part should be 'whirlpool'.\n    \"\"\"\n    hashed: str = whirlpool_hasher.hash(\"TestData123!\")\n    parts: list[str] = hashed.split(\"$\")\n    assert len(parts) == 2, \"Hash format is incorrect; expected 2 parts separated by '$'.\"\n    assert parts[0] == \"whirlpool\", \"Algorithm name in hash does not match 'whirlpool'.\""
            },
            "src/tests/test_whirlpool.py::test_whirlpool_verify_correct_data": {
                "testid": "src/tests/test_whirlpool.py::test_whirlpool_verify_correct_data",
                "result": "passed",
                "test_implementation": "def test_whirlpool_verify_correct_data(whirlpool_hasher: WhirlpoolHasher) -> None:\n    \"\"\"\n    Test the Whirlpool hasher's ability to verify correct data.\n\n    This test ensures that the WhirlpoolHasher can correctly hash a given\n    data string and subsequently verify that the hashed value matches\n    the original data.\n\n    Args:\n        whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\n    Asserts:\n        The verification of the hashed data against the original data\n        returns True.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = whirlpool_hasher.hash(data)\n    assert whirlpool_hasher.verify(data, hashed) is True, \"Verification failed for correct data.\""
            },
            "src/tests/test_whirlpool.py::test_whirlpool_verify_incorrect_data": {
                "testid": "src/tests/test_whirlpool.py::test_whirlpool_verify_incorrect_data",
                "result": "passed",
                "test_implementation": "def test_whirlpool_verify_incorrect_data(whirlpool_hasher: WhirlpoolHasher) -> None:\n    \"\"\"\n    Test the WhirlpoolHasher's verify method with incorrect data.\n\n    This test ensures that the verify method returns False when provided\n    with data that does not match the original hashed data.\n\n    Args:\n        whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\n    Asserts:\n        The verify method should return False when the provided data does not\n        match the hashed data.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = whirlpool_hasher.hash(data)\n    assert whirlpool_hasher.verify(\"WrongData\", hashed) is False, \"Verification incorrectly succeeded for wrong data.\""
            },
            "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_false": {
                "testid": "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_false",
                "result": "passed",
                "test_implementation": "def test_whirlpool_needs_rehash_false(whirlpool_hasher: WhirlpoolHasher) -> None:\n    \"\"\"\n    Test that the WhirlpoolHasher does not require rehashing for a freshly hashed password.\n\n    Args:\n        whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\n    Asserts:\n        The hashed password does not need rehashing.\n    \"\"\"\n    hashed: str = whirlpool_hasher.hash(\"TestData123!\")\n    assert whirlpool_hasher.needs_rehash(hashed) is False, \"needs_rehash incorrectly returned True for fresh hash.\""
            },
            "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_true": {
                "testid": "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_true",
                "result": "passed",
                "test_implementation": "def test_whirlpool_needs_rehash_true(whirlpool_hasher: WhirlpoolHasher) -> None:\n    \"\"\"\n    Test if the Whirlpool hasher correctly identifies when a hashed password needs rehashing.\n\n    This test simulates an outdated hash by modifying the algorithm name in the hashed string.\n    It then checks if the current WhirlpoolHasher instance correctly identifies that the old hash needs rehashing.\n\n    Args:\n        whirlpool_hasher (WhirlpoolHasher): The Whirlpool hasher instance to test against.\n\n    Asserts:\n        True if the `needs_rehash` method correctly identifies that the old hash\n        needs rehashing.\n    \"\"\"\n    data = \"TestData123!\"\n    hashed: str = whirlpool_hasher.hash(data)\n    # Simulate an outdated hash by changing the algorithm name\n    old_hashed = hashed.replace(\"whirlpool$\", \"oldalgo$\", 1)\n    assert whirlpool_hasher.needs_rehash(old_hashed) is True, \"needs_rehash failed to identify outdated hash.\""
            },
            "src/tests/test_whirlpool.py::test_whirlpool_invalid_hash_format": {
                "testid": "src/tests/test_whirlpool.py::test_whirlpool_invalid_hash_format",
                "result": "passed",
                "test_implementation": "def test_whirlpool_invalid_hash_format(whirlpool_hasher: WhirlpoolHasher) -> None:\n    \"\"\"\n    Test the Whirlpool hasher's verify method with an invalid hash format.\n\n    Args:\n        whirlpool_hasher (WhirlpoolHasher): An instance of the WhirlpoolHasher class.\n\n    Asserts:\n        The verify method should return False when provided with an invalid hash format.\n    \"\"\"\n    assert (\n        whirlpool_hasher.verify(\"TestData123!\", \"invalid$hash$format\") is False\n    ), \"Verification did not fail for invalid hash format.\""
            }
        },
        "SRS_document": "**Software Requirements Specification: Hash Forge Library**\n\n**Table of Contents:**\n1.  Introduction\n    1.1 Purpose\n    1.2 Scope\n    1.3 Definitions, Acronyms, and Abbreviations\n    1.4 References\n    1.5 Overview\n2.  Overall Description\n    2.1 Product Perspective\n    2.2 Product Functions\n    2.3 User Characteristics\n    2.4 Constraints\n    2.5 Assumptions and Dependencies\n3.  Specific Requirements\n    3.1 Functional Requirements\n        3.1.1 Hash Manager (`HashManager`)\n        3.1.2 Common Hasher Capabilities (Interface)\n        3.1.3 Dynamic Library Loading for Hashers\n        3.1.4 PBKDF2-SHA256 Hasher\n        3.1.5 PBKDF2-SHA1 Hasher\n        3.1.6 bcrypt Hasher (Standard)\n        3.1.7 bcrypt-SHA256 Hasher\n        3.1.8 Argon2 Hasher\n        3.1.9 Scrypt Hasher\n        3.1.10 Blake2b Hasher\n        3.1.11 Blake3 Hasher\n        3.1.12 Whirlpool Hasher\n        3.1.13 RIPEMD-160 Hasher\n    3.2 External Interface Requirements\n        3.2.1 Hashed String Formats\n    3.3 Non-Functional Requirements\n\n---\n\n## 1. Introduction\n\n### 1.1 Purpose\nThis Software Requirements Specification (SRS) document defines the requirements for the Hash Forge library. Its primary goal is to serve as the sole specification for software developers who will implement the library. Developer assessment will be based on their ability to create a functional system, matching all original test cases (public and private), using only this SRS and a subset of public test cases for verification during development.\n\nThis document must be exceptionally clear, unambiguous, functionally comprehensive, and appropriately abstracted to allow for independent design choices while ensuring all specified functionalities are met.\n\n### 1.2 Scope\nThe Hash Forge library is a Python utility designed to:\n*   Provide a unified interface for hashing strings using various cryptographic hash algorithms.\n*   Verify input strings against previously generated hash strings.\n*   Determine if existing hash strings need to be rehashed due to outdated parameters or algorithms.\n*   Support a collection of specific, common hashing algorithms including PBKDF2, bcrypt, Argon2, Scrypt, Blake2b, Blake3, Whirlpool, and RIPEMD-160.\n\n### 1.3 Definitions, Acronyms, and Abbreviations\n*   **SRS:** Software Requirements Specification\n*   **FR:** Functional Requirement\n*   **EIR:** External Interface Requirement\n*   **NFR:** Non-Functional Requirement\n*   **Hasher:** A component or class responsible for implementing a specific hash algorithm's logic (hashing, verification, rehash checking).\n*   **Hash String:** The string output produced by a hasher, typically including an algorithm identifier, parameters, salt (if applicable), and the actual hash value.\n*   **Preferred Hasher:** The hasher designated by the `HashManager` to be used for new hashing operations.\n*   **PBKDF2:** Password-Based Key Derivation Function 2\n*   **HMAC:** Hash-based Message Authentication Code\n*   **SHA:** Secure Hash Algorithm\n\n### 1.4 References\n*   Original README.md for Hash Forge (provided as context)\n*   Original source code of Hash Forge (provided as context)\n*   Original test suite for Hash Forge (provided as context)\n\n### 1.5 Overview\nThis SRS is organized into three main sections:\n*   **Section 1 (Introduction):** Provides context for the SRS document.\n*   **Section 2 (Overall Description):** Gives a high-level overview of the Hash Forge library, its functions, users, and constraints.\n*   **Section 3 (Specific Requirements):** Details all functional requirements, external interface requirements (notably hashed string formats), and non-functional requirements (if any, based on direct testability). This section is critical for development.\n\n---\n\n## 2. Overall Description\n\n### 2.1 Product Perspective\nHash Forge is a Python library intended to be integrated into other Python applications requiring secure password hashing or data integrity checks. It acts as a facade over various underlying hashing algorithm implementations.\n\n### 2.2 Product Functions\nThe Hash Forge library will provide the following core functionalities:\n*   **Hash Management:** A central manager (`HashManager`) to orchestrate hashing, verification, and rehash checks using multiple configured hashers.\n*   **String Hashing:** Generation of hash strings from input strings using a configured (preferred) hashing algorithm.\n*   **Hash Verification:** Comparison of an input string against a stored hash string to determine if they match. The system automatically detects the algorithm used for the stored hash.\n*   **Rehash Detection:** Assessment of whether a stored hash string should be rehashed (e.g., due to changes in algorithm parameters like iteration counts or if a different, stronger algorithm is now preferred).\n*   **Multi-Algorithm Support:** Support for a defined set of hashing algorithms, each with its own specific parameters and behaviors.\n*   **Extensibility (Implied):** The design suggests an interface (protocol) that individual hashers adhere to, allowing for potential future addition of new hashers (though adding new hashers is outside the scope of *this* SRS's implementation task).\n\n### 2.3 User Characteristics\nThe primary users of the Hash Forge library are Python developers building applications that require functionalities such as:\n*   Storing and verifying user passwords securely.\n*   Ensuring data integrity through hashing.\n*   Migrating from older hashing schemes to newer ones by detecting and rehashing.\n\n### 2.4 Constraints\n*   The system shall be implemented as a Python library.\n*   Hashed string formats for each algorithm must conform to the specifications in Section 3.2.1 to ensure interoperability and correct parsing by the `HashManager`.\n*   The system may rely on third-party Python packages for specific hashing algorithms (e.g., `bcrypt` for bcrypt, `argon2-cffi` for Argon2). The loading mechanism for these is described in FR-HASHER-LIB-LOAD.\n\n### 2.5 Assumptions and Dependencies\n*   Developers using the library are responsible for installing any necessary optional dependencies required by the specific hashers they choose to use (e.g., `pip install \"hash-forge[bcrypt]\"`). The library itself will indicate if a required backend library is missing.\n*   The underlying cryptographic primitives and libraries are assumed to be correctly implemented and secure. Hash Forge is responsible for their correct application as per the specified hashing schemes.\n\n---\n\n## 3. Specific Requirements\n\n### 3.1 Functional Requirements\n\n#### 3.1.1 Hash Manager (`HashManager`)\n\n*   **FR-HM-INIT:** The system shall allow initialization of a Hash Manager with one or more hasher instances.\n    *   If no hashers are provided during initialization, the system shall raise a `ValueError`.\n\n*   **FR-HM-PREFERRED-HASHER:** The Hash Manager shall designate the first hasher (in the order provided during initialization) as the \"preferred hasher\".\n\n*   **FR-HM-HASH:** The Hash Manager shall provide a method to hash an input string using its designated preferred hasher.\n    *   Input: plain text string.\n    *   Output: A hash string, formatted according to the preferred hasher's specification (see Section 3.2.1).\n\n*   **FR-HM-VERIFY:** The Hash Manager shall provide a method to verify an input string against a given hashed string.\n    *   It must automatically identify the hashing algorithm used for the `hashed_string` by inspecting its prefix.\n    *   It must then delegate the verification to the corresponding configured hasher instance.\n    *   Input: plain text string, hashed string.\n    *   Output: `True` if the string matches the hashed string according to the identified hasher, `False` otherwise.\n\n*   **FR-HM-VERIFY-UNKNOWN-ALGO:** If the algorithm prefix of the `hashed_string` provided to the Hash Manager's verify method does not match any of the configured hashers' algorithms, verification shall fail (return `False`).\n\n*   **FR-HM-VERIFY-INVALID-FORMAT:** If the `hashed_string` provided to the Hash Manager's verify method is malformed (e.g., its prefix identifies a known hasher, but the remainder of the string cannot be parsed by that hasher), verification shall fail (return `False`).\n\n*   **FR-HM-NEEDS-REHASH:** The Hash Manager shall provide a method to determine if a given hashed string needs to be rehashed.\n    *   It must automatically identify the hashing algorithm from the prefix of the `hashed_string`.\n    *   It must then delegate the rehash check to the corresponding configured hasher's `needs_rehash` logic.\n    *   Input: hashed string.\n    *   Output: `True` if the identified hasher determines a rehash is needed, `False` otherwise.\n\n*   **FR-HM-NEEDS-REHASH-UNKNOWN-ALGO:** If the algorithm prefix of the `hashed_string` provided to the Hash Manager's `needs_rehash` method does not match any of the configured hashers' algorithms, it shall be considered as needing a rehash (return `True`).\n\n*   **FR-HM-NEEDS-REHASH-INVALID-FORMAT:** If the `hashed_string` provided to the Hash Manager's `needs_rehash` method has an algorithm prefix that does not match any configured hasher, it shall be considered as needing a rehash (return `True`). If the prefix matches a known hasher but the rest of the string is malformed preventing the specific hasher from parsing its parameters, the result is delegated to the specific hasher (which typically returns `False` as per FR-HASHER-NEEDS-REHASH-INVALID-FORMAT).\n\n#### 3.1.2 Common Hasher Capabilities (Interface)\nEach specific hasher implementation must provide the following capabilities:\n\n*   **FR-HASHER-ALGORITHM-ID:** Each hasher shall expose a unique string identifier for its algorithm (e.g., \"pbkdf2_sha256\", \"bcrypt\"). This identifier is used as the prefix in generated hash strings.\n\n*   **FR-HASHER-INTERFACE-HASH:** Each hasher shall provide a mechanism to hash an input string.\n    *   Input: plain text string.\n    *   Output: An algorithm-specific formatted hash string (see Section 3.2.1 for formats).\n\n*   **FR-HASHER-INTERFACE-VERIFY:** Each hasher shall provide a mechanism to verify a plain text string against one of its algorithm-specific formatted hash strings.\n    *   Input: plain text string, hashed string.\n    *   Output: `True` if the string matches the hash, `False` otherwise.\n    *   Verification must fail (return `False`) if the algorithm identifier in the `hashed_string` does not match the hasher's own algorithm identifier.\n    *   Verification must fail (return `False`) if the `hashed_string` is malformed (e.g., missing parts, incorrect parameter types).\n\n*   **FR-HASHER-INTERFACE-NEEDS-REHASH:** Each hasher shall provide a mechanism to determine if a given algorithm-specific formatted hash string needs to be rehashed based on its current configuration parameters (e.g., iteration count, cost factors, algorithm version).\n    *   Input: hashed string.\n    *   Output: `True` if a rehash is needed, `False` otherwise.\n    *   The check should generally return `False` if the algorithm identifier in the `hashed_string` does not match the hasher's own (unless the hasher is designed to specifically trigger rehash for \"foreign\" but related hashes, which is not the case here).\n\n*   **FR-HASHER-NEEDS-REHASH-INVALID-FORMAT:** For a hasher-specific `needs_rehash` check, if the provided `_hashed_string` starts with the correct algorithm identifier for that hasher but is otherwise malformed (e.g., parameter components are missing or invalid such that they cannot be parsed), the hasher shall indicate that a rehash is not needed (return `False`).\n\n#### 3.1.3 Dynamic Library Loading for Hashers\n\n*   **FR-HASHER-LIB-LOAD:** For hashers that depend on external third-party libraries (e.g., Argon2, bcrypt, Whirlpool, RIPEMD-160, Blake3), the system shall attempt to dynamically load the required library module upon hasher initialization.\n    *   If the required library module is not found (i.e., not installed in the Python environment), the system shall raise an `ImportError`.\n    *   The error message for the `ImportError` should clearly indicate the name of the missing dependency and suggest how to install it (e.g., \"bcrypt is not installed. Please install bcrypt to use this hasher. (pip install hash-forge[bcrypt])\").\n\n#### 3.1.4 PBKDF2-SHA256 Hasher\nAlgorithm Identifier: `pbkdf2_sha256`\n\n*   **FR-PBKDF2_SHA256-INIT:** The system shall allow initialization of a PBKDF2-SHA256 hasher with configurable `iterations` (default: 100,000) and `salt_length` in bytes (default: 16).\n\n*   **FR-PBKDF2_SHA256-HASH:** The PBKDF2-SHA256 hasher shall produce a hash string for a given input string using PBKDF2 with HMAC-SHA256. The format is defined in EIR-PBKDF2_SHA256-FORMAT.\n    *   The salt used shall be randomly generated and of the configured `salt_length`.\n\n*   **FR-PBKDF2_SHA256-VERIFY:** The PBKDF2-SHA256 hasher shall verify an input string against a hash string (format EIR-PBKDF2_SHA256-FORMAT).\n    *   It returns `True` for a correct string/hash pair.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed or uses a different algorithm ID.\n\n*   **FR-PBKDF2_SHA256-NEEDS-REHASH:** The PBKDF2-SHA256 hasher shall determine if a rehash is needed.\n    *   Rehash is not needed (`False`) if the `iterations` in the hash string matches the hasher's current `iterations` configuration.\n    *   Rehash is needed (`True`) if the `iterations` in the hash string differs from the hasher's current `iterations` configuration.\n\n#### 3.1.5 PBKDF2-SHA1 Hasher\nAlgorithm Identifier: `pbkdf2_sha1`\n\n*   **FR-PBKDF2_SHA1-INIT:** The system shall allow initialization of a PBKDF2-SHA1 hasher with configurable `iterations` (default: 100,000) and `salt_length` in bytes (default: 16).\n\n*   **FR-PBKDF2_SHA1-HASH:** The PBKDF2-SHA1 hasher shall produce a hash string for a given input string using PBKDF2 with HMAC-SHA1. The format is defined in EIR-PBKDF2_SHA1-FORMAT.\n    *   The salt used shall be randomly generated and of the configured `salt_length`.\n\n*   **FR-PBKDF2_SHA1-VERIFY:** The PBKDF2-SHA1 hasher shall verify an input string against a hash string (format EIR-PBKDF2_SHA1-FORMAT).\n    *   It returns `True` for a correct string/hash pair.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed or uses a different algorithm ID.\n\n*   **FR-PBKDF2_SHA1-NEEDS-REHASH:** The PBKDF2-SHA1 hasher shall determine if a rehash is needed.\n    *   Rehash is not needed (`False`) if the `iterations` in the hash string matches the hasher's current `iterations` configuration.\n    *   Rehash is needed (`True`) if the `iterations` in the hash string differs from the hasher's current `iterations` configuration.\n\n#### 3.1.6 bcrypt Hasher (Standard)\nAlgorithm Identifier: `bcrypt`\n\n*   **FR-BCRYPT-INIT:** The system shall allow initialization of a bcrypt (standard) hasher with a configurable number of `rounds` (default: 12).\n\n*   **FR-BCRYPT-HASH:** The bcrypt (standard) hasher shall produce a hash string for a given input string. The format is defined in EIR-BCRYPT-FORMAT.\n    *   The input string (byte-encoded) is used directly with the underlying bcrypt algorithm.\n\n*   **FR-BCRYPT-VERIFY:** The bcrypt (standard) hasher shall verify an input string against a hash string (format EIR-BCRYPT-FORMAT).\n    *   It returns `True` for a correct string/hash pair.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed or uses a different algorithm ID.\n\n*   **FR-BCRYPT-NEEDS-REHASH:** The bcrypt (standard) hasher shall determine if a rehash is needed.\n    *   Rehash is not needed (`False`) if the `rounds` embedded in the hash string matches the hasher's current `rounds` configuration.\n    *   Rehash is needed (`True`) if the `rounds` in the hash string differs (implicitly, as bcrypt rounds are part of its standard output, though the test doesn't vary rounds for this specific hasher, this is standard bcrypt behavior and covered by the base class logic tested in `test_bcrypt_sha256.py::test_bcrypt_needs_rehash_true_for_rounds_mismatch` if rounds were changed). For the standard `BCryptHasher`, the provided tests only confirm `needs_rehash` is `False` for a fresh hash. The rehash based on rounds logic is from the parent.\n\n#### 3.1.7 bcrypt-SHA256 Hasher\nAlgorithm Identifier: `bcrypt_sha256`\n\n*   **FR-BCRYPT_SHA256-INIT:** The system shall allow initialization of a bcrypt-SHA256 hasher with a configurable number of `rounds` (default: 12).\n\n*   **FR-BCRYPT_SHA256-PREHASH:** The bcrypt-SHA256 hasher shall first compute the SHA256 hash of the input string (UTF-8 encoded), then take the hexadecimal representation of this SHA256 digest (as bytes), and use this hex representation as the input to the underlying bcrypt algorithm.\n\n*   **FR-BCRYPT_SHA256-HASH:** The bcrypt-SHA256 hasher shall produce a hash string. The format is defined in EIR-BCRYPT_SHA256-FORMAT.\n\n*   **FR-BCRYPT_SHA256-VERIFY:** The bcrypt-SHA256 hasher shall verify an input string against a hash string (format EIR-BCRYPT_SHA256-FORMAT), applying the same pre-hashing logic (FR-BCRYPT_SHA256-PREHASH) to the input string before bcrypt comparison.\n    *   It returns `True` for a correct string/hash pair.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed or uses a different algorithm ID.\n\n*   **FR-BCRYPT_SHA256-NEEDS-REHASH:** The bcrypt-SHA256 hasher shall determine if a rehash is needed.\n    *   Rehash is not needed (`False`) if the `rounds` embedded in the hash string matches the hasher's current `rounds` configuration.\n    *   Rehash is needed (`True`) if the `rounds` in the hash string differs from the hasher's current `rounds` configuration. (Note: test for this is missing in `test_bcrypt_sha256.py` but logic is in code).\n\n#### 3.1.8 Argon2 Hasher\nAlgorithm Identifier: `argon2` (Note: full Argon2 hash string includes type, e.g., `$argon2id$`)\n\n*   **FR-ARGON2-INIT:** The system shall allow initialization of an Argon2 hasher. It can be configured with optional Argon2 parameters: `time_cost`, `memory_cost`, `parallelism`, `hash_len`, `salt_len`. If not specified, the defaults of the underlying `argon2-cffi` library's `PasswordHasher` are used.\n\n*   **FR-ARGON2-HASH:** The Argon2 hasher shall produce a hash string for a given input string. The format is defined in EIR-ARGON2-FORMAT.\n\n*   **FR-ARGON2-VERIFY:** The Argon2 hasher shall verify an input string against a hash string (format EIR-ARGON2-FORMAT).\n    *   It returns `True` for a correct string/hash pair.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed (e.g., invalid for Argon2 library) or uses an algorithm identifier that doesn't start with `argon2`.\n\n*   **FR-ARGON2-NEEDS-REHASH:** The Argon2 hasher shall determine if a rehash is needed based on whether the parameters in the hash string (e.g., `time_cost`, `memory_cost`, `parallelism`, Argon2 type/version) are considered outdated by the underlying Argon2 library's `check_needs_rehash` method relative to the hasher's current configuration.\n    *   Rehash is not needed (`False`) if parameters are current.\n    *   Rehash is needed (`True`) if parameters are outdated (e.g., `time_cost` in hash is lower than current hasher's `time_cost`).\n\n#### 3.1.9 Scrypt Hasher\nAlgorithm Identifier: `scrypt`\n\n*   **FR-SCRYPT-INIT:** The system shall allow initialization of a Scrypt hasher with configurable parameters: `work_factor` (N, default: 2<sup>14</sup>), `block_size` (r, default: 8), `parallelism` (p, default: 5), `maxmem` (default: 0, unlimited), `dklen` (derived key length, default: 64 bytes), `salt_length` (default: 16 bytes).\n\n*   **FR-SCRYPT-HASH:** The Scrypt hasher shall produce a hash string for a given input string. The format is defined in EIR-SCRYPT-FORMAT.\n    *   The salt used shall be randomly generated to the configured `salt_length` and Base64 encoded.\n    *   The derived key (hash) shall be Base64 encoded.\n\n*   **FR-SCRYPT-VERIFY:** The Scrypt hasher shall verify an input string against a provided hashed string (format EIR-SCRYPT-FORMAT).\n    *   Verification is performed by re-hashing the input string using the hasher's current configuration parameters (`work_factor`, `block_size`, `parallelism`, etc.) and salt generation, then performing a constant-time comparison of this newly generated hash string against the provided `_hashed_string`.\n    *   It returns `True` if the provided `_hashed_string` exactly matches the newly generated hash.\n    *   It returns `False` otherwise (including if parameters differ, input string differs, or `_hashed_string` is malformed).\n\n*   **FR-SCRYPT-NEEDS-REHASH:** The Scrypt hasher shall determine if a rehash is needed by comparing the `work_factor` (N), `block_size` (r), and `parallelism` (p) values embedded in the hash string with its current configuration.\n    *   Rehash is not needed (`False`) if N, r, and p in the hash string match the hasher's current configuration.\n    *   Rehash is needed (`True`) if any of N, r, or p in the hash string differ from the hasher's current configuration.\n\n#### 3.1.10 Blake2b Hasher\nAlgorithm Identifier: `blake2b`\n\n*   **FR-BLAKE2B-INIT:** The system shall allow initialization of a Blake2b hasher with a mandatory `key` (string) and a configurable `digest_size` in bytes (default: 64).\n\n*   **FR-BLAKE2B-HASH:** The Blake2b hasher shall produce a hash string for a given input string using the configured `key` and `digest_size`. The format is defined in EIR-BLAKE2B-FORMAT.\n\n*   **FR-BLAKE2B-VERIFY:** The Blake2b hasher shall verify an input string against a hash string (format EIR-BLAKE2B-FORMAT), using the hasher's configured `key` and comparing the `digest_size` from the hash string.\n    *   It returns `True` for a correct string/hash pair if key and digest size match.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed, uses a different algorithm ID, or its embedded `digest_size` does not match the hasher's `digest_size`.\n\n*   **FR-BLAKE2B-NEEDS-REHASH:** The Blake2b hasher shall determine if a rehash is needed by comparing the `digest_size` embedded in the hash string with its current `digest_size` configuration. (The key is not part of the hash string, so rehash based on key change is not detectable this way).\n    *   Rehash is not needed (`False`) if the `digest_size` in the hash string matches the hasher's current configuration.\n    *   Rehash is needed (`True`) if the `digest_size` in the hash string differs.\n\n#### 3.1.11 Blake3 Hasher\nAlgorithm Identifier: `blake3`\n\n*   **FR-BLAKE3-INIT:** The system shall allow initialization of a Blake3 hasher. This implementation uses an unkeyed Blake3 hash.\n\n*   **FR-BLAKE3-HASH:** The Blake3 hasher shall produce a hash string for a given input string using unkeyed Blake3. The format is defined in EIR-BLAKE3-FORMAT.\n\n*   **FR-BLAKE3-VERIFY:** The Blake3 hasher shall verify an input string against a hash string (format EIR-BLAKE3-FORMAT).\n    *   It returns `True` for a correct string/hash pair.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed or uses a different algorithm ID.\n\n*   **FR-BLAKE3-NEEDS-REHASH:** The Blake3 hasher shall determine if a rehash is needed. Since this implementation uses unkeyed Blake3 with no configurable parameters affecting the hash output structure (beyond the algorithm itself), a rehash is generally only indicated if the algorithm identifier in the hash string does not match `blake3`.\n    *   Rehash is not needed (`False`) if the algorithm in the hash string is `blake3`.\n    *   Rehash is needed (`True`) if the algorithm in the hash string is not `blake3`.\n\n#### 3.1.12 Whirlpool Hasher\nAlgorithm Identifier: `whirlpool`\n\n*   **FR-WHIRLPOOL-INIT:** The system shall allow initialization of a Whirlpool hasher. (Note: The provided source uses SHA512 as an implementation basis for Whirlpool, but identifies as `whirlpool`).\n\n*   **FR-WHIRLPOOL-HASH:** The Whirlpool hasher shall produce a hash string for a given input string. The format is defined in EIR-WHIRLPOOL-FORMAT. (Internally, this might use SHA512, but the external representation is `whirlpool`).\n\n*   **FR-WHIRLPOOL-VERIFY:** The Whirlpool hasher shall verify an input string against a hash string (format EIR-WHIRLPOOL-FORMAT).\n    *   It returns `True` for a correct string/hash pair.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed or uses a different algorithm ID.\n\n*   **FR-WHIRLPOOL-NEEDS-REHASH:** The Whirlpool hasher shall determine if a rehash is needed.\n    *   Rehash is not needed (`False`) if the algorithm identifier in the hash string is `whirlpool`.\n    *   Rehash is needed (`True`) if the algorithm identifier in the hash string is not `whirlpool`.\n\n#### 3.1.13 RIPEMD-160 Hasher\nAlgorithm Identifier: `RIPEMD-160`\n\n*   **FR-RIPEMD160-INIT:** The system shall allow initialization of a RIPEMD-160 hasher.\n\n*   **FR-RIPEMD160-HASH:** The RIPEMD-160 hasher shall produce a hash string for a given input string. The format is defined in EIR-RIPEMD160-FORMAT.\n\n*   **FR-RIPEMD160-VERIFY:** The RIPEMD-160 hasher shall verify an input string against a hash string (format EIR-RIPEMD160-FORMAT).\n    *   It returns `True` for a correct string/hash pair.\n    *   It returns `False` for an incorrect string.\n    *   It returns `False` if the hash string is malformed or uses a different algorithm ID.\n\n*   **FR-RIPEMD160-NEEDS-REHASH:** The RIPEMD-160 hasher shall determine if a rehash is needed.\n    *   Rehash is not needed (`False`) if the algorithm identifier in the hash string is `RIPEMD-160`.\n    *   Rehash is needed (`True`) if the algorithm identifier in the hash string is not `RIPEMD-160`.\n\n### 3.2 External Interface Requirements\n\n#### 3.2.1 Hashed String Formats\nThe following formats define the structure of hash strings produced and consumed by the system. The `$` character is used as a delimiter. All textual parts (algorithm identifier, salt, hash value) are ASCII or compatible (e.g., hex, Base64).\n\n*   **EIR-PBKDF2_SHA256-FORMAT:** Hashed strings generated by the PBKDF2-SHA256 hasher SHALL adhere to the format:\n    `pbkdf2_sha256$<iterations>$<salt_hex>$<hash_hex>`\n    *   `<iterations>`: Integer number of iterations.\n    *   `<salt_hex>`: Hexadecimal representation of the salt.\n    *   `<hash_hex>`: Hexadecimal representation of the derived key.\n\n*   **EIR-PBKDF2_SHA1-FORMAT:** Hashed strings generated by the PBKDF2-SHA1 hasher SHALL adhere to the format:\n    `pbkdf2_sha1$<iterations>$<salt_hex>$<hash_hex>`\n    *   `<iterations>`: Integer number of iterations.\n    *   `<salt_hex>`: Hexadecimal representation of the salt.\n    *   `<hash_hex>`: Hexadecimal representation of the derived key.\n\n*   **EIR-BCRYPT-FORMAT:** Hashed strings generated by the bcrypt (standard) hasher SHALL start with `bcrypt$` followed by the standard bcrypt hash representation (e.g., `$2b$12$...`).\n    *   The full format SHALL match the regular expression: `^bcrypt\\$2[abxy]\\$\\d{2}\\$[./A-Za-z0-9]{53}$`\n\n*   **EIR-BCRYPT_SHA256-FORMAT:** Hashed strings generated by the bcrypt-SHA256 hasher SHALL start with `bcrypt_sha256$` followed by the standard bcrypt hash representation (e.g., `$2b$12$...`).\n    *   The full format SHALL match the regular expression: `^bcrypt_sha256\\$2[abxy]\\$\\d{2}\\$[./A-Za-z0-9]{53}$`\n\n*   **EIR-ARGON2-FORMAT:** Hashed strings generated by the Argon2 hasher SHALL start with `argon2` followed by the standard Argon2 encoded hash string (e.g., `$argon2id$v=19$m=...,t=...,p=...$<salt_b64>$<hash_b64>`).\n    *   The exact format of the suffix is determined by the underlying Argon2 library. The prefix for identification is `argon2`.\n\n*   **EIR-SCRYPT-FORMAT:** Hashed strings generated by the Scrypt hasher SHALL adhere to the format:\n    `scrypt$<work_factor>$<salt_b64>$<block_size>$<parallelism>$<hash_b64>`\n    *   `<work_factor>` (N): Integer CPU/memory cost parameter.\n    *   `<salt_b64>`: Base64 representation of the salt.\n    *   `<block_size>` (r): Integer block size parameter.\n    *   `<parallelism>` (p): Integer parallelization parameter.\n    *   `<hash_b64>`: Base64 representation of the derived key.\n\n*   **EIR-BLAKE2B-FORMAT:** Hashed strings generated by the Blake2b hasher SHALL adhere to the format:\n    `blake2b$<digest_size>$<hash_hex>`\n    *   `<digest_size>`: Integer digest size in bytes.\n    *   `<hash_hex>`: Hexadecimal representation of the hash.\n\n*   **EIR-BLAKE3-FORMAT:** Hashed strings generated by the Blake3 hasher SHALL adhere to the format:\n    `blake3$<hash_hex>`\n    *   `<hash_hex>`: Hexadecimal representation of the hash.\n\n*   **EIR-WHIRLPOOL-FORMAT:** Hashed strings generated by the Whirlpool hasher SHALL adhere to the format:\n    `whirlpool$<hash_hex>`\n    *   `<hash_hex>`: Hexadecimal representation of the hash.\n\n*   **EIR-RIPEMD160-FORMAT:** Hashed strings generated by the RIPEMD-160 hasher SHALL adhere to the format:\n    `RIPEMD-160$<hash_hex>`\n    *   `<hash_hex>`: Hexadecimal representation of the hash.\n\n### 3.3 Non-Functional Requirements\nNo specific non-functional requirements (e.g., performance, specific security compliance levels beyond algorithm choice, usability of the API) are mandated by explicitly corresponding original test cases. The primary NFR is correctness of the hashing and verification logic as per the functional requirements and standard cryptographic behaviors of the chosen algorithms.",
        "structured_requirements": [
            {
                "requirement_id": "FR-HM-INIT",
                "requirement_description": "The system shall allow initialization of a Hash Manager with one or more hasher instances.\n*   If no hashers are provided during initialization, the system shall raise a `ValueError`.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::hash_manager_instance",
                        "description": "(for successful initialization)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HM-PREFERRED-HASHER",
                "requirement_description": "The Hash Manager shall designate the first hasher (in the order provided during initialization) as the \"preferred hasher\".",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::test_hash_with_preferred_hasher",
                        "description": "(verified by checking the output format of the hash)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HM-HASH",
                "requirement_description": "The Hash Manager shall provide a method to hash an input string using its designated preferred hasher.\n*   Input: plain text string.\n*   Output: A hash string, formatted according to the preferred hasher's specification (see Section 3.2.1).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::test_hash_with_preferred_hasher",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HM-VERIFY",
                "requirement_description": "The Hash Manager shall provide a method to verify an input string against a given hashed string.\n*   It must automatically identify the hashing algorithm used for the `hashed_string` by inspecting its prefix.\n*   It must then delegate the verification to the corresponding configured hasher instance.\n*   Input: plain text string, hashed string.\n*   Output: `True` if the string matches the hashed string according to the identified hasher, `False` otherwise.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::test_verify_with_preferred_hasher",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_hash_manager.py::test_verify_incorrect_data",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::verify",
                        "description": ""
                    },
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::_get_hasher_by_hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HM-VERIFY-UNKNOWN-ALGO",
                "requirement_description": "If the algorithm prefix of the `hashed_string` provided to the Hash Manager's verify method does not match any of the configured hashers' algorithms, verification shall fail (return `False`).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::test_unknown_algorithm",
                        "description": "(verify part)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::verify",
                        "description": ""
                    },
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::_get_hasher_by_hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HM-VERIFY-INVALID-FORMAT",
                "requirement_description": "If the `hashed_string` provided to the Hash Manager's verify method is malformed (e.g., its prefix identifies a known hasher, but the remainder of the string cannot be parsed by that hasher), verification shall fail (return `False`).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::test_invalid_hashed_data_format",
                        "description": "(verify part)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HM-NEEDS-REHASH",
                "requirement_description": "The Hash Manager shall provide a method to determine if a given hashed string needs to be rehashed.\n*   It must automatically identify the hashing algorithm from the prefix of the `hashed_string`.\n*   It must then delegate the rehash check to the corresponding configured hasher's `needs_rehash` logic.\n*   Input: hashed string.\n*   Output: `True` if the identified hasher determines a rehash is needed, `False` otherwise.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::test_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_hash_manager.py::test_needs_rehash_true_due_to_iterations",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::needs_rehash",
                        "description": ""
                    },
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::_get_hasher_by_hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HM-NEEDS-REHASH-UNKNOWN-ALGO",
                "requirement_description": "If the algorithm prefix of the `hashed_string` provided to the Hash Manager's `needs_rehash` method does not match any of the configured hashers' algorithms, it shall be considered as needing a rehash (return `True`).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::test_unknown_algorithm",
                        "description": "(rehash part)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HM-NEEDS-REHASH-INVALID-FORMAT",
                "requirement_description": "If the `hashed_string` provided to the Hash Manager's `needs_rehash` method has an algorithm prefix that does not match any configured hasher, it shall be considered as needing a rehash (return `True`). If the prefix matches a known hasher but the rest of the string is malformed preventing the specific hasher from parsing its parameters, the result is delegated to the specific hasher (which typically returns `False` as per FR-HASHER-NEEDS-REHASH-INVALID-FORMAT).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_hash_manager.py::test_invalid_hashed_data_format",
                        "description": "(rehash part, covers the \"unknown prefix\" case)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/__init__.py::HashManager::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-HASHER-ALGORITHM-ID",
                "requirement_description": "Each hasher shall expose a unique string identifier for its algorithm (e.g., \"pbkdf2_sha256\", \"bcrypt\"). This identifier is used as the prefix in generated hash strings.",
                "test_traceability": [
                    {
                        "id": "All test_*_hash_format tests",
                        "description": "(implicitly, by checking the prefix)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/protocols.py::PHasher::algorithm",
                        "description": "(and all concrete hasher implementations)"
                    }
                ]
            },
            {
                "requirement_id": "FR-HASHER-INTERFACE-HASH",
                "requirement_description": "Each hasher shall provide a mechanism to hash an input string.\n*   Input: plain text string.\n*   Output: An algorithm-specific formatted hash string (see Section 3.2.1 for formats).",
                "test_traceability": [
                    {
                        "id": "All test_*_hash_format and test_*_verify_correct_data",
                        "description": "(as hashing is a prerequisite for verification tests)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/protocols.py::PHasher::hash",
                        "description": "(and all concrete hasher implementations)"
                    }
                ]
            },
            {
                "requirement_id": "FR-HASHER-INTERFACE-VERIFY",
                "requirement_description": "Each hasher shall provide a mechanism to verify a plain text string against one of its algorithm-specific formatted hash strings.\n*   Input: plain text string, hashed string.\n*   Output: `True` if the string matches the hash, `False` otherwise.\n*   Verification must fail (return `False`) if the algorithm identifier in the `hashed_string` does not match the hasher's own algorithm identifier.\n*   Verification must fail (return `False`) if the `hashed_string` is malformed (e.g., missing parts, incorrect parameter types).",
                "test_traceability": [
                    {
                        "id": "All test_*_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "test_*_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "test_*_invalid_hash_format",
                        "description": ""
                    },
                    {
                        "id": "test_bcrypt_sha256.py::test_bcrypt_unknown_algorithm",
                        "description": "(for algorithm mismatch)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/protocols.py::PHasher::verify",
                        "description": "(and all concrete hasher implementations)"
                    }
                ]
            },
            {
                "requirement_id": "FR-HASHER-INTERFACE-NEEDS-REHASH",
                "requirement_description": "Each hasher shall provide a mechanism to determine if a given algorithm-specific formatted hash string needs to be rehashed based on its current configuration parameters (e.g., iteration count, cost factors, algorithm version).\n*   Input: hashed string.\n*   Output: `True` if a rehash is needed, `False` otherwise.\n*   The check should generally return `False` if the algorithm identifier in the `hashed_string` does not match the hasher's own (unless the hasher is designed to specifically trigger rehash for \"foreign\" but related hashes, which is not the case here).",
                "test_traceability": [
                    {
                        "id": "All test_*_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "test_*_needs_rehash_true",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/protocols.py::PHasher::needs_rehash",
                        "description": "(and all concrete hasher implementations)"
                    }
                ]
            },
            {
                "requirement_id": "FR-HASHER-NEEDS-REHASH-INVALID-FORMAT",
                "requirement_description": "For a hasher-specific `needs_rehash` check, if the provided `_hashed_string` starts with the correct algorithm identifier for that hasher but is otherwise malformed (e.g., parameter components are missing or invalid such that they cannot be parsed), the hasher shall indicate that a rehash is not needed (return `False`).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_invalid_hash_format",
                        "description": "(rehash part)"
                    },
                    {
                        "id": "src/tests/test_argon2.py::test_argon2_invalid_hash_format",
                        "description": "(rehash part)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/*_hasher.py::*Hasher::needs_rehash",
                        "description": "methods that handle parsing errors."
                    }
                ]
            },
            {
                "requirement_id": "FR-HASHER-LIB-LOAD",
                "requirement_description": "For hashers that depend on external third-party libraries (e.g., Argon2, bcrypt, Whirlpool, RIPEMD-160, Blake3), the system shall attempt to dynamically load the required library module upon hasher initialization.\n*   If the required library module is not found (i.e., not installed in the Python environment), the system shall raise an `ImportError`.\n*   The error message for the `ImportError` should clearly indicate the name of the missing dependency and suggest how to install it (e.g., \"bcrypt is not installed. Please install bcrypt to use this hasher. (pip install hash-forge[bcrypt])\").",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/protocols.py::PHasher::load_library",
                        "description": ""
                    },
                    {
                        "id": "and __init__ methods",
                        "description": "of `Argon2Hasher`, `BCryptSha256Hasher`, `Ripemd160Hasher`, `WhirlpoolHasher`, `Blake3Hasher`."
                    }
                ]
            },
            {
                "requirement_id": "FR-PBKDF2_SHA256-INIT",
                "requirement_description": "The system shall allow initialization of a PBKDF2-SHA256 hasher with configurable `iterations` (default: 100,000) and `salt_length` in bytes (default: 16).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha256.py::pbkdf2_hasher",
                        "description": "(uses 100,000 iterations)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha256Hasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PBKDF2_SHA256-HASH",
                "requirement_description": "The PBKDF2-SHA256 hasher shall produce a hash string for a given input string using PBKDF2 with HMAC-SHA256. The format is defined in EIR-PBKDF2_SHA256-FORMAT.\n*   The salt used shall be randomly generated and of the configured `salt_length`.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha256Hasher::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PBKDF2_SHA256-VERIFY",
                "requirement_description": "The PBKDF2-SHA256 hasher shall verify an input string against a hash string (format EIR-PBKDF2_SHA256-FORMAT).\n*   It returns `True` for a correct string/hash pair.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed or uses a different algorithm ID.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_invalid_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha256Hasher::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PBKDF2_SHA256-NEEDS-REHASH",
                "requirement_description": "The PBKDF2-SHA256 hasher shall determine if a rehash is needed.\n*   Rehash is not needed (`False`) if the `iterations` in the hash string matches the hasher's current `iterations` configuration.\n*   Rehash is needed (`True`) if the `iterations` in the hash string differs from the hasher's current `iterations` configuration.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_true",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha256Hasher::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PBKDF2_SHA1-INIT",
                "requirement_description": "The system shall allow initialization of a PBKDF2-SHA1 hasher with configurable `iterations` (default: 100,000) and `salt_length` in bytes (default: 16).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha1.py::pbkdf2_hasher",
                        "description": "(uses 100,000 iterations)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha1Hasher",
                        "description": ""
                    },
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha256Hasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PBKDF2_SHA1-HASH",
                "requirement_description": "The PBKDF2-SHA1 hasher shall produce a hash string for a given input string using PBKDF2 with HMAC-SHA1. The format is defined in EIR-PBKDF2_SHA1-FORMAT.\n*   The salt used shall be randomly generated and of the configured `salt_length`.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha1Hasher::hash",
                        "description": "(inherited, but uses `sha1` digest)"
                    }
                ]
            },
            {
                "requirement_id": "FR-PBKDF2_SHA1-VERIFY",
                "requirement_description": "The PBKDF2-SHA1 hasher shall verify an input string against a hash string (format EIR-PBKDF2_SHA1-FORMAT).\n*   It returns `True` for a correct string/hash pair.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed or uses a different algorithm ID.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_invalid_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha1Hasher::verify",
                        "description": "(inherited)"
                    }
                ]
            },
            {
                "requirement_id": "FR-PBKDF2_SHA1-NEEDS-REHASH",
                "requirement_description": "The PBKDF2-SHA1 hasher shall determine if a rehash is needed.\n*   Rehash is not needed (`False`) if the `iterations` in the hash string matches the hasher's current `iterations` configuration.\n*   Rehash is needed (`True`) if the `iterations` in the hash string differs from the hasher's current `iterations` configuration.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_true",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/pbkdf2_hasher.py::PBKDF2Sha1Hasher::needs_rehash",
                        "description": "(inherited)"
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT-INIT",
                "requirement_description": "The system shall allow initialization of a bcrypt (standard) hasher with a configurable number of `rounds` (default: 12).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt.py::bcrypt_hasher",
                        "description": "(uses 12 rounds)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptHasher",
                        "description": ""
                    },
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptSha256Hasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT-HASH",
                "requirement_description": "The bcrypt (standard) hasher shall produce a hash string for a given input string. The format is defined in EIR-BCRYPT-FORMAT.\n*   The input string (byte-encoded) is used directly with the underlying bcrypt algorithm.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt.py::test_bcrypt_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptHasher::hash",
                        "description": "(method resolution to `BCryptSha256Hasher.hash` with `digest=None`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT-VERIFY",
                "requirement_description": "The bcrypt (standard) hasher shall verify an input string against a hash string (format EIR-BCRYPT-FORMAT).\n*   It returns `True` for a correct string/hash pair.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed or uses a different algorithm ID.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt.py::test_bcrypt_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_bcrypt.py::test_bcrypt_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_bcrypt.py::test_bcrypt_invalid_hash_format",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_bcrypt.py::test_bcrypt_unknown_algorithm",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptHasher::verify",
                        "description": "(method resolution)"
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT-NEEDS-REHASH",
                "requirement_description": "The bcrypt (standard) hasher shall determine if a rehash is needed.\n*   Rehash is not needed (`False`) if the `rounds` embedded in the hash string matches the hasher's current `rounds` configuration.\n*   Rehash is needed (`True`) if the `rounds` in the hash string differs (implicitly, as bcrypt rounds are part of its standard output, though the test doesn't vary rounds for this specific hasher, this is standard bcrypt behavior and covered by the base class logic tested in `test_bcrypt_sha256.py::test_bcrypt_needs_rehash_true_for_rounds_mismatch` if rounds were changed). For the standard `BCryptHasher`, the provided tests only confirm `needs_rehash` is `False` for a fresh hash. The rehash based on rounds logic is from the parent.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt.py::test_bcrypt_needs_rehash_false",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptHasher::needs_rehash",
                        "description": "(method resolution)"
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT_SHA256-INIT",
                "requirement_description": "The system shall allow initialization of a bcrypt-SHA256 hasher with a configurable number of `rounds` (default: 12).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::bcrypt_hasher",
                        "description": "(uses 12 rounds)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptSha256Hasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT_SHA256-PREHASH",
                "requirement_description": "The bcrypt-SHA256 hasher shall first compute the SHA256 hash of the input string (UTF-8 encoded), then take the hexadecimal representation of this SHA256 digest (as bytes), and use this hex representation as the input to the underlying bcrypt algorithm.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptSha256Hasher::hash",
                        "description": ""
                    },
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptSha256Hasher::_get_hexdigest",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT_SHA256-HASH",
                "requirement_description": "The bcrypt-SHA256 hasher shall produce a hash string. The format is defined in EIR-BCRYPT_SHA256-FORMAT.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptSha256Hasher::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT_SHA256-VERIFY",
                "requirement_description": "The bcrypt-SHA256 hasher shall verify an input string against a hash string (format EIR-BCRYPT_SHA256-FORMAT), applying the same pre-hashing logic (FR-BCRYPT_SHA256-PREHASH) to the input string before bcrypt comparison.\n*   It returns `True` for a correct string/hash pair.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed or uses a different algorithm ID.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_invalid_hash_format",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_unknown_algorithm",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptSha256Hasher::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BCRYPT_SHA256-NEEDS-REHASH",
                "requirement_description": "The bcrypt-SHA256 hasher shall determine if a rehash is needed.\n*   Rehash is not needed (`False`) if the `rounds` embedded in the hash string matches the hasher's current `rounds` configuration.\n*   Rehash is needed (`True`) if the `rounds` in the hash string differs from the hasher's current `rounds` configuration. (Note: test for this is missing in `test_bcrypt_sha256.py` but logic is in code).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_needs_rehash_false",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/bcrypt_hasher.py::BCryptSha256Hasher::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARGON2-INIT",
                "requirement_description": "The system shall allow initialization of an Argon2 hasher. It can be configured with optional Argon2 parameters: `time_cost`, `memory_cost`, `parallelism`, `hash_len`, `salt_len`. If not specified, the defaults of the underlying `argon2-cffi` library's `PasswordHasher` are used.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_argon2.py::argon2_hasher",
                        "description": "(uses library defaults)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/argon2_hasher.py::Argon2Hasher::__init__",
                        "description": ""
                    },
                    {
                        "id": "src/hash_forge/hashers/argon2_hasher.py::Argon2Hasher::_get_hasher",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARGON2-HASH",
                "requirement_description": "The Argon2 hasher shall produce a hash string for a given input string. The format is defined in EIR-ARGON2-FORMAT.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_argon2.py::test_argon2_verify_correct_data",
                        "description": "(hash is generated as part of test)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/argon2_hasher.py::Argon2Hasher::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARGON2-VERIFY",
                "requirement_description": "The Argon2 hasher shall verify an input string against a hash string (format EIR-ARGON2-FORMAT).\n*   It returns `True` for a correct string/hash pair.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed (e.g., invalid for Argon2 library) or uses an algorithm identifier that doesn't start with `argon2`.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_argon2.py::test_argon2_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_argon2.py::test_argon2_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_argon2.py::test_argon2_invalid_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/argon2_hasher.py::Argon2Hasher::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ARGON2-NEEDS-REHASH",
                "requirement_description": "The Argon2 hasher shall determine if a rehash is needed based on whether the parameters in the hash string (e.g., `time_cost`, `memory_cost`, `parallelism`, Argon2 type/version) are considered outdated by the underlying Argon2 library's `check_needs_rehash` method relative to the hasher's current configuration.\n*   Rehash is not needed (`False`) if parameters are current.\n*   Rehash is needed (`True`) if parameters are outdated (e.g., `time_cost` in hash is lower than current hasher's `time_cost`).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_argon2.py::test_argon2_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_argon2.py::test_argon2_needs_rehash_true",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/argon2_hasher.py::Argon2Hasher::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SCRYPT-INIT",
                "requirement_description": "The system shall allow initialization of a Scrypt hasher with configurable parameters: `work_factor` (N, default: 2<sup>14</sup>), `block_size` (r, default: 8), `parallelism` (p, default: 5), `maxmem` (default: 0, unlimited), `dklen` (derived key length, default: 64 bytes), `salt_length` (default: 16 bytes).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_scrypt.py::scrypt_hasher",
                        "description": "(uses defaults)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/scrypt_hasher.py::ScryptHasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SCRYPT-HASH",
                "requirement_description": "The Scrypt hasher shall produce a hash string for a given input string. The format is defined in EIR-SCRYPT-FORMAT.\n*   The salt used shall be randomly generated to the configured `salt_length` and Base64 encoded.\n*   The derived key (hash) shall be Base64 encoded.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_scrypt.py::test_scrypt_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/scrypt_hasher.py::ScryptHasher::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SCRYPT-VERIFY",
                "requirement_description": "The Scrypt hasher shall verify an input string against a provided hashed string (format EIR-SCRYPT-FORMAT).\n*   Verification is performed by re-hashing the input string using the hasher's current configuration parameters (`work_factor`, `block_size`, `parallelism`, etc.) and salt generation, then performing a constant-time comparison of this newly generated hash string against the provided `_hashed_string`.\n*   It returns `True` if the provided `_hashed_string` exactly matches the newly generated hash.\n*   It returns `False` otherwise (including if parameters differ, input string differs, or `_hashed_string` is malformed).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_scrypt.py::test_scrypt_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_scrypt.py::test_scrypt_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_scrypt.py::test_scrypt_invalid_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/scrypt_hasher.py::ScryptHasher::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-SCRYPT-NEEDS-REHASH",
                "requirement_description": "The Scrypt hasher shall determine if a rehash is needed by comparing the `work_factor` (N), `block_size` (r), and `parallelism` (p) values embedded in the hash string with its current configuration.\n*   Rehash is not needed (`False`) if N, r, and p in the hash string match the hasher's current configuration.\n*   Rehash is needed (`True`) if any of N, r, or p in the hash string differ from the hasher's current configuration.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_scrypt.py::test_scrypt_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_scrypt.py::test_scrypt_needs_rehash_true",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/scrypt_hasher.py::ScryptHasher::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BLAKE2B-INIT",
                "requirement_description": "The system shall allow initialization of a Blake2b hasher with a mandatory `key` (string) and a configurable `digest_size` in bytes (default: 64).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake2.py::blake2_hasher",
                        "description": "(uses a random key and default digest size)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/blake2_hasher.py::Blake2Hasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BLAKE2B-HASH",
                "requirement_description": "The Blake2b hasher shall produce a hash string for a given input string using the configured `key` and `digest_size`. The format is defined in EIR-BLAKE2B-FORMAT.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake2.py::test_hash_creation",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/blake2_hasher.py::Blake2Hasher::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BLAKE2B-VERIFY",
                "requirement_description": "The Blake2b hasher shall verify an input string against a hash string (format EIR-BLAKE2B-FORMAT), using the hasher's configured `key` and comparing the `digest_size` from the hash string.\n*   It returns `True` for a correct string/hash pair if key and digest size match.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed, uses a different algorithm ID, or its embedded `digest_size` does not match the hasher's `digest_size`.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake2.py::test_verify_hash_correct",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_blake2.py::test_verify_hash_incorrect",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_blake2.py::test_verify_fails_on_modified_hash",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/blake2_hasher.py::Blake2Hasher::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BLAKE2B-NEEDS-REHASH",
                "requirement_description": "The Blake2b hasher shall determine if a rehash is needed by comparing the `digest_size` embedded in the hash string with its current `digest_size` configuration. (The key is not part of the hash string, so rehash based on key change is not detectable this way).\n*   Rehash is not needed (`False`) if the `digest_size` in the hash string matches the hasher's current configuration.\n*   Rehash is needed (`True`) if the `digest_size` in the hash string differs.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake2.py::test_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_blake2.py::test_needs_rehash_true",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/blake2_hasher.py::Blake2Hasher::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BLAKE3-INIT",
                "requirement_description": "The system shall allow initialization of a Blake3 hasher. This implementation uses an unkeyed Blake3 hash.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake3.py::blake3_hasher",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/blake3_hasher.py::Blake3Hasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BLAKE3-HASH",
                "requirement_description": "The Blake3 hasher shall produce a hash string for a given input string using unkeyed Blake3. The format is defined in EIR-BLAKE3-FORMAT.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake3.py::test_hash_creation",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/blake3_hasher.py::Blake3Hasher::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BLAKE3-VERIFY",
                "requirement_description": "The Blake3 hasher shall verify an input string against a hash string (format EIR-BLAKE3-FORMAT).\n*   It returns `True` for a correct string/hash pair.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed or uses a different algorithm ID.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake3.py::test_verify_hash_correct",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_blake3.py::test_verify_hash_incorrect",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_blake3.py::test_verify_fails_on_modified_hash",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/blake3_hasher.py::Blake3Hasher::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BLAKE3-NEEDS-REHASH",
                "requirement_description": "The Blake3 hasher shall determine if a rehash is needed. Since this implementation uses unkeyed Blake3 with no configurable parameters affecting the hash output structure (beyond the algorithm itself), a rehash is generally only indicated if the algorithm identifier in the hash string does not match `blake3`.\n*   Rehash is not needed (`False`) if the algorithm in the hash string is `blake3`.\n*   Rehash is needed (`True`) if the algorithm in the hash string is not `blake3`.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/blake3_hasher.py::Blake3Hasher::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-WHIRLPOOL-INIT",
                "requirement_description": "The system shall allow initialization of a Whirlpool hasher. (Note: The provided source uses SHA512 as an implementation basis for Whirlpool, but identifies as `whirlpool`).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_whirlpool.py::whirlpool_hasher",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/whirlpool_hasher.py::WhirlpoolHasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-WHIRLPOOL-HASH",
                "requirement_description": "The Whirlpool hasher shall produce a hash string for a given input string. The format is defined in EIR-WHIRLPOOL-FORMAT. (Internally, this might use SHA512, but the external representation is `whirlpool`).",
                "test_traceability": [
                    {
                        "id": "src/tests/test_whirlpool.py::test_whirlpool_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/whirlpool_hasher.py::WhirlpoolHasher::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-WHIRLPOOL-VERIFY",
                "requirement_description": "The Whirlpool hasher shall verify an input string against a hash string (format EIR-WHIRLPOOL-FORMAT).\n*   It returns `True` for a correct string/hash pair.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed or uses a different algorithm ID.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_whirlpool.py::test_whirlpool_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_whirlpool.py::test_whirlpool_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_whirlpool.py::test_whirlpool_invalid_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/whirlpool_hasher.py::WhirlpoolHasher::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-WHIRLPOOL-NEEDS-REHASH",
                "requirement_description": "The Whirlpool hasher shall determine if a rehash is needed.\n*   Rehash is not needed (`False`) if the algorithm identifier in the hash string is `whirlpool`.\n*   Rehash is needed (`True`) if the algorithm identifier in the hash string is not `whirlpool`.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_true",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/whirlpool_hasher.py::WhirlpoolHasher::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RIPEMD160-INIT",
                "requirement_description": "The system shall allow initialization of a RIPEMD-160 hasher.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_ripemd160.py::ripemd160_hasher",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/ripemd160_hasher.py::Ripemd160Hasher::__init__",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RIPEMD160-HASH",
                "requirement_description": "The RIPEMD-160 hasher shall produce a hash string for a given input string. The format is defined in EIR-RIPEMD160-FORMAT.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_ripemd160.py::test_ripemd160_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/ripemd160_hasher.py::Ripemd160Hasher::hash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RIPEMD160-VERIFY",
                "requirement_description": "The RIPEMD-160 hasher shall verify an input string against a hash string (format EIR-RIPEMD160-FORMAT).\n*   It returns `True` for a correct string/hash pair.\n*   It returns `False` for an incorrect string.\n*   It returns `False` if the hash string is malformed or uses a different algorithm ID.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_ripemd160.py::test_ripemd160_verify_correct_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_ripemd160.py::test_ripemd160_verify_incorrect_data",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_ripemd160.py::test_ripemd160_invalid_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/ripemd160_hasher.py::Ripemd160Hasher::verify",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-RIPEMD160-NEEDS-REHASH",
                "requirement_description": "The RIPEMD-160 hasher shall determine if a rehash is needed.\n*   Rehash is not needed (`False`) if the algorithm identifier in the hash string is `RIPEMD-160`.\n*   Rehash is needed (`True`) if the algorithm identifier in the hash string is not `RIPEMD-160`.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_false",
                        "description": ""
                    },
                    {
                        "id": "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_true",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/hash_forge/hashers/ripemd160_hasher.py::Ripemd160Hasher::needs_rehash",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "EIR-PBKDF2_SHA256-FORMAT",
                "requirement_description": "Hashed strings generated by the PBKDF2-SHA256 hasher SHALL adhere to the format:\n`pbkdf2_sha256$<iterations>$<salt_hex>$<hash_hex>`\n*   `<iterations>`: Integer number of iterations.\n*   `<salt_hex>`: Hexadecimal representation of the salt.\n*   `<hash_hex>`: Hexadecimal representation of the derived key.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-PBKDF2_SHA1-FORMAT",
                "requirement_description": "Hashed strings generated by the PBKDF2-SHA1 hasher SHALL adhere to the format:\n`pbkdf2_sha1$<iterations>$<salt_hex>$<hash_hex>`\n*   `<iterations>`: Integer number of iterations.\n*   `<salt_hex>`: Hexadecimal representation of the salt.\n*   `<hash_hex>`: Hexadecimal representation of the derived key.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-BCRYPT-FORMAT",
                "requirement_description": "Hashed strings generated by the bcrypt (standard) hasher SHALL start with `bcrypt$` followed by the standard bcrypt hash representation (e.g., `$2b$12$...`).\n*   The full format SHALL match the regular expression: `^bcrypt\\$2[abxy]\\$\\d{2}\\$[./A-Za-z0-9]{53}$`",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt.py::test_bcrypt_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-BCRYPT_SHA256-FORMAT",
                "requirement_description": "Hashed strings generated by the bcrypt-SHA256 hasher SHALL start with `bcrypt_sha256$` followed by the standard bcrypt hash representation (e.g., `$2b$12$...`).\n*   The full format SHALL match the regular expression: `^bcrypt_sha256\\$2[abxy]\\$\\d{2}\\$[./A-Za-z0-9]{53}$`",
                "test_traceability": [
                    {
                        "id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-ARGON2-FORMAT",
                "requirement_description": "Hashed strings generated by the Argon2 hasher SHALL start with `argon2` followed by the standard Argon2 encoded hash string (e.g., `$argon2id$v=19$m=...,t=...,p=...$<salt_b64>$<hash_b64>`).\n*   The exact format of the suffix is determined by the underlying Argon2 library. The prefix for identification is `argon2`.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_argon2.py::test_argon2_verify_correct_data",
                        "description": "(The test confirms verification works, implying format compatibility. Argon2 output format is standard.)"
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-SCRYPT-FORMAT",
                "requirement_description": "Hashed strings generated by the Scrypt hasher SHALL adhere to the format:\n`scrypt$<work_factor>$<salt_b64>$<block_size>$<parallelism>$<hash_b64>`\n*   `<work_factor>` (N): Integer CPU/memory cost parameter.\n*   `<salt_b64>`: Base64 representation of the salt.\n*   `<block_size>` (r): Integer block size parameter.\n*   `<parallelism>` (p): Integer parallelization parameter.\n*   `<hash_b64>`: Base64 representation of the derived key.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_scrypt.py::test_scrypt_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-BLAKE2B-FORMAT",
                "requirement_description": "Hashed strings generated by the Blake2b hasher SHALL adhere to the format:\n`blake2b$<digest_size>$<hash_hex>`\n*   `<digest_size>`: Integer digest size in bytes.\n*   `<hash_hex>`: Hexadecimal representation of the hash.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake2.py::test_hash_creation",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-BLAKE3-FORMAT",
                "requirement_description": "Hashed strings generated by the Blake3 hasher SHALL adhere to the format:\n`blake3$<hash_hex>`\n*   `<hash_hex>`: Hexadecimal representation of the hash.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_blake3.py::test_hash_creation",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-WHIRLPOOL-FORMAT",
                "requirement_description": "Hashed strings generated by the Whirlpool hasher SHALL adhere to the format:\n`whirlpool$<hash_hex>`\n*   `<hash_hex>`: Hexadecimal representation of the hash.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_whirlpool.py::test_whirlpool_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-RIPEMD160-FORMAT",
                "requirement_description": "Hashed strings generated by the RIPEMD-160 hasher SHALL adhere to the format:\n`RIPEMD-160$<hash_hex>`\n*   `<hash_hex>`: Hexadecimal representation of the hash.",
                "test_traceability": [
                    {
                        "id": "src/tests/test_ripemd160.py::test_ripemd160_hash_format",
                        "description": ""
                    }
                ],
                "code_traceability": []
            }
        ],
        "commit_sha": "a213d0ab101286d143c4d140656286b12d7bb30a",
        "full_code_skeleton": "--- File: src/hash_forge/__init__.py ---\n```python\nclass HashManager:\n    def __init__(self, *hashers: PHasher) -> None:\n        \"\"\"\n        Initialize the HashForge instance with one or more hashers.\n\n        Args:\n            *hashers (PHasher): One or more hasher instances to be used by the HashForge.\n\n        Raises:\n            ValueError: If no hashers are provided.\n\n        Attributes:\n            hashers (Set[Tuple[str, PHasher]]): A set of tuples containing the algorithm name and the hasher instance.\n            preferred_hasher (PHasher): The first hasher provided, used as the preferred hasher.\n        \"\"\"\n        pass\n\n    def hash(self, string: str) -> str:\n        \"\"\"\n        Hashes the given string using the preferred hasher.\n\n        Args:\n            string (str): The string to be hashed.\n\n        Returns:\n            str: The hashed string.\n        \"\"\"\n        pass\n\n    def verify(self, string: str, hashed_string: str) -> bool:\n        \"\"\"\n        Verifies if a given string matches a hashed string using the appropriate hashing algorithm.\n\n        Args:\n            string (str): The plain text string to verify.\n            hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n\n        Raises:\n            IndexError: If the hashed string does not contain a valid algorithm identifier.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, hashed_string: str) -> bool:\n        \"\"\"\n        Determines if a given hashed string needs to be rehashed.\n\n        This method checks if the hashing algorithm used for the given hashed string\n        is the preferred algorithm or if the hashed string needs to be rehashed\n        according to the hasher's criteria.\n\n        Args:\n            hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n\n        Raises:\n            IndexError: If the hashed string format is invalid.\n        \"\"\"\n        pass\n\n    def _get_hasher_by_hash(self, hashed_string: str) -> PHasher | None:\n        \"\"\"\n        Retrieve the hasher instance that matches the given hashed string.\n\n        This method iterates through the available hashers and returns the first\n        hasher whose algorithm matches the beginning of the provided hashed string.\n\n        Args:\n            hashed_string (str): The hashed string to match against available hashers.\n\n        Returns:\n            PHasher | None: The hasher instance that matches the hashed string, or\n            None if no match is found.\n        \"\"\"\n        pass\n```\n--- File: src/hash_forge/protocols.py ---\n```python\nclass PHasher(Protocol):\n    algorithm: ClassVar[str]\n    library_module: ClassVar[str | None] = None\n\n    @abstractmethod\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Computes the hash of the given string.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The resulting hash as a string.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if the provided string matches the hashed string.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the original string matches the hashed string, False otherwise.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determine if a hashed string needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def load_library(self, name: str) -> ModuleType:\n        \"\"\"\n        Loads a library module by its name.\n\n        This function attempts to import a module specified by the `name` parameter.\n        If the module is not found, it raises an ImportError with a message indicating\n        the required third-party library to install.\n\n        Args:\n            name (str): The name of the module to import.\n\n        Returns:\n            ModuleType: The imported module.\n\n        Raises:\n            ImportError: If the module cannot be imported, with a message suggesting\n                        the required third-party library to install.\n        \"\"\"\n        pass\n```\n--- File: src/hash_forge/utils.py ---\n```python\ndef get_random_string(length: int, allowed_chars: str = RANDOM_STRING_CHARS) -> str:\n    \"\"\"\n    Return a securely generated random string.\n\n    The bit length of the returned value can be calculated with the formula:\n        log_2(len(allowed_chars)^length)\n\n    For example, with default `allowed_chars` (26+26+10), this gives:\n      * length: 12, bit length =~ 71 bits\n      * length: 22, bit length =~ 131 bits\n    \"\"\"\n    pass\n```\n--- File: src/hash_forge/hashers/pbkdf2_hasher.py ---\n```python\nclass PBKDF2Sha256Hasher(PHasher):\n    algorithm: ClassVar[str] = 'pbkdf2_sha256'\n    digest: ClassVar[Callable[..., Any]]\n\n    def __init__(self, iterations: int = 100_000, salt_length: int = 16) -> None:\n        pass\n\n    __slots__ = ('iterations', 'salt_length')\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes a given string using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$iterations$salt$hashed'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the hashed string using PBKDF2 algorithm.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against, formatted as 'algorithm$iterations$salt$hashed'.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if a hashed string needs to be rehashed based on the number of iterations.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if rehash is needed, False otherwise.\n        \"\"\"\n        pass\n\n\nclass PBKDF2Sha1Hasher(PBKDF2Sha256Hasher):\n    algorithm = \"pbkdf2_sha1\"\n    digest\n```\n--- File: src/hash_forge/hashers/argon2_hasher.py ---\n```python\nclass Argon2Hasher(PHasher):\n    algorithm: ClassVar[str] = \"argon2\"\n    library_module: ClassVar[str] = \"argon2\"\n\n    def __init__(\n        self,\n        time_cost: int | None = None,\n        salt_len: int | None = None,\n        memory_cost: int | None = None,\n        parallelism: int | None = None,\n        hash_len: int | None = None,\n    ) -> None:\n        \"\"\"\n        Initialize the Argon2Hasher with optional parameters for hashing configuration.\n\n        Args:\n            time_cost (int | None): The time cost parameter for Argon2. Defaults to None.\n            salt_len (int | None): The length of the salt. Defaults to None.\n            memory_cost (int | None): The memory cost parameter for Argon2. Defaults to None.\n            parallelism (int | None): The degree of parallelism for Argon2. Defaults to None.\n            hash_len (int | None): The length of the resulting hash. Defaults to None.\n        \"\"\"\n        pass\n\n    __slots__ = (\"argon2\", \"time_cost\", \"memory_cost\", \"parallelism\", \"hash_len\", \"salt_len\")\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using Argon2 algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, time cost, memory cost, parallelism, salt, and\n            hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the provided hashed string using Argon2.\n\n        Args:\n            _string (str): The plain string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed based on the current time cost.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n\n    def _get_hasher(self) -> Any:\n        \"\"\"\n        Creates and returns a configured instance of argon2.PasswordHasher.\n\n        This method uses the provided configuration parameters to set up the\n        PasswordHasher instance. The parameters that can be configured are:\n        - time_cost: The time cost parameter for the Argon2 algorithm.\n        - memory_cost: The memory cost parameter for the Argon2 algorithm.\n        - parallelism: The parallelism parameter for the Argon2 algorithm.\n        - hash_len: The length of the generated hash.\n        - salt_len: The length of the salt.\n\n        Returns:\n            argon2.PasswordHasher: A configured instance of the PasswordHasher.\n        \"\"\"\n        pass\n```\n--- File: src/hash_forge/hashers/ripemd160_hasher.py ---\n```python\nclass Ripemd160Hasher(PHasher):\n    algorithm: ClassVar[str] = \"RIPEMD-160\"\n    library_module: ClassVar[str] = \"Crypto.Hash.RIPEMD160\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the RIPEMD-160 hasher instance.\n\n        This method loads the RIPEMD-160 hashing library and assigns it to the\n        instance variable `self.ripemd160`.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the RIPEMD-160 algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$hashed_value'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed: str, /) -> bool:\n        \"\"\"\n        Verify if the provided string matches the given hashed value.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed (str): The hashed value to compare against, in the format 'algorithm$hash_value'.\n\n        Returns:\n            bool: True if the string matches the hashed value, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the algorithm used in the hashed string does not match\n                  the current algorithm, indicating that a rehash is needed.\n        \"\"\"\n        pass\n```\n--- File: src/hash_forge/hashers/whirlpool_hasher.py ---\n```python\nclass WhirlpoolHasher(PHasher):\n    algorithm: ClassVar[str] = 'whirlpool'\n    library_module: ClassVar[str] = 'Crypto.Hash.SHA512'\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the WhirlpoolHasher instance.\n\n        This constructor initializes the WhirlpoolHasher by loading the SHA-512\n        hashing library module.\n\n        Attributes:\n            sha512: The loaded SHA-512 hashing library module.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Computes the hash of the given string using the SHA-512 algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The resulting hash as a hexadecimal string prefixed with the algorithm name.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if the given string matches the given hash.\n\n        Args:\n            _string (str): The input string to verify.\n            _hashed_string (str): The hash to compare against.\n\n        Returns:\n            bool: True if the hash matches the input string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hash needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hash to check.\n\n        Returns:\n            bool: True if the hash needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n```\n--- File: src/hash_forge/hashers/blake2_hasher.py ---\n```python\nclass Blake2Hasher(PHasher):\n    algorithm: ClassVar[str] = 'blake2b'\n\n    def __init__(self, key: str, digest_size: int = 64) -> None:\n        \"\"\"\n        Initializes the Blake2Hasher with a key and an optional digest size.\n\n        Args:\n            key (str): The key to use for the hash function.\n            digest_size (int, optional): The size of the digest in bytes. Defaults to 64.\n        \"\"\"\n        pass\n\n    __slots__ = ('digest_size', 'key')\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the BLAKE2b algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, digest size, and hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the hashed string using BLAKE2b.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the plain text string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Checks if the hashed string needs to be rehashed based on the digest size.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n```\n--- File: src/hash_forge/hashers/bcrypt_hasher.py ---\n```python\nclass BCryptSha256Hasher(PHasher):\n    algorithm: ClassVar[str] = 'bcrypt_sha256'\n    library_module: ClassVar[str] = 'bcrypt'\n    digest: Callable[[bytes], Any] | None\n\n    def __init__(self, rounds: int = 12) -> None:\n        \"\"\"\n        Initializes the BcryptHasher with the specified number of rounds.\n\n        Args:\n            rounds (int, optional): The number of rounds to use for hashing. Defaults to 12.\n        \"\"\"\n        pass\n\n    __slots__ = ('rounds',)\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using bcrypt algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, rounds, salt, and hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if a given string matches the hashed string using bcrypt.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the plain text string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Check if the hashed string needs to be rehashed.\n\n        This method determines whether the provided hashed string needs to be rehashed\n        based on the algorithm and the number of rounds used during hashing.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def _get_hexdigest(_string: str, digest: Callable[[bytes], Any]) -> bytes:\n        \"\"\"\n        Generate a hexadecimal digest for a given string using the specified digest function.\n\n        Args:\n            _string (str): The input string to be hashed.\n            digest (Callable): A callable digest function (e.g., hashlib.sha256).\n\n        Returns:\n            bytes: The hexadecimal representation of the digest.\n        \"\"\"\n        pass\n\n\nclass BCryptHasher(BCryptSha256Hasher):\n    algorithm = 'bcrypt'\n    digest = None\n```\n--- File: src/hash_forge/hashers/scrypt_hasher.py ---\n```python\ndef _generate_salt(salt: int) -> str:\n    \"\"\"\n    Generates a base64-encoded salt string.\n\n    Args:\n        salt (int): The number of bytes to generate for the salt.\n\n    Returns:\n        str: A base64-encoded string representation of the generated salt.\n    \"\"\"\n    pass\n\n\nclass ScryptHasher(PHasher):\n    algorithm: ClassVar[str] = \"scrypt\"\n\n    def __init__(\n        self,\n        work_factor: int = 2**14,\n        block_size: int = 8,\n        parallelism: int = 5,\n        maxmem: int = 0,\n        dklen: int = 64,\n        salt_length: int = 16,\n    ) -> None:\n        \"\"\"\n        Initialize the ScryptHasher with the given parameters.\n\n        Args:\n            work_factor (int): The CPU/memory cost parameter. Default is 2**14.\n            block_size (int): The block size parameter. Default is 8.\n            parallelism (int): The parallelization parameter. Default is 5.\n            maxmem (int): The maximum memory to use in bytes. Default is 0 (no limit).\n            dklen (int): The length of the derived key. Default is 64.\n            salt_length (int): The length of the salt. Default is 16.\n        \"\"\"\n        pass\n\n    __slots__ = (\"work_factor\", \"block_size\", \"parallelism\", \"maxmem\", \"dklen\", \"salt_length\")\n\n    def hash(self, _string: str) -> str:\n        \"\"\"\n        Hashes the given string using the scrypt algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$work_factor$salt$block_size$parallelism$hashed_value'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str) -> bool:\n        \"\"\"\n        Verify if a given string matches the hashed string.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the original string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed based on the current\n        work factor, block size, and parallelism parameters.\n\n        Args:\n            _hashed_string (str): The hashed string to check, expected to be in the format\n                                  \"$<prefix>$<n>$<r>$<p>$<hash>\".\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n\n    def generate_salt(self) -> str:\n        \"\"\"\n        Generates a cryptographic salt.\n\n        Returns:\n            str: A string representing the generated salt.\n        \"\"\"\n        pass\n```\n--- File: src/hash_forge/hashers/blake3_hasher.py ---\n```python\nclass Blake3Hasher(PHasher):\n    algorithm: ClassVar[str] = \"blake3\"\n    library_module: ClassVar[str] = \"blake3\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the Blake3Hasher with a key and an optional digest size.\n\n        Args:\n            key (str): The key to use for the hash function.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the BLAKE3 algorithm and returns the result in a specific format.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format \"algorithm$hashed_hex\".\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if a given string matches a hashed string using the BLAKE3 algorithm.\n\n        Args:\n            _string (str): The input string to verify.\n            _hashed_string (str): The hashed string to compare against, in the format 'algorithm$hashed_value'.\n\n        Returns:\n            bool: True if the input string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Checks if the hashed string needs to be rehashed based on the digest size.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "src/hash_forge/__init__.py",
                "code": "class HashManager:\n    def __init__(self, *hashers: PHasher) -> None:\n        \"\"\"\n        Initialize the HashForge instance with one or more hashers.\n\n        Args:\n            *hashers (PHasher): One or more hasher instances to be used by the HashForge.\n\n        Raises:\n            ValueError: If no hashers are provided.\n\n        Attributes:\n            hashers (Set[Tuple[str, PHasher]]): A set of tuples containing the algorithm name and the hasher instance.\n            preferred_hasher (PHasher): The first hasher provided, used as the preferred hasher.\n        \"\"\"\n        pass\n\n    def hash(self, string: str) -> str:\n        \"\"\"\n        Hashes the given string using the preferred hasher.\n\n        Args:\n            string (str): The string to be hashed.\n\n        Returns:\n            str: The hashed string.\n        \"\"\"\n        pass\n\n    def verify(self, string: str, hashed_string: str) -> bool:\n        \"\"\"\n        Verifies if a given string matches a hashed string using the appropriate hashing algorithm.\n\n        Args:\n            string (str): The plain text string to verify.\n            hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n\n        Raises:\n            IndexError: If the hashed string does not contain a valid algorithm identifier.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, hashed_string: str) -> bool:\n        \"\"\"\n        Determines if a given hashed string needs to be rehashed.\n\n        This method checks if the hashing algorithm used for the given hashed string\n        is the preferred algorithm or if the hashed string needs to be rehashed\n        according to the hasher's criteria.\n\n        Args:\n            hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n\n        Raises:\n            IndexError: If the hashed string format is invalid.\n        \"\"\"\n        pass\n\n    def _get_hasher_by_hash(self, hashed_string: str) -> PHasher | None:\n        \"\"\"\n        Retrieve the hasher instance that matches the given hashed string.\n\n        This method iterates through the available hashers and returns the first\n        hasher whose algorithm matches the beginning of the provided hashed string.\n\n        Args:\n            hashed_string (str): The hashed string to match against available hashers.\n\n        Returns:\n            PHasher | None: The hasher instance that matches the hashed string, or\n            None if no match is found.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "src/hash_forge/protocols.py",
                "code": "class PHasher(Protocol):\n    algorithm: ClassVar[str]\n    library_module: ClassVar[str | None] = None\n\n    @abstractmethod\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Computes the hash of the given string.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The resulting hash as a string.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if the provided string matches the hashed string.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the original string matches the hashed string, False otherwise.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determine if a hashed string needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def load_library(self, name: str) -> ModuleType:\n        \"\"\"\n        Loads a library module by its name.\n\n        This function attempts to import a module specified by the `name` parameter.\n        If the module is not found, it raises an ImportError with a message indicating\n        the required third-party library to install.\n\n        Args:\n            name (str): The name of the module to import.\n\n        Returns:\n            ModuleType: The imported module.\n\n        Raises:\n            ImportError: If the module cannot be imported, with a message suggesting\n                        the required third-party library to install.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "src/hash_forge/utils.py",
                "code": "def get_random_string(length: int, allowed_chars: str = RANDOM_STRING_CHARS) -> str:\n    \"\"\"\n    Return a securely generated random string.\n\n    The bit length of the returned value can be calculated with the formula:\n        log_2(len(allowed_chars)^length)\n\n    For example, with default `allowed_chars` (26+26+10), this gives:\n      * length: 12, bit length =~ 71 bits\n      * length: 22, bit length =~ 131 bits\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "src/hash_forge/hashers/pbkdf2_hasher.py",
                "code": "class PBKDF2Sha256Hasher(PHasher):\n    algorithm: ClassVar[str] = 'pbkdf2_sha256'\n    digest: ClassVar[Callable[..., Any]]\n\n    def __init__(self, iterations: int = 100_000, salt_length: int = 16) -> None:\n        pass\n\n    __slots__ = ('iterations', 'salt_length')\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes a given string using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$iterations$salt$hashed'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the hashed string using PBKDF2 algorithm.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against, formatted as 'algorithm$iterations$salt$hashed'.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if a hashed string needs to be rehashed based on the number of iterations.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if rehash is needed, False otherwise.\n        \"\"\"\n        pass\n\n\nclass PBKDF2Sha1Hasher(PBKDF2Sha256Hasher):\n    algorithm = \"pbkdf2_sha1\"\n    digest\n"
            },
            {
                "file_path": "src/hash_forge/hashers/argon2_hasher.py",
                "code": "class Argon2Hasher(PHasher):\n    algorithm: ClassVar[str] = \"argon2\"\n    library_module: ClassVar[str] = \"argon2\"\n\n    def __init__(\n        self,\n        time_cost: int | None = None,\n        salt_len: int | None = None,\n        memory_cost: int | None = None,\n        parallelism: int | None = None,\n        hash_len: int | None = None,\n    ) -> None:\n        \"\"\"\n        Initialize the Argon2Hasher with optional parameters for hashing configuration.\n\n        Args:\n            time_cost (int | None): The time cost parameter for Argon2. Defaults to None.\n            salt_len (int | None): The length of the salt. Defaults to None.\n            memory_cost (int | None): The memory cost parameter for Argon2. Defaults to None.\n            parallelism (int | None): The degree of parallelism for Argon2. Defaults to None.\n            hash_len (int | None): The length of the resulting hash. Defaults to None.\n        \"\"\"\n        pass\n\n    __slots__ = (\"argon2\", \"time_cost\", \"memory_cost\", \"parallelism\", \"hash_len\", \"salt_len\")\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using Argon2 algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, time cost, memory cost, parallelism, salt, and\n            hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the provided hashed string using Argon2.\n\n        Args:\n            _string (str): The plain string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed based on the current time cost.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n\n    def _get_hasher(self) -> Any:\n        \"\"\"\n        Creates and returns a configured instance of argon2.PasswordHasher.\n\n        This method uses the provided configuration parameters to set up the\n        PasswordHasher instance. The parameters that can be configured are:\n        - time_cost: The time cost parameter for the Argon2 algorithm.\n        - memory_cost: The memory cost parameter for the Argon2 algorithm.\n        - parallelism: The parallelism parameter for the Argon2 algorithm.\n        - hash_len: The length of the generated hash.\n        - salt_len: The length of the salt.\n\n        Returns:\n            argon2.PasswordHasher: A configured instance of the PasswordHasher.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "src/hash_forge/hashers/ripemd160_hasher.py",
                "code": "class Ripemd160Hasher(PHasher):\n    algorithm: ClassVar[str] = \"RIPEMD-160\"\n    library_module: ClassVar[str] = \"Crypto.Hash.RIPEMD160\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the RIPEMD-160 hasher instance.\n\n        This method loads the RIPEMD-160 hashing library and assigns it to the\n        instance variable `self.ripemd160`.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the RIPEMD-160 algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$hashed_value'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed: str, /) -> bool:\n        \"\"\"\n        Verify if the provided string matches the given hashed value.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed (str): The hashed value to compare against, in the format 'algorithm$hash_value'.\n\n        Returns:\n            bool: True if the string matches the hashed value, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the algorithm used in the hashed string does not match\n                  the current algorithm, indicating that a rehash is needed.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "src/hash_forge/hashers/whirlpool_hasher.py",
                "code": "class WhirlpoolHasher(PHasher):\n    algorithm: ClassVar[str] = 'whirlpool'\n    library_module: ClassVar[str] = 'Crypto.Hash.SHA512'\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the WhirlpoolHasher instance.\n\n        This constructor initializes the WhirlpoolHasher by loading the SHA-512\n        hashing library module.\n\n        Attributes:\n            sha512: The loaded SHA-512 hashing library module.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Computes the hash of the given string using the SHA-512 algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The resulting hash as a hexadecimal string prefixed with the algorithm name.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if the given string matches the given hash.\n\n        Args:\n            _string (str): The input string to verify.\n            _hashed_string (str): The hash to compare against.\n\n        Returns:\n            bool: True if the hash matches the input string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hash needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hash to check.\n\n        Returns:\n            bool: True if the hash needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "src/hash_forge/hashers/blake2_hasher.py",
                "code": "class Blake2Hasher(PHasher):\n    algorithm: ClassVar[str] = 'blake2b'\n\n    def __init__(self, key: str, digest_size: int = 64) -> None:\n        \"\"\"\n        Initializes the Blake2Hasher with a key and an optional digest size.\n\n        Args:\n            key (str): The key to use for the hash function.\n            digest_size (int, optional): The size of the digest in bytes. Defaults to 64.\n        \"\"\"\n        pass\n\n    __slots__ = ('digest_size', 'key')\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the BLAKE2b algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, digest size, and hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the hashed string using BLAKE2b.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the plain text string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Checks if the hashed string needs to be rehashed based on the digest size.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "src/hash_forge/hashers/bcrypt_hasher.py",
                "code": "class BCryptSha256Hasher(PHasher):\n    algorithm: ClassVar[str] = 'bcrypt_sha256'\n    library_module: ClassVar[str] = 'bcrypt'\n    digest: Callable[[bytes], Any] | None\n\n    def __init__(self, rounds: int = 12) -> None:\n        \"\"\"\n        Initializes the BcryptHasher with the specified number of rounds.\n\n        Args:\n            rounds (int, optional): The number of rounds to use for hashing. Defaults to 12.\n        \"\"\"\n        pass\n\n    __slots__ = ('rounds',)\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using bcrypt algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, rounds, salt, and hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if a given string matches the hashed string using bcrypt.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the plain text string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Check if the hashed string needs to be rehashed.\n\n        This method determines whether the provided hashed string needs to be rehashed\n        based on the algorithm and the number of rounds used during hashing.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def _get_hexdigest(_string: str, digest: Callable[[bytes], Any]) -> bytes:\n        \"\"\"\n        Generate a hexadecimal digest for a given string using the specified digest function.\n\n        Args:\n            _string (str): The input string to be hashed.\n            digest (Callable): A callable digest function (e.g., hashlib.sha256).\n\n        Returns:\n            bytes: The hexadecimal representation of the digest.\n        \"\"\"\n        pass\n\n\nclass BCryptHasher(BCryptSha256Hasher):\n    algorithm = 'bcrypt'\n    digest = None\n"
            },
            {
                "file_path": "src/hash_forge/hashers/scrypt_hasher.py",
                "code": "def _generate_salt(salt: int) -> str:\n    \"\"\"\n    Generates a base64-encoded salt string.\n\n    Args:\n        salt (int): The number of bytes to generate for the salt.\n\n    Returns:\n        str: A base64-encoded string representation of the generated salt.\n    \"\"\"\n    pass\n\n\nclass ScryptHasher(PHasher):\n    algorithm: ClassVar[str] = \"scrypt\"\n\n    def __init__(\n        self,\n        work_factor: int = 2**14,\n        block_size: int = 8,\n        parallelism: int = 5,\n        maxmem: int = 0,\n        dklen: int = 64,\n        salt_length: int = 16,\n    ) -> None:\n        \"\"\"\n        Initialize the ScryptHasher with the given parameters.\n\n        Args:\n            work_factor (int): The CPU/memory cost parameter. Default is 2**14.\n            block_size (int): The block size parameter. Default is 8.\n            parallelism (int): The parallelization parameter. Default is 5.\n            maxmem (int): The maximum memory to use in bytes. Default is 0 (no limit).\n            dklen (int): The length of the derived key. Default is 64.\n            salt_length (int): The length of the salt. Default is 16.\n        \"\"\"\n        pass\n\n    __slots__ = (\"work_factor\", \"block_size\", \"parallelism\", \"maxmem\", \"dklen\", \"salt_length\")\n\n    def hash(self, _string: str) -> str:\n        \"\"\"\n        Hashes the given string using the scrypt algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$work_factor$salt$block_size$parallelism$hashed_value'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str) -> bool:\n        \"\"\"\n        Verify if a given string matches the hashed string.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the original string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed based on the current\n        work factor, block size, and parallelism parameters.\n\n        Args:\n            _hashed_string (str): The hashed string to check, expected to be in the format\n                                  \"$<prefix>$<n>$<r>$<p>$<hash>\".\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n\n    def generate_salt(self) -> str:\n        \"\"\"\n        Generates a cryptographic salt.\n\n        Returns:\n            str: A string representing the generated salt.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "src/hash_forge/hashers/blake3_hasher.py",
                "code": "class Blake3Hasher(PHasher):\n    algorithm: ClassVar[str] = \"blake3\"\n    library_module: ClassVar[str] = \"blake3\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the Blake3Hasher with a key and an optional digest size.\n\n        Args:\n            key (str): The key to use for the hash function.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the BLAKE3 algorithm and returns the result in a specific format.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format \"algorithm$hashed_hex\".\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if a given string matches a hashed string using the BLAKE3 algorithm.\n\n        Args:\n            _string (str): The input string to verify.\n            _hashed_string (str): The hashed string to compare against, in the format 'algorithm$hashed_value'.\n\n        Returns:\n            bool: True if the input string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Checks if the hashed string needs to be rehashed based on the digest size.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: hash_forge/__init__.py ---\n```python\nclass HashManager:\n    def __init__(self, *hashers: 'PHasher') -> None:\n        \"\"\"\n        Initialize the HashForge instance with one or more hashers.\n\n        Args:\n            *hashers (PHasher): One or more hasher instances to be used by the HashForge.\n\n        Raises:\n            ValueError: If no hashers are provided.\n\n        Attributes:\n            hashers (Set[Tuple[str, PHasher]]): A set of tuples containing the algorithm name and the hasher instance.\n            preferred_hasher (PHasher): The first hasher provided, used as the preferred hasher.\n        \"\"\"\n        pass\n\n    def hash(self, string: str) -> str:\n        \"\"\"\n        Hashes the given string using the preferred hasher.\n\n        Args:\n            string (str): The string to be hashed.\n\n        Returns:\n            str: The hashed string.\n        \"\"\"\n        pass\n\n    def verify(self, string: str, hashed_string: str) -> bool:\n        \"\"\"\n        Verifies if a given string matches a hashed string using the appropriate hashing algorithm.\n\n        Args:\n            string (str): The plain text string to verify.\n            hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n\n        Raises:\n            IndexError: If the hashed string does not contain a valid algorithm identifier.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, hashed_string: str) -> bool:\n        \"\"\"\n        Determines if a given hashed string needs to be rehashed.\n\n        This method checks if the hashing algorithm used for the given hashed string\n        is the preferred algorithm or if the hashed string needs to be rehashed\n        according to the hasher's criteria.\n\n        Args:\n            hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n\n        Raises:\n            IndexError: If the hashed string format is invalid.\n        \"\"\"\n        pass\n```\n--- File: hash_forge/protocols.py ---\n```python\nclass PHasher:\n    algorithm: 'ClassVar[str]'\n    library_module: 'ClassVar[str | None]' = None\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Computes the hash of the given string.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The resulting hash as a string.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if the provided string matches the hashed string.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the original string matches the hashed string, False otherwise.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determine if a hashed string needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def load_library(self, name: str) -> 'ModuleType':\n        \"\"\"\n        Loads a library module by its name.\n\n        This function attempts to import a module specified by the `name` parameter.\n        If the module is not found, it raises an ImportError with a message indicating\n        the required third-party library to install.\n\n        Args:\n            name (str): The name of the module to import.\n\n        Returns:\n            ModuleType: The imported module.\n\n        Raises:\n            ImportError: If the module cannot be imported, with a message suggesting\n                        the required third-party library to install.\n        \"\"\"\n        pass\n```\n--- File: hash_forge/hashers/argon2_hasher.py ---\n```python\nclass Argon2Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = \"argon2\"\n    library_module: 'ClassVar[str]' = \"argon2\"\n\n    def __init__(\n        self,\n        time_cost: int | None = None,\n        salt_len: int | None = None,\n        memory_cost: int | None = None,\n        parallelism: int | None = None,\n        hash_len: int | None = None,\n    ) -> None:\n        \"\"\"\n        Initialize the Argon2Hasher with optional parameters for hashing configuration.\n\n        Args:\n            time_cost (int | None): The time cost parameter for Argon2. Defaults to None.\n            salt_len (int | None): The length of the salt. Defaults to None.\n            memory_cost (int | None): The memory cost parameter for Argon2. Defaults to None.\n            parallelism (int | None): The degree of parallelism for Argon2. Defaults to None.\n            hash_len (int | None): The length of the resulting hash. Defaults to None.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using Argon2 algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, time cost, memory cost, parallelism, salt, and\n            hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the provided hashed string using Argon2.\n\n        Args:\n            _string (str): The plain string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed based on the current time cost.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n```\n--- File: hash_forge/hashers/bcrypt_hasher.py ---\n```python\nclass BCryptSha256Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = 'bcrypt_sha256'\n    library_module: 'ClassVar[str]' = 'bcrypt'\n    digest: 'Callable[[bytes], Any] | None'\n\n    def __init__(self, rounds: int = 12) -> None:\n        \"\"\"\n        Initializes the BcryptHasher with the specified number of rounds.\n\n        Args:\n            rounds (int, optional): The number of rounds to use for hashing. Defaults to 12.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using bcrypt algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, rounds, salt, and hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if a given string matches the hashed string using bcrypt.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the plain text string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Check if the hashed string needs to be rehashed.\n\n        This method determines whether the provided hashed string needs to be rehashed\n        based on the algorithm and the number of rounds used during hashing.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n\nclass BCryptHasher(BCryptSha256Hasher):\n    algorithm: 'ClassVar[str]' = 'bcrypt'\n    digest: 'Callable[[bytes], Any] | None' = None\n```\n--- File: hash_forge/hashers/blake2_hasher.py ---\n```python\nclass Blake2Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = 'blake2b'\n\n    def __init__(self, key: str, digest_size: int = 64) -> None:\n        \"\"\"\n        Initializes the Blake2Hasher with a key and an optional digest size.\n\n        Args:\n            key (str): The key to use for the hash function.\n            digest_size (int, optional): The size of the digest in bytes. Defaults to 64.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the BLAKE2b algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, digest size, and hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the hashed string using BLAKE2b.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the plain text string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Checks if the hashed string needs to be rehashed based on the digest size.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n```\n--- File: hash_forge/hashers/blake3_hasher.py ---\n```python\nclass Blake3Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = \"blake3\"\n    library_module: 'ClassVar[str]' = \"blake3\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the Blake3Hasher with a key and an optional digest size.\n\n        Args:\n            key (str): The key to use for the hash function.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the BLAKE3 algorithm and returns the result in a specific format.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format \"algorithm$hashed_hex\".\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if a given string matches a hashed string using the BLAKE3 algorithm.\n\n        Args:\n            _string (str): The input string to verify.\n            _hashed_string (str): The hashed string to compare against, in the format 'algorithm$hashed_value'.\n\n        Returns:\n            bool: True if the input string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n```\n--- File: hash_forge/hashers/pbkdf2_hasher.py ---\n```python\nclass PBKDF2Sha256Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = 'pbkdf2_sha256'\n    digest: 'ClassVar[Callable[..., Any]]'\n\n    def __init__(self, iterations: int = 100_000, salt_length: int = 16) -> None:\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes a given string using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$iterations$salt$hashed'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the hashed string using PBKDF2 algorithm.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against, formatted as 'algorithm$iterations$salt$hashed'.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if a hashed string needs to be rehashed based on the number of iterations.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if rehash is needed, False otherwise.\n        \"\"\"\n        pass\n\nclass PBKDF2Sha1Hasher(PBKDF2Sha256Hasher):\n    algorithm: 'ClassVar[str]' = \"pbkdf2_sha1\"\n    # digest: 'ClassVar[Callable[..., Any]]' # Value removed, type inherited. Implementer provides new value.\n```\n--- File: hash_forge/hashers/ripemd160_hasher.py ---\n```python\nclass Ripemd160Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = \"RIPEMD-160\"\n    library_module: 'ClassVar[str]' = \"Crypto.Hash.RIPEMD160\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the RIPEMD-160 hasher instance.\n\n        This method loads the RIPEMD-160 hashing library and assigns it to the\n        instance variable `self.ripemd160`.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the RIPEMD-160 algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$hashed_value'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed: str, /) -> bool:\n        \"\"\"\n        Verify if the provided string matches the given hashed value.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed (str): The hashed value to compare against, in the format 'algorithm$hash_value'.\n\n        Returns:\n            bool: True if the string matches the hashed value, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the algorithm used in the hashed string does not match\n                  the current algorithm, indicating that a rehash is needed.\n        \"\"\"\n        pass\n```\n--- File: hash_forge/hashers/scrypt_hasher.py ---\n```python\nclass ScryptHasher(PHasher):\n    algorithm: 'ClassVar[str]' = \"scrypt\"\n\n    def __init__(\n        self,\n        work_factor: int = 2**14,\n        block_size: int = 8,\n        parallelism: int = 5,\n        maxmem: int = 0,\n        dklen: int = 64,\n        salt_length: int = 16,\n    ) -> None:\n        \"\"\"\n        Initialize the ScryptHasher with the given parameters.\n\n        Args:\n            work_factor (int): The CPU/memory cost parameter. Default is 2**14.\n            block_size (int): The block size parameter. Default is 8.\n            parallelism (int): The parallelization parameter. Default is 5.\n            maxmem (int): The maximum memory to use in bytes. Default is 0 (no limit).\n            dklen (int): The length of the derived key. Default is 64.\n            salt_length (int): The length of the salt. Default is 16.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str) -> str:\n        \"\"\"\n        Hashes the given string using the scrypt algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$work_factor$salt$block_size$parallelism$hashed_value'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str) -> bool:\n        \"\"\"\n        Verify if a given string matches the hashed string.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the original string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed based on the current\n        work factor, block size, and parallelism parameters.\n\n        Args:\n            _hashed_string (str): The hashed string to check, expected to be in the format\n                                  \"$<prefix>$<n>$<r>$<p>$<hash>\".\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n```\n--- File: hash_forge/hashers/whirlpool_hasher.py ---\n```python\nclass WhirlpoolHasher(PHasher):\n    algorithm: 'ClassVar[str]' = 'whirlpool'\n    library_module: 'ClassVar[str]' = 'Crypto.Hash.SHA512'\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the WhirlpoolHasher instance.\n\n        This constructor initializes the WhirlpoolHasher by loading the SHA-512\n        hashing library module.\n\n        Attributes:\n            sha512: The loaded SHA-512 hashing library module.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Computes the hash of the given string using the SHA-512 algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The resulting hash as a hexadecimal string prefixed with the algorithm name.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if the given string matches the given hash.\n\n        Args:\n            _string (str): The input string to verify.\n            _hashed_string (str): The hash to compare against.\n\n        Returns:\n            bool: True if the hash matches the input string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hash needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hash to check.\n\n        Returns:\n            bool: True if the hash needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "hash_forge/__init__.py",
                "code": "class HashManager:\n    def __init__(self, *hashers: 'PHasher') -> None:\n        \"\"\"\n        Initialize the HashForge instance with one or more hashers.\n\n        Args:\n            *hashers (PHasher): One or more hasher instances to be used by the HashForge.\n\n        Raises:\n            ValueError: If no hashers are provided.\n\n        Attributes:\n            hashers (Set[Tuple[str, PHasher]]): A set of tuples containing the algorithm name and the hasher instance.\n            preferred_hasher (PHasher): The first hasher provided, used as the preferred hasher.\n        \"\"\"\n        pass\n\n    def hash(self, string: str) -> str:\n        \"\"\"\n        Hashes the given string using the preferred hasher.\n\n        Args:\n            string (str): The string to be hashed.\n\n        Returns:\n            str: The hashed string.\n        \"\"\"\n        pass\n\n    def verify(self, string: str, hashed_string: str) -> bool:\n        \"\"\"\n        Verifies if a given string matches a hashed string using the appropriate hashing algorithm.\n\n        Args:\n            string (str): The plain text string to verify.\n            hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n\n        Raises:\n            IndexError: If the hashed string does not contain a valid algorithm identifier.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, hashed_string: str) -> bool:\n        \"\"\"\n        Determines if a given hashed string needs to be rehashed.\n\n        This method checks if the hashing algorithm used for the given hashed string\n        is the preferred algorithm or if the hashed string needs to be rehashed\n        according to the hasher's criteria.\n\n        Args:\n            hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n\n        Raises:\n            IndexError: If the hashed string format is invalid.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "hash_forge/protocols.py",
                "code": "class PHasher:\n    algorithm: 'ClassVar[str]'\n    library_module: 'ClassVar[str | None]' = None\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Computes the hash of the given string.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The resulting hash as a string.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if the provided string matches the hashed string.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the original string matches the hashed string, False otherwise.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determine if a hashed string needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        pass\n\n    def load_library(self, name: str) -> 'ModuleType':\n        \"\"\"\n        Loads a library module by its name.\n\n        This function attempts to import a module specified by the `name` parameter.\n        If the module is not found, it raises an ImportError with a message indicating\n        the required third-party library to install.\n\n        Args:\n            name (str): The name of the module to import.\n\n        Returns:\n            ModuleType: The imported module.\n\n        Raises:\n            ImportError: If the module cannot be imported, with a message suggesting\n                        the required third-party library to install.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "hash_forge/hashers/argon2_hasher.py",
                "code": "class Argon2Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = \"argon2\"\n    library_module: 'ClassVar[str]' = \"argon2\"\n\n    def __init__(\n        self,\n        time_cost: int | None = None,\n        salt_len: int | None = None,\n        memory_cost: int | None = None,\n        parallelism: int | None = None,\n        hash_len: int | None = None,\n    ) -> None:\n        \"\"\"\n        Initialize the Argon2Hasher with optional parameters for hashing configuration.\n\n        Args:\n            time_cost (int | None): The time cost parameter for Argon2. Defaults to None.\n            salt_len (int | None): The length of the salt. Defaults to None.\n            memory_cost (int | None): The memory cost parameter for Argon2. Defaults to None.\n            parallelism (int | None): The degree of parallelism for Argon2. Defaults to None.\n            hash_len (int | None): The length of the resulting hash. Defaults to None.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using Argon2 algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, time cost, memory cost, parallelism, salt, and\n            hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the provided hashed string using Argon2.\n\n        Args:\n            _string (str): The plain string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed based on the current time cost.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "hash_forge/hashers/bcrypt_hasher.py",
                "code": "class BCryptSha256Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = 'bcrypt_sha256'\n    library_module: 'ClassVar[str]' = 'bcrypt'\n    digest: 'Callable[[bytes], Any] | None'\n\n    def __init__(self, rounds: int = 12) -> None:\n        \"\"\"\n        Initializes the BcryptHasher with the specified number of rounds.\n\n        Args:\n            rounds (int, optional): The number of rounds to use for hashing. Defaults to 12.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using bcrypt algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, rounds, salt, and hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if a given string matches the hashed string using bcrypt.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the plain text string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Check if the hashed string needs to be rehashed.\n\n        This method determines whether the provided hashed string needs to be rehashed\n        based on the algorithm and the number of rounds used during hashing.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n\nclass BCryptHasher(BCryptSha256Hasher):\n    algorithm: 'ClassVar[str]' = 'bcrypt'\n    digest: 'Callable[[bytes], Any] | None' = None\n"
            },
            {
                "file_path": "hash_forge/hashers/blake2_hasher.py",
                "code": "class Blake2Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = 'blake2b'\n\n    def __init__(self, key: str, digest_size: int = 64) -> None:\n        \"\"\"\n        Initializes the Blake2Hasher with a key and an optional digest size.\n\n        Args:\n            key (str): The key to use for the hash function.\n            digest_size (int, optional): The size of the digest in bytes. Defaults to 64.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the BLAKE2b algorithm.\n\n        Args:\n            _string (str): The string to be hashed.\n\n        Returns:\n            str: The formatted hash string containing the algorithm, digest size, and hashed value.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the hashed string using BLAKE2b.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the plain text string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Checks if the hashed string needs to be rehashed based on the digest size.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "hash_forge/hashers/blake3_hasher.py",
                "code": "class Blake3Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = \"blake3\"\n    library_module: 'ClassVar[str]' = \"blake3\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the Blake3Hasher with a key and an optional digest size.\n\n        Args:\n            key (str): The key to use for the hash function.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the BLAKE3 algorithm and returns the result in a specific format.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format \"algorithm$hashed_hex\".\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verify if a given string matches a hashed string using the BLAKE3 algorithm.\n\n        Args:\n            _string (str): The input string to verify.\n            _hashed_string (str): The hashed string to compare against, in the format 'algorithm$hashed_value'.\n\n        Returns:\n            bool: True if the input string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "hash_forge/hashers/pbkdf2_hasher.py",
                "code": "class PBKDF2Sha256Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = 'pbkdf2_sha256'\n    digest: 'ClassVar[Callable[..., Any]]'\n\n    def __init__(self, iterations: int = 100_000, salt_length: int = 16) -> None:\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes a given string using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$iterations$salt$hashed'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if a given string matches the hashed string using PBKDF2 algorithm.\n\n        Args:\n            _string (str): The plain text string to verify.\n            _hashed_string (str): The hashed string to compare against, formatted as 'algorithm$iterations$salt$hashed'.\n\n        Returns:\n            bool: True if the string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if a hashed string needs to be rehashed based on the number of iterations.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if rehash is needed, False otherwise.\n        \"\"\"\n        pass\n\nclass PBKDF2Sha1Hasher(PBKDF2Sha256Hasher):\n    algorithm: 'ClassVar[str]' = \"pbkdf2_sha1\"\n    # digest: 'ClassVar[Callable[..., Any]]' # Value removed, type inherited. Implementer provides new value.\n"
            },
            {
                "file_path": "hash_forge/hashers/ripemd160_hasher.py",
                "code": "class Ripemd160Hasher(PHasher):\n    algorithm: 'ClassVar[str]' = \"RIPEMD-160\"\n    library_module: 'ClassVar[str]' = \"Crypto.Hash.RIPEMD160\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the RIPEMD-160 hasher instance.\n\n        This method loads the RIPEMD-160 hashing library and assigns it to the\n        instance variable `self.ripemd160`.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Hashes the given string using the RIPEMD-160 algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$hashed_value'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed: str, /) -> bool:\n        \"\"\"\n        Verify if the provided string matches the given hashed value.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed (str): The hashed value to compare against, in the format 'algorithm$hash_value'.\n\n        Returns:\n            bool: True if the string matches the hashed value, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hashed string to check.\n\n        Returns:\n            bool: True if the algorithm used in the hashed string does not match\n                  the current algorithm, indicating that a rehash is needed.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "hash_forge/hashers/scrypt_hasher.py",
                "code": "class ScryptHasher(PHasher):\n    algorithm: 'ClassVar[str]' = \"scrypt\"\n\n    def __init__(\n        self,\n        work_factor: int = 2**14,\n        block_size: int = 8,\n        parallelism: int = 5,\n        maxmem: int = 0,\n        dklen: int = 64,\n        salt_length: int = 16,\n    ) -> None:\n        \"\"\"\n        Initialize the ScryptHasher with the given parameters.\n\n        Args:\n            work_factor (int): The CPU/memory cost parameter. Default is 2**14.\n            block_size (int): The block size parameter. Default is 8.\n            parallelism (int): The parallelization parameter. Default is 5.\n            maxmem (int): The maximum memory to use in bytes. Default is 0 (no limit).\n            dklen (int): The length of the derived key. Default is 64.\n            salt_length (int): The length of the salt. Default is 16.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str) -> str:\n        \"\"\"\n        Hashes the given string using the scrypt algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The hashed string in the format 'algorithm$work_factor$salt$block_size$parallelism$hashed_value'.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str) -> bool:\n        \"\"\"\n        Verify if a given string matches the hashed string.\n\n        Args:\n            _string (str): The original string to verify.\n            _hashed_string (str): The hashed string to compare against.\n\n        Returns:\n            bool: True if the original string matches the hashed string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str) -> bool:\n        \"\"\"\n        Determines if the given hashed string needs to be rehashed based on the current\n        work factor, block size, and parallelism parameters.\n\n        Args:\n            _hashed_string (str): The hashed string to check, expected to be in the format\n                                  \"$<prefix>$<n>$<r>$<p>$<hash>\".\n\n        Returns:\n            bool: True if the hashed string needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n"
            },
            {
                "file_path": "hash_forge/hashers/whirlpool_hasher.py",
                "code": "class WhirlpoolHasher(PHasher):\n    algorithm: 'ClassVar[str]' = 'whirlpool'\n    library_module: 'ClassVar[str]' = 'Crypto.Hash.SHA512'\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes the WhirlpoolHasher instance.\n\n        This constructor initializes the WhirlpoolHasher by loading the SHA-512\n        hashing library module.\n\n        Attributes:\n            sha512: The loaded SHA-512 hashing library module.\n        \"\"\"\n        pass\n\n    def hash(self, _string: str, /) -> str:\n        \"\"\"\n        Computes the hash of the given string using the SHA-512 algorithm.\n\n        Args:\n            _string (str): The input string to be hashed.\n\n        Returns:\n            str: The resulting hash as a hexadecimal string prefixed with the algorithm name.\n        \"\"\"\n        pass\n\n    def verify(self, _string: str, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Verifies if the given string matches the given hash.\n\n        Args:\n            _string (str): The input string to verify.\n            _hashed_string (str): The hash to compare against.\n\n        Returns:\n            bool: True if the hash matches the input string, False otherwise.\n        \"\"\"\n        pass\n\n    def needs_rehash(self, _hashed_string: str, /) -> bool:\n        \"\"\"\n        Determines if the given hash needs to be rehashed.\n\n        Args:\n            _hashed_string (str): The hash to check.\n\n        Returns:\n            bool: True if the hash needs to be rehashed, False otherwise.\n        \"\"\"\n        pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "src/tests/test_hash_manager.py::test_hash_with_preferred_hasher",
                "covers": [
                    "hash_forge.HashManager.__init__ - instantiation with multiple hashers",
                    "hash_forge.HashManager.hash - happy path with preferred hasher",
                    "hash_forge.HashManager.verify - happy path with preferred hasher",
                    "hash_forge.hashers.PBKDF2Sha256Hasher.__init__ - instantiation as part of HashManager",
                    "hash_forge.hashers.BCryptSha256Hasher.__init__ - instantiation as part of HashManager",
                    "hash_forge.hashers.Argon2Hasher.__init__ - instantiation as part of HashManager",
                    "hash_forge.hashers.ScryptHasher.__init__ - instantiation as part of HashManager",
                    "hash_forge.hashers.Ripemd160Hasher.__init__ - instantiation as part of HashManager",
                    "hash_forge.hashers.Blake2Hasher.__init__ - instantiation as part of HashManager",
                    "hash_forge.hashers.WhirlpoolHasher.__init__ - instantiation as part of HashManager"
                ]
            },
            {
                "test_id": "src/tests/test_hash_manager.py::test_needs_rehash_false",
                "covers": [
                    "hash_forge.HashManager.needs_rehash - happy path (false) for preferred hasher's hash"
                ]
            },
            {
                "test_id": "src/tests/test_argon2.py::test_argon2_verify_correct_data",
                "covers": [
                    "hash_forge.hashers.Argon2Hasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.Argon2Hasher.hash - happy path",
                    "hash_forge.hashers.Argon2Hasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_argon2.py::test_argon2_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.Argon2Hasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_bcrypt.py::test_bcrypt_verify_correct_data",
                "covers": [
                    "hash_forge.hashers.BCryptHasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.BCryptHasher.hash - happy path",
                    "hash_forge.hashers.BCryptHasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_bcrypt.py::test_bcrypt_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.BCryptHasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_verify_correct_data",
                "covers": [
                    "hash_forge.hashers.BCryptSha256Hasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.BCryptSha256Hasher.hash - happy path",
                    "hash_forge.hashers.BCryptSha256Hasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_bcrypt_sha256.py::test_bcrypt_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.BCryptSha256Hasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_verify_correct_data",
                "covers": [
                    "hash_forge.hashers.PBKDF2Sha256Hasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.PBKDF2Sha256Hasher.hash - happy path",
                    "hash_forge.hashers.PBKDF2Sha256Hasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_pbkdf2_sha256.py::test_pbkdf2_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.PBKDF2Sha256Hasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_verify_correct_data",
                "covers": [
                    "hash_forge.hashers.PBKDF2Sha1Hasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.PBKDF2Sha1Hasher.hash - happy path",
                    "hash_forge.hashers.PBKDF2Sha1Hasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_pbkdf2_sha1.py::test_pbkdf2_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.PBKDF2Sha1Hasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_scrypt.py::test_scrypt_verify_correct_data",
                "covers": [
                    "hash_forge.hashers.ScryptHasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.ScryptHasher.hash - happy path",
                    "hash_forge.hashers.ScryptHasher.generate_salt - exercised by hash method",
                    "hash_forge.hashers.ScryptHasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_scrypt.py::test_scrypt_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.ScryptHasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_blake2.py::test_verify_hash_correct",
                "covers": [
                    "hash_forge.hashers.Blake2Hasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.Blake2Hasher.hash - happy path",
                    "hash_forge.hashers.Blake2Hasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_blake2.py::test_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.Blake2Hasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_ripemd160.py::test_ripemd160_verify_correct_data",
                "covers": [
                    "hash_forge.hashers.Ripemd160Hasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.Ripemd160Hasher.hash - happy path",
                    "hash_forge.hashers.Ripemd160Hasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_ripemd160.py::test_ripemd160_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.Ripemd160Hasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_whirlpool.py::test_whirlpool_verify_correct_data",
                "covers": [
                    "hash_forge.hashers.WhirlpoolHasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.WhirlpoolHasher.hash - happy path",
                    "hash_forge.hashers.WhirlpoolHasher.verify - happy path for correct data"
                ]
            },
            {
                "test_id": "src/tests/test_whirlpool.py::test_whirlpool_needs_rehash_false",
                "covers": [
                    "hash_forge.hashers.WhirlpoolHasher.needs_rehash - happy path (false) with current parameters"
                ]
            },
            {
                "test_id": "src/tests/test_blake3.py::test_verify_hash_correct",
                "covers": [
                    "hash_forge.hashers.Blake3Hasher.__init__ - direct instantiation for happy path test",
                    "hash_forge.hashers.Blake3Hasher.hash - happy path",
                    "hash_forge.hashers.Blake3Hasher.verify - happy path for correct data"
                ]
            }
        ]
    },
    {
        "idx": 88793,
        "repo_name": "silvio-machado_br-eval",
        "url": "https://github.com/silvio-machado/br-eval",
        "description": "Python library for validation and formatting of Brazilian data such as CPF, CNPJ, postal codes (CEP), phone numbers, and vehicle license plates.",
        "stars": 9,
        "forks": 1,
        "language": "python",
        "size": 41,
        "created_at": "2024-10-18T00:21:17+00:00",
        "updated_at": "2024-10-25T13:38:38+00:00",
        "pypi_info": {
            "name": "br-eval",
            "version": "1.0.1",
            "url": "https://files.pythonhosted.org/packages/f0/95/dddd54f3884ce0630a1f8c62739f12187622ab665d8784f53ef4bb4f26f3/br_eval-1.0.1.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 19,
            "comment_ratio": 0.24500768049155147,
            "pyfile_content_length": 43039,
            "pyfile_code_lines": 1302,
            "test_file_exist": true,
            "test_file_content_length": 13483,
            "pytest_framework": true,
            "test_case_num": 50,
            "metadata_path": [
                "setup.py"
            ],
            "readme_content_length": 5848,
            "llm_reason": "The project is highly suitable as a benchmark. \n**Positive Aspects:**\n1.  **Self-Contained & Independent:** The library's core functionality (validating, formatting, and generating Brazilian identifiers like CPF, CNPJ, CEP, Plates, Phone numbers) is algorithmic and pattern-based. It does not require internet access, external APIs, or complex external services for its operation or testing. Dependencies for an AI-rebuilt solution would likely be limited to the Python standard library (e.g., `re`, `random`).\n2.  **Clear & Well-Defined Functionality:** The README provides clear descriptions and usage examples for each data type and its associated functions (validate, format, generate). The scope of what needs to be rebuilt is specific.\n3.  **Testable & Verifiable Output:** The project includes a comprehensive suite of unit tests (`tests/test_*.py` files using `unittest`). These tests cover various valid and invalid inputs, formatting, and generation, providing excellent means to verify an AI-rebuilt solution.\n4.  **No Graphical User Interface (GUI):** It is a Python library, intended for programmatic use, not a GUI application.\n5.  **Appropriate Complexity & Scope (Medium):** The project involves implementing several distinct modules, each with specific logic: checksum algorithms for CPF/CNPJ, regex for plate and phone patterns, string manipulation for formatting, and rule-based generation. It also includes a hierarchy of custom exceptions. This is non-trivial, offering a good challenge, but not excessively complex or large. A human could replicate it in hours to a few days.\n6.  **Well-Understood Problem Domain:** Validation and formatting of national/regional identifiers is a common programming task. The Brazilian-specific rules are the core of the problem, and the README provides context.\n7.  **Predominantly Code-Based Solution:** The task is to generate Python code for the library's functions and custom exceptions.\n8.  **Modular Structure:** The codebase is well-organized into modules for each data type and a separate exceptions package, which is a good pattern for an AI to replicate.\n\n**Negative Aspects or Concerns:**\n*   The primary challenge for an AI rebuilding this from scratch based *only* on the README would be to correctly implement the specific Brazilian validation algorithms (e.g., CPF/CNPJ checksums) and detailed rules (e.g., all valid phone DDDs, exact plate patterns) if they are not explicitly provided or easily inferable. However, since the task is to *recreate this specific project*, the existing project's behavior (and tests) would serve as the definitive specification. This is more of a note on how the benchmark prompt would be framed rather than a flaw in the project itself.\n\nOverall, the project is an excellent candidate because it's a realistic, self-contained utility library with clear requirements, a manageable scope for an AI to build from scratch, and robust testability.",
            "llm_project_type": "Utility library for data validation, formatting, and generation",
            "llm_rating": 90,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "silvio-machado_br-eval",
            "finish_test": true,
            "test_case_result": {
                "tests/test_cep.py::TestCEP::test_clean_cep": "passed",
                "tests/test_cep.py::TestCEP::test_format_cep": "passed",
                "tests/test_cep.py::TestCEP::test_generate_cep": "passed",
                "tests/test_cep.py::TestCEP::test_generate_formatted_cep": "passed",
                "tests/test_cep.py::TestCEP::test_invalid_character_cep": "passed",
                "tests/test_cep.py::TestCEP::test_invalid_length_cep": "passed",
                "tests/test_cep.py::TestCEP::test_valid_cep": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_all_same_digits": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_clean_cnpj": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_format_cnpj": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_invalid_characters_cnpj": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_invalid_length_cnpj": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_invalid_verification_digits": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_repeated_digits_cnpj": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj_generator": "passed",
                "tests/test_cnpj.py::TestCNPJ::test_whitespace_cnpj": "passed",
                "tests/test_cpf.py::TestCPF::test_all_same_digits": "passed",
                "tests/test_cpf.py::TestCPF::test_clean_cpf": "passed",
                "tests/test_cpf.py::TestCPF::test_format_invalid_characters_cpf": "passed",
                "tests/test_cpf.py::TestCPF::test_format_invalid_length_cpf": "passed",
                "tests/test_cpf.py::TestCPF::test_invalid_characters_cpf": "passed",
                "tests/test_cpf.py::TestCPF::test_invalid_first_digit": "passed",
                "tests/test_cpf.py::TestCPF::test_invalid_length_cpf": "passed",
                "tests/test_cpf.py::TestCPF::test_invalid_second_digit": "passed",
                "tests/test_cpf.py::TestCPF::test_repeated_digits_cpf": "passed",
                "tests/test_cpf.py::TestCPF::test_valid_cpf": "passed",
                "tests/test_cpf.py::TestCPF::test_valid_cpf_generator": "passed",
                "tests/test_cpf.py::TestCPF::test_whitespace_cpf": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_clean_phone_number": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_format_phone_number": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_generate_phone_number": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_invalid_characters": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_invalid_ddd": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_invalid_length": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_invalid_number_pattern": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_valid_landline_numbers": "passed",
                "tests/test_phone.py::TestPhoneNumber::test_valid_mobile_numbers": "passed",
                "tests/test_plate.py::TestPlate::test_format_plate": "passed",
                "tests/test_plate.py::TestPlate::test_format_plate_invalid_length": "passed",
                "tests/test_plate.py::TestPlate::test_generate_mercosul_car_plate": "passed",
                "tests/test_plate.py::TestPlate::test_generate_mercosul_motorcycle_plate": "passed",
                "tests/test_plate.py::TestPlate::test_generate_old_plate": "passed",
                "tests/test_plate.py::TestPlate::test_generate_plate_formatted": "passed",
                "tests/test_plate.py::TestPlate::test_invalid_plate_characters": "passed",
                "tests/test_plate.py::TestPlate::test_invalid_plate_length": "passed",
                "tests/test_plate.py::TestPlate::test_invalid_plate_pattern": "passed",
                "tests/test_plate.py::TestPlate::test_valid_mercosul_car_plate": "passed",
                "tests/test_plate.py::TestPlate::test_valid_mercosul_motorcycle_plate": "passed",
                "tests/test_plate.py::TestPlate::test_valid_old_plate": "passed"
            },
            "success_count": 50,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "unknown_count": 0,
            "total_count": 50,
            "success_rate": 1.0,
            "coverage_report": {
                "covered_lines": 287,
                "num_statements": 330,
                "percent_covered": 85.4066985645933,
                "percent_covered_display": "85",
                "missing_lines": 43,
                "excluded_lines": 0,
                "num_branches": 88,
                "num_partial_branches": 12,
                "covered_branches": 70,
                "missing_branches": 18
            },
            "coverage_result": {}
        },
        "codelines_count": 1302,
        "codefiles_count": 19,
        "code_length": 43039,
        "test_files_count": 5,
        "test_code_length": 13483,
        "class_diagram": "@startuml\nclass TestCNPJ {\n    test_valid_cnpj(): void\n    test_valid_cnpj_generator(): void\n    test_invalid_verification_digits(): void\n    test_repeated_digits_cnpj(): void\n    test_invalid_characters_cnpj(): void\n    test_invalid_length_cnpj(): void\n    test_clean_cnpj(): void\n    test_format_cnpj(): void\n    test_whitespace_cnpj(): void\n    test_all_same_digits(): void\n}\nclass TestPlate {\n    test_valid_old_plate(): void\n    test_valid_mercosul_car_plate(): void\n    test_valid_mercosul_motorcycle_plate(): void\n    test_invalid_plate_length(): void\n    test_invalid_plate_characters(): void\n    test_invalid_plate_pattern(): void\n    test_format_plate(): void\n    test_format_plate_invalid_length(): void\n    test_generate_old_plate(): void\n    test_generate_mercosul_car_plate(): void\n    test_generate_mercosul_motorcycle_plate(): void\n    test_generate_plate_formatted(): void\n}\nclass TestCEP {\n    test_valid_cep(): void\n    test_invalid_length_cep(): void\n    test_invalid_character_cep(): void\n    test_format_cep(): void\n    test_clean_cep(): void\n    test_generate_cep(): void\n    test_generate_formatted_cep(): void\n}\nclass TestPhoneNumber {\n    test_valid_mobile_numbers(): void\n    test_valid_landline_numbers(): void\n    test_invalid_characters(): void\n    test_invalid_length(): void\n    test_invalid_ddd(): void\n    test_invalid_number_pattern(): void\n    test_format_phone_number(): void\n    test_generate_phone_number(): void\n    test_clean_phone_number(): void\n}\nclass TestCPF {\n    test_valid_cpf(): void\n    test_valid_cpf_generator(): void\n    test_invalid_first_digit(): void\n    test_invalid_second_digit(): void\n    test_repeated_digits_cpf(): void\n    test_invalid_characters_cpf(): void\n    test_invalid_length_cpf(): void\n    test_whitespace_cpf(): void\n    test_clean_cpf(): void\n    test_format_invalid_length_cpf(): void\n    test_format_invalid_characters_cpf(): void\n    test_all_same_digits(): void\n}\nclass CEPError {\n}\nclass InvalidCEPError {\n    __str__(): void\n}\nclass InvalidLengthCEPError {\n    __init__(length): void\n    __str__(): void\n}\nclass InvalidCharacterCEPError {\n    __str__(): void\n}\nclass CPFError {\n}\nclass InvalidCPFError {\n}\nclass FirstDigitInvalidError {\n    __str__(): void\n}\nclass SecondDigitInvalidError {\n    __str__(): void\n}\nclass RepeatedDigitsCPFError {\n    __str__(): void\n}\nclass InvalidFormatCPFError {\n    __str__(): void\n}\nclass InvalidLengthCPFError {\n    __init__(length): void\n    __str__(): void\n}\nclass CNPJError {\n}\nclass InvalidCNPJError {\n}\nclass RepeatedDigitsCNPJError {\n    __str__(): void\n}\nclass InvalidFormatCNPJError {\n    __str__(): void\n}\nclass InvalidLengthCNPJError {\n    __init__(length): void\n    __str__(): void\n}\nclass PlateError {\n}\nclass InvalidPlateError {\n    __str__(): void\n}\nclass InvalidFormatPlateError {\n    __str__(): void\n}\nclass InvalidCharacterPlateError {\n    __str__(): void\n}\nclass InvalidLengthPlateError {\n    __str__(): void\n}\nclass PhoneNumberError {\n}\nclass InvalidPhoneNumberError {\n    __str__(): void\n}\nclass InvalidLengthPhoneNumberError {\n    __init__(length): void\n    __str__(): void\n}\nclass InvalidCharacterPhoneNumberError {\n    __str__(): void\n}\nclass InvalidDDDPhoneNumberError {\n    __init__(ddd): void\n    __str__(): void\n}\nCPFError <|-- InvalidLengthCPFError\nPhoneNumberError <|-- InvalidCharacterPhoneNumberError\nPlateError <|-- InvalidLengthPlateError\nCPFError <|-- InvalidFormatCPFError\nPhoneNumberError <|-- InvalidLengthPhoneNumberError\nCPFError <|-- InvalidCPFError\nPlateError <|-- InvalidFormatPlateError\nCNPJError <|-- InvalidCNPJError\nCEPError <|-- InvalidCharacterCEPError\nCEPError <|-- InvalidLengthCEPError\nCNPJError <|-- InvalidFormatCNPJError\nInvalidCPFError <|-- SecondDigitInvalidError\nPlateError <|-- InvalidPlateError\nCNPJError <|-- InvalidLengthCNPJError\nPlateError <|-- InvalidCharacterPlateError\nInvalidCPFError <|-- RepeatedDigitsCPFError\nInvalidCNPJError <|-- RepeatedDigitsCNPJError\nInvalidCPFError <|-- FirstDigitInvalidError\nCEPError <|-- InvalidCEPError\nPhoneNumberError <|-- InvalidDDDPhoneNumberError\nPhoneNumberError <|-- InvalidPhoneNumberError\n@enduml",
        "structure": [
            {
                "file": "setup.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/test_cnpj.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestCNPJ",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_cnpj",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_valid_cnpj_generator",
                                "docstring": "This test will generate 10 valid CNPJs and check if they are valid.\nAlso generates 10 formatted CNPJs and checks if they are valid.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_verification_digits",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_repeated_digits_cnpj",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_characters_cnpj",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_length_cnpj",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_clean_cnpj",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_format_cnpj",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_whitespace_cnpj",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_all_same_digits",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_plate.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestPlate",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_old_plate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_valid_mercosul_car_plate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_valid_mercosul_motorcycle_plate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_plate_length",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_plate_characters",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_plate_pattern",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_format_plate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_format_plate_invalid_length",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_generate_old_plate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_generate_mercosul_car_plate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_generate_mercosul_motorcycle_plate",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_generate_plate_formatted",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_cep.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestCEP",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_cep",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_length_cep",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_character_cep",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_format_cep",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_clean_cep",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_generate_cep",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_generate_formatted_cep",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/test_phone.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestPhoneNumber",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_mobile_numbers",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_valid_landline_numbers",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_characters",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_length",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_ddd",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_number_pattern",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_format_phone_number",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_generate_phone_number",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_clean_phone_number",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/test_cpf.py",
                "functions": [],
                "classes": [
                    {
                        "name": "TestCPF",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "test_valid_cpf",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_valid_cpf_generator",
                                "docstring": "This test will generate 10 valid CPFs and check if they are valid.\nAlsto generates 10 formatted CPFs and checks if they are valid.",
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_first_digit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_second_digit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_repeated_digits_cpf",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_characters_cpf",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_invalid_length_cpf",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_whitespace_cpf",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_clean_cpf",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_format_invalid_length_cpf",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_format_invalid_characters_cpf",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "test_all_same_digits",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "br_eval/cpf.py",
                "functions": [
                    {
                        "name": "format_cpf",
                        "docstring": "Formats a CPF string in the pattern XXX.XXX.XXX-XX.\n\nArgs:\n    cpf (str): The CPF string with exactly 11 digits.\n\nReturns:\n    str: The formatted CPF string.\n\nRaises:\n    ValueError: If the CPF does not have exactly 11 digits.",
                        "comments": null,
                        "args": [
                            "cpf"
                        ]
                    },
                    {
                        "name": "clean_cpf",
                        "docstring": "Removes all non-digit characters from the CPF.\nRaises an exception if the CPF does not have 11 digits after cleaning.",
                        "comments": null,
                        "args": [
                            "cpf"
                        ]
                    },
                    {
                        "name": "validate_cpf",
                        "docstring": "Validates a CPF by checking the verification digits.\nRaises specific exceptions for different validation errors.",
                        "comments": null,
                        "args": [
                            "cpf"
                        ]
                    },
                    {
                        "name": "generate_cpf",
                        "docstring": "Generates a valid CPF number.\n\nArgs:\n    formatted (bool):\n        If True, returns the CPF in the formatted pattern XXX.XXX.XXX-XX.\n        If False, returns the CPF as a numeric string.\n\nReturns:\n    str: A valid CPF number.",
                        "comments": null,
                        "args": [
                            "formatted"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "br_eval/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "br_eval/cnpj.py",
                "functions": [
                    {
                        "name": "clean_cnpj",
                        "docstring": "Removes all non-digit characters from the CNPJ.\nRaises an exception if the CNPJ does not have 14 digits after cleaning.",
                        "comments": null,
                        "args": [
                            "cnpj"
                        ]
                    },
                    {
                        "name": "format_cnpj",
                        "docstring": "Formats a CNPJ string into the pattern XX.XXX.XXX/YYYY-ZZ.\n\nArgs:\n    cnpj (str): The CNPJ string with exactly 14 digits.\n\nReturns:\n    str: The formatted CNPJ string.\n\nRaises:\n    InvalidLengthCNPJError: If the CNPJ does not have exactly 14 digits.",
                        "comments": null,
                        "args": [
                            "cnpj"
                        ]
                    },
                    {
                        "name": "validate_cnpj",
                        "docstring": "Validates a CNPJ by checking the verification digits.\nRaises specific exceptions for different validation errors.\n\nArgs:\n    cnpj (str): The CNPJ string to validate.\n\nReturns:\n    bool: True if the CNPJ is valid.\n\nRaises:\n    RepeatedDigitsCNPJError: If all digits are the same.\n    InvalidCNPJError: If the verification digits do not match.",
                        "comments": null,
                        "args": [
                            "cnpj"
                        ]
                    },
                    {
                        "name": "generate_cnpj",
                        "docstring": "Generates a valid CNPJ number.\n\nArgs:\n    formatted (bool):\n        True, returns the CNPJ in the formatted pattern XX.XXX.XXX/YYYY-ZZ.\n        False, returns the CNPJ as a numeric string.\n\nReturns:\n    str: A valid CNPJ number.",
                        "comments": null,
                        "args": [
                            "formatted"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "br_eval/cep.py",
                "functions": [
                    {
                        "name": "clean_cep",
                        "docstring": "Remove all non-numeric characters from the CEP.\nChecks if the CEP has exactly 8 digits after cleaning.\n\nArgs:\n    cep (str): The CEP to be cleaned.\n\nReturns:\n    str: The CEP containing only numeric digits.\n\nRaises:\n    InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n    InvalidCharacterCEPError: If the CEP contains letters.",
                        "comments": null,
                        "args": [
                            "cep"
                        ]
                    },
                    {
                        "name": "format_cep",
                        "docstring": "Format the CEP in the 'XXXXX-XXX' pattern.\n\nArgs:\n    cep (str): The CEP containing 8 digits.\n\nReturns:\n    str: The formatted CEP.\n\nRaises:\n    InvalidLengthCEPError: If the CEP does not have exactly 8 digits.",
                        "comments": null,
                        "args": [
                            "cep"
                        ]
                    },
                    {
                        "name": "validate_cep",
                        "docstring": "Validate the CEP by checking if it is in the correct format.\n\nArgs:\n    cep (str): The CEP to be validated.\n\nReturns:\n    bool: True if the CEP is valid.\n\nRaises:\n    CEPError: Specific exceptions if the CEP is invalid.",
                        "comments": null,
                        "args": [
                            "cep"
                        ]
                    },
                    {
                        "name": "generate_cep",
                        "docstring": "Generate a valid random CEP.\n\nArgs:\n    formatted (bool): If True, returns the CEP formatted with a hyphen.\n\nReturns:\n    str: The generated CEP.",
                        "comments": null,
                        "args": [
                            "formatted"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "br_eval/plate.py",
                "functions": [
                    {
                        "name": "validate_plate",
                        "docstring": "Validates a Brazilian vehicle plate, considering old and new\nMercosul standards.\nRaises specific exceptions for different validation errors.\n\nArgs:\n    plate (str): The plate string to validate.\n\nReturns:\n    str: The type of plate validated (\n        e.g., 'Old', 'Mercosul Car', 'Mercosul Motorcycle'\n    ).\n\nRaises:\n    InvalidLengthPlateError: If the plate does not have 7 characters.\n    InvalidCharacterPlateError: If the plate contains invalid characters.\n    InvalidPlateError: If the plate does not match any valid pattern.",
                        "comments": null,
                        "args": [
                            "plate"
                        ]
                    },
                    {
                        "name": "format_plate",
                        "docstring": "Formats the plate string by inserting a hyphen.\n\nArgs:\n    plate (str): The plate string to format.\n\nReturns:\n    str: The formatted plate string (e.g., 'ABC-1234').",
                        "comments": null,
                        "args": [
                            "plate"
                        ]
                    },
                    {
                        "name": "generate_old_plate",
                        "docstring": "Generates a valid vehicle plate in the old format (ABC1234).\n\nArgs:\n    formatted (bool): If True, returns the plate with a hyphen (ABC-1234).\n\nReturns:\n    str: A generated vehicle plate.",
                        "comments": null,
                        "args": [
                            "formatted"
                        ]
                    },
                    {
                        "name": "generate_mercosul_car_plate",
                        "docstring": "Generates a valid vehicle plate in the Mercosul car format (ABC1D23).\n\nArgs:\n    formatted (bool): If True, returns the plate with a hyphen (ABC-1D23).\n\nReturns:\n    str: A generated vehicle plate.",
                        "comments": null,
                        "args": [
                            "formatted"
                        ]
                    },
                    {
                        "name": "generate_mercosul_motorcycle_plate",
                        "docstring": "Generates valid vehicle plate in the Mercosul motorcycle format (ABC12D3).\n\nArgs:\n    formatted (bool): If True, returns the plate with a hyphen (ABC-12D3).\n\nReturns:\n    str: A generated vehicle plate.",
                        "comments": null,
                        "args": [
                            "formatted"
                        ]
                    },
                    {
                        "name": "generate_plate",
                        "docstring": "Generates a vehicle plate of the specified type.\n\nArgs:\n    plate_type (str): Type of plate to generate (\n        'old', 'mercosul_car', 'mercosul_motorcycle'\n    ).\n    formatted (bool): If True, returns the plate with a hyphen.\n\nReturns:\n    str: A generated vehicle plate.\n\nRaises:\n    ValueError: If the plate_type is not recognized.",
                        "comments": null,
                        "args": [
                            "plate_type",
                            "formatted"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "br_eval/phone.py",
                "functions": [
                    {
                        "name": "clean_phone_number",
                        "docstring": "Removes all non-digit characters from the phone number.\nRaises InvalidCharacterPhoneNumberError if letters are present.\n\nArgs:\n    phone_number (str): The phone number to clean.\n\nReturns:\n    str: The cleaned phone number containing only digits.\n\nRaises:\n    InvalidCharacterPhoneNumberError: If the phone number contains letters.",
                        "comments": null,
                        "args": [
                            "phone_number"
                        ]
                    },
                    {
                        "name": "validate_phone_number",
                        "docstring": "Validates a Brazilian phone number.\n\nArgs:\n    phone_number (str): The phone number to validate.\n\nReturns:\n    dict: A dictionary with keys 'type' and 'formatted_number'.\n          'type' can be 'mobile' or 'landline'.\n\nRaises:\n    InvalidLengthPhoneNumberError\n    InvalidCharacterPhoneNumberError\n    InvalidDDDPhoneNumberError\n    InvalidPhoneNumberError",
                        "comments": null,
                        "args": [
                            "phone_number"
                        ]
                    },
                    {
                        "name": "format_phone_number",
                        "docstring": "Formats the phone number into a standard format.\n\nArgs:\n    phone_number (str): The phone number to format.\n    international (bool): If True, includes the country code '+55'.\n\nReturns:\n    str: The formatted phone number.",
                        "comments": null,
                        "args": [
                            "phone_number",
                            "international"
                        ]
                    },
                    {
                        "name": "generate_phone_number",
                        "docstring": "Generates a valid Brazilian phone number.\n\nArgs:\n    phone_type (str): 'mobile' or 'landline'.\n    formatted (bool): If True, returns the formatted phone number.\n    international (bool): If True, includes the country code '+55'.\n\nReturns:\n    str: The generated phone number.",
                        "comments": null,
                        "args": [
                            "phone_type",
                            "formatted",
                            "international"
                        ]
                    }
                ],
                "classes": []
            },
            {
                "file": "br_eval/exceptions/cep_exceptions.py",
                "functions": [],
                "classes": [
                    {
                        "name": "CEPError",
                        "docstring": "Base class for CEP exceptions.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "InvalidCEPError",
                        "docstring": "Exception for invalid CEPs.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidLengthCEPError",
                        "docstring": "Exception for CEPs with incorrect number of digits.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "length"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidCharacterCEPError",
                        "docstring": "Exception for CEPs with invalid characters.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "br_eval/exceptions/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "br_eval/exceptions/cpf_exceptions.py",
                "functions": [],
                "classes": [
                    {
                        "name": "CPFError",
                        "docstring": "Base class for CPF exceptions.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "InvalidCPFError",
                        "docstring": "Exception raised when the CPF is invalid.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "FirstDigitInvalidError",
                        "docstring": "Exception raised when the first verification digit is incorrect.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "SecondDigitInvalidError",
                        "docstring": "Exception raised when the second verification digit is incorrect.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "RepeatedDigitsCPFError",
                        "docstring": "Exception for CPFs with all digits equal.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidFormatCPFError",
                        "docstring": "Exception raised when the CPF contains non-numeric characters.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidLengthCPFError",
                        "docstring": "Exception raised when the CPF does not have 11 digits.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "length"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "br_eval/exceptions/cnpj_exceptions.py",
                "functions": [],
                "classes": [
                    {
                        "name": "CNPJError",
                        "docstring": "Base class for CNPJ exceptions.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "InvalidCNPJError",
                        "docstring": "Exception raised when the CNPJ is invalid.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "RepeatedDigitsCNPJError",
                        "docstring": "Exception for CNPJs with all digits equal.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidFormatCNPJError",
                        "docstring": "Exception raised when the CNPJ contains invalid characters.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidLengthCNPJError",
                        "docstring": "Exception raised when the CNPJ does not have 14 digits.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "length"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "br_eval/exceptions/plate_exceptions.py",
                "functions": [],
                "classes": [
                    {
                        "name": "PlateError",
                        "docstring": "Base class for plate exceptions.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "InvalidPlateError",
                        "docstring": "Exception raised when the plate is invalid.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidFormatPlateError",
                        "docstring": "Exception raised when the plate has invalid characters or length.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidCharacterPlateError",
                        "docstring": "Exception raised when the plate contains invalid characters.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidLengthPlateError",
                        "docstring": "Exception raised when the plate does not have 7 characters.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "br_eval/exceptions/phone_exceptions.py",
                "functions": [],
                "classes": [
                    {
                        "name": "PhoneNumberError",
                        "docstring": "Base class for phone number exceptions.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "InvalidPhoneNumberError",
                        "docstring": "Exception raised when the phone number is invalid.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidLengthPhoneNumberError",
                        "docstring": "Exception raised when the phone number has an invalid length.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "length"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidCharacterPhoneNumberError",
                        "docstring": "Exception raised when the phone number contains invalid characters.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "InvalidDDDPhoneNumberError",
                        "docstring": "Exception raised when the DDD code is invalid.",
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "ddd"
                                ]
                            },
                            {
                                "name": "__str__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            }
        ],
        "test_cases": {
            "tests/test_cep.py::TestCEP::test_clean_cep": {
                "testid": "tests/test_cep.py::TestCEP::test_clean_cep",
                "result": "passed",
                "test_implementation": "    def test_clean_cep(self):\n        cleaned = clean_cep('01001-000')\n        self.assertEqual(cleaned, '01001000')"
            },
            "tests/test_cep.py::TestCEP::test_format_cep": {
                "testid": "tests/test_cep.py::TestCEP::test_format_cep",
                "result": "passed",
                "test_implementation": "    def test_format_cep(self):\n        formatted = format_cep('01001000')\n        self.assertEqual(formatted, '01001-000')"
            },
            "tests/test_cep.py::TestCEP::test_generate_cep": {
                "testid": "tests/test_cep.py::TestCEP::test_generate_cep",
                "result": "passed",
                "test_implementation": "    def test_generate_cep(self):\n        for _ in range(100):\n            cep = generate_cep()\n            self.assertTrue(validate_cep(cep))"
            },
            "tests/test_cep.py::TestCEP::test_generate_formatted_cep": {
                "testid": "tests/test_cep.py::TestCEP::test_generate_formatted_cep",
                "result": "passed",
                "test_implementation": "    def test_generate_formatted_cep(self):\n        cep_formatted = generate_cep(formatted=True)\n        self.assertEqual(len(cep_formatted), 9)\n        self.assertEqual(cep_formatted[5], '-')\n        self.assertTrue(validate_cep(cep_formatted))"
            },
            "tests/test_cep.py::TestCEP::test_invalid_character_cep": {
                "testid": "tests/test_cep.py::TestCEP::test_invalid_character_cep",
                "result": "passed",
                "test_implementation": "    def test_invalid_character_cep(self):\n        with self.assertRaises(InvalidCharacterCEPError):\n            validate_cep('01A01-000')\n        with self.assertRaises(InvalidCharacterCEPError):\n            validate_cep('ABCDE-FGH')\n        with self.assertRaises(InvalidCharacterCEPError):\n            validate_cep('94445abcx162')"
            },
            "tests/test_cep.py::TestCEP::test_invalid_length_cep": {
                "testid": "tests/test_cep.py::TestCEP::test_invalid_length_cep",
                "result": "passed",
                "test_implementation": "    def test_invalid_length_cep(self):\n        with self.assertRaises(InvalidLengthCEPError):\n            validate_cep('1234567')\n        with self.assertRaises(InvalidLengthCEPError):\n            validate_cep('123456789')"
            },
            "tests/test_cep.py::TestCEP::test_valid_cep": {
                "testid": "tests/test_cep.py::TestCEP::test_valid_cep",
                "result": "passed",
                "test_implementation": "    def test_valid_cep(self):\n        self.assertTrue(validate_cep('01001-000'))\n        self.assertTrue(validate_cep('30140071'))\n        self.assertTrue(validate_cep('01001000'))"
            },
            "tests/test_cnpj.py::TestCNPJ::test_all_same_digits": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_all_same_digits",
                "result": "passed",
                "test_implementation": "    def test_all_same_digits(self):\n        with self.assertRaises(RepeatedDigitsCNPJError):\n            validate_cnpj(\"00000000000000\")\n        with self.assertRaises(RepeatedDigitsCNPJError):\n            validate_cnpj(\"99.999.999/9999-99\")"
            },
            "tests/test_cnpj.py::TestCNPJ::test_clean_cnpj": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_clean_cnpj",
                "result": "passed",
                "test_implementation": "    def test_clean_cnpj(self):\n        cnpj_clean = clean_cnpj(\"13.347.016/0001-17\")\n        self.assertEqual(cnpj_clean, \"13347016000117\")"
            },
            "tests/test_cnpj.py::TestCNPJ::test_format_cnpj": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_format_cnpj",
                "result": "passed",
                "test_implementation": "    def test_format_cnpj(self):\n        cnpj_formatted = format_cnpj(\"13347016000117\")\n        self.assertEqual(cnpj_formatted, \"13.347.016/0001-17\")"
            },
            "tests/test_cnpj.py::TestCNPJ::test_invalid_characters_cnpj": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_invalid_characters_cnpj",
                "result": "passed",
                "test_implementation": "    def test_invalid_characters_cnpj(self):\n        with self.assertRaises(InvalidLengthCNPJError):\n            validate_cnpj(\"13.347.016/0001-1A\")"
            },
            "tests/test_cnpj.py::TestCNPJ::test_invalid_length_cnpj": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_invalid_length_cnpj",
                "result": "passed",
                "test_implementation": "    def test_invalid_length_cnpj(self):\n        with self.assertRaises(InvalidLengthCNPJError):\n            validate_cnpj(\"13.347.016/0001-1\")\n        with self.assertRaises(InvalidLengthCNPJError):\n            validate_cnpj(\"13.347.016/0001-177\")"
            },
            "tests/test_cnpj.py::TestCNPJ::test_invalid_verification_digits": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_invalid_verification_digits",
                "result": "passed",
                "test_implementation": "    def test_invalid_verification_digits(self):\n        with self.assertRaises(InvalidCNPJError):\n            validate_cnpj(\"13.347.016/0001-18\")  # Altered last digit"
            },
            "tests/test_cnpj.py::TestCNPJ::test_repeated_digits_cnpj": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_repeated_digits_cnpj",
                "result": "passed",
                "test_implementation": "    def test_repeated_digits_cnpj(self):\n        with self.assertRaises(RepeatedDigitsCNPJError):\n            validate_cnpj(\"11.111.111/1111-11\")\n        with self.assertRaises(RepeatedDigitsCNPJError):\n            validate_cnpj(\"22222222222222\")"
            },
            "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj",
                "result": "passed",
                "test_implementation": "    def test_valid_cnpj(self):\n        self.assertTrue(validate_cnpj(\"13.347.016/0001-17\"))  # Example CNPJ\n        self.assertTrue(validate_cnpj(\"00.000.000/0001-91\"))\n        self.assertTrue(validate_cnpj(\"00000000000191\"))"
            },
            "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj_generator": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj_generator",
                "result": "passed",
                "test_implementation": "    def test_valid_cnpj_generator(self):\n        \"\"\"\n        This test will generate 10 valid CNPJs and check if they are valid.\n        Also generates 10 formatted CNPJs and checks if they are valid.\n        \"\"\"\n        for _ in range(10):\n            cnpj = generate_cnpj()\n            self.assertTrue(validate_cnpj(cnpj))\n        for _ in range(10):\n            cnpj = generate_cnpj(formatted=True)\n            self.assertTrue(validate_cnpj(cnpj))"
            },
            "tests/test_cnpj.py::TestCNPJ::test_whitespace_cnpj": {
                "testid": "tests/test_cnpj.py::TestCNPJ::test_whitespace_cnpj",
                "result": "passed",
                "test_implementation": "    def test_whitespace_cnpj(self):\n        self.assertTrue(validate_cnpj(\" 13.347.016/0001-17 \"))"
            },
            "tests/test_cpf.py::TestCPF::test_all_same_digits": {
                "testid": "tests/test_cpf.py::TestCPF::test_all_same_digits",
                "result": "passed",
                "test_implementation": "    def test_all_same_digits(self):\n        with self.assertRaises(RepeatedDigitsCPFError):\n            validate_cpf(\"000.000.000-00\")\n        with self.assertRaises(RepeatedDigitsCPFError):\n            validate_cpf(\"99999999999\")"
            },
            "tests/test_cpf.py::TestCPF::test_clean_cpf": {
                "testid": "tests/test_cpf.py::TestCPF::test_clean_cpf",
                "result": "passed",
                "test_implementation": "    def test_clean_cpf(self):\n        cpf_formatted = format_cpf(\"14538220620\")\n        self.assertEqual(cpf_formatted, \"145.382.206-20\")"
            },
            "tests/test_cpf.py::TestCPF::test_format_invalid_characters_cpf": {
                "testid": "tests/test_cpf.py::TestCPF::test_format_invalid_characters_cpf",
                "result": "passed",
                "test_implementation": "    def test_format_invalid_characters_cpf(self):\n        with self.assertRaises(InvalidFormatCPFError):\n            clean_cpf(\"145.382.206-2A\")"
            },
            "tests/test_cpf.py::TestCPF::test_format_invalid_length_cpf": {
                "testid": "tests/test_cpf.py::TestCPF::test_format_invalid_length_cpf",
                "result": "passed",
                "test_implementation": "    def test_format_invalid_length_cpf(self):\n        with self.assertRaises(InvalidLengthCPFError):\n            clean_cpf(\"1453822062\")  # Only 10 digits"
            },
            "tests/test_cpf.py::TestCPF::test_invalid_characters_cpf": {
                "testid": "tests/test_cpf.py::TestCPF::test_invalid_characters_cpf",
                "result": "passed",
                "test_implementation": "    def test_invalid_characters_cpf(self):\n        with self.assertRaises(InvalidLengthCPFError):\n            validate_cpf(\"529.982.247-2\")"
            },
            "tests/test_cpf.py::TestCPF::test_invalid_first_digit": {
                "testid": "tests/test_cpf.py::TestCPF::test_invalid_first_digit",
                "result": "passed",
                "test_implementation": "    def test_invalid_first_digit(self):\n        with self.assertRaises(InvalidCPFError) as context:\n            validate_cpf(\"145.382.206-30\")  # Altered first verification digit\n        self.assertIn(\"First verification digit does not match\", str(context.exception))"
            },
            "tests/test_cpf.py::TestCPF::test_invalid_length_cpf": {
                "testid": "tests/test_cpf.py::TestCPF::test_invalid_length_cpf",
                "result": "passed",
                "test_implementation": "    def test_invalid_length_cpf(self):\n        with self.assertRaises(InvalidLengthCPFError):\n            validate_cpf(\"529.982.247-2\")\n        with self.assertRaises(InvalidLengthCPFError):\n            validate_cpf(\"529.982.247-255\")"
            },
            "tests/test_cpf.py::TestCPF::test_invalid_second_digit": {
                "testid": "tests/test_cpf.py::TestCPF::test_invalid_second_digit",
                "result": "passed",
                "test_implementation": "    def test_invalid_second_digit(self):\n        with self.assertRaises(InvalidCPFError) as context:\n            validate_cpf(\"145.382.206-21\")  # Altered second verification digit\n        self.assertIn(\"Second verification digit does not match\", str(context.exception))"
            },
            "tests/test_cpf.py::TestCPF::test_repeated_digits_cpf": {
                "testid": "tests/test_cpf.py::TestCPF::test_repeated_digits_cpf",
                "result": "passed",
                "test_implementation": "    def test_repeated_digits_cpf(self):\n        with self.assertRaises(RepeatedDigitsCPFError):\n            validate_cpf(\"111.111.111-11\")\n        with self.assertRaises(RepeatedDigitsCPFError):\n            validate_cpf(\"22222222222\")"
            },
            "tests/test_cpf.py::TestCPF::test_valid_cpf": {
                "testid": "tests/test_cpf.py::TestCPF::test_valid_cpf",
                "result": "passed",
                "test_implementation": "    def test_valid_cpf(self):\n        self.assertTrue(validate_cpf(\"145.382.206-20\"))  # Valid CPF from the example\n        self.assertTrue(validate_cpf(\"529.982.247-25\"))\n        self.assertTrue(validate_cpf(\"52998224725\"))"
            },
            "tests/test_cpf.py::TestCPF::test_valid_cpf_generator": {
                "testid": "tests/test_cpf.py::TestCPF::test_valid_cpf_generator",
                "result": "passed",
                "test_implementation": "    def test_valid_cpf_generator(self):\n        \"\"\"\n        This test will generate 10 valid CPFs and check if they are valid.\n        Alsto generates 10 formatted CPFs and checks if they are valid.\n        \"\"\"\n        for _ in range(10):\n            cpf = generate_cpf()\n            self.assertTrue(validate_cpf(cpf))\n        for _ in range(10):\n            cpf = generate_cpf(formatted=True)\n            self.assertTrue(validate_cpf(cpf))"
            },
            "tests/test_cpf.py::TestCPF::test_whitespace_cpf": {
                "testid": "tests/test_cpf.py::TestCPF::test_whitespace_cpf",
                "result": "passed",
                "test_implementation": "    def test_whitespace_cpf(self):\n        self.assertTrue(validate_cpf(\" 145.382.206-20 \"))"
            },
            "tests/test_phone.py::TestPhoneNumber::test_clean_phone_number": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_clean_phone_number",
                "result": "passed",
                "test_implementation": "    def test_clean_phone_number(self):\n        cleaned = clean_phone_number('+55 (11) 98765-4321')\n        self.assertEqual(cleaned, '5511987654321')"
            },
            "tests/test_phone.py::TestPhoneNumber::test_format_phone_number": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_format_phone_number",
                "result": "passed",
                "test_implementation": "    def test_format_phone_number(self):\n        formatted = format_phone_number('11987654321')\n        self.assertEqual(formatted, '(11) 98765-4321')"
            },
            "tests/test_phone.py::TestPhoneNumber::test_generate_phone_number": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_generate_phone_number",
                "result": "passed",
                "test_implementation": "    def test_generate_phone_number(self):\n        for _ in range(100):\n            number = generate_phone_number(phone_type='mobile')\n            result = validate_phone_number(number)\n            self.assertEqual(result['type'], 'mobile')\n        \n        for _ in range(100):\n            number = generate_phone_number(phone_type='landline')\n            result = validate_phone_number(number)\n            self.assertEqual(result['type'], 'landline')"
            },
            "tests/test_phone.py::TestPhoneNumber::test_invalid_characters": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_invalid_characters",
                "result": "passed",
                "test_implementation": "    def test_invalid_characters(self):\n        with self.assertRaises(InvalidCharacterPhoneNumberError):\n            validate_phone_number('(11) 9a876-5432')"
            },
            "tests/test_phone.py::TestPhoneNumber::test_invalid_ddd": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_invalid_ddd",
                "result": "passed",
                "test_implementation": "    def test_invalid_ddd(self):\n        with self.assertRaises(InvalidDDDPhoneNumberError):\n            validate_phone_number('00123456789')"
            },
            "tests/test_phone.py::TestPhoneNumber::test_invalid_length": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_invalid_length",
                "result": "passed",
                "test_implementation": "    def test_invalid_length(self):\n        with self.assertRaises(InvalidLengthPhoneNumberError):\n            validate_phone_number('12345')\n        with self.assertRaises(InvalidLengthPhoneNumberError):\n            validate_phone_number('123456789012')"
            },
            "tests/test_phone.py::TestPhoneNumber::test_invalid_number_pattern": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_invalid_number_pattern",
                "result": "passed",
                "test_implementation": "    def test_invalid_number_pattern(self):\n        with self.assertRaises(InvalidLengthPhoneNumberError):\n            validate_phone_number('119123456')  # Incorrect length\n        with self.assertRaises(InvalidPhoneNumberError):\n            validate_phone_number('11812345678')  # Mobile number not starting with '9'"
            },
            "tests/test_phone.py::TestPhoneNumber::test_valid_landline_numbers": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_valid_landline_numbers",
                "result": "passed",
                "test_implementation": "    def test_valid_landline_numbers(self):\n        self.assertEqual(\n            validate_phone_number('1131234567')['type'],\n            'landline'\n        )\n        self.assertEqual(\n            validate_phone_number('(31) 3123-4567')['type'],\n            'landline'\n        )"
            },
            "tests/test_phone.py::TestPhoneNumber::test_valid_mobile_numbers": {
                "testid": "tests/test_phone.py::TestPhoneNumber::test_valid_mobile_numbers",
                "result": "passed",
                "test_implementation": "    def test_valid_mobile_numbers(self):\n        self.assertEqual(\n            validate_phone_number('11987654321')['type'],\n            'mobile'\n        )\n        self.assertEqual(\n            validate_phone_number('+55 (21) 99876-5432')['type'],\n            'mobile'\n        )\n        self.assertEqual(\n            validate_phone_number('61 9 9876-5432')['type'],\n            'mobile'\n        )"
            },
            "tests/test_plate.py::TestPlate::test_format_plate": {
                "testid": "tests/test_plate.py::TestPlate::test_format_plate",
                "result": "passed",
                "test_implementation": "    def test_format_plate(self):\n        self.assertEqual(format_plate('ABC1234'), 'ABC-1234')\n        self.assertEqual(format_plate('ABC1D23'), 'ABC-1D23')\n        self.assertEqual(format_plate('ABC12D3'), 'ABC-12D3')"
            },
            "tests/test_plate.py::TestPlate::test_format_plate_invalid_length": {
                "testid": "tests/test_plate.py::TestPlate::test_format_plate_invalid_length",
                "result": "passed",
                "test_implementation": "    def test_format_plate_invalid_length(self):\n        with self.assertRaises(InvalidLengthPlateError):\n            format_plate('ABC123')"
            },
            "tests/test_plate.py::TestPlate::test_generate_mercosul_car_plate": {
                "testid": "tests/test_plate.py::TestPlate::test_generate_mercosul_car_plate",
                "result": "passed",
                "test_implementation": "    def test_generate_mercosul_car_plate(self):\n        for _ in range(100):\n            plate = generate_mercosul_car_plate()\n            plate_type = validate_plate(plate)\n            self.assertEqual(plate_type, 'Mercosul Car')"
            },
            "tests/test_plate.py::TestPlate::test_generate_mercosul_motorcycle_plate": {
                "testid": "tests/test_plate.py::TestPlate::test_generate_mercosul_motorcycle_plate",
                "result": "passed",
                "test_implementation": "    def test_generate_mercosul_motorcycle_plate(self):\n        for _ in range(100):\n            plate = generate_mercosul_motorcycle_plate()\n            plate_type = validate_plate(plate)\n            self.assertEqual(plate_type, 'Mercosul Motorcycle')"
            },
            "tests/test_plate.py::TestPlate::test_generate_old_plate": {
                "testid": "tests/test_plate.py::TestPlate::test_generate_old_plate",
                "result": "passed",
                "test_implementation": "    def test_generate_old_plate(self):\n        for _ in range(100):\n            plate = generate_old_plate()\n            plate_type = validate_plate(plate)\n            self.assertEqual(plate_type, 'Old')"
            },
            "tests/test_plate.py::TestPlate::test_generate_plate_formatted": {
                "testid": "tests/test_plate.py::TestPlate::test_generate_plate_formatted",
                "result": "passed",
                "test_implementation": "    def test_generate_plate_formatted(self):\n        plate = generate_old_plate(formatted=True)\n        self.assertEqual(len(plate), 8)  # 'ABC-1234'\n        plate_type = validate_plate(plate)\n        self.assertEqual(plate_type, 'Old')"
            },
            "tests/test_plate.py::TestPlate::test_invalid_plate_characters": {
                "testid": "tests/test_plate.py::TestPlate::test_invalid_plate_characters",
                "result": "passed",
                "test_implementation": "    def test_invalid_plate_characters(self):\n        with self.assertRaises(InvalidCharacterPlateError):\n            validate_plate('AB@1234')\n        with self.assertRaises(InvalidCharacterPlateError):\n            validate_plate('ABC12*3')"
            },
            "tests/test_plate.py::TestPlate::test_invalid_plate_length": {
                "testid": "tests/test_plate.py::TestPlate::test_invalid_plate_length",
                "result": "passed",
                "test_implementation": "    def test_invalid_plate_length(self):\n        with self.assertRaises(InvalidLengthPlateError):\n            validate_plate('AB1234')\n        with self.assertRaises(InvalidLengthPlateError):\n            validate_plate('ABCDEFGH')"
            },
            "tests/test_plate.py::TestPlate::test_invalid_plate_pattern": {
                "testid": "tests/test_plate.py::TestPlate::test_invalid_plate_pattern",
                "result": "passed",
                "test_implementation": "    def test_invalid_plate_pattern(self):\n        with self.assertRaises(InvalidPlateError):\n            validate_plate('ABCD123')\n        with self.assertRaises(InvalidPlateError):\n            validate_plate('1234ABC')"
            },
            "tests/test_plate.py::TestPlate::test_valid_mercosul_car_plate": {
                "testid": "tests/test_plate.py::TestPlate::test_valid_mercosul_car_plate",
                "result": "passed",
                "test_implementation": "    def test_valid_mercosul_car_plate(self):\n        self.assertEqual(validate_plate('ABC1D23'), 'Mercosul Car')\n        self.assertEqual(validate_plate('abc1d23'), 'Mercosul Car')\n        self.assertEqual(validate_plate('ABC-1D23'), 'Mercosul Car')"
            },
            "tests/test_plate.py::TestPlate::test_valid_mercosul_motorcycle_plate": {
                "testid": "tests/test_plate.py::TestPlate::test_valid_mercosul_motorcycle_plate",
                "result": "passed",
                "test_implementation": "    def test_valid_mercosul_motorcycle_plate(self):\n        self.assertEqual(validate_plate('ABC12D3'), 'Mercosul Motorcycle')\n        self.assertEqual(validate_plate('abc12d3'), 'Mercosul Motorcycle')\n        self.assertEqual(validate_plate('ABC-12D3'), 'Mercosul Motorcycle')"
            },
            "tests/test_plate.py::TestPlate::test_valid_old_plate": {
                "testid": "tests/test_plate.py::TestPlate::test_valid_old_plate",
                "result": "passed",
                "test_implementation": "    def test_valid_old_plate(self):\n        self.assertEqual(validate_plate('ABC1234'), 'Old')\n        self.assertEqual(validate_plate('abc1234'), 'Old')\n        self.assertEqual(validate_plate('ABC-1234'), 'Old')"
            }
        },
        "SRS_document": "**Software Requirements Specification: Brazilian Data Utilities Library**\n\n**Table of Contents**\n1.  Introduction\n    1.1 Purpose\n    1.2 Scope\n    1.3 Definitions, Acronyms, and Abbreviations\n    1.4 References\n    1.5 Overview\n2.  Overall Description\n    2.1 Product Perspective\n    2.2 Product Functions\n    2.3 User Characteristics\n    2.4 Constraints\n    2.5 Assumptions and Dependencies\n    2.6 Usage Limitations (Disclaimers)\n3.  Specific Requirements\n    3.1 Functional Requirements\n        3.1.1 Common Functional Requirements\n        3.1.2 CPF (Cadastro de Pessoas Físicas) Data Management\n        3.1.3 CNPJ (Cadastro Nacional da Pessoa Jurídica) Data Management\n        3.1.4 Vehicle Plate Data Management\n        3.1.5 CEP (Código de Endereçamento Postal) Data Management\n        3.1.6 Phone Number Data Management\n    3.2 Non-Functional Requirements\n    3.3 External Interface Requirements\n    3.4 Custom Exception Definitions\n        3.4.1 CPF Exceptions\n        3.4.2 CNPJ Exceptions\n        3.4.3 Plate Exceptions\n        3.4.4 CEP Exceptions\n        3.4.5 Phone Number Exceptions\nAppendix A: Valid DDD Codes (Conceptual)\n\n---\n\n**1. Introduction**\n\n**1.1 Purpose**\nThis Software Requirements Specification (SRS) document outlines the functional and non-functional requirements for the Brazilian Data Utilities Library. The primary goal of this SRS is to serve as a definitive guide for software developers who will implement this library. Their implementation will be assessed based on its adherence to these requirements, verified by a comprehensive set of public and private test cases. This document aims to be exceptionally clear, unambiguous, functionally comprehensive, and appropriately abstracted to allow for independent design and implementation choices while ensuring all specified functionalities are met.\n\n**1.2 Scope**\nThe software to be developed is a Python library providing utilities for:\n*   Validation of Brazilian common identifiers and codes: CPF, CNPJ, Vehicle License Plates, Postal Codes (CEP), and Phone Numbers.\n*   Formatting of these identifiers and codes into standard Brazilian representations.\n*   Generation of syntactically valid (but not necessarily real) samples of these identifiers and codes for testing and development purposes.\n*   Sanitization (cleaning) of input strings for these identifiers and codes.\n\nThe library is intended for use in software development and testing environments.\n\n**1.3 Definitions, Acronyms, and Abbreviations**\n*   **SRS:** Software Requirements Specification\n*   **CPF:** Cadastro de Pessoas Físicas (Brazilian individual taxpayer registry ID)\n*   **CNPJ:** Cadastro Nacional da Pessoa Jurídica (Brazilian legal entity taxpayer registry ID)\n*   **CEP:** Código de Endereçamento Postal (Brazilian postal code)\n*   **Plate:** Vehicle License Plate\n*   **Mercosul Plate:** Newer vehicle license plate standard used in Mercosur countries, including Brazil.\n*   **DDD:** Discagem Direta a Distância (Brazilian area code for phone numbers)\n*   **API:** Application Programming Interface\n\n**1.4 References**\n*   Original README.md (provided)\n*   Original source code (`br_eval` package, provided for LLM contextual understanding only)\n*   Original test cases (`tests` directory, provided for LLM contextual understanding and SRS derivation)\n\n**1.5 Overview**\nThis SRS is organized into three main sections:\n*   **Section 1 (Introduction):** Provides an overview of the SRS, its purpose, scope, definitions, and references.\n*   **Section 2 (Overall Description):** Describes the general factors affecting the product and its requirements, including product perspective, functions, user characteristics, constraints, and assumptions.\n*   **Section 3 (Specific Requirements):** Details all specific requirements, including functional requirements for each data type (CPF, CNPJ, Plate, CEP, Phone), non-functional requirements (if any with test backing), external interface requirements, and definitions of custom exceptions.\n\n---\n\n**2. Overall Description**\n\n**2.1 Product Perspective**\nThe Brazilian Data Utilities Library is a self-contained Python library. It is intended to be imported and used by other Python applications that need to handle Brazilian-specific data formats for validation, formatting, or generation. It does not have its own user interface beyond the programmatic API it exposes.\n\n**2.2 Product Functions**\nThe library will provide the following key functionalities:\n*   **CPF Management:** Validation, formatting, sanitization, and generation of CPF numbers.\n*   **CNPJ Management:** Validation, formatting, sanitization, and generation of CNPJ numbers.\n*   **Vehicle Plate Management:** Validation (old and Mercosul patterns), formatting, and generation of various plate types.\n*   **CEP Management:** Validation, formatting, sanitization, and generation of CEP codes.\n*   **Phone Number Management:** Validation (mobile and landline), formatting (including international option), sanitization, and generation of Brazilian phone numbers.\n*   **Error Handling:** Consistent error reporting through a hierarchy of custom exceptions for specific invalid conditions.\n\n**2.3 User Characteristics**\nThe primary users of this library are software developers building applications that require processing of Brazilian data. Users are expected to have proficiency in Python programming and understand how to integrate third-party libraries.\n\n**2.4 Constraints**\n*   **CSTR-01:** The library must be compatible with Python version 3.6 or higher.\n*   **CSTR-02:** The validation logic for CPF and CNPJ must adhere to the standard Brazilian algorithms for verifier digit calculation.\n*   **CSTR-03:** Vehicle plate validation must recognize standard Brazilian \"Old\" format plates and \"Mercosul\" (Car and Motorcycle) format plates based on their defined structural patterns.\n*   **CSTR-04:** Phone number validation must use a predefined set of valid Brazilian DDD codes (see Appendix A for conceptual list).\n*   **CSTR-05:** CEP validation is structural (length, numeric characters) and does not verify existence against a real address database.\n\n**2.5 Assumptions and Dependencies**\n*   **ASMP-01:** The system operates on string inputs for all identifiers and codes.\n*   **ASMP-02:** The library does not require external network access or databases for its core validation, formatting, or generation functions (e.g., CEP validation is local).\n\n**2.6 Usage Limitations (Disclaimers)**\nThe following disclaimers regarding the use of data generated by this library must be understood by users:\n*   **DISC-01 (CPF/CNPJ):** Generated CPF and CNPJ numbers are intended solely for development and testing purposes. They do not correspond to real individuals or companies and must not be used in production systems, official registrations, or for any illegal or fraudulent activities.\n*   **DISC-02 (Vehicle Plates):** Generated vehicle plates are intended solely for development and testing purposes. They do not correspond to real vehicles and must not be used in production systems, official documents, registrations, or any activities involving real-world entities.\n*   **DISC-03 (CEP):** Generated CEPs do not necessarily correspond to real addresses and are intended solely for development and testing purposes. They must not be used in production systems, official documents, or registrations involving real-world entities.\n*   **DISC-04 (Phone Numbers):** Generated phone numbers are intended solely for development and testing purposes. They do not correspond to real individuals or active lines and must not be used in production systems, official documents, registrations, marketing campaigns, telemarketing, or any activities involving real-world entities.\n\n---\n\n**3. Specific Requirements**\n\n**3.1 Functional Requirements**\n\n**3.1.1 Common Functional Requirements**\n\n*   **FR-COM-EXC-01:** The system shall use specific custom exceptions (detailed in Section 3.4) to indicate distinct error conditions encountered during processing.\n\n**3.1.2 CPF (Cadastro de Pessoas Físicas) Data Management**\n\n**3.1.2.1 CPF Sanitization (Cleaning)**\n*   **FR-CPF-CLN-01:** The system shall provide a capability to sanitize a CPF string by removing all non-numeric characters.\n    *   **Description:** This process prepares the CPF for numerical validation or formatting. Leading/trailing whitespace should also be implicitly handled or removed by this process if it affects digit extraction.\n*   **FR-CPF-CLN-02:** If an input CPF string intended for sanitization (specifically for `clean_cpf` behavior) contains any alphabetic characters, the system shall raise an `InvalidFormatCPFError`.\n*   **FR-CPF-CLN-03:** After removing non-numeric characters, if the resulting CPF string does not consist of exactly 11 digits, the system shall raise an `InvalidLengthCPFError` when `clean_cpf` is directly invoked.\n\n**3.1.2.2 CPF Validation**\n*   **FR-CPF-VAL-01:** The system shall validate a given CPF string to determine its authenticity according to Brazilian CPF rules.\n    *   **Description:** Validation succeeds if all checks pass, returning a boolean True. Any failure results in a specific exception. Input CPF can be formatted (e.g., \"XXX.XXX.XXX-XX\"), unformatted (\"XXXXXXXXXXX\"), or contain leading/trailing whitespace.\n*   **FR-CPF-VAL-02:** As part of validation, if the sanitized CPF (11 digits) consists of all identical digits (e.g., \"11111111111\", \"00000000000\"), the system shall raise a `RepeatedDigitsCPFError`.\n*   **FR-CPF-VAL-03:** The system shall calculate the first CPF verifier digit based on a weighted sum of the first 9 digits of the sanitized CPF, conforming to the standard Brazilian CPF algorithm. If the calculated first verifier digit does not match the 10th digit of the sanitized CPF, the system shall raise an `InvalidCPFError` with a message indicating failure at the first verifier digit.\n*   **FR-CPF-VAL-04:** The system shall calculate the second CPF verifier digit based on a weighted sum of the first 10 digits (including the correct first verifier digit) of the sanitized CPF, conforming to the standard Brazilian CPF algorithm. If the calculated second verifier digit does not match the 11th digit of the sanitized CPF, the system shall raise an `InvalidCPFError` with a message indicating failure at the second verifier digit.\n*   **FR-CPF-VAL-05:** If, after sanitization, the CPF string does not contain exactly 11 digits, the validation shall fail by raising an `InvalidLengthCPFError` (this is raised by the underlying `clean_cpf` call within `validate_cpf`).\n*   **FR-CPF-VAL-06:** If the input CPF string for validation contains alphabetic characters, the validation shall fail by raising an `InvalidFormatCPFError` (this is raised by the underlying `clean_cpf` call within `validate_cpf`).\n\n**3.1.2.3 CPF Formatting**\n*   **FR-CPF-FMT-01:** The system shall format a CPF string consisting of 11 numeric digits into the standard \"XXX.XXX.XXX-XX\" pattern.\n    *   **Description:** Input can be a string of 11 digits or a string that, after removing all non-digit characters, results in 11 digits.\n*   **FR-CPF-FMT-02:** If the input CPF string, after removal of non-digit characters, does not result in exactly 11 digits, the formatting function shall raise a `ValueError`.\n    *   **Description:** This is specific to the `format_cpf` function's direct behavior.\n\n**3.1.2.4 CPF Generation**\n*   **FR-CPF-GEN-01:** The system shall generate syntactically valid 11-digit CPF numbers.\n    *   **Description:** Generated CPFs must pass the system's CPF validation logic (FR-CPF-VAL-01).\n*   **FR-CPF-GEN-02:** The CPF generation function shall offer an option to return the generated CPF in the \"XXX.XXX.XXX-XX\" formatted pattern.\n\n**3.1.3 CNPJ (Cadastro Nacional da Pessoa Jurídica) Data Management**\n\n**3.1.3.1 CNPJ Sanitization (Cleaning)**\n*   **FR-CNPJ-CLN-01:** The system shall provide a capability to sanitize a CNPJ string by removing all non-numeric characters.\n    *   **Description:** Prepares the CNPJ for numerical validation or formatting.\n*   **FR-CNPJ-CLN-02:** After removing non-numeric characters, if the resulting CNPJ string does not consist of exactly 14 digits, the system shall raise an `InvalidLengthCNPJError`.\n*   **FR-CNPJ-CLN-03:** If the CNPJ string, after removal of non-numeric characters, still contains non-digit characters (e.g. if it was not purely numeric and punctuation to begin with, and `re.sub(r'\\D', '', cnpj)` was bypassed or failed), the system shall raise an `InvalidFormatCNPJError`. (Note: given `re.sub(r'\\D', '', cnpj)`, this path is hard to reach unless `isdigit()` fails for other reasons, but the code has the check).\n    *   **Description:** The `clean_cnpj` function has a `cnpj_numbers.isdigit()` check.\n\n**3.1.3.2 CNPJ Validation**\n*   **FR-CNPJ-VAL-01:** The system shall validate a given CNPJ string to determine its authenticity according to Brazilian CNPJ rules.\n    *   **Description:** Validation succeeds if all checks pass, returning boolean True. Failures raise specific exceptions. Input CNPJ can be formatted, unformatted, or contain leading/trailing whitespace.\n*   **FR-CNPJ-VAL-02:** As part of validation, if the sanitized CNPJ (14 digits) consists of all identical digits (e.g., \"11111111111111\"), the system shall raise a `RepeatedDigitsCNPJError`.\n*   **FR-CNPJ-VAL-03:** The system shall calculate the two CNPJ verifier digits based on weighted sums of the preceding digits, conforming to the standard Brazilian CNPJ algorithm. If the calculated verifier digits do not match the last two digits of the sanitized CNPJ, the system shall raise an `InvalidCNPJError`.\n*   **FR-CNPJ-VAL-04:** If, after sanitization, the CNPJ string does not contain exactly 14 digits, the validation shall fail by raising an `InvalidLengthCNPJError` (via `clean_cnpj`).\n\n**3.1.3.3 CNPJ Formatting**\n*   **FR-CNPJ-FMT-01:** The system shall format a CNPJ string consisting of 14 numeric digits (or a string that cleans to 14 digits) into the \"XX.XXX.XXX/XXXX-XX\" pattern.\n*   **FR-CNPJ-FMT-02:** The formatting function internally uses the sanitization logic (FR-CNPJ-CLN-01, FR-CNPJ-CLN-02). If sanitization fails (e.g., due to incorrect length after cleaning), the relevant exceptions (`InvalidLengthCNPJError`) shall be propagated.\n\n**3.1.3.4 CNPJ Generation**\n*   **FR-CNPJ-GEN-01:** The system shall generate syntactically valid 14-digit CNPJ numbers.\n    *   **Description:** Generated CNPJs must pass the system's CNPJ validation logic (FR-CNPJ-VAL-01). Base numbers for generation may include a fixed \"0001\" for the branch identifier part.\n*   **FR-CNPJ-GEN-02:** The CNPJ generation function shall offer an option to return the generated CNPJ in the \"XX.XXX.XXX/XXXX-XX\" formatted pattern.\n\n**3.1.4 Vehicle Plate Data Management**\n\n**3.1.4.1 Vehicle Plate Sanitization (Implicit)**\n*   **FR-PLT-CLN-01:** Before validation or formatting, input plate strings shall be sanitized by converting them to uppercase, removing any hyphens, and stripping leading/trailing whitespace.\n\n**3.1.4.2 Vehicle Plate Validation**\n*   **FR-PLT-VAL-01:** The system shall validate a given vehicle plate string.\n    *   **Description:** If valid, the function returns a string indicating the plate type ('Old', 'Mercosul Car', 'Mercosul Motorcycle'). Invalid plates raise specific exceptions.\n*   **FR-PLT-VAL-02:** If the input plate is not a string, the system shall raise an `InvalidFormatPlateError`.\n*   **FR-PLT-VAL-03:** After sanitization (FR-PLT-CLN-01), if the plate string does not consist of exactly 7 characters, the system shall raise an `InvalidLengthPlateError`.\n*   **FR-PLT-VAL-04:** After sanitization, if the 7-character plate string contains any characters other than uppercase letters (A-Z) or digits (0-9), the system shall raise an `InvalidCharacterPlateError`.\n*   **FR-PLT-VAL-05:** The system shall identify a sanitized 7-character alphanumeric plate as 'Old' if it matches the pattern LLLNNNN (3 letters followed by 4 numbers).\n*   **FR-PLT-VAL-06:** The system shall identify a sanitized 7-character alphanumeric plate as 'Mercosul Car' if it matches the pattern LLLNLNN (3 letters, 1 number, 1 letter, 2 numbers).\n*   **FR-PLT-VAL-07:** The system shall identify a sanitized 7-character alphanumeric plate as 'Mercosul Motorcycle' if it matches the pattern LLLNNLN (3 letters, 2 numbers, 1 letter, 1 number).\n*   **FR-PLT-VAL-08:** If a sanitized 7-character alphanumeric plate does not match any of the recognized patterns (Old, Mercosul Car, Mercosul Motorcycle), the system shall raise an `InvalidPlateError`.\n\n**3.1.4.3 Vehicle Plate Formatting**\n*   **FR-PLT-FMT-01:** The system shall format a 7-character alphanumeric plate string (after sanitization per FR-PLT-CLN-01) by inserting a hyphen between the third and fourth characters, resulting in an \"AAA-BBBB\" pattern.\n*   **FR-PLT-FMT-02:** If the input plate string, after sanitization, does not have exactly 7 characters, the formatting function shall raise an `InvalidLengthPlateError`.\n\n**3.1.4.4 Vehicle Plate Generation**\n*   **FR-PLT-GEN-01:** The system shall generate valid vehicle plates of the 'Old' format (LLLNNNN).\n*   **FR-PLT-GEN-02:** The system shall generate valid vehicle plates of the 'Mercosul Car' format (LLLNLNN).\n*   **FR-PLT-GEN-03:** The system shall generate valid vehicle plates of the 'Mercosul Motorcycle' format (LLLNNLN).\n*   **FR-PLT-GEN-04:** All plate generation functions shall offer an option to return the plate in a formatted \"AAA-BBBB\" pattern.\n*   **FR-PLT-GEN-05:** The system shall provide a generic plate generation function that accepts a `plate_type` argument ('old', 'mercosul_car', 'mercosul_motorcycle') and delegates to the corresponding specific generator.\n*   **FR-PLT-GEN-06:** If the generic plate generation function (FR-PLT-GEN-05) receives an unrecognized `plate_type`, it shall raise a `ValueError`.\n\n**3.1.5 CEP (Código de Endereçamento Postal) Data Management**\n\n**3.1.5.1 CEP Sanitization (Cleaning)**\n*   **FR-CEP-CLN-01:** The system shall provide a capability to sanitize a CEP string.\n    *   **Description:** This involves checking for letters before numeric conversion, then removing all non-numeric characters.\n*   **FR-CEP-CLN-02:** If the input CEP string contains any alphabetic characters, the sanitization process shall raise an `InvalidCharacterCEPError`.\n*   **FR-CEP-CLN-03:** After removing non-numeric characters, if the resulting string does not consist of exactly 8 digits, the sanitization process shall raise an `InvalidLengthCEPError`.\n*   **FR-CEP-CLN-04:** After removing non-numeric characters and checking length, if the resulting 8-character string is not composed entirely of digits, the sanitization process shall raise an `InvalidCharacterCEPError`.\n\n**3.1.5.2 CEP Validation**\n*   **FR-CEP-VAL-01:** The system shall validate a given CEP string.\n    *   **Description:** Validation is structural. It uses the sanitization logic (FR-CEP-CLN-01 to FR-CEP-CLN-04). If sanitization succeeds (i.e., the cleaned CEP is 8 digits and numeric), validation returns `True`. Otherwise, relevant exceptions from cleaning are raised.\n*   **FR-CEP-VAL-02:** If the input CEP for validation leads to `InvalidCharacterCEPError` during sanitization (e.g., contains letters), this exception shall be propagated.\n*   **FR-CEP-VAL-03:** If the input CEP for validation leads to `InvalidLengthCEPError` during sanitization (e.g., not 8 digits after cleaning), this exception shall be propagated.\n\n**3.1.5.3 CEP Formatting**\n*   **FR-CEP-FMT-01:** The system shall format a CEP string (that sanitizes to 8 numeric digits) into the \"XXXXX-XXX\" pattern.\n*   **FR-CEP-FMT-02:** The CEP formatting function internally uses the sanitization logic. If sanitization fails, the relevant exceptions (e.g., `InvalidLengthCEPError`, `InvalidCharacterCEPError`) shall be propagated.\n\n**3.1.5.4 CEP Generation**\n*   **FR-CEP-GEN-01:** The system shall generate random 8-digit numeric strings representing CEPs.\n    *   **Description:** Generated CEPs must be structurally valid per FR-CEP-VAL-01.\n*   **FR-CEP-GEN-02:** The CEP generation function shall offer an option to return the generated CEP in the \"XXXXX-XXX\" formatted pattern.\n\n**3.1.6 Phone Number Data Management**\n\n**3.1.6.1 Phone Number Sanitization (Cleaning)**\n*   **FR-PHN-CLN-01:** The system shall provide a capability to sanitize a phone number string by removing all non-numeric characters.\n*   **FR-PHN-CLN-02:** If the input phone number string for sanitization contains any alphabetic characters, the system shall raise an `InvalidCharacterPhoneNumberError`.\n\n**3.1.6.2 Phone Number Validation**\n*   **FR-PHN-VAL-01:** The system shall validate a given Brazilian phone number string.\n    *   **Description:** If valid, returns a dictionary containing the phone 'type' ('mobile' or 'landline'), 'ddd' (2-digit area code), and 'number' (the local part of the number). Invalid numbers raise specific exceptions. Input can be in various formats (e.g., with/without parentheses, spaces, hyphens, or country code).\n*   **FR-PHN-VAL-02:** The validation process shall correctly handle and remove an optional \"55\" Brazilian country code prefix from the sanitized number if present and the remaining length is appropriate for a national number.\n*   **FR-PHN-VAL-03:** After sanitization and optional country code removal, if the resulting number of digits is not 10 or 11, the system shall raise an `InvalidLengthPhoneNumberError`.\n*   **FR-PHN-VAL-04:** The system shall extract the first two digits of the (potentially country-code-stripped) 10 or 11-digit number as the DDD. This DDD must be one of the predefined valid Brazilian DDD codes (see Appendix A / CSTR-04). If not, an `InvalidDDDPhoneNumberError` shall be raised.\n*   **FR-PHN-VAL-05:** If the number (DDD + local part) has 11 digits, the local part (9 digits) must start with '9' to be considered a 'mobile' number. If not (e.g., local part is 9 digits but starts with '8'), an `InvalidPhoneNumberError` shall be raised.\n*   **FR-PHN-VAL-06:** If the number (DDD + local part) has 10 digits, the local part (8 digits) must start with '2', '3', '4', or '5' to be considered a 'landline' number. If not, an `InvalidPhoneNumberError` shall be raised.\n*   **FR-PHN-VAL-07:** If the length of the local number part is inconsistent with expected patterns (e.g., 11 total digits but local part is not 9 digits, or 10 total digits but local part is not 8 digits), an `InvalidPhoneNumberError` or `InvalidLengthPhoneNumberError` shall be raised.\n*   **FR-PHN-VAL-08:** If the input phone number for validation leads to an `InvalidCharacterPhoneNumberError` during sanitization (e.g., contains letters), this exception shall be propagated.\n\n**3.1.6.3 Phone Number Formatting**\n*   **FR-PHN-FMT-01:** The system shall format a valid phone number (validated by FR-PHN-VAL-01) into a standard Brazilian pattern.\n    *   Mobile: `(XX) 9XXXX-XXXX`\n    *   Landline: `(XX) XXXX-XXXX`\n    *   Where XX is the DDD.\n*   **FR-PHN-FMT-02:** The phone number formatting function shall offer an option to include the \"+55\" international country code prefix.\n    *   **Description:** Example: `+55 (XX) 9XXXX-XXXX` or `+55 (XX) XXXX-XXXX`.\n*   **FR-PHN-FMT-03:** The phone number formatting function internally validates the input number. If validation fails, the relevant exceptions (e.g., `InvalidLengthPhoneNumberError`, `InvalidDDDPhoneNumberError`) shall be propagated.\n\n**3.1.6.4 Phone Number Generation**\n*   **FR-PHN-GEN-01:** The system shall generate syntactically valid Brazilian phone numbers.\n    *   **Description:** Generated numbers must use a valid DDD from the predefined set and adhere to mobile or landline structural rules.\n*   **FR-PHN-GEN-02:** The generation function shall accept a `phone_type` argument to specify 'mobile' or 'landline' generation.\n    *   Mobile: 9-digit local number, starting with '9'.\n    *   Landline: 8-digit local number, starting with '2', '3', '4', or '5'.\n*   **FR-PHN-GEN-03:** If an unrecognized `phone_type` is provided to the generation function, it shall raise a `ValueError`.\n*   **FR-PHN-GEN-04:** The generation function shall offer an option (`formatted=True`) to return the generated number in the standard Brazilian formatted pattern (FR-PHN-FMT-01).\n*   **FR-PHN-GEN-05:** The generation function shall offer an option (`international=True`) to affect the output format.\n    *   If `formatted=True` and `international=True`, the output is internationally formatted (e.g., `+55 (XX) ...`).\n    *   If `formatted=False` and `international=True`, the output is a raw digit string prefixed with \"55\".\n\n**3.2 Non-Functional Requirements**\n*   **NFR-01:** No specific non-functional requirements (e.g., performance, security, usability) have been identified with directly corresponding, explicit original test cases that validate them. Therefore, no NFRs are formally specified here.\n\n**3.3 External Interface Requirements**\n*   **EIR-01 (CPF Format):** The standard human-readable format for CPF is \"XXX.XXX.XXX-XX\". The system should be able to parse inputs that may include this formatting and should be able to generate outputs in this format.\n*   **EIR-02 (CNPJ Format):** The standard human-readable format for CNPJ is \"XX.XXX.XXX/XXXX-XX\". The system should be able to parse inputs with this formatting and generate outputs in this format.\n*   **EIR-03 (Plate Format):** Vehicle plates are commonly represented as \"AAA-NNNN\" (Old) or \"AAA-XNNN\" patterns (Mercosul, where X can be letter or number). The system should handle inputs with/without the hyphen and format outputs with a hyphen.\n*   **EIR-04 (CEP Format):** The standard human-readable format for CEP is \"XXXXX-XXX\". The system should parse inputs with this formatting and generate outputs in this format.\n*   **EIR-05 (Phone Formats):** Brazilian phone numbers have common representations like \"(XX) YYYYY-YYYY\" (mobile) or \"(XX) YYYY-YYYY\" (landline), and internationally as \"+55 ...\". The system should parse various common input styles and format to these standards.\n\n**3.4 Custom Exception Definitions**\nThe system shall use the following custom exceptions to report errors. Each specific functional requirement above details when these are raised.\n\n**3.4.1 CPF Exceptions (`br_eval/exceptions/cpf_exceptions.py`)**\n*   **EXC-CPF-01: `CPFError`:** Base class for CPF-related exceptions.\n*   **EXC-CPF-02: `InvalidCPFError` (inherits `CPFError`):** General exception for an invalid CPF, often due to verifier digit mismatch.\n    *   **Message for 1st digit mismatch:** \"First verification digit does not match.\"\n    *   **Message for 2nd digit mismatch:** \"Second verification digit does not match.\"\n*   **EXC-CPF-03: `RepeatedDigitsCPFError` (inherits `InvalidCPFError`):** Raised when a CPF consists of all identical digits.\n    *   **Message:** \"CPF cannot have all digits equal.\"\n*   **EXC-CPF-04: `InvalidFormatCPFError` (inherits `CPFError`):** Raised when a CPF string contains non-numeric characters where not expected by `clean_cpf` (specifically letters).\n    *   **Message:** \"CPF must contain only numbers.\" (from `br_eval/cpf.py::clean_cpf` exception), or \"CPF contains letters.\"\n*   **EXC-CPF-05: `InvalidLengthCPFError` (inherits `CPFError`):** Raised when a CPF string, after sanitization, does not have the required 11 digits.\n    *   **Message:** \"CPF must have 11 digits. Current length: {length}.\"\n\n**3.4.2 CNPJ Exceptions (`br_eval/exceptions/cnpj_exceptions.py`)**\n*   **EXC-CNPJ-01: `CNPJError`:** Base class for CNPJ-related exceptions.\n*   **EXC-CNPJ-02: `InvalidCNPJError` (inherits `CNPJError`):** General exception for an invalid CNPJ, typically due to verifier digit mismatch.\n    *   **Message:** \"Verification digits do not match.\"\n*   **EXC-CNPJ-03: `RepeatedDigitsCNPJError` (inherits `InvalidCNPJError`):** Raised when a CNPJ consists of all identical digits.\n    *   **Message:** \"CNPJ cannot have all digits equal.\"\n*   **EXC-CNPJ-04: `InvalidFormatCNPJError` (inherits `CNPJError`):** Raised when a CNPJ string contains non-numeric characters where not expected by `clean_cnpj`.\n    *   **Message:** \"CNPJ must contain only numbers.\"\n*   **EXC-CNPJ-05: `InvalidLengthCNPJError` (inherits `CNPJError`):** Raised when a CNPJ string, after sanitization, does not have the required 14 digits.\n    *   **Message:** \"CNPJ must have 14 digits. Current length: {length}.\"\n\n**3.4.3 Plate Exceptions (`br_eval/exceptions/plate_exceptions.py`)**\n*   **EXC-PLT-01: `PlateError`:** Base class for plate-related exceptions.\n*   **EXC-PLT-02: `InvalidPlateError` (inherits `PlateError`):** Raised when a plate string does not match any recognized Brazilian plate pattern after basic structural checks pass.\n    *   **Message:** \"Invalid plate format.\" (as per `InvalidPlateError.__str__`)\n*   **EXC-PLT-03: `InvalidFormatPlateError` (inherits `PlateError`):** Raised if the input plate is not a string.\n    *   **Message:** \"Plate must be a string of 7 alphanumeric characters.\"\n*   **EXC-PLT-04: `InvalidCharacterPlateError` (inherits `PlateError`):** Raised when a sanitized plate string contains characters other than A-Z or 0-9.\n    *   **Message:** \"Plate contains invalid characters.\"\n*   **EXC-PLT-05: `InvalidLengthPlateError` (inherits `PlateError`):** Raised when a sanitized plate string does not have exactly 7 characters.\n    *   **Message:** \"Plate must have exactly 7 characters.\"\n\n**3.4.4 CEP Exceptions (`br_eval/exceptions/cep_exceptions.py`)**\n*   **EXC-CEP-01: `CEPError`:** Base class for CEP-related exceptions.\n*   **EXC-CEP-02: `InvalidCEPError` (inherits `CEPError`):** General exception for an invalid CEP format (though current implementation uses more specific errors).\n    *   **Message:** \"Invalid CEP format.\"\n*   **EXC-CEP-03: `InvalidLengthCEPError` (inherits `CEPError`):** Raised when a CEP string, after sanitization, does not have 8 digits.\n    *   **Message:** \"CEP must have 8 digits. Current length: {length}.\"\n*   **EXC-CEP-04: `InvalidCharacterCEPError` (inherits `CEPError`):** Raised when a CEP string contains non-numeric characters (specifically letters before cleaning, or if `isdigit()` fails after cleaning for other reasons).\n    *   **Message:** \"CEP must contain only numbers.\" or \"CEP contains letters.\"\n\n**3.4.5 Phone Number Exceptions (`br_eval/exceptions/phone_exceptions.py`)**\n*   **EXC-PHN-01: `PhoneNumberError`:** Base class for phone number exceptions.\n*   **EXC-PHN-02: `InvalidPhoneNumberError` (inherits `PhoneNumberError`):** Raised for general invalid phone number patterns not covered by more specific exceptions (e.g., mobile not starting with '9', landline not starting with '2'-'5').\n    *   **Message:** \"Invalid phone number format.\" or \"Phone number does not match mobile or landline patterns.\"\n*   **EXC-PHN-03: `InvalidLengthPhoneNumberError` (inherits `PhoneNumberError`):** Raised when a sanitized phone number (after potential country code removal) does not have 10 or 11 digits.\n    *   **Message:** \"Phone number has an invalid length: {length} digits.\"\n*   **EXC-PHN-04: `InvalidCharacterPhoneNumberError` (inherits `PhoneNumberError`):** Raised when a phone number string contains non-numeric characters (e.g., letters).\n    *   **Message:** \"Phone number must contain only digits.\" or \"Phone number contains letters.\"\n*   **EXC-PHN-05: `InvalidDDDPhoneNumberError` (inherits `PhoneNumberError`):** Raised when the extracted 2-digit DDD is not in the predefined list of valid DDDs.\n    *   **Message:** \"Invalid DDD code: {ddd}.\"\n\n---\n\n**Appendix A: Valid DDD Codes (Conceptual)**\n\nThe system relies on a predefined, static set of 2-digit Brazilian DDD (Direct Distance Dialing) codes for phone number validation. This set includes codes such as '11', '21', '31', '41', '51', '61', '71', '81', '91', and many others covering all Brazilian states. An attempt to validate or format a phone number with a DDD not present in this internal list will result in an `InvalidDDDPhoneNumberError`. The exact list is an internal data structure of the library.\n(Reference: `br_eval/phone.py::VALID_DDD_CODES` in the original source).",
        "structured_requirements": [
            {
                "requirement_id": "CSTR-01",
                "requirement_description": "The library must be compatible with Python version 3.6 or higher.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "setup.py::python_requires",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "CSTR-02",
                "requirement_description": "The validation logic for CPF and CNPJ must adhere to the standard Brazilian algorithms for verifier digit calculation.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::validate_cpf",
                        "description": ""
                    },
                    {
                        "id": "br_eval/cnpj.py::validate_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "CSTR-03",
                "requirement_description": "Vehicle plate validation must recognize standard Brazilian \"Old\" format plates and \"Mercosul\" (Car and Motorcycle) format plates based on their defined structural patterns.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "CSTR-04",
                "requirement_description": "Phone number validation must use a predefined set of valid Brazilian DDD codes (see Appendix A for conceptual list).",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::VALID_DDD_CODES",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "CSTR-05",
                "requirement_description": "CEP validation is structural (length, numeric characters) and does not verify existence against a real address database.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::validate_cep",
                        "description": ""
                    },
                    {
                        "id": "br_eval/cep.py::clean_cep",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "ASMP-01",
                "requirement_description": "The system operates on string inputs for all identifiers and codes.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "ASMP-02",
                "requirement_description": "The library does not require external network access or databases for its core validation, formatting, or generation functions (e.g., CEP validation is local).",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "DISC-01",
                "requirement_description": "Generated CPF and CNPJ numbers are intended solely for development and testing purposes. They do not correspond to real individuals or companies and must not be used in production systems, official registrations, or for any illegal or fraudulent activities.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "README.md",
                        "description": "\"IMPORTANT NOTICE\" sections."
                    }
                ]
            },
            {
                "requirement_id": "DISC-02",
                "requirement_description": "Generated vehicle plates are intended solely for development and testing purposes. They do not correspond to real vehicles and must not be used in production systems, official documents, registrations, or any activities involving real-world entities.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "README.md",
                        "description": "\"IMPORTANT NOTICE\" sections."
                    }
                ]
            },
            {
                "requirement_id": "DISC-03",
                "requirement_description": "Generated CEPs do not necessarily correspond to real addresses and are intended solely for development and testing purposes. They must not be used in production systems, official documents, or registrations involving real-world entities.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "README.md",
                        "description": "\"IMPORTANT NOTICE\" sections."
                    }
                ]
            },
            {
                "requirement_id": "DISC-04",
                "requirement_description": "Generated phone numbers are intended solely for development and testing purposes. They do not correspond to real individuals or active lines and must not be used in production systems, official documents, registrations, marketing campaigns, telemarketing, or any activities involving real-world entities.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "README.md",
                        "description": "\"IMPORTANT NOTICE\" sections."
                    }
                ]
            },
            {
                "requirement_id": "FR-COM-EXC-01",
                "requirement_description": "The system shall use specific custom exceptions (detailed in Section 3.4) to indicate distinct error conditions encountered during processing.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_invalid_first_digit",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_repeated_digits_cnpj",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/exceptions/",
                        "description": "directory"
                    },
                    {
                        "id": "various raise statements in br_eval/",
                        "description": "modules"
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-CLN-01",
                "requirement_description": "The system shall provide a capability to sanitize a CPF string by removing all non-numeric characters.\n    *   **Description:** This process prepares the CPF for numerical validation or formatting. Leading/trailing whitespace should also be implicitly handled or removed by this process if it affects digit extraction.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_valid_cpf",
                        "description": "with \"145.382.206-20\""
                    },
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_whitespace_cpf",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_format_invalid_length_cpf",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_format_invalid_characters_cpf",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::clean_cpf",
                        "description": "(primary)"
                    },
                    {
                        "id": "br_eval/cpf.py::format_cpf",
                        "description": "(uses `re.sub`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-CLN-02",
                "requirement_description": "If an input CPF string intended for sanitization (specifically for `clean_cpf` behavior) contains any alphabetic characters, the system shall raise an `InvalidFormatCPFError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_format_invalid_characters_cpf",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::clean_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-CLN-03",
                "requirement_description": "After removing non-numeric characters, if the resulting CPF string does not consist of exactly 11 digits, the system shall raise an `InvalidLengthCPFError` when `clean_cpf` is directly invoked.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_format_invalid_length_cpf",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::clean_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-VAL-01",
                "requirement_description": "The system shall validate a given CPF string to determine its authenticity according to Brazilian CPF rules.\n    *   **Description:** Validation succeeds if all checks pass, returning a boolean True. Any failure results in a specific exception. Input CPF can be formatted (e.g., \"XXX.XXX.XXX-XX\"), unformatted (\"XXXXXXXXXXX\"), or contain leading/trailing whitespace.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_valid_cpf",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_whitespace_cpf",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::validate_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-VAL-02",
                "requirement_description": "As part of validation, if the sanitized CPF (11 digits) consists of all identical digits (e.g., \"11111111111\", \"00000000000\"), the system shall raise a `RepeatedDigitsCPFError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_repeated_digits_cpf",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_all_same_digits",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::validate_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-VAL-03",
                "requirement_description": "The system shall calculate the first CPF verifier digit based on a weighted sum of the first 9 digits of the sanitized CPF, conforming to the standard Brazilian CPF algorithm. If the calculated first verifier digit does not match the 10th digit of the sanitized CPF, the system shall raise an `InvalidCPFError` with a message indicating failure at the first verifier digit.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_invalid_first_digit",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::validate_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-VAL-04",
                "requirement_description": "The system shall calculate the second CPF verifier digit based on a weighted sum of the first 10 digits (including the correct first verifier digit) of the sanitized CPF, conforming to the standard Brazilian CPF algorithm. If the calculated second verifier digit does not match the 11th digit of the sanitized CPF, the system shall raise an `InvalidCPFError` with a message indicating failure at the second verifier digit.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_invalid_second_digit",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::validate_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-VAL-05",
                "requirement_description": "If, after sanitization, the CPF string does not contain exactly 11 digits, the validation shall fail by raising an `InvalidLengthCPFError` (this is raised by the underlying `clean_cpf` call within `validate_cpf`).",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_invalid_characters_cpf",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_invalid_length_cpf",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::validate_cpf",
                        "description": "(via `br_eval/cpf.py::clean_cpf`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-VAL-06",
                "requirement_description": "If the input CPF string for validation contains alphabetic characters, the validation shall fail by raising an `InvalidFormatCPFError` (this is raised by the underlying `clean_cpf` call within `validate_cpf`).",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::validate_cpf",
                        "description": "(via `br_eval/cpf.py::clean_cpf`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-FMT-01",
                "requirement_description": "The system shall format a CPF string consisting of 11 numeric digits into the standard \"XXX.XXX.XXX-XX\" pattern.\n    *   **Description:** Input can be a string of 11 digits or a string that, after removing all non-digit characters, results in 11 digits.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_clean_cpf",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::format_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-FMT-02",
                "requirement_description": "If the input CPF string, after removal of non-digit characters, does not result in exactly 11 digits, the formatting function shall raise a `ValueError`.\n    *   **Description:** This is specific to the `format_cpf` function's direct behavior.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::format_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-GEN-01",
                "requirement_description": "The system shall generate syntactically valid 11-digit CPF numbers.\n    *   **Description:** Generated CPFs must pass the system's CPF validation logic (FR-CPF-VAL-01).",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_valid_cpf_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::generate_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CPF-GEN-02",
                "requirement_description": "The CPF generation function shall offer an option to return the generated CPF in the \"XXX.XXX.XXX-XX\" formatted pattern.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_valid_cpf_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cpf.py::generate_cpf",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-CLN-01",
                "requirement_description": "The system shall provide a capability to sanitize a CNPJ string by removing all non-numeric characters.\n    *   **Description:** Prepares the CNPJ for numerical validation or formatting.",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_clean_cnpj",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::clean_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-CLN-02",
                "requirement_description": "After removing non-numeric characters, if the resulting CNPJ string does not consist of exactly 14 digits, the system shall raise an `InvalidLengthCNPJError`.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::clean_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-CLN-03",
                "requirement_description": "If the CNPJ string, after removal of non-numeric characters, still contains non-digit characters (e.g. if it was not purely numeric and punctuation to begin with, and `re.sub(r'\\D', '', cnpj)` was bypassed or failed), the system shall raise an `InvalidFormatCNPJError`. (Note: given `re.sub(r'\\D', '', cnpj)`, this path is hard to reach unless `isdigit()` fails for other reasons, but the code has the check).\n    *   **Description:** The `clean_cnpj` function has a `cnpj_numbers.isdigit()` check.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::clean_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-VAL-01",
                "requirement_description": "The system shall validate a given CNPJ string to determine its authenticity according to Brazilian CNPJ rules.\n    *   **Description:** Validation succeeds if all checks pass, returning boolean True. Failures raise specific exceptions. Input CNPJ can be formatted, unformatted, or contain leading/trailing whitespace.",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_whitespace_cnpj",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::validate_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-VAL-02",
                "requirement_description": "As part of validation, if the sanitized CNPJ (14 digits) consists of all identical digits (e.g., \"11111111111111\"), the system shall raise a `RepeatedDigitsCNPJError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_repeated_digits_cnpj",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_all_same_digits",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::validate_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-VAL-03",
                "requirement_description": "The system shall calculate the two CNPJ verifier digits based on weighted sums of the preceding digits, conforming to the standard Brazilian CNPJ algorithm. If the calculated verifier digits do not match the last two digits of the sanitized CNPJ, the system shall raise an `InvalidCNPJError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_invalid_verification_digits",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::validate_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-VAL-04",
                "requirement_description": "If, after sanitization, the CNPJ string does not contain exactly 14 digits, the validation shall fail by raising an `InvalidLengthCNPJError` (via `clean_cnpj`).",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_invalid_characters_cnpj",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_invalid_length_cnpj",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::validate_cnpj",
                        "description": "(via `br_eval/cnpj.py::clean_cnpj`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-FMT-01",
                "requirement_description": "The system shall format a CNPJ string consisting of 14 numeric digits (or a string that cleans to 14 digits) into the \"XX.XXX.XXX/XXXX-XX\" pattern.",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_format_cnpj",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::format_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-FMT-02",
                "requirement_description": "The formatting function internally uses the sanitization logic (FR-CNPJ-CLN-01, FR-CNPJ-CLN-02). If sanitization fails (e.g., due to incorrect length after cleaning), the relevant exceptions (`InvalidLengthCNPJError`) shall be propagated.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::format_cnpj",
                        "description": "(calls `br_eval/cnpj.py::clean_cnpj`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-GEN-01",
                "requirement_description": "The system shall generate syntactically valid 14-digit CNPJ numbers.\n    *   **Description:** Generated CNPJs must pass the system's CNPJ validation logic (FR-CNPJ-VAL-01). Base numbers for generation may include a fixed \"0001\" for the branch identifier part.",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::generate_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CNPJ-GEN-02",
                "requirement_description": "The CNPJ generation function shall offer an option to return the generated CNPJ in the \"XX.XXX.XXX/XXXX-XX\" formatted pattern.",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cnpj.py::generate_cnpj",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-CLN-01",
                "requirement_description": "Before validation or formatting, input plate strings shall be sanitized by converting them to uppercase, removing any hyphens, and stripping leading/trailing whitespace.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    },
                    {
                        "id": "br_eval/plate.py::format_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-VAL-01",
                "requirement_description": "The system shall validate a given vehicle plate string.\n    *   **Description:** If valid, the function returns a string indicating the plate type ('Old', 'Mercosul Car', 'Mercosul Motorcycle'). Invalid plates raise specific exceptions.",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_valid_old_plate",
                        "description": ""
                    },
                    {
                        "id": "tests/test_plate.py::TestPlate::test_valid_mercosul_car_plate",
                        "description": ""
                    },
                    {
                        "id": "tests/test_plate.py::TestPlate::test_valid_mercosul_motorcycle_plate",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-VAL-02",
                "requirement_description": "If the input plate is not a string, the system shall raise an `InvalidFormatPlateError`.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-VAL-03",
                "requirement_description": "After sanitization (FR-PLT-CLN-01), if the plate string does not consist of exactly 7 characters, the system shall raise an `InvalidLengthPlateError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_invalid_plate_length",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-VAL-04",
                "requirement_description": "After sanitization, if the 7-character plate string contains any characters other than uppercase letters (A-Z) or digits (0-9), the system shall raise an `InvalidCharacterPlateError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_invalid_plate_characters",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-VAL-05",
                "requirement_description": "The system shall identify a sanitized 7-character alphanumeric plate as 'Old' if it matches the pattern LLLNNNN (3 letters followed by 4 numbers).",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_valid_old_plate",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-VAL-06",
                "requirement_description": "The system shall identify a sanitized 7-character alphanumeric plate as 'Mercosul Car' if it matches the pattern LLLNLNN (3 letters, 1 number, 1 letter, 2 numbers).",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_valid_mercosul_car_plate",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-VAL-07",
                "requirement_description": "The system shall identify a sanitized 7-character alphanumeric plate as 'Mercosul Motorcycle' if it matches the pattern LLLNNLN (3 letters, 2 numbers, 1 letter, 1 number).",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_valid_mercosul_motorcycle_plate",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-VAL-08",
                "requirement_description": "If a sanitized 7-character alphanumeric plate does not match any of the recognized patterns (Old, Mercosul Car, Mercosul Motorcycle), the system shall raise an `InvalidPlateError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_invalid_plate_pattern",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::validate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-FMT-01",
                "requirement_description": "The system shall format a 7-character alphanumeric plate string (after sanitization per FR-PLT-CLN-01) by inserting a hyphen between the third and fourth characters, resulting in an \"AAA-BBBB\" pattern.",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_format_plate",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::format_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-FMT-02",
                "requirement_description": "If the input plate string, after sanitization, does not have exactly 7 characters, the formatting function shall raise an `InvalidLengthPlateError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_format_plate_invalid_length",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::format_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-GEN-01",
                "requirement_description": "The system shall generate valid vehicle plates of the 'Old' format (LLLNNNN).",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_generate_old_plate",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::generate_old_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-GEN-02",
                "requirement_description": "The system shall generate valid vehicle plates of the 'Mercosul Car' format (LLLNLNN).",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_generate_mercosul_car_plate",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::generate_mercosul_car_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-GEN-03",
                "requirement_description": "The system shall generate valid vehicle plates of the 'Mercosul Motorcycle' format (LLLNNLN).",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_generate_mercosul_motorcycle_plate",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::generate_mercosul_motorcycle_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-GEN-04",
                "requirement_description": "All plate generation functions shall offer an option to return the plate in a formatted \"AAA-BBBB\" pattern.",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_generate_plate_formatted",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::generate_old_plate",
                        "description": ""
                    },
                    {
                        "id": "generate_mercosul_car_plate",
                        "description": ""
                    },
                    {
                        "id": "generate_mercosul_motorcycle_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-GEN-05",
                "requirement_description": "The system shall provide a generic plate generation function that accepts a `plate_type` argument ('old', 'mercosul_car', 'mercosul_motorcycle') and delegates to the corresponding specific generator.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::generate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PLT-GEN-06",
                "requirement_description": "If the generic plate generation function (FR-PLT-GEN-05) receives an unrecognized `plate_type`, it shall raise a `ValueError`.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/plate.py::generate_plate",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-CLN-01",
                "requirement_description": "The system shall provide a capability to sanitize a CEP string.\n    *   **Description:** This involves checking for letters before numeric conversion, then removing all non-numeric characters.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_clean_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::clean_cep",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-CLN-02",
                "requirement_description": "If the input CEP string contains any alphabetic characters, the sanitization process shall raise an `InvalidCharacterCEPError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_invalid_character_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::clean_cep",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-CLN-03",
                "requirement_description": "After removing non-numeric characters, if the resulting string does not consist of exactly 8 digits, the sanitization process shall raise an `InvalidLengthCEPError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_invalid_length_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::clean_cep",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-CLN-04",
                "requirement_description": "After removing non-numeric characters and checking length, if the resulting 8-character string is not composed entirely of digits, the sanitization process shall raise an `InvalidCharacterCEPError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_invalid_character_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::clean_cep",
                        "description": "(`if not cep_numbers.isdigit()`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-VAL-01",
                "requirement_description": "The system shall validate a given CEP string.\n    *   **Description:** Validation is structural. It uses the sanitization logic (FR-CEP-CLN-01 to FR-CEP-CLN-04). If sanitization succeeds (i.e., the cleaned CEP is 8 digits and numeric), validation returns `True`. Otherwise, relevant exceptions from cleaning are raised.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_valid_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::validate_cep",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-VAL-02",
                "requirement_description": "If the input CEP for validation leads to `InvalidCharacterCEPError` during sanitization (e.g., contains letters), this exception shall be propagated.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_invalid_character_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::validate_cep",
                        "description": "(via `br_eval/cep.py::clean_cep`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-VAL-03",
                "requirement_description": "If the input CEP for validation leads to `InvalidLengthCEPError` during sanitization (e.g., not 8 digits after cleaning), this exception shall be propagated.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_invalid_length_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::validate_cep",
                        "description": "(via `br_eval/cep.py::clean_cep`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-FMT-01",
                "requirement_description": "The system shall format a CEP string (that sanitizes to 8 numeric digits) into the \"XXXXX-XXX\" pattern.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_format_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::format_cep",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-FMT-02",
                "requirement_description": "The CEP formatting function internally uses the sanitization logic. If sanitization fails, the relevant exceptions (e.g., `InvalidLengthCEPError`, `InvalidCharacterCEPError`) shall be propagated.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::format_cep",
                        "description": "(calls `br_eval/cep.py::clean_cep`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-GEN-01",
                "requirement_description": "The system shall generate random 8-digit numeric strings representing CEPs.\n    *   **Description:** Generated CEPs must be structurally valid per FR-CEP-VAL-01.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_generate_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::generate_cep",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CEP-GEN-02",
                "requirement_description": "The CEP generation function shall offer an option to return the generated CEP in the \"XXXXX-XXX\" formatted pattern.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_generate_formatted_cep",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/cep.py::generate_cep",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-CLN-01",
                "requirement_description": "The system shall provide a capability to sanitize a phone number string by removing all non-numeric characters.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_clean_phone_number",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::clean_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-CLN-02",
                "requirement_description": "If the input phone number string for sanitization contains any alphabetic characters, the system shall raise an `InvalidCharacterPhoneNumberError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_invalid_characters",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::clean_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-VAL-01",
                "requirement_description": "The system shall validate a given Brazilian phone number string.\n    *   **Description:** If valid, returns a dictionary containing the phone 'type' ('mobile' or 'landline'), 'ddd' (2-digit area code), and 'number' (the local part of the number). Invalid numbers raise specific exceptions. Input can be in various formats (e.g., with/without parentheses, spaces, hyphens, or country code).",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_valid_mobile_numbers",
                        "description": ""
                    },
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_valid_landline_numbers",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::validate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-VAL-02",
                "requirement_description": "The validation process shall correctly handle and remove an optional \"55\" Brazilian country code prefix from the sanitized number if present and the remaining length is appropriate for a national number.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_valid_mobile_numbers",
                        "description": "(with \"+55 (21) 99876-5432\")"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::validate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-VAL-03",
                "requirement_description": "After sanitization and optional country code removal, if the resulting number of digits is not 10 or 11, the system shall raise an `InvalidLengthPhoneNumberError`.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_invalid_length",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::validate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-VAL-04",
                "requirement_description": "The system shall extract the first two digits of the (potentially country-code-stripped) 10 or 11-digit number as the DDD. This DDD must be one of the predefined valid Brazilian DDD codes (see Appendix A / CSTR-04). If not, an `InvalidDDDPhoneNumberError` shall be raised.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_invalid_ddd",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::validate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-VAL-05",
                "requirement_description": "If the number (DDD + local part) has 11 digits, the local part (9 digits) must start with '9' to be considered a 'mobile' number. If not (e.g., local part is 9 digits but starts with '8'), an `InvalidPhoneNumberError` shall be raised.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_valid_mobile_numbers",
                        "description": ""
                    },
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_invalid_number_pattern",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::validate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-VAL-06",
                "requirement_description": "If the number (DDD + local part) has 10 digits, the local part (8 digits) must start with '2', '3', '4', or '5' to be considered a 'landline' number. If not, an `InvalidPhoneNumberError` shall be raised.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_valid_landline_numbers",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::validate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-VAL-07",
                "requirement_description": "If the length of the local number part is inconsistent with expected patterns (e.g., 11 total digits but local part is not 9 digits, or 10 total digits but local part is not 8 digits), an `InvalidPhoneNumberError` or `InvalidLengthPhoneNumberError` shall be raised.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_invalid_number_pattern",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::validate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-VAL-08",
                "requirement_description": "If the input phone number for validation leads to an `InvalidCharacterPhoneNumberError` during sanitization (e.g., contains letters), this exception shall be propagated.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_invalid_characters",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::validate_phone_number",
                        "description": "(via `br_eval/phone.py::clean_phone_number`)"
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-FMT-01",
                "requirement_description": "The system shall format a valid phone number (validated by FR-PHN-VAL-01) into a standard Brazilian pattern.\n    *   Mobile: `(XX) 9XXXX-XXXX`\n    *   Landline: `(XX) XXXX-XXXX`\n    *   Where XX is the DDD.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_format_phone_number",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::format_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-FMT-02",
                "requirement_description": "The phone number formatting function shall offer an option to include the \"+55\" international country code prefix.\n    *   **Description:** Example: `+55 (XX) 9XXXX-XXXX` or `+55 (XX) XXXX-XXXX`.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::format_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-FMT-03",
                "requirement_description": "The phone number formatting function internally validates the input number. If validation fails, the relevant exceptions (e.g., `InvalidLengthPhoneNumberError`, `InvalidDDDPhoneNumberError`) shall be propagated.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::format_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-GEN-01",
                "requirement_description": "The system shall generate syntactically valid Brazilian phone numbers.\n    *   **Description:** Generated numbers must use a valid DDD from the predefined set and adhere to mobile or landline structural rules.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_generate_phone_number",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::generate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-GEN-02",
                "requirement_description": "The generation function shall accept a `phone_type` argument to specify 'mobile' or 'landline' generation.\n    *   Mobile: 9-digit local number, starting with '9'.\n    *   Landline: 8-digit local number, starting with '2', '3', '4', or '5'.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_generate_phone_number",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::generate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-GEN-03",
                "requirement_description": "If an unrecognized `phone_type` is provided to the generation function, it shall raise a `ValueError`.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::generate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-GEN-04",
                "requirement_description": "The generation function shall offer an option (`formatted=True`) to return the generated number in the standard Brazilian formatted pattern (FR-PHN-FMT-01).",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::generate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PHN-GEN-05",
                "requirement_description": "The generation function shall offer an option (`international=True`) to affect the output format.\n    *   If `formatted=True` and `international=True`, the output is internationally formatted (e.g., `+55 (XX) ...`).\n    *   If `formatted=False` and `international=True`, the output is a raw digit string prefixed with \"55\".",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "br_eval/phone.py::generate_phone_number",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "NFR-01",
                "requirement_description": "No specific non-functional requirements (e.g., performance, security, usability) have been identified with directly corresponding, explicit original test cases that validate them. Therefore, no NFRs are formally specified here.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-01",
                "requirement_description": "The standard human-readable format for CPF is \"XXX.XXX.XXX-XX\". The system should be able to parse inputs that may include this formatting and should be able to generate outputs in this format.",
                "test_traceability": [
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_clean_cpf",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cpf.py::TestCPF::test_valid_cpf",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-02",
                "requirement_description": "The standard human-readable format for CNPJ is \"XX.XXX.XXX/XXXX-XX\". The system should be able to parse inputs with this formatting and generate outputs in this format.",
                "test_traceability": [
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_format_cnpj",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-03",
                "requirement_description": "Vehicle plates are commonly represented as \"AAA-NNNN\" (Old) or \"AAA-XNNN\" patterns (Mercosul, where X can be letter or number). The system should handle inputs with/without the hyphen and format outputs with a hyphen.",
                "test_traceability": [
                    {
                        "id": "tests/test_plate.py::TestPlate::test_format_plate",
                        "description": ""
                    },
                    {
                        "id": "tests/test_plate.py::TestPlate::test_valid_old_plate",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-04",
                "requirement_description": "The standard human-readable format for CEP is \"XXXXX-XXX\". The system should parse inputs with this formatting and generate outputs in this format.",
                "test_traceability": [
                    {
                        "id": "tests/test_cep.py::TestCEP::test_format_cep",
                        "description": ""
                    },
                    {
                        "id": "tests/test_cep.py::TestCEP::test_valid_cep",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EIR-05",
                "requirement_description": "Brazilian phone numbers have common representations like \"(XX) YYYYY-YYYY\" (mobile) or \"(XX) YYYY-YYYY\" (landline), and internationally as \"+55 ...\". The system should parse various common input styles and format to these standards.",
                "test_traceability": [
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_format_phone_number",
                        "description": ""
                    },
                    {
                        "id": "tests/test_phone.py::TestPhoneNumber::test_valid_mobile_numbers",
                        "description": ""
                    }
                ],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CPF-01",
                "requirement_description": "`CPFError`:** Base class for CPF-related exceptions.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CPF-02",
                "requirement_description": "`InvalidCPFError` (inherits `CPFError`):** General exception for an invalid CPF, often due to verifier digit mismatch.\n    *   **Message for 1st digit mismatch:** \"First verification digit does not match.\"\n    *   **Message for 2nd digit mismatch:** \"Second verification digit does not match.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CPF-03",
                "requirement_description": "`RepeatedDigitsCPFError` (inherits `InvalidCPFError`):** Raised when a CPF consists of all identical digits.\n    *   **Message:** \"CPF cannot have all digits equal.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CPF-04",
                "requirement_description": "`InvalidFormatCPFError` (inherits `CPFError`):** Raised when a CPF string contains non-numeric characters where not expected by `clean_cpf` (specifically letters).\n    *   **Message:** \"CPF must contain only numbers.\" (from `br_eval/cpf.py::clean_cpf` exception), or \"CPF contains letters.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CPF-05",
                "requirement_description": "`InvalidLengthCPFError` (inherits `CPFError`):** Raised when a CPF string, after sanitization, does not have the required 11 digits.\n    *   **Message:** \"CPF must have 11 digits. Current length: {length}.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CNPJ-01",
                "requirement_description": "`CNPJError`:** Base class for CNPJ-related exceptions.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CNPJ-02",
                "requirement_description": "`InvalidCNPJError` (inherits `CNPJError`):** General exception for an invalid CNPJ, typically due to verifier digit mismatch.\n    *   **Message:** \"Verification digits do not match.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CNPJ-03",
                "requirement_description": "`RepeatedDigitsCNPJError` (inherits `InvalidCNPJError`):** Raised when a CNPJ consists of all identical digits.\n    *   **Message:** \"CNPJ cannot have all digits equal.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CNPJ-04",
                "requirement_description": "`InvalidFormatCNPJError` (inherits `CNPJError`):** Raised when a CNPJ string contains non-numeric characters where not expected by `clean_cnpj`.\n    *   **Message:** \"CNPJ must contain only numbers.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CNPJ-05",
                "requirement_description": "`InvalidLengthCNPJError` (inherits `CNPJError`):** Raised when a CNPJ string, after sanitization, does not have the required 14 digits.\n    *   **Message:** \"CNPJ must have 14 digits. Current length: {length}.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PLT-01",
                "requirement_description": "`PlateError`:** Base class for plate-related exceptions.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PLT-02",
                "requirement_description": "`InvalidPlateError` (inherits `PlateError`):** Raised when a plate string does not match any recognized Brazilian plate pattern after basic structural checks pass.\n    *   **Message:** \"Invalid plate format.\" (as per `InvalidPlateError.__str__`)",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PLT-03",
                "requirement_description": "`InvalidFormatPlateError` (inherits `PlateError`):** Raised if the input plate is not a string.\n    *   **Message:** \"Plate must be a string of 7 alphanumeric characters.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PLT-04",
                "requirement_description": "`InvalidCharacterPlateError` (inherits `PlateError`):** Raised when a sanitized plate string contains characters other than A-Z or 0-9.\n    *   **Message:** \"Plate contains invalid characters.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PLT-05",
                "requirement_description": "`InvalidLengthPlateError` (inherits `PlateError`):** Raised when a sanitized plate string does not have exactly 7 characters.\n    *   **Message:** \"Plate must have exactly 7 characters.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CEP-01",
                "requirement_description": "`CEPError`:** Base class for CEP-related exceptions.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CEP-02",
                "requirement_description": "`InvalidCEPError` (inherits `CEPError`):** General exception for an invalid CEP format (though current implementation uses more specific errors).\n    *   **Message:** \"Invalid CEP format.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CEP-03",
                "requirement_description": "`InvalidLengthCEPError` (inherits `CEPError`):** Raised when a CEP string, after sanitization, does not have 8 digits.\n    *   **Message:** \"CEP must have 8 digits. Current length: {length}.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-CEP-04",
                "requirement_description": "`InvalidCharacterCEPError` (inherits `CEPError`):** Raised when a CEP string contains non-numeric characters (specifically letters before cleaning, or if `isdigit()` fails after cleaning for other reasons).\n    *   **Message:** \"CEP must contain only numbers.\" or \"CEP contains letters.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PHN-01",
                "requirement_description": "`PhoneNumberError`:** Base class for phone number exceptions.",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PHN-02",
                "requirement_description": "`InvalidPhoneNumberError` (inherits `PhoneNumberError`):** Raised for general invalid phone number patterns not covered by more specific exceptions (e.g., mobile not starting with '9', landline not starting with '2'-'5').\n    *   **Message:** \"Invalid phone number format.\" or \"Phone number does not match mobile or landline patterns.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PHN-03",
                "requirement_description": "`InvalidLengthPhoneNumberError` (inherits `PhoneNumberError`):** Raised when a sanitized phone number (after potential country code removal) does not have 10 or 11 digits.\n    *   **Message:** \"Phone number has an invalid length: {length} digits.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PHN-04",
                "requirement_description": "`InvalidCharacterPhoneNumberError` (inherits `PhoneNumberError`):** Raised when a phone number string contains non-numeric characters (e.g., letters).\n    *   **Message:** \"Phone number must contain only digits.\" or \"Phone number contains letters.\"",
                "test_traceability": [],
                "code_traceability": []
            },
            {
                "requirement_id": "EXC-PHN-05",
                "requirement_description": "`InvalidDDDPhoneNumberError` (inherits `PhoneNumberError`):** Raised when the extracted 2-digit DDD is not in the predefined list of valid DDDs.\n    *   **Message:** \"Invalid DDD code: {ddd}.\"",
                "test_traceability": [],
                "code_traceability": []
            }
        ],
        "commit_sha": "4914085f2e47e2919db2b511a3cd971015adb3a1",
        "full_code_skeleton": "--- File: br_eval/cpf.py ---\n```python\ndef format_cpf(cpf: str) -> str:\n    \"\"\"\n    Formats a CPF string in the pattern XXX.XXX.XXX-XX.\n\n    Args:\n        cpf (str): The CPF string with exactly 11 digits.\n\n    Returns:\n        str: The formatted CPF string.\n\n    Raises:\n        ValueError: If the CPF does not have exactly 11 digits.\n    \"\"\"\n    pass\n\ndef clean_cpf(cpf: str) -> str:\n    \"\"\"\n    Removes all non-digit characters from the CPF.\n    Raises an exception if the CPF does not have 11 digits after cleaning.\n    \"\"\"\n    pass\n\ndef validate_cpf(cpf: str) -> bool:\n    \"\"\"\n    Validates a CPF by checking the verification digits.\n    Raises specific exceptions for different validation errors.\n    \"\"\"\n    pass\n\ndef generate_cpf(formatted: bool = False) -> str:\n    \"\"\"\n    Generates a valid CPF number.\n\n    Args:\n        formatted (bool):\n            If True, returns the CPF in the formatted pattern XXX.XXX.XXX-XX.\n            If False, returns the CPF as a numeric string.\n\n    Returns:\n        str: A valid CPF number.\n    \"\"\"\n    pass\n```\n--- File: br_eval/cnpj.py ---\n```python\ndef clean_cnpj(cnpj):\n    \"\"\"\n    Removes all non-digit characters from the CNPJ.\n    Raises an exception if the CNPJ does not have 14 digits after cleaning.\n    \"\"\"\n    pass\n\ndef format_cnpj(cnpj):\n    \"\"\"\n    Formats a CNPJ string into the pattern XX.XXX.XXX/YYYY-ZZ.\n\n    Args:\n        cnpj (str): The CNPJ string with exactly 14 digits.\n\n    Returns:\n        str: The formatted CNPJ string.\n\n    Raises:\n        InvalidLengthCNPJError: If the CNPJ does not have exactly 14 digits.\n    \"\"\"\n    pass\n\ndef validate_cnpj(cnpj):\n    \"\"\"\n    Validates a CNPJ by checking the verification digits.\n    Raises specific exceptions for different validation errors.\n\n    Args:\n        cnpj (str): The CNPJ string to validate.\n\n    Returns:\n        bool: True if the CNPJ is valid.\n\n    Raises:\n        RepeatedDigitsCNPJError: If all digits are the same.\n        InvalidCNPJError: If the verification digits do not match.\n    \"\"\"\n    pass\n\ndef generate_cnpj(formatted=False):\n    \"\"\"\n    Generates a valid CNPJ number.\n\n    Args:\n        formatted (bool):\n            True, returns the CNPJ in the formatted pattern XX.XXX.XXX/YYYY-ZZ.\n            False, returns the CNPJ as a numeric string.\n\n    Returns:\n        str: A valid CNPJ number.\n    \"\"\"\n    pass\n```\n--- File: br_eval/cep.py ---\n```python\ndef clean_cep(cep):\n    \"\"\"\n    Remove all non-numeric characters from the CEP.\n    Checks if the CEP has exactly 8 digits after cleaning.\n\n    Args:\n        cep (str): The CEP to be cleaned.\n\n    Returns:\n        str: The CEP containing only numeric digits.\n\n    Raises:\n        InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n        InvalidCharacterCEPError: If the CEP contains letters.\n    \"\"\"\n    pass\n\ndef format_cep(cep):\n    \"\"\"\n    Format the CEP in the 'XXXXX-XXX' pattern.\n\n    Args:\n        cep (str): The CEP containing 8 digits.\n\n    Returns:\n        str: The formatted CEP.\n\n    Raises:\n        InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n    \"\"\"\n    pass\n\ndef validate_cep(cep):\n    \"\"\"\n    Validate the CEP by checking if it is in the correct format.\n\n    Args:\n        cep (str): The CEP to be validated.\n\n    Returns:\n        bool: True if the CEP is valid.\n\n    Raises:\n        CEPError: Specific exceptions if the CEP is invalid.\n    \"\"\"\n    pass\n\ndef generate_cep(formatted=False):\n    \"\"\"\n    Generate a valid random CEP.\n\n    Args:\n        formatted (bool): If True, returns the CEP formatted with a hyphen.\n\n    Returns:\n        str: The generated CEP.\n    \"\"\"\n    pass\n```\n--- File: br_eval/plate.py ---\n```python\ndef validate_plate(plate):\n    \"\"\"\n    Validates a Brazilian vehicle plate, considering old and new\n    Mercosul standards.\n    Raises specific exceptions for different validation errors.\n\n    Args:\n        plate (str): The plate string to validate.\n\n    Returns:\n        str: The type of plate validated (\n            e.g., 'Old', 'Mercosul Car', 'Mercosul Motorcycle'\n        ).\n\n    Raises:\n        InvalidLengthPlateError: If the plate does not have 7 characters.\n        InvalidCharacterPlateError: If the plate contains invalid characters.\n        InvalidPlateError: If the plate does not match any valid pattern.\n    \"\"\"\n    pass\n\ndef format_plate(plate):\n    \"\"\"\n    Formats the plate string by inserting a hyphen.\n\n    Args:\n        plate (str): The plate string to format.\n\n    Returns:\n        str: The formatted plate string (e.g., 'ABC-1234').\n    \"\"\"\n    pass\n\ndef generate_old_plate(formatted=False):\n    \"\"\"\n    Generates a valid vehicle plate in the old format (ABC1234).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-1234).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_mercosul_car_plate(formatted=False):\n    \"\"\"\n    Generates a valid vehicle plate in the Mercosul car format (ABC1D23).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-1D23).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_mercosul_motorcycle_plate(formatted=False):\n    \"\"\"\n    Generates valid vehicle plate in the Mercosul motorcycle format (ABC12D3).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-12D3).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_plate(plate_type='old', formatted=False):\n    \"\"\"\n    Generates a vehicle plate of the specified type.\n\n    Args:\n        plate_type (str): Type of plate to generate (\n            'old', 'mercosul_car', 'mercosul_motorcycle'\n        ).\n        formatted (bool): If True, returns the plate with a hyphen.\n\n    Returns:\n        str: A generated vehicle plate.\n\n    Raises:\n        ValueError: If the plate_type is not recognized.\n    \"\"\"\n    pass\n```\n--- File: br_eval/phone.py ---\n```python\ndef clean_phone_number(phone_number: str) -> str:\n    \"\"\"\n    Removes all non-digit characters from the phone number.\n    Raises InvalidCharacterPhoneNumberError if letters are present.\n\n    Args:\n        phone_number (str): The phone number to clean.\n\n    Returns:\n        str: The cleaned phone number containing only digits.\n\n    Raises:\n        InvalidCharacterPhoneNumberError: If the phone number contains letters.\n    \"\"\"\n    pass\n\ndef validate_phone_number(phone_number):\n    \"\"\"\n    Validates a Brazilian phone number.\n\n    Args:\n        phone_number (str): The phone number to validate.\n\n    Returns:\n        dict: A dictionary with keys 'type' and 'formatted_number'.\n              'type' can be 'mobile' or 'landline'.\n\n    Raises:\n        InvalidLengthPhoneNumberError\n        InvalidCharacterPhoneNumberError\n        InvalidDDDPhoneNumberError\n        InvalidPhoneNumberError\n    \"\"\"\n    pass\n\ndef format_phone_number(phone_number, international=False):\n    \"\"\"\n    Formats the phone number into a standard format.\n\n    Args:\n        phone_number (str): The phone number to format.\n        international (bool): If True, includes the country code '+55'.\n\n    Returns:\n        str: The formatted phone number.\n    \"\"\"\n    pass\n\ndef generate_phone_number(\n        phone_type='mobile', formatted=False, international=False\n):\n    \"\"\"\n    Generates a valid Brazilian phone number.\n\n    Args:\n        phone_type (str): 'mobile' or 'landline'.\n        formatted (bool): If True, returns the formatted phone number.\n        international (bool): If True, includes the country code '+55'.\n\n    Returns:\n        str: The generated phone number.\n    \"\"\"\n    pass\n```\n--- File: br_eval/exceptions/cep_exceptions.py ---\n```python\nclass CEPError(Exception):\n    \"\"\"Base class for CEP exceptions.\"\"\"\n    pass\n\nclass InvalidCEPError(CEPError):\n    \"\"\"Exception for invalid CEPs.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCEPError(CEPError):\n    \"\"\"Exception for CEPs with incorrect number of digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n\nclass InvalidCharacterCEPError(CEPError):\n    \"\"\"Exception for CEPs with invalid characters.\"\"\"\n    def __str__(self):\n        pass\n```\n--- File: br_eval/exceptions/cpf_exceptions.py ---\n```python\nclass CPFError(Exception):\n    \"\"\"Base class for CPF exceptions.\"\"\"\n    pass\n\nclass InvalidCPFError(CPFError):\n    \"\"\"Exception raised when the CPF is invalid.\"\"\"\n    pass\n\nclass FirstDigitInvalidError(InvalidCPFError):\n    \"\"\"Exception raised when the first verification digit is incorrect.\"\"\"\n    def __str__(self):\n        pass\n\nclass SecondDigitInvalidError(InvalidCPFError):\n    \"\"\"Exception raised when the second verification digit is incorrect.\"\"\"\n    def __str__(self):\n        pass\n\nclass RepeatedDigitsCPFError(InvalidCPFError):\n    \"\"\"Exception for CPFs with all digits equal.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidFormatCPFError(CPFError):\n    \"\"\"Exception raised when the CPF contains non-numeric characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCPFError(CPFError):\n    \"\"\"Exception raised when the CPF does not have 11 digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n```\n--- File: br_eval/exceptions/cnpj_exceptions.py ---\n```python\nclass CNPJError(Exception):\n    \"\"\"Base class for CNPJ exceptions.\"\"\"\n    pass\n\nclass InvalidCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ is invalid.\"\"\"\n    pass\n\nclass RepeatedDigitsCNPJError(InvalidCNPJError):\n    \"\"\"Exception for CNPJs with all digits equal.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidFormatCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ does not have 14 digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n```\n--- File: br_eval/exceptions/plate_exceptions.py ---\n```python\nclass PlateError(Exception):\n    \"\"\"Base class for plate exceptions.\"\"\"\n    pass\n\nclass InvalidPlateError(PlateError):\n    \"\"\"Exception raised when the plate is invalid.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidFormatPlateError(PlateError):\n    \"\"\"Exception raised when the plate has invalid characters or length.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidCharacterPlateError(PlateError):\n    \"\"\"Exception raised when the plate contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthPlateError(PlateError):\n    \"\"\"Exception raised when the plate does not have 7 characters.\"\"\"\n    def __str__(self):\n        pass\n```\n--- File: br_eval/exceptions/phone_exceptions.py ---\n```python\nclass PhoneNumberError(Exception):\n    \"\"\"Base class for phone number exceptions.\"\"\"\n    pass\n\nclass InvalidPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number is invalid.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number has an invalid length.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n\nclass InvalidCharacterPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidDDDPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the DDD code is invalid.\"\"\"\n    def __init__(self, ddd):\n        pass\n\n    def __str__(self):\n        pass\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "br_eval/cpf.py",
                "code": "def format_cpf(cpf: str) -> str:\n    \"\"\"\n    Formats a CPF string in the pattern XXX.XXX.XXX-XX.\n\n    Args:\n        cpf (str): The CPF string with exactly 11 digits.\n\n    Returns:\n        str: The formatted CPF string.\n\n    Raises:\n        ValueError: If the CPF does not have exactly 11 digits.\n    \"\"\"\n    pass\n\ndef clean_cpf(cpf: str) -> str:\n    \"\"\"\n    Removes all non-digit characters from the CPF.\n    Raises an exception if the CPF does not have 11 digits after cleaning.\n    \"\"\"\n    pass\n\ndef validate_cpf(cpf: str) -> bool:\n    \"\"\"\n    Validates a CPF by checking the verification digits.\n    Raises specific exceptions for different validation errors.\n    \"\"\"\n    pass\n\ndef generate_cpf(formatted: bool = False) -> str:\n    \"\"\"\n    Generates a valid CPF number.\n\n    Args:\n        formatted (bool):\n            If True, returns the CPF in the formatted pattern XXX.XXX.XXX-XX.\n            If False, returns the CPF as a numeric string.\n\n    Returns:\n        str: A valid CPF number.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/cnpj.py",
                "code": "def clean_cnpj(cnpj):\n    \"\"\"\n    Removes all non-digit characters from the CNPJ.\n    Raises an exception if the CNPJ does not have 14 digits after cleaning.\n    \"\"\"\n    pass\n\ndef format_cnpj(cnpj):\n    \"\"\"\n    Formats a CNPJ string into the pattern XX.XXX.XXX/YYYY-ZZ.\n\n    Args:\n        cnpj (str): The CNPJ string with exactly 14 digits.\n\n    Returns:\n        str: The formatted CNPJ string.\n\n    Raises:\n        InvalidLengthCNPJError: If the CNPJ does not have exactly 14 digits.\n    \"\"\"\n    pass\n\ndef validate_cnpj(cnpj):\n    \"\"\"\n    Validates a CNPJ by checking the verification digits.\n    Raises specific exceptions for different validation errors.\n\n    Args:\n        cnpj (str): The CNPJ string to validate.\n\n    Returns:\n        bool: True if the CNPJ is valid.\n\n    Raises:\n        RepeatedDigitsCNPJError: If all digits are the same.\n        InvalidCNPJError: If the verification digits do not match.\n    \"\"\"\n    pass\n\ndef generate_cnpj(formatted=False):\n    \"\"\"\n    Generates a valid CNPJ number.\n\n    Args:\n        formatted (bool):\n            True, returns the CNPJ in the formatted pattern XX.XXX.XXX/YYYY-ZZ.\n            False, returns the CNPJ as a numeric string.\n\n    Returns:\n        str: A valid CNPJ number.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/cep.py",
                "code": "def clean_cep(cep):\n    \"\"\"\n    Remove all non-numeric characters from the CEP.\n    Checks if the CEP has exactly 8 digits after cleaning.\n\n    Args:\n        cep (str): The CEP to be cleaned.\n\n    Returns:\n        str: The CEP containing only numeric digits.\n\n    Raises:\n        InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n        InvalidCharacterCEPError: If the CEP contains letters.\n    \"\"\"\n    pass\n\ndef format_cep(cep):\n    \"\"\"\n    Format the CEP in the 'XXXXX-XXX' pattern.\n\n    Args:\n        cep (str): The CEP containing 8 digits.\n\n    Returns:\n        str: The formatted CEP.\n\n    Raises:\n        InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n    \"\"\"\n    pass\n\ndef validate_cep(cep):\n    \"\"\"\n    Validate the CEP by checking if it is in the correct format.\n\n    Args:\n        cep (str): The CEP to be validated.\n\n    Returns:\n        bool: True if the CEP is valid.\n\n    Raises:\n        CEPError: Specific exceptions if the CEP is invalid.\n    \"\"\"\n    pass\n\ndef generate_cep(formatted=False):\n    \"\"\"\n    Generate a valid random CEP.\n\n    Args:\n        formatted (bool): If True, returns the CEP formatted with a hyphen.\n\n    Returns:\n        str: The generated CEP.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/plate.py",
                "code": "def validate_plate(plate):\n    \"\"\"\n    Validates a Brazilian vehicle plate, considering old and new\n    Mercosul standards.\n    Raises specific exceptions for different validation errors.\n\n    Args:\n        plate (str): The plate string to validate.\n\n    Returns:\n        str: The type of plate validated (\n            e.g., 'Old', 'Mercosul Car', 'Mercosul Motorcycle'\n        ).\n\n    Raises:\n        InvalidLengthPlateError: If the plate does not have 7 characters.\n        InvalidCharacterPlateError: If the plate contains invalid characters.\n        InvalidPlateError: If the plate does not match any valid pattern.\n    \"\"\"\n    pass\n\ndef format_plate(plate):\n    \"\"\"\n    Formats the plate string by inserting a hyphen.\n\n    Args:\n        plate (str): The plate string to format.\n\n    Returns:\n        str: The formatted plate string (e.g., 'ABC-1234').\n    \"\"\"\n    pass\n\ndef generate_old_plate(formatted=False):\n    \"\"\"\n    Generates a valid vehicle plate in the old format (ABC1234).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-1234).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_mercosul_car_plate(formatted=False):\n    \"\"\"\n    Generates a valid vehicle plate in the Mercosul car format (ABC1D23).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-1D23).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_mercosul_motorcycle_plate(formatted=False):\n    \"\"\"\n    Generates valid vehicle plate in the Mercosul motorcycle format (ABC12D3).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-12D3).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_plate(plate_type='old', formatted=False):\n    \"\"\"\n    Generates a vehicle plate of the specified type.\n\n    Args:\n        plate_type (str): Type of plate to generate (\n            'old', 'mercosul_car', 'mercosul_motorcycle'\n        ).\n        formatted (bool): If True, returns the plate with a hyphen.\n\n    Returns:\n        str: A generated vehicle plate.\n\n    Raises:\n        ValueError: If the plate_type is not recognized.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/phone.py",
                "code": "def clean_phone_number(phone_number: str) -> str:\n    \"\"\"\n    Removes all non-digit characters from the phone number.\n    Raises InvalidCharacterPhoneNumberError if letters are present.\n\n    Args:\n        phone_number (str): The phone number to clean.\n\n    Returns:\n        str: The cleaned phone number containing only digits.\n\n    Raises:\n        InvalidCharacterPhoneNumberError: If the phone number contains letters.\n    \"\"\"\n    pass\n\ndef validate_phone_number(phone_number):\n    \"\"\"\n    Validates a Brazilian phone number.\n\n    Args:\n        phone_number (str): The phone number to validate.\n\n    Returns:\n        dict: A dictionary with keys 'type' and 'formatted_number'.\n              'type' can be 'mobile' or 'landline'.\n\n    Raises:\n        InvalidLengthPhoneNumberError\n        InvalidCharacterPhoneNumberError\n        InvalidDDDPhoneNumberError\n        InvalidPhoneNumberError\n    \"\"\"\n    pass\n\ndef format_phone_number(phone_number, international=False):\n    \"\"\"\n    Formats the phone number into a standard format.\n\n    Args:\n        phone_number (str): The phone number to format.\n        international (bool): If True, includes the country code '+55'.\n\n    Returns:\n        str: The formatted phone number.\n    \"\"\"\n    pass\n\ndef generate_phone_number(\n        phone_type='mobile', formatted=False, international=False\n):\n    \"\"\"\n    Generates a valid Brazilian phone number.\n\n    Args:\n        phone_type (str): 'mobile' or 'landline'.\n        formatted (bool): If True, returns the formatted phone number.\n        international (bool): If True, includes the country code '+55'.\n\n    Returns:\n        str: The generated phone number.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/exceptions/cep_exceptions.py",
                "code": "class CEPError(Exception):\n    \"\"\"Base class for CEP exceptions.\"\"\"\n    pass\n\nclass InvalidCEPError(CEPError):\n    \"\"\"Exception for invalid CEPs.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCEPError(CEPError):\n    \"\"\"Exception for CEPs with incorrect number of digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n\nclass InvalidCharacterCEPError(CEPError):\n    \"\"\"Exception for CEPs with invalid characters.\"\"\"\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/exceptions/cpf_exceptions.py",
                "code": "class CPFError(Exception):\n    \"\"\"Base class for CPF exceptions.\"\"\"\n    pass\n\nclass InvalidCPFError(CPFError):\n    \"\"\"Exception raised when the CPF is invalid.\"\"\"\n    pass\n\nclass FirstDigitInvalidError(InvalidCPFError):\n    \"\"\"Exception raised when the first verification digit is incorrect.\"\"\"\n    def __str__(self):\n        pass\n\nclass SecondDigitInvalidError(InvalidCPFError):\n    \"\"\"Exception raised when the second verification digit is incorrect.\"\"\"\n    def __str__(self):\n        pass\n\nclass RepeatedDigitsCPFError(InvalidCPFError):\n    \"\"\"Exception for CPFs with all digits equal.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidFormatCPFError(CPFError):\n    \"\"\"Exception raised when the CPF contains non-numeric characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCPFError(CPFError):\n    \"\"\"Exception raised when the CPF does not have 11 digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/exceptions/cnpj_exceptions.py",
                "code": "class CNPJError(Exception):\n    \"\"\"Base class for CNPJ exceptions.\"\"\"\n    pass\n\nclass InvalidCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ is invalid.\"\"\"\n    pass\n\nclass RepeatedDigitsCNPJError(InvalidCNPJError):\n    \"\"\"Exception for CNPJs with all digits equal.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidFormatCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ does not have 14 digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/exceptions/plate_exceptions.py",
                "code": "class PlateError(Exception):\n    \"\"\"Base class for plate exceptions.\"\"\"\n    pass\n\nclass InvalidPlateError(PlateError):\n    \"\"\"Exception raised when the plate is invalid.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidFormatPlateError(PlateError):\n    \"\"\"Exception raised when the plate has invalid characters or length.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidCharacterPlateError(PlateError):\n    \"\"\"Exception raised when the plate contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthPlateError(PlateError):\n    \"\"\"Exception raised when the plate does not have 7 characters.\"\"\"\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/exceptions/phone_exceptions.py",
                "code": "class PhoneNumberError(Exception):\n    \"\"\"Base class for phone number exceptions.\"\"\"\n    pass\n\nclass InvalidPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number is invalid.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number has an invalid length.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n\nclass InvalidCharacterPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidDDDPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the DDD code is invalid.\"\"\"\n    def __init__(self, ddd):\n        pass\n\n    def __str__(self):\n        pass\n"
            }
        ],
        "minimal_code_skeleton": "--- File: br_eval/cep.py ---\n```python\ndef clean_cep(cep):\n    \"\"\"\n    Remove all non-numeric characters from the CEP.\n    Checks if the CEP has exactly 8 digits after cleaning.\n\n    Args:\n        cep (str): The CEP to be cleaned.\n\n    Returns:\n        str: The CEP containing only numeric digits.\n\n    Raises:\n        InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n        InvalidCharacterCEPError: If the CEP contains letters.\n    \"\"\"\n    pass\n\ndef format_cep(cep):\n    \"\"\"\n    Format the CEP in the 'XXXXX-XXX' pattern.\n\n    Args:\n        cep (str): The CEP containing 8 digits.\n\n    Returns:\n        str: The formatted CEP.\n\n    Raises:\n        InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n    \"\"\"\n    pass\n\ndef validate_cep(cep):\n    \"\"\"\n    Validate the CEP by checking if it is in the correct format.\n\n    Args:\n        cep (str): The CEP to be validated.\n\n    Returns:\n        bool: True if the CEP is valid.\n\n    Raises:\n        CEPError: Specific exceptions if the CEP is invalid.\n    \"\"\"\n    pass\n\ndef generate_cep(formatted=False):\n    \"\"\"\n    Generate a valid random CEP.\n\n    Args:\n        formatted (bool): If True, returns the CEP formatted with a hyphen.\n\n    Returns:\n        str: The generated CEP.\n    \"\"\"\n    pass\n```\n--- File: br_eval/cnpj.py ---\n```python\ndef clean_cnpj(cnpj):\n    \"\"\"\n    Removes all non-digit characters from the CNPJ.\n    Raises an exception if the CNPJ does not have 14 digits after cleaning.\n    \"\"\"\n    pass\n\ndef format_cnpj(cnpj):\n    \"\"\"\n    Formats a CNPJ string into the pattern XX.XXX.XXX/YYYY-ZZ.\n\n    Args:\n        cnpj (str): The CNPJ string with exactly 14 digits.\n\n    Returns:\n        str: The formatted CNPJ string.\n\n    Raises:\n        InvalidLengthCNPJError: If the CNPJ does not have exactly 14 digits.\n    \"\"\"\n    pass\n\ndef validate_cnpj(cnpj):\n    \"\"\"\n    Validates a CNPJ by checking the verification digits.\n    Raises specific exceptions for different validation errors.\n\n    Args:\n        cnpj (str): The CNPJ string to validate.\n\n    Returns:\n        bool: True if the CNPJ is valid.\n\n    Raises:\n        RepeatedDigitsCNPJError: If all digits are the same.\n        InvalidCNPJError: If the verification digits do not match.\n    \"\"\"\n    pass\n\ndef generate_cnpj(formatted=False):\n    \"\"\"\n    Generates a valid CNPJ number.\n\n    Args:\n        formatted (bool):\n            True, returns the CNPJ in the formatted pattern XX.XXX.XXX/YYYY-ZZ.\n            False, returns the CNPJ as a numeric string.\n\n    Returns:\n        str: A valid CNPJ number.\n    \"\"\"\n    pass\n```\n--- File: br_eval/cpf.py ---\n```python\ndef format_cpf(cpf: str) -> str:\n    \"\"\"\n    Formats a CPF string in the pattern XXX.XXX.XXX-XX.\n\n    Args:\n        cpf (str): The CPF string with exactly 11 digits.\n\n    Returns:\n        str: The formatted CPF string.\n\n    Raises:\n        ValueError: If the CPF does not have exactly 11 digits.\n    \"\"\"\n    pass\n\ndef clean_cpf(cpf: str) -> str:\n    \"\"\"\n    Removes all non-digit characters from the CPF.\n    Raises an exception if the CPF does not have 11 digits after cleaning.\n    \"\"\"\n    pass\n\ndef validate_cpf(cpf: str) -> bool:\n    \"\"\"\n    Validates a CPF by checking the verification digits.\n    Raises specific exceptions for different validation errors.\n    \"\"\"\n    pass\n\ndef generate_cpf(formatted: bool = False) -> str:\n    \"\"\"\n    Generates a valid CPF number.\n\n    Args:\n        formatted (bool):\n            If True, returns the CPF in the formatted pattern XXX.XXX.XXX-XX.\n            If False, returns the CPF as a numeric string.\n\n    Returns:\n        str: A valid CPF number.\n    \"\"\"\n    pass\n```\n--- File: br_eval/exceptions/cep_exceptions.py ---\n```python\nclass CEPError(Exception):\n    \"\"\"Base class for CEP exceptions.\"\"\"\n    pass\n\nclass InvalidLengthCEPError(CEPError):\n    \"\"\"Exception for CEPs with incorrect number of digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n\nclass InvalidCharacterCEPError(CEPError):\n    \"\"\"Exception for CEPs with invalid characters.\"\"\"\n    def __str__(self):\n        pass\n```\n--- File: br_eval/exceptions/cnpj_exceptions.py ---\n```python\nclass CNPJError(Exception):\n    \"\"\"Base class for CNPJ exceptions.\"\"\"\n    pass\n\nclass InvalidCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ is invalid.\"\"\"\n    pass\n\nclass RepeatedDigitsCNPJError(InvalidCNPJError):\n    \"\"\"Exception for CNPJs with all digits equal.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ does not have 14 digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n```\n--- File: br_eval/exceptions/cpf_exceptions.py ---\n```python\nclass CPFError(Exception):\n    \"\"\"Base class for CPF exceptions.\"\"\"\n    pass\n\nclass InvalidCPFError(CPFError):\n    \"\"\"Exception raised when the CPF is invalid.\"\"\"\n    pass\n\nclass RepeatedDigitsCPFError(InvalidCPFError):\n    \"\"\"Exception for CPFs with all digits equal.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidFormatCPFError(CPFError):\n    \"\"\"Exception raised when the CPF contains non-numeric characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCPFError(CPFError):\n    \"\"\"Exception raised when the CPF does not have 11 digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n```\n--- File: br_eval/exceptions/phone_exceptions.py ---\n```python\nclass PhoneNumberError(Exception):\n    \"\"\"Base class for phone number exceptions.\"\"\"\n    pass\n\nclass InvalidPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number is invalid.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number has an invalid length.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n\nclass InvalidCharacterPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidDDDPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the DDD code is invalid.\"\"\"\n    def __init__(self, ddd):\n        pass\n\n    def __str__(self):\n        pass\n```\n--- File: br_eval/exceptions/plate_exceptions.py ---\n```python\nclass PlateError(Exception):\n    \"\"\"Base class for plate exceptions.\"\"\"\n    pass\n\nclass InvalidPlateError(PlateError):\n    \"\"\"Exception raised when the plate is invalid.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidCharacterPlateError(PlateError):\n    \"\"\"Exception raised when the plate contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthPlateError(PlateError):\n    \"\"\"Exception raised when the plate does not have 7 characters.\"\"\"\n    def __str__(self):\n        pass\n```\n--- File: br_eval/phone.py ---\n```python\ndef clean_phone_number(phone_number: str) -> str:\n    \"\"\"\n    Removes all non-digit characters from the phone number.\n    Raises InvalidCharacterPhoneNumberError if letters are present.\n\n    Args:\n        phone_number (str): The phone number to clean.\n\n    Returns:\n        str: The cleaned phone number containing only digits.\n\n    Raises:\n        InvalidCharacterPhoneNumberError: If the phone number contains letters.\n    \"\"\"\n    pass\n\ndef validate_phone_number(phone_number):\n    \"\"\"\n    Validates a Brazilian phone number.\n\n    Args:\n        phone_number (str): The phone number to validate.\n\n    Returns:\n        dict: A dictionary with keys 'type' and 'formatted_number'.\n              'type' can be 'mobile' or 'landline'.\n\n    Raises:\n        InvalidLengthPhoneNumberError\n        InvalidCharacterPhoneNumberError\n        InvalidDDDPhoneNumberError\n        InvalidPhoneNumberError\n    \"\"\"\n    pass\n\ndef format_phone_number(phone_number, international=False):\n    \"\"\"\n    Formats the phone number into a standard format.\n\n    Args:\n        phone_number (str): The phone number to format.\n        international (bool): If True, includes the country code '+55'.\n\n    Returns:\n        str: The formatted phone number.\n    \"\"\"\n    pass\n\ndef generate_phone_number(\n        phone_type='mobile', formatted=False, international=False\n):\n    \"\"\"\n    Generates a valid Brazilian phone number.\n\n    Args:\n        phone_type (str): 'mobile' or 'landline'.\n        formatted (bool): If True, returns the formatted phone number.\n        international (bool): If True, includes the country code '+55'.\n\n    Returns:\n        str: The generated phone number.\n    \"\"\"\n    pass\n```\n--- File: br_eval/plate.py ---\n```python\ndef validate_plate(plate):\n    \"\"\"\n    Validates a Brazilian vehicle plate, considering old and new\n    Mercosul standards.\n    Raises specific exceptions for different validation errors.\n\n    Args:\n        plate (str): The plate string to validate.\n\n    Returns:\n        str: The type of plate validated (\n            e.g., 'Old', 'Mercosul Car', 'Mercosul Motorcycle'\n        ).\n\n    Raises:\n        InvalidLengthPlateError: If the plate does not have 7 characters.\n        InvalidCharacterPlateError: If the plate contains invalid characters.\n        InvalidPlateError: If the plate does not match any valid pattern.\n    \"\"\"\n    pass\n\ndef format_plate(plate):\n    \"\"\"\n    Formats the plate string by inserting a hyphen.\n\n    Args:\n        plate (str): The plate string to format.\n\n    Returns:\n        str: The formatted plate string (e.g., 'ABC-1234').\n    \"\"\"\n    pass\n\ndef generate_old_plate(formatted=False):\n    \"\"\"\n    Generates a valid vehicle plate in the old format (ABC1234).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-1234).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_mercosul_car_plate(formatted=False):\n    \"\"\"\n    Generates a valid vehicle plate in the Mercosul car format (ABC1D23).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-1D23).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_mercosul_motorcycle_plate(formatted=False):\n    \"\"\"\n    Generates valid vehicle plate in the Mercosul motorcycle format (ABC12D3).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-12D3).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "br_eval/cep.py",
                "code": "def clean_cep(cep):\n    \"\"\"\n    Remove all non-numeric characters from the CEP.\n    Checks if the CEP has exactly 8 digits after cleaning.\n\n    Args:\n        cep (str): The CEP to be cleaned.\n\n    Returns:\n        str: The CEP containing only numeric digits.\n\n    Raises:\n        InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n        InvalidCharacterCEPError: If the CEP contains letters.\n    \"\"\"\n    pass\n\ndef format_cep(cep):\n    \"\"\"\n    Format the CEP in the 'XXXXX-XXX' pattern.\n\n    Args:\n        cep (str): The CEP containing 8 digits.\n\n    Returns:\n        str: The formatted CEP.\n\n    Raises:\n        InvalidLengthCEPError: If the CEP does not have exactly 8 digits.\n    \"\"\"\n    pass\n\ndef validate_cep(cep):\n    \"\"\"\n    Validate the CEP by checking if it is in the correct format.\n\n    Args:\n        cep (str): The CEP to be validated.\n\n    Returns:\n        bool: True if the CEP is valid.\n\n    Raises:\n        CEPError: Specific exceptions if the CEP is invalid.\n    \"\"\"\n    pass\n\ndef generate_cep(formatted=False):\n    \"\"\"\n    Generate a valid random CEP.\n\n    Args:\n        formatted (bool): If True, returns the CEP formatted with a hyphen.\n\n    Returns:\n        str: The generated CEP.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/cnpj.py",
                "code": "def clean_cnpj(cnpj):\n    \"\"\"\n    Removes all non-digit characters from the CNPJ.\n    Raises an exception if the CNPJ does not have 14 digits after cleaning.\n    \"\"\"\n    pass\n\ndef format_cnpj(cnpj):\n    \"\"\"\n    Formats a CNPJ string into the pattern XX.XXX.XXX/YYYY-ZZ.\n\n    Args:\n        cnpj (str): The CNPJ string with exactly 14 digits.\n\n    Returns:\n        str: The formatted CNPJ string.\n\n    Raises:\n        InvalidLengthCNPJError: If the CNPJ does not have exactly 14 digits.\n    \"\"\"\n    pass\n\ndef validate_cnpj(cnpj):\n    \"\"\"\n    Validates a CNPJ by checking the verification digits.\n    Raises specific exceptions for different validation errors.\n\n    Args:\n        cnpj (str): The CNPJ string to validate.\n\n    Returns:\n        bool: True if the CNPJ is valid.\n\n    Raises:\n        RepeatedDigitsCNPJError: If all digits are the same.\n        InvalidCNPJError: If the verification digits do not match.\n    \"\"\"\n    pass\n\ndef generate_cnpj(formatted=False):\n    \"\"\"\n    Generates a valid CNPJ number.\n\n    Args:\n        formatted (bool):\n            True, returns the CNPJ in the formatted pattern XX.XXX.XXX/YYYY-ZZ.\n            False, returns the CNPJ as a numeric string.\n\n    Returns:\n        str: A valid CNPJ number.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/cpf.py",
                "code": "def format_cpf(cpf: str) -> str:\n    \"\"\"\n    Formats a CPF string in the pattern XXX.XXX.XXX-XX.\n\n    Args:\n        cpf (str): The CPF string with exactly 11 digits.\n\n    Returns:\n        str: The formatted CPF string.\n\n    Raises:\n        ValueError: If the CPF does not have exactly 11 digits.\n    \"\"\"\n    pass\n\ndef clean_cpf(cpf: str) -> str:\n    \"\"\"\n    Removes all non-digit characters from the CPF.\n    Raises an exception if the CPF does not have 11 digits after cleaning.\n    \"\"\"\n    pass\n\ndef validate_cpf(cpf: str) -> bool:\n    \"\"\"\n    Validates a CPF by checking the verification digits.\n    Raises specific exceptions for different validation errors.\n    \"\"\"\n    pass\n\ndef generate_cpf(formatted: bool = False) -> str:\n    \"\"\"\n    Generates a valid CPF number.\n\n    Args:\n        formatted (bool):\n            If True, returns the CPF in the formatted pattern XXX.XXX.XXX-XX.\n            If False, returns the CPF as a numeric string.\n\n    Returns:\n        str: A valid CPF number.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/exceptions/cep_exceptions.py",
                "code": "class CEPError(Exception):\n    \"\"\"Base class for CEP exceptions.\"\"\"\n    pass\n\nclass InvalidLengthCEPError(CEPError):\n    \"\"\"Exception for CEPs with incorrect number of digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n\nclass InvalidCharacterCEPError(CEPError):\n    \"\"\"Exception for CEPs with invalid characters.\"\"\"\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/exceptions/cnpj_exceptions.py",
                "code": "class CNPJError(Exception):\n    \"\"\"Base class for CNPJ exceptions.\"\"\"\n    pass\n\nclass InvalidCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ is invalid.\"\"\"\n    pass\n\nclass RepeatedDigitsCNPJError(InvalidCNPJError):\n    \"\"\"Exception for CNPJs with all digits equal.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCNPJError(CNPJError):\n    \"\"\"Exception raised when the CNPJ does not have 14 digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/exceptions/cpf_exceptions.py",
                "code": "class CPFError(Exception):\n    \"\"\"Base class for CPF exceptions.\"\"\"\n    pass\n\nclass InvalidCPFError(CPFError):\n    \"\"\"Exception raised when the CPF is invalid.\"\"\"\n    pass\n\nclass RepeatedDigitsCPFError(InvalidCPFError):\n    \"\"\"Exception for CPFs with all digits equal.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidFormatCPFError(CPFError):\n    \"\"\"Exception raised when the CPF contains non-numeric characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthCPFError(CPFError):\n    \"\"\"Exception raised when the CPF does not have 11 digits.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/exceptions/phone_exceptions.py",
                "code": "class PhoneNumberError(Exception):\n    \"\"\"Base class for phone number exceptions.\"\"\"\n    pass\n\nclass InvalidPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number is invalid.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number has an invalid length.\"\"\"\n    def __init__(self, length):\n        pass\n\n    def __str__(self):\n        pass\n\nclass InvalidCharacterPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the phone number contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidDDDPhoneNumberError(PhoneNumberError):\n    \"\"\"Exception raised when the DDD code is invalid.\"\"\"\n    def __init__(self, ddd):\n        pass\n\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/exceptions/plate_exceptions.py",
                "code": "class PlateError(Exception):\n    \"\"\"Base class for plate exceptions.\"\"\"\n    pass\n\nclass InvalidPlateError(PlateError):\n    \"\"\"Exception raised when the plate is invalid.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidCharacterPlateError(PlateError):\n    \"\"\"Exception raised when the plate contains invalid characters.\"\"\"\n    def __str__(self):\n        pass\n\nclass InvalidLengthPlateError(PlateError):\n    \"\"\"Exception raised when the plate does not have 7 characters.\"\"\"\n    def __str__(self):\n        pass\n"
            },
            {
                "file_path": "br_eval/phone.py",
                "code": "def clean_phone_number(phone_number: str) -> str:\n    \"\"\"\n    Removes all non-digit characters from the phone number.\n    Raises InvalidCharacterPhoneNumberError if letters are present.\n\n    Args:\n        phone_number (str): The phone number to clean.\n\n    Returns:\n        str: The cleaned phone number containing only digits.\n\n    Raises:\n        InvalidCharacterPhoneNumberError: If the phone number contains letters.\n    \"\"\"\n    pass\n\ndef validate_phone_number(phone_number):\n    \"\"\"\n    Validates a Brazilian phone number.\n\n    Args:\n        phone_number (str): The phone number to validate.\n\n    Returns:\n        dict: A dictionary with keys 'type' and 'formatted_number'.\n              'type' can be 'mobile' or 'landline'.\n\n    Raises:\n        InvalidLengthPhoneNumberError\n        InvalidCharacterPhoneNumberError\n        InvalidDDDPhoneNumberError\n        InvalidPhoneNumberError\n    \"\"\"\n    pass\n\ndef format_phone_number(phone_number, international=False):\n    \"\"\"\n    Formats the phone number into a standard format.\n\n    Args:\n        phone_number (str): The phone number to format.\n        international (bool): If True, includes the country code '+55'.\n\n    Returns:\n        str: The formatted phone number.\n    \"\"\"\n    pass\n\ndef generate_phone_number(\n        phone_type='mobile', formatted=False, international=False\n):\n    \"\"\"\n    Generates a valid Brazilian phone number.\n\n    Args:\n        phone_type (str): 'mobile' or 'landline'.\n        formatted (bool): If True, returns the formatted phone number.\n        international (bool): If True, includes the country code '+55'.\n\n    Returns:\n        str: The generated phone number.\n    \"\"\"\n    pass\n"
            },
            {
                "file_path": "br_eval/plate.py",
                "code": "def validate_plate(plate):\n    \"\"\"\n    Validates a Brazilian vehicle plate, considering old and new\n    Mercosul standards.\n    Raises specific exceptions for different validation errors.\n\n    Args:\n        plate (str): The plate string to validate.\n\n    Returns:\n        str: The type of plate validated (\n            e.g., 'Old', 'Mercosul Car', 'Mercosul Motorcycle'\n        ).\n\n    Raises:\n        InvalidLengthPlateError: If the plate does not have 7 characters.\n        InvalidCharacterPlateError: If the plate contains invalid characters.\n        InvalidPlateError: If the plate does not match any valid pattern.\n    \"\"\"\n    pass\n\ndef format_plate(plate):\n    \"\"\"\n    Formats the plate string by inserting a hyphen.\n\n    Args:\n        plate (str): The plate string to format.\n\n    Returns:\n        str: The formatted plate string (e.g., 'ABC-1234').\n    \"\"\"\n    pass\n\ndef generate_old_plate(formatted=False):\n    \"\"\"\n    Generates a valid vehicle plate in the old format (ABC1234).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-1234).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_mercosul_car_plate(formatted=False):\n    \"\"\"\n    Generates a valid vehicle plate in the Mercosul car format (ABC1D23).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-1D23).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n\ndef generate_mercosul_motorcycle_plate(formatted=False):\n    \"\"\"\n    Generates valid vehicle plate in the Mercosul motorcycle format (ABC12D3).\n\n    Args:\n        formatted (bool): If True, returns the plate with a hyphen (ABC-12D3).\n\n    Returns:\n        str: A generated vehicle plate.\n    \"\"\"\n    pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "tests/test_cep.py::TestCEP::test_generate_cep",
                "covers": [
                    "br_eval.cep.generate_cep - happy path (unformatted)",
                    "br_eval.cep.validate_cep - happy path (validation of generated CEP)",
                    "br_eval.cep.clean_cep - happy path (implicit via validate_cep)"
                ]
            },
            {
                "test_id": "tests/test_cep.py::TestCEP::test_format_cep",
                "covers": [
                    "br_eval.cep.format_cep - happy path",
                    "br_eval.cep.clean_cep - happy path (implicit via format_cep)"
                ]
            },
            {
                "test_id": "tests/test_cnpj.py::TestCNPJ::test_valid_cnpj_generator",
                "covers": [
                    "br_eval.cnpj.generate_cnpj - happy path (unformatted and formatted options)",
                    "br_eval.cnpj.validate_cnpj - happy path (validation of generated CNPJs)",
                    "br_eval.cnpj.clean_cnpj - happy path (implicit via internal calls during generation/validation)",
                    "br_eval.cnpj.format_cnpj - happy path (implicit via internal calls during generation)"
                ]
            },
            {
                "test_id": "tests/test_cpf.py::TestCPF::test_valid_cpf_generator",
                "covers": [
                    "br_eval.cpf.generate_cpf - happy path (unformatted and formatted options)",
                    "br_eval.cpf.validate_cpf - happy path (validation of generated CPFs)",
                    "br_eval.cpf.clean_cpf - happy path (implicit via internal calls during generation/validation)",
                    "br_eval.cpf.format_cpf - happy path (implicit via internal calls during generation)"
                ]
            },
            {
                "test_id": "tests/test_phone.py::TestPhoneNumber::test_generate_phone_number",
                "covers": [
                    "br_eval.phone.generate_phone_number - happy path (mobile and landline types)",
                    "br_eval.phone.validate_phone_number - happy path (validation of generated numbers)",
                    "br_eval.phone.clean_phone_number - happy path (implicit via validate_phone_number during generation)"
                ]
            },
            {
                "test_id": "tests/test_phone.py::TestPhoneNumber::test_format_phone_number",
                "covers": [
                    "br_eval.phone.format_phone_number - happy path",
                    "br_eval.phone.validate_phone_number - happy path (implicit via format_phone_number)",
                    "br_eval.phone.clean_phone_number - happy path (implicit via format_phone_number)"
                ]
            },
            {
                "test_id": "tests/test_plate.py::TestPlate::test_generate_old_plate",
                "covers": [
                    "br_eval.plate.generate_old_plate - happy path",
                    "br_eval.plate.validate_plate - happy path (validation of generated 'Old' plate)"
                ]
            },
            {
                "test_id": "tests/test_plate.py::TestPlate::test_generate_mercosul_car_plate",
                "covers": [
                    "br_eval.plate.generate_mercosul_car_plate - happy path",
                    "br_eval.plate.validate_plate - happy path (validation of generated 'Mercosul Car' plate)"
                ]
            },
            {
                "test_id": "tests/test_plate.py::TestPlate::test_generate_mercosul_motorcycle_plate",
                "covers": [
                    "br_eval.plate.generate_mercosul_motorcycle_plate - happy path",
                    "br_eval.plate.validate_plate - happy path (validation of generated 'Mercosul Motorcycle' plate)"
                ]
            },
            {
                "test_id": "tests/test_plate.py::TestPlate::test_format_plate",
                "covers": [
                    "br_eval.plate.format_plate - happy path"
                ]
            }
        ]
    },
    {
        "idx": 49864,
        "repo_name": "sklearn-compat_sklearn-compat",
        "url": "https://github.com/sklearn-compat/sklearn-compat",
        "description": "Ease multi-version support for scikit-learn compatible library",
        "stars": 8,
        "forks": 5,
        "language": "python",
        "size": 254,
        "created_at": "2024-11-20T22:21:36+00:00",
        "updated_at": "2025-04-23T12:12:22+00:00",
        "pypi_info": {
            "name": "sklearn-compat",
            "version": "0.1.3",
            "url": "https://files.pythonhosted.org/packages/23/7e/c5b43911ca5813eadfe1fd3f0a5e97f4f23c5006fba935b1172f350b4d17/sklearn_compat-0.1.3.tar.gz"
        },
        "judge_info": {
            "language": "python",
            "is_good_project": false,
            "reason": "",
            "python_file_num": 41,
            "comment_ratio": 0.27261462205700127,
            "pyfile_content_length": 60602,
            "pyfile_code_lines": 1614,
            "test_file_exist": true,
            "test_file_content_length": 21803,
            "pytest_framework": true,
            "test_case_num": 43,
            "metadata_path": [
                "pyproject.toml"
            ],
            "readme_content_length": 17498,
            "llm_reason": "The project `sklearn-compat` is a Python library designed to provide a compatibility layer for scikit-learn utilities across different versions of scikit-learn. \n\n**Positive Aspects:**\n*   **Self-Contained & Independent (for AI-rebuilt solution):** The library's core logic, once dependencies (`scikit-learn`, `numpy`) are installed, does not require an active internet connection or external APIs/services for its operation. Dependencies are standard PyPI packages. No specialized hardware is needed.\n*   **Clear & Well-Defined Functionality:** The README clearly explains the project's purpose and provides numerous examples of how its shims adapt code for different scikit-learn versions. This serves as a strong basis for specifying the AI's task to re-implement these compatibility functions.\n*   **Testable & Verifiable Output:** The project includes a comprehensive test suite that can be directly used or adapted to verify the AI's implementation. The behavior of each compatibility shim is objectively testable against expected outcomes or by ensuring it correctly interacts with different scikit-learn versions.\n*   **No Graphical User Interface (GUI):** It is a library, and interactions are through Python imports and function calls.\n*   **Appropriate Complexity, Scope & Difficulty:** Re-implementing the set of compatibility shims is a non-trivial task involving understanding API changes, version checking, and conditional logic. However, the scope is manageable, and individual shims are not overly complex algorithmically. The main file `_sklearn_compat.py` centralizes much of the logic.\n*   **Well-Understood Problem Domain:** API compatibility and writing shims are common software engineering tasks. The context is the scikit-learn library, which is well-documented.\n*   **Predominantly Code-Based Solution:** The task is to generate Python code for the library's functions and classes.\n\n**Negative Aspects or Concerns:**\n*   **Dependency on `scikit-learn` versions for testing:** While the `sklearn-compat` logic itself is self-contained, thoroughly testing the AI-rebuilt solution would require an environment where different versions of `scikit-learn` can be installed or mocked. This is an important consideration for the benchmark's execution environment.\n*   **Specification Granularity:** Precisely specifying every shim's behavior across all targeted scikit-learn versions could be verbose. However, the project's README and existing tests provide a strong foundation, and the task can be framed as replicating the observed behavior and API of the original `sklearn-compat`.\n\nOverall, `sklearn-compat` is a strong candidate. The task for the AI would be to recreate this library, ensuring that its functions provide the documented compatibility for scikit-learn versions (e.g., 1.2 through 1.6+ as detailed in the README). The difficulty is assessed as Medium because it requires careful handling of version differences and implementing a fair number of distinct utility shims, but does not involve highly complex algorithmic design.",
            "llm_project_type": "Python utility library for scikit-learn version compatibility",
            "llm_rating": 85,
            "llm_difficulty": "Medium"
        },
        "tests": {
            "repo_name": "sklearn-compat_sklearn-compat",
            "finish_test": true,
            "test_case_result": {
                "tests/test_base.py::test_is_clusterer": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_cloneable0]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_cloneable1]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_tags_renamed]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_valid_tag_types]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_repr]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_no_attributes_set_in_init]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_score_takes_y]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_overwrite_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_dont_overwrite_parameters]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_fit_returns_self]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_readonly_memmap_input]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_unfitted]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_do_not_raise_errors_in_init_or_set_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_n_features_in_after_fitting]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_mixin_order]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_positive_only_tag_during_fit]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_dtypes]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_complex_data]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_dtype_object]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_empty_data_messages]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_pipeline_consistency]": "skipped",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_nan_inf]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_tag]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_array]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_matrix]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_pickle]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_pickle(readonly_memmap=True)]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_f_contiguous_array_estimator]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_classifier_data_not_an_array]": "skipped",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_one_label]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_one_label_sample_weights]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_classes]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_partial_fit_n_features]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train(readonly_memmap=True)]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train(readonly_memmap=True,X_dtype=float32)]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_regression_target]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_supervised_y_no_nan]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_supervised_y_2d]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_non_transformer_estimators_n_iter]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_decision_proba_consistency]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_parameters_default_constructible]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_1sample]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_1feature]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_get_params_invariance]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_set_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_dict_unchanged]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_idempotent]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_check_is_fitted]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_n_features_in]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_fit1d]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_predict1d]": "passed",
                "tests/test_common.py::test_basic_estimator[Classifier()-check_requires_y_none]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_cloneable0]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_cloneable1]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_tags_renamed]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_valid_tag_types]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_repr]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_no_attributes_set_in_init]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_score_takes_y]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_overwrite_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_dont_overwrite_parameters]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_fit_returns_self]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_readonly_memmap_input]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_unfitted]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_do_not_raise_errors_in_init_or_set_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_n_features_in_after_fitting]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_mixin_order]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_positive_only_tag_during_fit]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_dtypes]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_complex_data]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_dtype_object]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_empty_data_messages]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_pipeline_consistency]": "skipped",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_nan_inf]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_tag]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_array]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_matrix]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_pickle]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_pickle(readonly_memmap=True)]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_f_contiguous_array_estimator]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train(readonly_memmap=True)]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_regressor_data_not_an_array]": "skipped",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_partial_fit_n_features]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_no_decision_function]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_supervised_y_2d]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_supervised_y_no_nan]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_int]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_non_transformer_estimators_n_iter]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_parameters_default_constructible]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_1sample]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_1feature]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_get_params_invariance]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_set_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_dict_unchanged]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_idempotent]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_check_is_fitted]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_n_features_in]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_fit1d]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_predict1d]": "passed",
                "tests/test_common.py::test_basic_estimator[Regressor()-check_requires_y_none]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_cloneable0]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_cloneable1]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_tags_renamed]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_valid_tag_types]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_repr]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_no_attributes_set_in_init]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_score_takes_y]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_overwrite_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_dont_overwrite_parameters]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_fit_returns_self]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_readonly_memmap_input]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_unfitted]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_do_not_raise_errors_in_init_or_set_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_n_features_in_after_fitting]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_mixin_order]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_positive_only_tag_during_fit]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_dtypes]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_complex_data]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_dtype_object]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_empty_data_messages]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_pipeline_consistency]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_nan_inf]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_tag]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_array]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_matrix]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_pickle]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_pickle(readonly_memmap=True)]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_f_contiguous_array_estimator]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_data_not_an_array]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_general]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_preserve_dtypes]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_general(readonly_memmap=True)]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_transformers_unfitted]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_n_iter]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_parameters_default_constructible]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_methods_sample_order_invariance]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_methods_subset_invariance]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_1sample]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_1feature]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_get_params_invariance]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_set_params]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_dict_unchanged]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_idempotent]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_check_is_fitted]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_n_features_in]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_fit1d]": "passed",
                "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_predict1d]": "passed",
                "tests/test_sklearn_compat.py::test__sklearn_compat": "passed",
                "tests/test_version.py::test_version": "passed",
                "tests/utils/_test_common/test_instance_generator.py::test__construct_instances": "passed",
                "tests/utils/test_chunking.py::test_chunk_generator": "passed",
                "tests/utils/test_chunking.py::test_gen_batches": "passed",
                "tests/utils/test_chunking.py::test_gen_even_slices": "passed",
                "tests/utils/test_chunking.py::test_get_chunk_n_rows": "passed",
                "tests/utils/test_estimator_checks.py::test_check_estimator": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_cloneable0]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_cloneable1]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_tags_renamed]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_valid_tag_types]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_repr]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_no_attributes_set_in_init]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_score_takes_y]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_overwrite_params]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dont_overwrite_parameters]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_fit_returns_self]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_readonly_memmap_input]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_unfitted]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_do_not_raise_errors_in_init_or_set_params]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_n_features_in_after_fitting]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_mixin_order]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_positive_only_tag_during_fit]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_dtypes]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_complex_data]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dtype_object]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_empty_data_messages]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_pipeline_consistency]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_nan_inf]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_tag]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_array]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_matrix]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_pickle]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_pickle(readonly_memmap=True)]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_f_contiguous_array_estimator]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_data_not_an_array]": "xfailed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_general]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_preserve_dtypes]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_general(readonly_memmap=True)]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformers_unfitted]": "xfailed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_n_iter]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_parameters_default_constructible]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_methods_sample_order_invariance]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_methods_subset_invariance]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_1sample]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_1feature]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_get_params_invariance]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_set_params]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dict_unchanged]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_idempotent]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_check_is_fitted]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_n_features_in]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit1d]": "passed",
                "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_predict1d]": "passed",
                "tests/utils/test_extmath.py::test__approximate_mode": "passed",
                "tests/utils/test_extmath.py::test_safe_sqr": "passed",
                "tests/utils/test_fixes.py::test__in_unstable_openblas_configuration": "passed",
                "tests/utils/test_fixes.py::test__IS_WASM": "passed",
                "tests/utils/test_fixes.py::test__IS_32BIT": "passed",
                "tests/utils/test_indexing.py::test__determine_key_type": "passed",
                "tests/utils/test_indexing.py::test__safe_indexing": "passed",
                "tests/utils/test_indexing.py::test__get_column_indices": "skipped",
                "tests/utils/test_indexing.py::test_resample": "passed",
                "tests/utils/test_indexing.py::test_shuffle": "passed",
                "tests/utils/test_mask.py::test_safe_mask": "passed",
                "tests/utils/test_mask.py::test_axis0_safe_slice": "passed",
                "tests/utils/test_mask.py::test_indices_to_mask": "passed",
                "tests/utils/test_metadata_routing.py::test_process_routing": "passed",
                "tests/utils/test_metadata_routing.py::test_raise_for_params": "passed",
                "tests/utils/test_metadata_routing.py::test__raise_for_params_not_implemented": "skipped",
                "tests/utils/test_missing.py::test_is_scalar_nan": "passed",
                "tests/utils/test_missing.py::test_is_pandas_na": "passed",
                "tests/utils/test_optional_dependencies.py::test_check_matplotlib_support": "passed",
                "tests/utils/test_optional_dependencies.py::test_check_pandas_support": "passed",
                "tests/utils/test_param_validation.py::test_validate_params": "passed",
                "tests/utils/test_tags.py::test_get_tags": "passed",
                "tests/utils/test_tags.py::test_patched_more_tags": "skipped",
                "tests/utils/test_user_interface.py::test_print_elapsed_time": "passed",
                "tests/utils/test_validation.py::test_check_array_ensure_all_finite": "passed",
                "tests/utils/test_validation.py::test_check_X_y_ensure_all_finite": "passed",
                "tests/utils/test_validation.py::test_validate_data[True]": "passed",
                "tests/utils/test_validation.py::test_validate_data[False]": "passed",
                "tests/utils/test_validation.py::test_validate_data_skip_check_array[list]": "passed",
                "tests/utils/test_validation.py::test_validate_data_skip_check_array[dataframe]": "skipped",
                "tests/utils/test_validation.py::test_check_n_features": "passed",
                "tests/utils/test_validation.py::test_check_feature_names": "skipped",
                "tests/utils/test_validation.py::test__to_object_array": "passed",
                "tests/utils/test_validation.py::test__is_fitted": "passed"
            },
            "success_count": 231,
            "failed_count": 0,
            "error_count": 0,
            "skipped_count": 9,
            "unknown_count": 0,
            "total_count": 240,
            "success_rate": 0.9625,
            "coverage_report": {
                "covered_lines": 59,
                "num_statements": 239,
                "percent_covered": 22.105263157894736,
                "percent_covered_display": "22",
                "missing_lines": 180,
                "excluded_lines": 0,
                "num_branches": 46,
                "num_partial_branches": 4,
                "covered_branches": 4,
                "missing_branches": 42
            },
            "coverage_result": {}
        },
        "codelines_count": 1614,
        "codefiles_count": 41,
        "code_length": 60602,
        "test_files_count": 19,
        "test_code_length": 21803,
        "class_diagram": "@startuml\nclass Classifier {\n    _parameter_constraints: Unknown\n    _estimator_type: Unknown\n    __init__(seed): void\n    fit(X, y): void\n    predict(X): void\n    _more_tags(): void\n    __sklearn_tags__(): void\n}\nclass Regressor {\n    _parameter_constraints: Unknown\n    _estimator_type: Unknown\n    __init__(seed): void\n    fit(X, y): void\n    predict(X): void\n    _more_tags(): void\n    __sklearn_tags__(): void\n}\nclass Transformer {\n    _parameter_constraints: Unknown\n    _estimator_type: Unknown\n    __init__(with_mean, with_std): void\n    fit(X, y): void\n    transform(X): void\n}\nclass MyEstimator {\n    fit(X, y): void\n    transform(X): void\n}\n@enduml",
        "structure": [
            {
                "file": "tests/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/test_base.py",
                "functions": [
                    {
                        "name": "test_is_clusterer",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_sklearn_compat.py",
                "functions": [
                    {
                        "name": "test__sklearn_compat",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/test_common.py",
                "functions": [
                    {
                        "name": "test_basic_estimator",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "estimator",
                            "check"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "Classifier",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "seed"
                                ]
                            },
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            },
                            {
                                "name": "predict",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X"
                                ]
                            },
                            {
                                "name": "_more_tags",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__sklearn_tags__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": [
                            "_parameter_constraints",
                            "_estimator_type"
                        ]
                    },
                    {
                        "name": "Regressor",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "seed"
                                ]
                            },
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            },
                            {
                                "name": "predict",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X"
                                ]
                            },
                            {
                                "name": "_more_tags",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__sklearn_tags__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": [
                            "_parameter_constraints",
                            "_estimator_type"
                        ]
                    },
                    {
                        "name": "Transformer",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "with_mean",
                                    "with_std"
                                ]
                            },
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            },
                            {
                                "name": "transform",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X"
                                ]
                            }
                        ],
                        "attributes": [
                            "_parameter_constraints",
                            "_estimator_type"
                        ]
                    }
                ]
            },
            {
                "file": "tests/test_version.py",
                "functions": [
                    {
                        "name": "test_version",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_missing.py",
                "functions": [
                    {
                        "name": "test_is_scalar_nan",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_is_pandas_na",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_param_validation.py",
                "functions": [
                    {
                        "name": "test_validate_params",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/utils/test_indexing.py",
                "functions": [
                    {
                        "name": "test__determine_key_type",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test__safe_indexing",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test__get_column_indices",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_resample",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_shuffle",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_optional_dependencies.py",
                "functions": [
                    {
                        "name": "test_check_matplotlib_support",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_check_pandas_support",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_user_interface.py",
                "functions": [
                    {
                        "name": "test_print_elapsed_time",
                        "docstring": "Check that we can import `_print_elapsed_time` from the right module.\n\nThis change has been done in scikit-learn 1.5.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_estimator_checks.py",
                "functions": [
                    {
                        "name": "test_check_estimator",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_parametrize_with_checks",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "estimator",
                            "check"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "MyEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            },
                            {
                                "name": "transform",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/utils/test_chunking.py",
                "functions": [
                    {
                        "name": "test_chunk_generator",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_gen_batches",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_gen_even_slices",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_get_chunk_n_rows",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_mask.py",
                "functions": [
                    {
                        "name": "test_safe_mask",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_axis0_safe_slice",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_indices_to_mask",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_fixes.py",
                "functions": [
                    {
                        "name": "test__in_unstable_openblas_configuration",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test__IS_WASM",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test__IS_32BIT",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_validation.py",
                "functions": [
                    {
                        "name": "test_check_array_ensure_all_finite",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_check_X_y_ensure_all_finite",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_validate_data",
                        "docstring": "Check the behaviour of `validate_data`.\n\nThis change has been introduced in scikit-learn 1.6.",
                        "comments": null,
                        "args": [
                            "ensure_all_finite"
                        ]
                    },
                    {
                        "name": "test_validate_data_skip_check_array",
                        "docstring": null,
                        "comments": null,
                        "args": [
                            "container_type"
                        ]
                    },
                    {
                        "name": "test_check_n_features",
                        "docstring": "Check the behaviour of `_check_n_features`.\n\nThis change has been introduced in scikit-learn 1.6.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_check_feature_names",
                        "docstring": "Check the behaviour of `_check_feature_names`.\n\nThis change has been introduced in scikit-learn 1.6.",
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test__to_object_array",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test__is_fitted",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": [
                    {
                        "name": "MyEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            },
                            {
                                "name": "transform",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "MyEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            },
                            {
                                "name": "transform",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "MyEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "MyEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/utils/test_tags.py",
                "functions": [
                    {
                        "name": "test_get_tags",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_patched_more_tags",
                        "docstring": "Non-regression test for:\nhttps://github.com/sklearn-compat/sklearn-compat/issues/27\n\nThe regression can be spotted when an estimator dynamically updates the tags\nbased on the inner estimator.",
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": [
                    {
                        "name": "MyEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "_more_tags",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__sklearn_tags__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "MyEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "allow_nan"
                                ]
                            },
                            {
                                "name": "_more_tags",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__sklearn_tags__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "MetaEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "estimator"
                                ]
                            },
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y"
                                ]
                            },
                            {
                                "name": "_more_tags",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            },
                            {
                                "name": "__sklearn_tags__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/utils/test_extmath.py",
                "functions": [
                    {
                        "name": "test__approximate_mode",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_safe_sqr",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_multiclass.py",
                "functions": [
                    {
                        "name": "type_of_target",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "tests/utils/test_metadata_routing.py",
                "functions": [
                    {
                        "name": "test_process_routing",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test_raise_for_params",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    },
                    {
                        "name": "test__raise_for_params_not_implemented",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": [
                    {
                        "name": "ExampleClassifier",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y",
                                    "sample_weight"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "MetaClassifier",
                        "docstring": null,
                        "comments": null,
                        "methods": [
                            {
                                "name": "__init__",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "estimator"
                                ]
                            },
                            {
                                "name": "fit",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self",
                                    "X",
                                    "y",
                                    "sample_weight"
                                ]
                            },
                            {
                                "name": "get_metadata_routing",
                                "docstring": null,
                                "comments": null,
                                "args": [
                                    "self"
                                ]
                            }
                        ],
                        "attributes": []
                    },
                    {
                        "name": "DummyEstimator",
                        "docstring": null,
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "tests/utils/_test_common/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "tests/utils/_test_common/test_instance_generator.py",
                "functions": [
                    {
                        "name": "test__construct_instances",
                        "docstring": null,
                        "comments": null,
                        "args": []
                    }
                ],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/_sklearn_compat.py",
                "functions": [
                    {
                        "name": "_dataclass_args",
                        "docstring": null,
                        "comments": "\nThe following code does not depend on the sklearn version\n\ntags infrastructure",
                        "args": []
                    },
                    {
                        "name": "get_tags",
                        "docstring": "Get estimator tags in a consistent format across different sklearn versions.\n\nThis function provides compatibility between sklearn versions before and after 1.6.\nIt returns either a Tags object (sklearn >= 1.6) or a converted Tags object from\nthe dictionary format (sklearn < 1.6) containing metadata about the estimator's\nrequirements and capabilities.\n\nParameters\n----------\nestimator : estimator object\n    A scikit-learn estimator instance.\n\nReturns\n-------\ntags : Tags\n    An object containing metadata about the estimator's requirements and\n    capabilities (e.g., input types, fitting requirements, classifier/regressor\n    specific tags).",
                        "comments": null,
                        "args": [
                            "estimator"
                        ]
                    },
                    {
                        "name": "_to_new_tags",
                        "docstring": "Utility function convert old tags (dictionary) to new tags (dataclass).",
                        "comments": null,
                        "args": [
                            "old_tags",
                            "estimator"
                        ]
                    }
                ],
                "classes": [
                    {
                        "name": "InputTags",
                        "docstring": "Tags for the input data.\n\nParameters\n----------\none_d_array : bool, default=False\n    Whether the input can be a 1D array.\n\ntwo_d_array : bool, default=True\n    Whether the input can be a 2D array. Note that most common\n    tests currently run only if this flag is set to ``True``.\n\nthree_d_array : bool, default=False\n    Whether the input can be a 3D array.\n\nsparse : bool, default=False\n    Whether the input can be a sparse matrix.\n\ncategorical : bool, default=False\n    Whether the input can be categorical.\n\nstring : bool, default=False\n    Whether the input can be an array-like of strings.\n\ndict : bool, default=False\n    Whether the input can be a dictionary.\n\npositive_only : bool, default=False\n    Whether the estimator requires positive X.\n\nallow_nan : bool, default=False\n    Whether the estimator supports data with missing values encoded as `np.nan`.\n\npairwise : bool, default=False\n    This boolean attribute indicates whether the data (`X`),\n    :term:`fit` and similar methods consists of pairwise measures\n    over samples rather than a feature representation for each\n    sample.  It is usually `True` where an estimator has a\n    `metric` or `affinity` or `kernel` parameter with value\n    'precomputed'. Its primary purpose is to support a\n    :term:`meta-estimator` or a cross validation procedure that\n    extracts a sub-sample of data intended for a pairwise\n    estimator, where the data needs to be indexed on both axes.\n    Specifically, this tag is used by\n    `sklearn.utils.metaestimators._safe_split` to slice rows and\n    columns.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "TargetTags",
                        "docstring": "Tags for the target data.\n\nParameters\n----------\nrequired : bool\n    Whether the estimator requires y to be passed to `fit`,\n    `fit_predict` or `fit_transform` methods. The tag is ``True``\n    for estimators inheriting from `~sklearn.base.RegressorMixin`\n    and `~sklearn.base.ClassifierMixin`.\n\none_d_labels : bool, default=False\n    Whether the input is a 1D labels (y).\n\ntwo_d_labels : bool, default=False\n    Whether the input is a 2D labels (y).\n\npositive_only : bool, default=False\n    Whether the estimator requires a positive y (only applicable\n    for regression).\n\nmulti_output : bool, default=False\n    Whether a regressor supports multi-target outputs or a classifier supports\n    multi-class multi-output.\n\nsingle_output : bool, default=True\n    Whether the target can be single-output. This can be ``False`` if the\n    estimator supports only multi-output cases.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "TransformerTags",
                        "docstring": "Tags for the transformer.\n\nParameters\n----------\npreserves_dtype : list[str], default=[\"float64\"]\n    Applies only on transformers. It corresponds to the data types\n    which will be preserved such that `X_trans.dtype` is the same\n    as `X.dtype` after calling `transformer.transform(X)`. If this\n    list is empty, then the transformer is not expected to\n    preserve the data type. The first value in the list is\n    considered as the default data type, corresponding to the data\n    type of the output when the input data type is not going to be\n    preserved.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "ClassifierTags",
                        "docstring": "Tags for the classifier.\n\nParameters\n----------\npoor_score : bool, default=False\n    Whether the estimator fails to provide a \"reasonable\" test-set\n    score, which currently for classification is an accuracy of\n    0.83 on ``make_blobs(n_samples=300, random_state=0)``. The\n    datasets and values are based on current estimators in scikit-learn\n    and might be replaced by something more systematic.\n\nmulti_class : bool, default=True\n    Whether the classifier can handle multi-class\n    classification. Note that all classifiers support binary\n    classification. Therefore this flag indicates whether the\n    classifier is a binary-classifier-only or not.\n\nmulti_label : bool, default=False\n    Whether the classifier supports multi-label output.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "RegressorTags",
                        "docstring": "Tags for the regressor.\n\nParameters\n----------\npoor_score : bool, default=False\n    Whether the estimator fails to provide a \"reasonable\" test-set\n    score, which currently for regression is an R2 of 0.5 on\n    ``make_regression(n_samples=200, n_features=10,\n    n_informative=1, bias=5.0, noise=20, random_state=42)``. The\n    dataset and values are based on current estimators in scikit-learn\n    and might be replaced by something more systematic.\n\nmulti_label : bool, default=False\n    Whether the regressor supports multilabel output.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    },
                    {
                        "name": "Tags",
                        "docstring": "Tags for the estimator.\n\nSee :ref:`estimator_tags` for more information.\n\nParameters\n----------\nestimator_type : str or None\n    The type of the estimator. Can be one of:\n    - \"classifier\"\n    - \"regressor\"\n    - \"transformer\"\n    - \"clusterer\"\n    - \"outlier_detector\"\n    - \"density_estimator\"\n\ntarget_tags : :class:`TargetTags`\n    The target(y) tags.\n\ntransformer_tags : :class:`TransformerTags` or None\n    The transformer tags.\n\nclassifier_tags : :class:`ClassifierTags` or None\n    The classifier tags.\n\nregressor_tags : :class:`RegressorTags` or None\n    The regressor tags.\n\narray_api_support : bool, default=False\n    Whether the estimator supports Array API compatible inputs.\n\nno_validation : bool, default=False\n    Whether the estimator skips input-validation. This is only meant for\n    stateless and dummy transformers!\n\nnon_deterministic : bool, default=False\n    Whether the estimator is not deterministic given a fixed ``random_state``.\n\nrequires_fit : bool, default=True\n    Whether the estimator requires to be fitted before calling one of\n    `transform`, `predict`, `predict_proba`, or `decision_function`.\n\n_skip_test : bool, default=False\n    Whether to skip common tests entirely. Don't use this unless\n    you have a *very good* reason.\n\ninput_tags : :class:`InputTags`\n    The input data(X) tags.",
                        "comments": null,
                        "methods": [],
                        "attributes": []
                    }
                ]
            },
            {
                "file": "src/sklearn_compat/base.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_indexing.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/extmath.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_chunking.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_param_validation.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/multiclass.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_missing.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/__init__.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/validation.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_optional_dependencies.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/estimator_checks.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_tags.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/metadata_routing.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/fixes.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_mask.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_user_interface.py",
                "functions": [],
                "classes": []
            },
            {
                "file": "src/sklearn_compat/utils/_test_common/instance_generator.py",
                "functions": [],
                "classes": []
            }
        ],
        "test_cases": {
            "tests/test_base.py::test_is_clusterer": {
                "testid": "tests/test_base.py::test_is_clusterer",
                "result": "passed",
                "test_implementation": "def test_is_clusterer():\n    assert is_clusterer(KMeans())\n    assert not is_clusterer(LinearRegression())"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_cloneable0]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_cloneable0]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_cloneable1]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_cloneable1]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_tags_renamed]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_tags_renamed]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_valid_tag_types]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_valid_tag_types]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_repr]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_repr]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_no_attributes_set_in_init]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_no_attributes_set_in_init]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_score_takes_y]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_score_takes_y]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_overwrite_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_overwrite_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_dont_overwrite_parameters]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_dont_overwrite_parameters]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_fit_returns_self]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_fit_returns_self]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_readonly_memmap_input]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_readonly_memmap_input]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_unfitted]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_unfitted]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_do_not_raise_errors_in_init_or_set_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_do_not_raise_errors_in_init_or_set_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_n_features_in_after_fitting]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_n_features_in_after_fitting]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_mixin_order]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_mixin_order]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_positive_only_tag_during_fit]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_positive_only_tag_during_fit]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_dtypes]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_dtypes]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_complex_data]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_complex_data]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_dtype_object]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_dtype_object]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_empty_data_messages]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_empty_data_messages]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_pipeline_consistency]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_pipeline_consistency]",
                "result": "skipped",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_nan_inf]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_nan_inf]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_tag]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_tag]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_array]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_array]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_matrix]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimator_sparse_matrix]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_pickle]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_pickle]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_pickle(readonly_memmap=True)]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_pickle(readonly_memmap=True)]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_f_contiguous_array_estimator]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_f_contiguous_array_estimator]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_classifier_data_not_an_array]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_classifier_data_not_an_array]",
                "result": "skipped",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_one_label]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_one_label]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_one_label_sample_weights]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_one_label_sample_weights]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_classes]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_classes]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_partial_fit_n_features]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_partial_fit_n_features]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train(readonly_memmap=True)]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train(readonly_memmap=True)]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train(readonly_memmap=True,X_dtype=float32)]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_train(readonly_memmap=True,X_dtype=float32)]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_regression_target]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_classifiers_regression_target]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_supervised_y_no_nan]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_supervised_y_no_nan]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_supervised_y_2d]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_supervised_y_2d]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_non_transformer_estimators_n_iter]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_non_transformer_estimators_n_iter]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_decision_proba_consistency]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_decision_proba_consistency]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_parameters_default_constructible]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_parameters_default_constructible]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_1sample]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_1sample]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_1feature]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_1feature]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_get_params_invariance]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_get_params_invariance]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_set_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_set_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_dict_unchanged]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_dict_unchanged]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_idempotent]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_idempotent]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_check_is_fitted]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_fit_check_is_fitted]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_n_features_in]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_n_features_in]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_fit1d]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_fit1d]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_predict1d]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_fit2d_predict1d]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Classifier()-check_requires_y_none]": {
                "testid": "tests/test_common.py::test_basic_estimator[Classifier()-check_requires_y_none]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_cloneable0]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_cloneable0]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_cloneable1]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_cloneable1]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_tags_renamed]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_tags_renamed]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_valid_tag_types]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_valid_tag_types]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_repr]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_repr]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_no_attributes_set_in_init]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_no_attributes_set_in_init]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_score_takes_y]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_score_takes_y]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_overwrite_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_overwrite_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_dont_overwrite_parameters]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_dont_overwrite_parameters]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_fit_returns_self]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_fit_returns_self]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_readonly_memmap_input]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_readonly_memmap_input]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_unfitted]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_unfitted]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_do_not_raise_errors_in_init_or_set_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_do_not_raise_errors_in_init_or_set_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_n_features_in_after_fitting]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_n_features_in_after_fitting]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_mixin_order]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_mixin_order]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_positive_only_tag_during_fit]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_positive_only_tag_during_fit]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_dtypes]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_dtypes]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_complex_data]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_complex_data]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_dtype_object]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_dtype_object]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_empty_data_messages]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_empty_data_messages]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_pipeline_consistency]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_pipeline_consistency]",
                "result": "skipped",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_nan_inf]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_nan_inf]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_tag]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_tag]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_array]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_array]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_matrix]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimator_sparse_matrix]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_pickle]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_pickle]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_pickle(readonly_memmap=True)]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_pickle(readonly_memmap=True)]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_f_contiguous_array_estimator]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_f_contiguous_array_estimator]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train(readonly_memmap=True)]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train(readonly_memmap=True)]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_regressor_data_not_an_array]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_regressor_data_not_an_array]",
                "result": "skipped",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_partial_fit_n_features]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_estimators_partial_fit_n_features]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_no_decision_function]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_no_decision_function]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_supervised_y_2d]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_supervised_y_2d]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_supervised_y_no_nan]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_supervised_y_no_nan]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_int]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_regressors_int]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_non_transformer_estimators_n_iter]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_non_transformer_estimators_n_iter]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_parameters_default_constructible]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_parameters_default_constructible]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_1sample]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_1sample]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_1feature]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_1feature]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_get_params_invariance]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_get_params_invariance]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_set_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_set_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_dict_unchanged]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_dict_unchanged]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_idempotent]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_idempotent]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_check_is_fitted]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_fit_check_is_fitted]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_n_features_in]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_n_features_in]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_fit1d]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_fit1d]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_predict1d]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_fit2d_predict1d]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Regressor()-check_requires_y_none]": {
                "testid": "tests/test_common.py::test_basic_estimator[Regressor()-check_requires_y_none]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_cloneable0]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_cloneable0]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_cloneable1]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_cloneable1]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_tags_renamed]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_tags_renamed]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_valid_tag_types]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_valid_tag_types]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_repr]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_repr]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_no_attributes_set_in_init]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_no_attributes_set_in_init]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_score_takes_y]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_score_takes_y]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_overwrite_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_overwrite_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_dont_overwrite_parameters]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_dont_overwrite_parameters]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_fit_returns_self]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_fit_returns_self]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_readonly_memmap_input]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_readonly_memmap_input]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_unfitted]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_unfitted]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_do_not_raise_errors_in_init_or_set_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_do_not_raise_errors_in_init_or_set_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_n_features_in_after_fitting]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_n_features_in_after_fitting]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_mixin_order]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_mixin_order]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_positive_only_tag_during_fit]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_positive_only_tag_during_fit]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_dtypes]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_dtypes]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_complex_data]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_complex_data]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_dtype_object]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_dtype_object]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_empty_data_messages]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_empty_data_messages]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_pipeline_consistency]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_pipeline_consistency]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_nan_inf]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_nan_inf]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_tag]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_tag]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_array]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_array]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_matrix]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimator_sparse_matrix]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_pickle]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_pickle]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_pickle(readonly_memmap=True)]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_estimators_pickle(readonly_memmap=True)]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_f_contiguous_array_estimator]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_f_contiguous_array_estimator]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_data_not_an_array]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_data_not_an_array]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_general]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_general]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_preserve_dtypes]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_preserve_dtypes]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_general(readonly_memmap=True)]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_general(readonly_memmap=True)]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_transformers_unfitted]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_transformers_unfitted]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_n_iter]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_transformer_n_iter]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_parameters_default_constructible]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_parameters_default_constructible]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_methods_sample_order_invariance]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_methods_sample_order_invariance]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_methods_subset_invariance]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_methods_subset_invariance]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_1sample]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_1sample]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_1feature]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_1feature]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_get_params_invariance]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_get_params_invariance]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_set_params]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_set_params]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_dict_unchanged]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_dict_unchanged]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_idempotent]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_idempotent]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_check_is_fitted]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_fit_check_is_fitted]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_n_features_in]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_n_features_in]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_fit1d]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_fit1d]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_predict1d]": {
                "testid": "tests/test_common.py::test_basic_estimator[Transformer()-check_fit2d_predict1d]",
                "result": "passed",
                "test_implementation": "def test_basic_estimator(estimator, check):\n    return check(estimator)"
            },
            "tests/test_sklearn_compat.py::test__sklearn_compat": {
                "testid": "tests/test_sklearn_compat.py::test__sklearn_compat",
                "result": "passed",
                "test_implementation": "def test__sklearn_compat():\n    # smoke test to trigger the import\n    from sklearn_compat import _sklearn_compat"
            },
            "tests/test_version.py::test_version": {
                "testid": "tests/test_version.py::test_version",
                "result": "passed",
                "test_implementation": "def test_version():\n    assert isinstance(__version__, str)"
            },
            "tests/utils/_test_common/test_instance_generator.py::test__construct_instances": {
                "testid": "tests/utils/_test_common/test_instance_generator.py::test__construct_instances",
                "result": "passed",
                "test_implementation": "def test__construct_instances():\n    list(iter(_construct_instances(LinearRegression)))"
            },
            "tests/utils/test_chunking.py::test_chunk_generator": {
                "testid": "tests/utils/test_chunking.py::test_chunk_generator",
                "result": "passed",
                "test_implementation": "def test_chunk_generator():\n    gen_chunk = chunk_generator(range(10), 3)\n    assert len(next(gen_chunk)) == 3"
            },
            "tests/utils/test_chunking.py::test_gen_batches": {
                "testid": "tests/utils/test_chunking.py::test_gen_batches",
                "result": "passed",
                "test_implementation": "def test_gen_batches():\n    batches = list(gen_batches(7, 3))\n    assert batches == [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]"
            },
            "tests/utils/test_chunking.py::test_gen_even_slices": {
                "testid": "tests/utils/test_chunking.py::test_gen_even_slices",
                "result": "passed",
                "test_implementation": "def test_gen_even_slices():\n    batches = list(gen_even_slices(10, 1))\n    assert batches == [slice(0, 10, None)]"
            },
            "tests/utils/test_chunking.py::test_get_chunk_n_rows": {
                "testid": "tests/utils/test_chunking.py::test_get_chunk_n_rows",
                "result": "passed",
                "test_implementation": "def test_get_chunk_n_rows():\n    assert get_chunk_n_rows(10) == 107374182"
            },
            "tests/utils/test_estimator_checks.py::test_check_estimator": {
                "testid": "tests/utils/test_estimator_checks.py::test_check_estimator",
                "result": "passed",
                "test_implementation": "def test_check_estimator():\n    check_estimator(\n        MyEstimator(),\n        expected_failed_checks=failing_checks,\n    )"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_cloneable0]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_cloneable0]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_cloneable1]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_cloneable1]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_tags_renamed]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_tags_renamed]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_valid_tag_types]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_valid_tag_types]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_repr]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_repr]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_no_attributes_set_in_init]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_no_attributes_set_in_init]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_score_takes_y]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_score_takes_y]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_overwrite_params]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_overwrite_params]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dont_overwrite_parameters]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dont_overwrite_parameters]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_fit_returns_self]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_fit_returns_self]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_readonly_memmap_input]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_readonly_memmap_input]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_unfitted]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_unfitted]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_do_not_raise_errors_in_init_or_set_params]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_do_not_raise_errors_in_init_or_set_params]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_n_features_in_after_fitting]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_n_features_in_after_fitting]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_mixin_order]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_mixin_order]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_positive_only_tag_during_fit]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_positive_only_tag_during_fit]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_dtypes]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_dtypes]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_complex_data]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_complex_data]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dtype_object]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dtype_object]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_empty_data_messages]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_empty_data_messages]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_pipeline_consistency]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_pipeline_consistency]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_nan_inf]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_nan_inf]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_tag]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_tag]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_array]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_array]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_matrix]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_sparse_matrix]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_pickle]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_pickle]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_pickle(readonly_memmap=True)]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimators_pickle(readonly_memmap=True)]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_f_contiguous_array_estimator]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_f_contiguous_array_estimator]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_data_not_an_array]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_data_not_an_array]",
                "result": "xfailed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_general]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_general]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_preserve_dtypes]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_preserve_dtypes]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_general(readonly_memmap=True)]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_general(readonly_memmap=True)]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformers_unfitted]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformers_unfitted]",
                "result": "xfailed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_n_iter]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_transformer_n_iter]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_parameters_default_constructible]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_parameters_default_constructible]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_methods_sample_order_invariance]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_methods_sample_order_invariance]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_methods_subset_invariance]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_methods_subset_invariance]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_1sample]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_1sample]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_1feature]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_1feature]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_get_params_invariance]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_get_params_invariance]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_set_params]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_set_params]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dict_unchanged]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_dict_unchanged]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_idempotent]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_idempotent]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_check_is_fitted]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit_check_is_fitted]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_n_features_in]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_n_features_in]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit1d]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit1d]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_predict1d]": {
                "testid": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_fit2d_predict1d]",
                "result": "passed",
                "test_implementation": "def test_parametrize_with_checks(estimator, check):\n    check(estimator)"
            },
            "tests/utils/test_extmath.py::test__approximate_mode": {
                "testid": "tests/utils/test_extmath.py::test__approximate_mode",
                "result": "passed",
                "test_implementation": "def test__approximate_mode():\n    result = _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)\n    np.testing.assert_array_equal(result, np.array([2, 1]))"
            },
            "tests/utils/test_extmath.py::test_safe_sqr": {
                "testid": "tests/utils/test_extmath.py::test_safe_sqr",
                "result": "passed",
                "test_implementation": "def test_safe_sqr():\n    result = safe_sqr(np.array([1, 2, 3]))\n    np.testing.assert_array_equal(result, np.array([1, 4, 9]))"
            },
            "tests/utils/test_fixes.py::test__in_unstable_openblas_configuration": {
                "testid": "tests/utils/test_fixes.py::test__in_unstable_openblas_configuration",
                "result": "passed",
                "test_implementation": "def test__in_unstable_openblas_configuration():\n    _in_unstable_openblas_configuration()"
            },
            "tests/utils/test_fixes.py::test__IS_WASM": {
                "testid": "tests/utils/test_fixes.py::test__IS_WASM",
                "result": "passed",
                "test_implementation": "def test__IS_WASM():\n    assert not _IS_WASM"
            },
            "tests/utils/test_fixes.py::test__IS_32BIT": {
                "testid": "tests/utils/test_fixes.py::test__IS_32BIT",
                "result": "passed",
                "test_implementation": "def test__IS_32BIT():\n    assert not _IS_32BIT"
            },
            "tests/utils/test_indexing.py::test__determine_key_type": {
                "testid": "tests/utils/test_indexing.py::test__determine_key_type",
                "result": "passed",
                "test_implementation": "def test__determine_key_type():\n    assert _determine_key_type(np.arange(10)) == \"int\""
            },
            "tests/utils/test_indexing.py::test__safe_indexing": {
                "testid": "tests/utils/test_indexing.py::test__safe_indexing",
                "result": "passed",
                "test_implementation": "def test__safe_indexing():\n    array = np.arange(10).reshape(2, 5)\n    np.testing.assert_allclose(_safe_indexing(array, 1, axis=1), array[:, 1])"
            },
            "tests/utils/test_indexing.py::test__get_column_indices": {
                "testid": "tests/utils/test_indexing.py::test__get_column_indices",
                "result": "skipped",
                "test_implementation": "def test__get_column_indices():\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    assert np.array_equal(_get_column_indices(df, key=\"b\"), [1])"
            },
            "tests/utils/test_indexing.py::test_resample": {
                "testid": "tests/utils/test_indexing.py::test_resample",
                "result": "passed",
                "test_implementation": "def test_resample():\n    array = np.arange(10)\n    resampled_array = resample(array, n_samples=20, replace=True, random_state=0)\n    assert len(resampled_array) == 20"
            },
            "tests/utils/test_indexing.py::test_shuffle": {
                "testid": "tests/utils/test_indexing.py::test_shuffle",
                "result": "passed",
                "test_implementation": "def test_shuffle():\n    array = np.arange(10)\n    shuffled_array = shuffle(array, random_state=0)\n    assert len(shuffled_array) == len(array)"
            },
            "tests/utils/test_mask.py::test_safe_mask": {
                "testid": "tests/utils/test_mask.py::test_safe_mask",
                "result": "passed",
                "test_implementation": "def test_safe_mask():\n    data = np.arange(1, 6).reshape(-1, 1)\n    condition = [False, True, True, False, True]\n    mask = safe_mask(data, condition)\n    np.testing.assert_array_equal(data[mask], np.array([[2], [3], [5]]))"
            },
            "tests/utils/test_mask.py::test_axis0_safe_slice": {
                "testid": "tests/utils/test_mask.py::test_axis0_safe_slice",
                "result": "passed",
                "test_implementation": "def test_axis0_safe_slice():\n    X = np.random.randn(5, 3)\n    mask = np.array([True, False, True, False, True])\n    result = axis0_safe_slice(X, mask, X.shape[0])\n    np.testing.assert_array_equal(result, X[mask])"
            },
            "tests/utils/test_mask.py::test_indices_to_mask": {
                "testid": "tests/utils/test_mask.py::test_indices_to_mask",
                "result": "passed",
                "test_implementation": "def test_indices_to_mask():\n    indices = [1, 2, 3, 4]\n    mask = indices_to_mask(indices, 5)\n    np.testing.assert_array_equal(mask, np.array([False, True, True, True, True]))"
            },
            "tests/utils/test_metadata_routing.py::test_process_routing": {
                "testid": "tests/utils/test_metadata_routing.py::test_process_routing",
                "result": "passed",
                "test_implementation": "def test_process_routing():\n    from sklearn.utils.metadata_routing import MetadataRouter, MethodMapping\n\n    class ExampleClassifier(ClassifierMixin, BaseEstimator):\n        def fit(self, X, y, sample_weight=None):\n            self.sample_weight_ = sample_weight\n            # all classifiers need to expose a classes_ attribute once they're fit.\n            self.classes_ = np.array([0, 1])\n            return self\n\n    class MetaClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):\n        def __init__(self, estimator):\n            self.estimator = estimator\n\n        def fit(self, X, y, sample_weight=None):\n            routed_params = process_routing(self, \"fit\", sample_weight=sample_weight)\n            self.estimator_ = clone(self.estimator).fit(\n                X, y, **routed_params.estimator.fit\n            )\n            return self\n\n        def get_metadata_routing(self):\n            router = MetadataRouter(owner=self.__class__.__name__).add(\n                estimator=self.estimator,\n                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"fit\"),\n            )\n            return router\n\n    with config_context(enable_metadata_routing=True):\n        est = MetaClassifier(ExampleClassifier().set_fit_request(sample_weight=True))\n        sample_weight = [1, 2, 3]\n        est.fit(None, None, sample_weight=sample_weight)\n        assert est.estimator_.sample_weight_ == sample_weight"
            },
            "tests/utils/test_metadata_routing.py::test_raise_for_params": {
                "testid": "tests/utils/test_metadata_routing.py::test_raise_for_params",
                "result": "passed",
                "test_implementation": "def test_raise_for_params():\n    class DummyEstimator(BaseEstimator):\n        pass\n\n    est = DummyEstimator()\n    params = {\"invalid_param\": 42}\n\n    with pytest.raises(ValueError, match=\"Passing extra keyword arguments\"):\n        _raise_for_params(params, est, \"fit\")"
            },
            "tests/utils/test_metadata_routing.py::test__raise_for_params_not_implemented": {
                "testid": "tests/utils/test_metadata_routing.py::test__raise_for_params_not_implemented",
                "result": "skipped",
                "test_implementation": "def test__raise_for_params_not_implemented():\n    with pytest.raises(NotImplementedError):\n        _raise_for_params(None, None, \"fit\")\n    with pytest.raises(NotImplementedError):\n        process_routing(None, \"fit\")"
            },
            "tests/utils/test_missing.py::test_is_scalar_nan": {
                "testid": "tests/utils/test_missing.py::test_is_scalar_nan",
                "result": "passed",
                "test_implementation": "def test_is_scalar_nan():\n    assert is_scalar_nan(float(\"nan\"))"
            },
            "tests/utils/test_missing.py::test_is_pandas_na": {
                "testid": "tests/utils/test_missing.py::test_is_pandas_na",
                "result": "passed",
                "test_implementation": "def test_is_pandas_na():\n    assert not is_pandas_na(float(\"nan\"))"
            },
            "tests/utils/test_optional_dependencies.py::test_check_matplotlib_support": {
                "testid": "tests/utils/test_optional_dependencies.py::test_check_matplotlib_support",
                "result": "passed",
                "test_implementation": "def test_check_matplotlib_support():\n    is_matplotlib_installed = False\n    try:\n        import matplotlib  # noqa: F401\n\n        is_matplotlib_installed = True\n    except ImportError:\n        pass\n\n    if is_matplotlib_installed:\n        check_matplotlib_support(\"sklearn_compat\")\n    else:\n        with pytest.raises(ImportError):\n            check_matplotlib_support(\"sklearn_compat\")"
            },
            "tests/utils/test_optional_dependencies.py::test_check_pandas_support": {
                "testid": "tests/utils/test_optional_dependencies.py::test_check_pandas_support",
                "result": "passed",
                "test_implementation": "def test_check_pandas_support():\n    is_pandas_installed = False\n    try:\n        import pandas  # noqa: F401\n\n        is_pandas_installed = True\n    except ImportError:\n        pass\n\n    if is_pandas_installed:\n        check_pandas_support(\"sklearn_compat\")\n    else:\n        with pytest.raises(ImportError):\n            check_pandas_support(\"sklearn_compat\")"
            },
            "tests/utils/test_param_validation.py::test_validate_params": {
                "testid": "tests/utils/test_param_validation.py::test_validate_params",
                "result": "passed",
                "test_implementation": "def test_validate_params():\n    @validate_params({\"x\": [int, float]}, prefer_skip_nested_validation=True)\n    def func(x):\n        return x\n\n    func(1)\n    func(1.0)\n    with pytest.raises(InvalidParameterError):\n        func(\"a\")"
            },
            "tests/utils/test_tags.py::test_get_tags": {
                "testid": "tests/utils/test_tags.py::test_get_tags",
                "result": "passed",
                "test_implementation": "def test_get_tags():\n    class MyEstimator(BaseEstimator):\n        def _more_tags(self):\n            return {\"requires_fit\": False}\n\n        def __sklearn_tags__(self):\n            tags = super().__sklearn_tags__()\n            tags.requires_fit = False\n            return tags\n\n    tags = get_tags(MyEstimator())\n    assert not tags.requires_fit"
            },
            "tests/utils/test_tags.py::test_patched_more_tags": {
                "testid": "tests/utils/test_tags.py::test_patched_more_tags",
                "result": "skipped",
                "test_implementation": "def test_patched_more_tags():\n    \"\"\"Non-regression test for:\n    https://github.com/sklearn-compat/sklearn-compat/issues/27\n\n    The regression can be spotted when an estimator dynamically updates the tags\n    based on the inner estimator.\n    \"\"\"\n\n    class MyEstimator(BaseEstimator):\n        def __init__(self, allow_nan=False):\n            self.allow_nan = allow_nan\n\n        def _more_tags(self):\n            return {\"allow_nan\": self.allow_nan}\n\n        def __sklearn_tags__(self):\n            tags = super().__sklearn_tags__()\n            tags.input_tags.allow_nan = self.allow_nan\n            return tags\n\n    class MetaEstimator(BaseEstimator):\n        def __init__(self, estimator):\n            self.estimator = estimator\n\n        def fit(self, X, y=None):\n            return self\n\n        def _more_tags(self):\n            return {\n                \"allow_nan\": get_tags(self.estimator).input_tags.allow_nan,\n            }\n\n        def __sklearn_tags__(self):\n            tags = super().__sklearn_tags__()\n            tags.input_tags.allow_nan = get_tags(self.estimator).input_tags.allow_nan\n            return tags\n\n    est_no_support_nan = MetaEstimator(estimator=MyEstimator(allow_nan=False))\n    est_support_nan = MetaEstimator(estimator=MyEstimator(allow_nan=True))\n\n    assert not get_tags(est_no_support_nan).input_tags.allow_nan\n    assert get_tags(est_support_nan).input_tags.allow_nan\n\n    from sklearn_compat._sklearn_compat import _patched_more_tags\n\n    test_to_fail = [{\"some_check\": True}]\n    _patched_more_tags(est_no_support_nan, expected_failed_checks=test_to_fail)\n    _patched_more_tags(est_support_nan, expected_failed_checks=test_to_fail)\n\n    # check that patching the instance to add the test to fail should not overwrite\n    # other tags\n    assert not get_tags(est_no_support_nan).input_tags.allow_nan\n    assert get_tags(est_support_nan).input_tags.allow_nan\n    # check that accessing the _xfail_checks via the old _get_tags API report the\n    # test to be skipped\n    assert est_no_support_nan._get_tags()[\"_xfail_checks\"] == test_to_fail\n    assert est_support_nan._get_tags()[\"_xfail_checks\"] == test_to_fail"
            },
            "tests/utils/test_user_interface.py::test_print_elapsed_time": {
                "testid": "tests/utils/test_user_interface.py::test_print_elapsed_time",
                "result": "passed",
                "test_implementation": "def test_print_elapsed_time():\n    \"\"\"Check that we can import `_print_elapsed_time` from the right module.\n\n    This change has been done in scikit-learn 1.5.\n    \"\"\"\n    import sys\n    from io import StringIO\n\n    stdout = StringIO()\n    sys.stdout = stdout\n    with _print_elapsed_time(\"sklearn_compat\", \"testing\"):\n        time.sleep(0.1)\n    sys.stdout = sys.__stdout__\n    output = stdout.getvalue()\n    assert output.startswith(\"[sklearn_compat] .....\")"
            },
            "tests/utils/test_validation.py::test_check_array_ensure_all_finite": {
                "testid": "tests/utils/test_validation.py::test_check_array_ensure_all_finite",
                "result": "passed",
                "test_implementation": "def test_check_array_ensure_all_finite():\n    X = [[1, 2, 3, np.nan]]\n    with pytest.raises(ValueError, match=\"contains NaN\"):\n        check_array(X, ensure_all_finite=True)\n    assert isinstance(check_array(X, ensure_all_finite=False), np.ndarray)"
            },
            "tests/utils/test_validation.py::test_check_X_y_ensure_all_finite": {
                "testid": "tests/utils/test_validation.py::test_check_X_y_ensure_all_finite",
                "result": "passed",
                "test_implementation": "def test_check_X_y_ensure_all_finite():\n    X, y = [[1, 2, 3, np.nan]], [1]\n    with pytest.raises(ValueError, match=\"contains NaN\"):\n        check_X_y(X, y, ensure_all_finite=True)\n    X_, y_ = check_X_y(X, y, ensure_all_finite=False)\n    assert isinstance(X_, np.ndarray) and isinstance(y_, np.ndarray)"
            },
            "tests/utils/test_validation.py::test_validate_data[True]": {
                "testid": "tests/utils/test_validation.py::test_validate_data[True]",
                "result": "passed",
                "test_implementation": "def test_validate_data(ensure_all_finite):\n    \"\"\"Check the behaviour of `validate_data`.\n\n    This change has been introduced in scikit-learn 1.6.\n    \"\"\"\n\n    class MyEstimator(TransformerMixin, BaseEstimator):\n        def fit(self, X, y=None):\n            X = validate_data(self, X=X, y=y, ensure_all_finite=ensure_all_finite)\n            return self\n\n        def transform(self, X):\n            X = validate_data(self, X=X, ensure_all_finite=ensure_all_finite)\n            return X\n\n    X = [[1, 2, 3, 4]]\n    est = MyEstimator()\n    assert isinstance(est.fit_transform(X), np.ndarray)\n\n    X = [[1, 2, 3, np.nan]]\n    est = MyEstimator()\n    if ensure_all_finite:\n        with pytest.raises(ValueError, match=\"contains NaN\"):\n            est.fit_transform(X)\n    else:\n        assert isinstance(est.fit_transform(X), np.ndarray)"
            },
            "tests/utils/test_validation.py::test_validate_data[False]": {
                "testid": "tests/utils/test_validation.py::test_validate_data[False]",
                "result": "passed",
                "test_implementation": "def test_validate_data(ensure_all_finite):\n    \"\"\"Check the behaviour of `validate_data`.\n\n    This change has been introduced in scikit-learn 1.6.\n    \"\"\"\n\n    class MyEstimator(TransformerMixin, BaseEstimator):\n        def fit(self, X, y=None):\n            X = validate_data(self, X=X, y=y, ensure_all_finite=ensure_all_finite)\n            return self\n\n        def transform(self, X):\n            X = validate_data(self, X=X, ensure_all_finite=ensure_all_finite)\n            return X\n\n    X = [[1, 2, 3, 4]]\n    est = MyEstimator()\n    assert isinstance(est.fit_transform(X), np.ndarray)\n\n    X = [[1, 2, 3, np.nan]]\n    est = MyEstimator()\n    if ensure_all_finite:\n        with pytest.raises(ValueError, match=\"contains NaN\"):\n            est.fit_transform(X)\n    else:\n        assert isinstance(est.fit_transform(X), np.ndarray)"
            },
            "tests/utils/test_validation.py::test_validate_data_skip_check_array[list]": {
                "testid": "tests/utils/test_validation.py::test_validate_data_skip_check_array[list]",
                "result": "passed",
                "test_implementation": "def test_validate_data_skip_check_array(container_type):\n    class MyEstimator(TransformerMixin, BaseEstimator):\n        def fit(self, X, y=None):\n            X = validate_data(self, X=X, y=y, skip_check_array=True)\n            return self\n\n        def transform(self, X):\n            X = validate_data(self, X=X, skip_check_array=True)\n            return X\n\n    X = _convert_container(\n        [[1, 2, 3, 4]], container_type, columns_name=[\"a\", \"b\", \"c\", \"d\"]\n    )\n    est = MyEstimator()\n    X_trans = est.fit_transform(X)\n    assert isinstance(X_trans, type(X))\n    assert est.n_features_in_ == 4\n    if container_type == \"dataframe\":\n        np.testing.assert_array_equal(est.feature_names_in_, [\"a\", \"b\", \"c\", \"d\"])"
            },
            "tests/utils/test_validation.py::test_validate_data_skip_check_array[dataframe]": {
                "testid": "tests/utils/test_validation.py::test_validate_data_skip_check_array[dataframe]",
                "result": "skipped",
                "test_implementation": "def test_validate_data_skip_check_array(container_type):\n    class MyEstimator(TransformerMixin, BaseEstimator):\n        def fit(self, X, y=None):\n            X = validate_data(self, X=X, y=y, skip_check_array=True)\n            return self\n\n        def transform(self, X):\n            X = validate_data(self, X=X, skip_check_array=True)\n            return X\n\n    X = _convert_container(\n        [[1, 2, 3, 4]], container_type, columns_name=[\"a\", \"b\", \"c\", \"d\"]\n    )\n    est = MyEstimator()\n    X_trans = est.fit_transform(X)\n    assert isinstance(X_trans, type(X))\n    assert est.n_features_in_ == 4\n    if container_type == \"dataframe\":\n        np.testing.assert_array_equal(est.feature_names_in_, [\"a\", \"b\", \"c\", \"d\"])"
            },
            "tests/utils/test_validation.py::test_check_n_features": {
                "testid": "tests/utils/test_validation.py::test_check_n_features",
                "result": "passed",
                "test_implementation": "def test_check_n_features():\n    \"\"\"Check the behaviour of `_check_n_features`.\n\n    This change has been introduced in scikit-learn 1.6.\n    \"\"\"\n\n    class MyEstimator(TransformerMixin, BaseEstimator):\n        def fit(self, X, y=None):\n            X = _check_n_features(self, X, reset=True)\n            return self\n\n    X = [[1, 2, 3, 4]]\n    est = MyEstimator().fit(X)\n    assert est.n_features_in_ == 4"
            },
            "tests/utils/test_validation.py::test_check_feature_names": {
                "testid": "tests/utils/test_validation.py::test_check_feature_names",
                "result": "skipped",
                "test_implementation": "def test_check_feature_names():\n    \"\"\"Check the behaviour of `_check_feature_names`.\n\n    This change has been introduced in scikit-learn 1.6.\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    class MyEstimator(TransformerMixin, BaseEstimator):\n        def fit(self, X, y=None):\n            X = _check_feature_names(self, X, reset=True)\n            return self\n\n    X = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    est = MyEstimator().fit(X)\n    np.testing.assert_array_equal(est.feature_names_in_, [\"a\", \"b\", \"c\", \"d\"])"
            },
            "tests/utils/test_validation.py::test__to_object_array": {
                "testid": "tests/utils/test_validation.py::test__to_object_array",
                "result": "passed",
                "test_implementation": "def test__to_object_array():\n    result = _to_object_array([np.array([0]), np.array([1])])\n    assert isinstance(result, np.ndarray)\n    assert result.dtype == object"
            },
            "tests/utils/test_validation.py::test__is_fitted": {
                "testid": "tests/utils/test_validation.py::test__is_fitted",
                "result": "passed",
                "test_implementation": "def test__is_fitted():\n    estimator = BaseEstimator()\n    assert not _is_fitted(estimator)"
            }
        },
        "SRS_document": "**Software Requirements Specification**\n\n**Project: sklearn-compat**\n\n**Primary Goal of this SRS Document:**\nThis Software Requirements Specification (SRS) document will serve a critical role in assessing software developers. Developers will be provided with this SRS document and a designated subset of the original test cases (public tests). Their objective is to develop a complete, functional software project based solely on this SRS. Their final success will be rigorously measured by whether their implementation passes all original test cases, including a comprehensive set of private tests not initially provided to them.\n\n**Table of Contents**\n1.  Introduction\n    1.1 Purpose\n    1.2 Scope\n    1.3 Definitions, Acronyms, and Abbreviations\n    1.4 References\n    1.5 Overview\n2.  Overall Description\n    2.1 Product Perspective\n    2.2 Product Functions\n    2.3 User Characteristics\n    2.4 Constraints\n    2.5 Assumptions and Dependencies\n3.  Specific Requirements\n    3.1 Functional Requirements\n        3.1.1 General Utilities\n        3.1.2 Base Utilities (`sklearn_compat.base`)\n        3.1.3 Validation Utilities (`sklearn_compat.utils.validation`)\n        3.1.4 Parameter Validation Utilities (`sklearn_compat.utils._param_validation`)\n        3.1.5 Tagging System Utilities (`sklearn_compat.utils` and `sklearn_compat.utils._tags`)\n        3.1.6 Estimator Checking Utilities (`sklearn_compat.utils.estimator_checks`)\n        3.1.7 Mathematical Utilities (`sklearn_compat.utils.extmath`)\n        3.1.8 Multiclass Utilities (`sklearn_compat.utils.multiclass`)\n        3.1.9 Fixes Utilities (`sklearn_compat.utils.fixes`)\n        3.1.10 Chunking Utilities (`sklearn_compat.utils._chunking`)\n        3.1.11 Indexing Utilities (`sklearn_compat.utils._indexing`)\n        3.1.12 Masking Utilities (`sklearn_compat.utils._mask`)\n        3.1.13 Missing Value Utilities (`sklearn_compat.utils._missing`)\n        3.1.14 User Interface Utilities (`sklearn_compat.utils._user_interface`)\n        3.1.15 Optional Dependency Checking Utilities (`sklearn_compat.utils._optional_dependencies`)\n        3.1.16 Metadata Routing Utilities (`sklearn_compat.utils.metadata_routing`)\n        3.1.17 Test Common Utilities (`sklearn_compat.utils._test_common.instance_generator`)\n    3.2 Non-Functional Requirements\n        3.2.1 Versioning\n\n---\n\n**1. Introduction**\n\n**1.1 Purpose**\nThis Software Requirements Specification (SRS) document defines the requirements for the `sklearn-compat` library. The primary purpose of this document is to provide a clear, unambiguous, and comprehensive set of functional and non-functional requirements to be used for assessing software developers. Developers will use this SRS to design and implement the `sklearn-compat` library.\n\n**1.2 Scope**\nThe `sklearn-compat` library is a Python package designed to assist developers in creating scikit-learn compatible estimators that support multiple versions of scikit-learn. It achieves this by providing a compatibility layer for various utility functions and conventions that have changed across different scikit-learn releases. This SRS covers all functional capabilities necessary to replicate the behavior of the `sklearn-compat` library as described herein, focusing on its external interactions and compatibility shims.\n\n**1.3 Definitions, Acronyms, and Abbreviations**\n*   **SRS:** Software Requirements Specification\n*   **sklearn:** scikit-learn, a machine learning library in Python.\n*   **Estimator:** A scikit-learn object that learns from data; it can be a classifier, regressor, transformer, etc.\n*   **API:** Application Programming Interface\n*   **FR:** Functional Requirement\n*   **NFR:** Non-Functional Requirement\n\n**1.4 References**\n*   Original `sklearn-compat` README.md (provided as context)\n*   Original `sklearn-compat` source code (provided as context)\n*   Original `sklearn-compat` test suite (provided as context for test traceability)\n\n**1.5 Overview**\nThis document is organized into three main sections:\n*   **Section 1 (Introduction):** Provides an overview of the SRS, its purpose, scope, definitions, and references.\n*   **Section 2 (Overall Description):** Describes the product in general, including its perspective, functions, user characteristics, constraints, and dependencies.\n*   **Section 3 (Specific Requirements):** Details all functional and non-functional requirements of the software. Functional requirements are grouped by utility area.\n\n---\n\n**2. Overall Description**\n\n**2.1 Product Perspective**\n`sklearn-compat` is a utility library for Python developers building third-party packages that depend on scikit-learn. It acts as a compatibility layer, abstracting away version-specific differences in scikit-learn's internal utilities and APIs. This allows developers to write code that works seamlessly across a range of scikit-learn versions without including extensive conditional logic for version handling in their own codebases. The library is designed to be lightweight and can also be vendored (i.e., its core logic can be directly embedded into other projects).\n\n**2.2 Product Functions**\nThe `sklearn-compat` library provides the following key categories of functionalities:\n*   **Version-Agnostic Utility Access:** Offers stable import paths and consistent interfaces for scikit-learn utility functions whose location or signature may have changed across versions. This includes utilities for validation, math operations, indexing, etc.\n*   **Parameter Compatibility:** Adapts function calls to handle parameter name changes or additions in scikit-learn utilities (e.g., `ensure_all_finite` vs. `force_all_finite`).\n*   **Backported Functionality:** Provides implementations of functionalities or objects (e.g., `is_clusterer`, tag dataclasses) for older scikit-learn versions that did not originally include them.\n*   **Estimator Tag Management:** Supplies tools to consistently retrieve and manage estimator tags across different scikit-learn versions, including conversion from older tag formats.\n*   **Estimator Checking Compatibility:** Offers wrappers for scikit-learn's estimator checking utilities to ensure they work correctly with estimators designed for cross-version compatibility, particularly regarding how test failures are specified.\n\n**2.3 User Characteristics**\nThe primary users of `sklearn-compat` are Python developers who maintain or create libraries that extend or depend on scikit-learn and need to ensure their software functions correctly with multiple scikit-learn versions. Users are expected to be familiar with Python and the scikit-learn API.\n\n**2.4 Constraints**\n*   The system must be implemented in Python.\n*   The system must be compatible with scikit-learn versions 1.2 and newer. (This refers to the target scikit-learn versions the `sklearn-compat` library helps support).\n*   The system should aim to have minimal dependencies beyond scikit-learn itself.\n\n**2.5 Assumptions and Dependencies**\n*   The end-user environment will have a compatible version of Python installed.\n*   A version of scikit-learn (>=1.2) will be installed in the environment where `sklearn-compat` or the user's library (using `sklearn-compat`) is run.\n\n---\n\n**3. Specific Requirements**\n\n**3.1 Functional Requirements**\n\n**3.1.1 General Utilities**\n*   **FR-GEN-IMP-001:** The system shall be importable as a Python package named `sklearn_compat`.\n\n**3.1.2 Base Utilities (`sklearn_compat.base`)**\n*   **FR-BASE-IC-001:** The system shall provide a function `is_clusterer(estimator)` that returns `True` if the given estimator object is a clusterer, and `False` otherwise. This function must provide this check consistently for scikit-learn versions 1.2+.\n*   **FR-BASE-FC-001:** The system shall provide a decorator `_fit_context(prefer_skip_nested_validation)` for use on estimator `fit` methods. This decorator shall ensure that the `fit` method is executed within the appropriate parameter validation context provided by scikit-learn, supporting versions 1.2+.\n\n**3.1.3 Validation Utilities (`sklearn_compat.utils.validation`)**\n*   **FR-VAL-VD-001:** The system shall provide a function `validate_data(estimator, X, y=None, reset=True, validate_separately=False, skip_check_array=False, ensure_all_finite=None, **kwargs)`. This function shall serve as a version-agnostic wrapper for scikit-learn's data validation logic.\n    *   It must accept an `ensure_all_finite` parameter. If `ensure_all_finite` is `True`, it should raise an error if data contains NaN or infinite values. If `False`, it should permit them. The library will internally map this to `force_all_finite` for older scikit-learn versions.\n    *   It must correctly handle the `skip_check_array=True` parameter, allowing it to bypass array checks and preserve original data types if specified, while still performing feature name and number checks.\n    *   It must provide consistent data validation behavior for scikit-learn versions 1.2+.\n*   **FR-VAL-CA-001:** The system shall provide a function `check_array(array, ..., ensure_all_finite=None, ...)` that acts as a version-agnostic wrapper for scikit-learn's `check_array`.\n    *   It must accept an `ensure_all_finite` parameter, which controls whether NaN/infinite values are permitted. The library will internally map this to `force_all_finite` for older scikit-learn versions.\n*   **FR-VAL-CXY-001:** The system shall provide a function `check_X_y(X, y, ..., ensure_all_finite=None, ...)` that acts as a version-agnostic wrapper for scikit-learn's `check_X_y`.\n    *   It must accept an `ensure_all_finite` parameter, which controls whether NaN/infinite values are permitted in X and y. The library will internally map this to `force_all_finite` for older scikit-learn versions.\n*   **FR-VAL-CNF-001:** The system shall provide a function `_check_n_features(estimator, X, reset)` that provides a consistent interface to scikit-learn's `_check_n_features` utility across scikit-learn versions 1.2+, adapting to its change from an estimator method to a standalone function.\n*   **FR-VAL-CFN-001:** The system shall provide a function `_check_feature_names(estimator, X, reset)` that provides a consistent interface to scikit-learn's `_check_feature_names` utility across scikit-learn versions 1.2+, adapting to its change from an estimator method to a standalone function.\n*   **FR-VAL-TOA-001:** The system shall provide a function `_to_object_array(array_list)` that ensures access to the `_to_object_array` utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-VAL-IF-001:** The system shall provide a function `_is_fitted(estimator, attributes=None, all_or_any=all)` that consistently determines if an estimator is fitted, ensuring availability for scikit-learn versions < 1.4.\n*   **FR-VAL-IPD-001:** The system shall provide a function `_is_pandas_df(X)` that consistently determines if X is a pandas DataFrame, ensuring availability for scikit-learn versions < 1.4.\n\n**3.1.4 Parameter Validation Utilities (`sklearn_compat.utils._param_validation`)**\n*   **FR-PVAL-VP-001:** The system shall provide a decorator `validate_params(parameter_constraints, prefer_skip_nested_validation)` for use on functions. This decorator shall enable parameter validation according to the provided constraints, compatible with scikit-learn 1.2+.\n\n**3.1.5 Tagging System Utilities (`sklearn_compat.utils` and `sklearn_compat.utils._tags`)**\n*   **FR-TAG-GT-001:** The system shall provide a function `get_tags(estimator)` that retrieves metadata tags for a given scikit-learn estimator.\n    *   The function must return tags in a consistent object format (conforming to the structure of `sklearn_compat.utils.Tags` and its nested dataclasses) across all supported scikit-learn versions (1.2+).\n    *   For scikit-learn versions prior to 1.6 (which used a dictionary-based tag system), the function shall convert these dictionary tags into the consistent object format.\n    *   The function must correctly retrieve tags even if the estimator's `_more_tags` method has been dynamically patched (e.g., by `check_estimator` compatibility shims).\n*   **FR-TAG-DC-001:** The system shall provide an `InputTags` dataclass for defining or representing input data (X) characteristic tags (e.g., `one_d_array`, `sparse`, `allow_nan`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.\n*   **FR-TAG-DC-002:** The system shall provide a `TargetTags` dataclass for defining or representing target data (y) characteristic tags (e.g., `required`, `multi_output`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.\n*   **FR-TAG-DC-003:** The system shall provide a `TransformerTags` dataclass for defining or representing transformer-specific tags (e.g., `preserves_dtype`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.\n*   **FR-TAG-DC-004:** The system shall provide a `ClassifierTags` dataclass for defining or representing classifier-specific tags (e.g., `poor_score`, `multi_class`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.\n*   **FR-TAG-DC-005:** The system shall provide a `RegressorTags` dataclass for defining or representing regressor-specific tags (e.g., `poor_score`, `multi_label`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.\n*   **FR-TAG-DC-006:** The system shall provide a `Tags` dataclass that aggregates all other tag dataclasses (`InputTags`, `TargetTags`, etc.) and common estimator tags (e.g., `estimator_type`, `requires_fit`). This ensures a consistent top-level tag object, especially for scikit-learn versions < 1.6.\n\n**3.1.6 Estimator Checking Utilities (`sklearn_compat.utils.estimator_checks`)**\n*   **FR-ESTC-CE-001:** The system shall provide a function `check_estimator(estimator, generate_only=False, ..., expected_failed_checks=None, ...)` for testing scikit-learn compatibility of an estimator.\n    *   It must accept an `expected_failed_checks` parameter (a dictionary mapping check names to reasons for failure).\n    *   For scikit-learn versions < 1.6 (which used `_xfail_checks` tag), this function must dynamically patch the provided estimator's `_more_tags` method to include the `expected_failed_checks` as `_xfail_checks`, allowing tests to be skipped appropriately.\n*   **FR-ESTC-PC-001:** The system shall provide a decorator `parametrize_with_checks(estimators, ..., expected_failed_checks=None, ...)` for use with Pytest to generate test cases from scikit-learn's common estimator checks.\n    *   It must accept an `expected_failed_checks` parameter (a callable that returns a dictionary mapping check names to reasons for failure for a given estimator).\n    *   For scikit-learn versions < 1.6, this function must dynamically patch each estimator's `_more_tags` method to include its `expected_failed_checks` as `_xfail_checks`.\n\n**3.1.7 Mathematical Utilities (`sklearn_compat.utils.extmath`)**\n*   **FR-EXTM-SS-001:** The system shall provide a function `safe_sqr(X)` that ensures access to the `safe_sqr` utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-EXTM-AM-001:** The system shall provide a function `_approximate_mode(class_counts, n_draws, rng)` that ensures access to the `_approximate_mode` utility, which was moved within scikit-learn modules in version 1.5.\n\n**3.1.8 Multiclass Utilities (`sklearn_compat.utils.multiclass`)**\n*   **FR-MULT-TOT-001:** The system shall provide a function `type_of_target(y, input_name=\"\", raise_unknown=False)` that determines the type of a target variable `y`.\n    *   It must support the `raise_unknown` parameter. If `True` and the target type is unknown, it should raise a `ValueError`. If `False` (or not provided) it should return 'unknown'. This behavior must be consistent for scikit-learn versions 1.2+ (parameter introduced later in sklearn).\n\n**3.1.9 Fixes Utilities (`sklearn_compat.utils.fixes`)**\n*   **FR-FIX-UOC-001:** The system shall provide a function `_in_unstable_openblas_configuration()` that ensures access to the `_in_unstable_openblas_configuration` utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-FIX-I32-001:** The system shall provide a constant `_IS_32BIT` that indicates if the Python interpreter is 32-bit. This ensures its availability from a consistent path, mirroring its move in scikit-learn 1.5.\n*   **FR-FIX-IWS-001:** The system shall provide a constant `_IS_WASM` that indicates if the Python environment is WebAssembly. This ensures its availability from a consistent path, mirroring its introduction/move in scikit-learn 1.5.\n\n**3.1.10 Chunking Utilities (`sklearn_compat.utils._chunking`)**\n*   **FR-CHNK-GB-001:** The system shall provide a function `gen_batches(n, batch_size)` that generates slice objects for batch processing. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-CHNK-GES-001:** The system shall provide a function `gen_even_slices(n, n_packs)` that generates slice objects for splitting data into even packs. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-CHNK-GCNR-001:** The system shall provide a function `get_chunk_n_rows(row_bytes, max_n_rows=None, working_memory=None)` that calculates the number of rows per chunk for processing large datasets. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-CHNK-CG-001:** The system shall provide a function `chunk_generator(iterator, chunk_size)` that yields chunks of data from an iterator. This provides a consistent interface to this utility, which was renamed (from `_chunk_generator`) and moved in scikit-learn 1.5.\n\n**3.1.11 Indexing Utilities (`sklearn_compat.utils._indexing`)**\n*   **FR-INDX-DKT-001:** The system shall provide a function `_determine_key_type(key)` that determines the type of a key for indexing. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-INDX-SI-001:** The system shall provide a function `_safe_indexing(X, indices, axis=0)` for safe indexing of array-like objects. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-INDX-SA-001:** The system shall provide a function `_safe_assign(X_dst, X_src, row_indexer, col_indexer)` for safe assignment to array-like objects. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-INDX-GCI-001:** The system shall provide a function `_get_column_indices(X, key)` that retrieves column indices from data structures like pandas DataFrames. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-INDX-RS-001:** The system shall provide a function `resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None)` for resampling arrays. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-INDX-SH-001:** The system shall provide a function `shuffle(*arrays, random_state=None, n_samples=None)` for shuffling arrays consistently. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n\n**3.1.12 Masking Utilities (`sklearn_compat.utils._mask`)**\n*   **FR-MASK-SM-001:** The system shall provide a function `safe_mask(data, mask)` for safely applying a boolean mask to data. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-MASK-ASS-001:** The system shall provide a function `axis0_safe_slice(X, mask, n_samples)` for safely slicing an array along axis 0 using a mask. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-MASK-ITM-001:** The system shall provide a function `indices_to_mask(indices, mask_length)` that converts a list of indices to a boolean mask. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n\n**3.1.13 Missing Value Utilities (`sklearn_compat.utils._missing`)**\n*   **FR-MISS-ISN-001:** The system shall provide a function `is_scalar_nan(x)` that checks if a scalar value is NaN. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-MISS-IPN-001:** The system shall provide a function `is_pandas_na(x)` that checks if a value is a pandas missing value indicator (e.g., `np.nan`, `pd.NA`). This provides a consistent interface to this utility, which was renamed (from `_is_pandas_na`) and moved in scikit-learn 1.5.\n\n**3.1.14 User Interface Utilities (`sklearn_compat.utils._user_interface`)**\n*   **FR-UI-PET-001:** The system shall provide a context manager `_print_elapsed_time(title, process_name)` that prints the time elapsed for operations within its scope. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n\n**3.1.15 Optional Dependency Checking Utilities (`sklearn_compat.utils._optional_dependencies`)**\n*   **FR-OPTD-CMS-001:** The system shall provide a function `check_matplotlib_support(caller_name)` that checks for Matplotlib availability and raises an ImportError if not found. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n*   **FR-OPTD-CPS-001:** The system shall provide a function `check_pandas_support(caller_name)` that checks for Pandas availability and raises an ImportError if not found. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.\n\n**3.1.16 Metadata Routing Utilities (`sklearn_compat.utils.metadata_routing`)**\n*   **FR-META-PR-001:** The system shall provide a function `process_routing(obj, method, **kwargs)` for handling metadata routing in scikit-learn estimators.\n    *   This function must adapt to the signature changes of `process_routing` introduced in scikit-learn 1.4 (where `sample_weight` became part of `**kwargs`).\n    *   For scikit-learn versions < 1.3 (where metadata routing was not available), calls to this function should raise a `NotImplementedError`.\n*   **FR-META-RFP-001:** The system shall provide a function `_raise_for_params(params, owner, method)` used in metadata routing to raise errors for unexpected parameters.\n    *   This function ensures availability for scikit-learn versions from 1.3 onwards (parameter validation behavior differs from 1.4).\n    *   For scikit-learn versions < 1.3, calls to this function should raise a `NotImplementedError`.\n\n**3.1.17 Test Common Utilities (`sklearn_compat.utils._test_common.instance_generator`)**\n*   **FR-TEST-CI-001:** The system shall provide a function `_construct_instances(Estimator)` that generates instances of an estimator class, typically for testing purposes. This function backports or provides a consistent interface to this utility for scikit-learn versions 1.2+.\n\n**3.2 Non-Functional Requirements**\n\n**3.2.1 Versioning**\n*   **NFR-GEN-VER-001:** The system shall expose its own version information as a string attribute named `__version__` at the top level of the package.\n",
        "structured_requirements": [
            {
                "requirement_id": "FR-GEN-IMP-001",
                "requirement_description": "The system shall be importable as a Python package named `sklearn_compat`.",
                "test_traceability": [
                    {
                        "id": "tests/test_sklearn_compat.py::test__sklearn_compat",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/__init__.py",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BASE-IC-001",
                "requirement_description": "The system shall provide a function `is_clusterer(estimator)` that returns `True` if the given estimator object is a clusterer, and `False` otherwise. This function must provide this check consistently for scikit-learn versions 1.2+.",
                "test_traceability": [
                    {
                        "id": "tests/test_base.py::test_is_clusterer",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::is_clusterer",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-BASE-FC-001",
                "requirement_description": "The system shall provide a decorator `_fit_context(prefer_skip_nested_validation)` for use on estimator `fit` methods. This decorator shall ensure that the `fit` method is executed within the appropriate parameter validation context provided by scikit-learn, supporting versions 1.2+.",
                "test_traceability": [
                    {
                        "id": "tests/test_common.py::test_basic_estimator",
                        "description": "(indirectly, via `Classifier.fit`, `Regressor.fit`, `Transformer.fit`)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_fit_context",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-VAL-VD-001",
                "requirement_description": "The system shall provide a function `validate_data(estimator, X, y=None, reset=True, validate_separately=False, skip_check_array=False, ensure_all_finite=None, **kwargs)`. This function shall serve as a version-agnostic wrapper for scikit-learn's data validation logic.\n    *   It must accept an `ensure_all_finite` parameter. If `ensure_all_finite` is `True`, it should raise an error if data contains NaN or infinite values. If `False`, it should permit them. The library will internally map this to `force_all_finite` for older scikit-learn versions.\n    *   It must correctly handle the `skip_check_array=True` parameter, allowing it to bypass array checks and preserve original data types if specified, while still performing feature name and number checks.\n    *   It must provide consistent data validation behavior for scikit-learn versions 1.2+.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_validation.py::test_validate_data",
                        "description": ""
                    },
                    {
                        "id": "tests/utils/test_validation.py::test_validate_data_skip_check_array",
                        "description": ""
                    },
                    {
                        "id": "tests/test_common.py::test_basic_estimator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::validate_data",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-VAL-CA-001",
                "requirement_description": "The system shall provide a function `check_array(array, ..., ensure_all_finite=None, ...)` that acts as a version-agnostic wrapper for scikit-learn's `check_array`.\n    *   It must accept an `ensure_all_finite` parameter, which controls whether NaN/infinite values are permitted. The library will internally map this to `force_all_finite` for older scikit-learn versions.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_validation.py::test_check_array_ensure_all_finite",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::check_array",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-VAL-CXY-001",
                "requirement_description": "The system shall provide a function `check_X_y(X, y, ..., ensure_all_finite=None, ...)` that acts as a version-agnostic wrapper for scikit-learn's `check_X_y`.\n    *   It must accept an `ensure_all_finite` parameter, which controls whether NaN/infinite values are permitted in X and y. The library will internally map this to `force_all_finite` for older scikit-learn versions.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_validation.py::test_check_X_y_ensure_all_finite",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::check_X_y",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-VAL-CNF-001",
                "requirement_description": "The system shall provide a function `_check_n_features(estimator, X, reset)` that provides a consistent interface to scikit-learn's `_check_n_features` utility across scikit-learn versions 1.2+, adapting to its change from an estimator method to a standalone function.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_validation.py::test_check_n_features",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_check_n_features",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-VAL-CFN-001",
                "requirement_description": "The system shall provide a function `_check_feature_names(estimator, X, reset)` that provides a consistent interface to scikit-learn's `_check_feature_names` utility across scikit-learn versions 1.2+, adapting to its change from an estimator method to a standalone function.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_validation.py::test_check_feature_names",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_check_feature_names",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-VAL-TOA-001",
                "requirement_description": "The system shall provide a function `_to_object_array(array_list)` that ensures access to the `_to_object_array` utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_validation.py::test__to_object_array",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_to_object_array",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-VAL-IF-001",
                "requirement_description": "The system shall provide a function `_is_fitted(estimator, attributes=None, all_or_any=all)` that consistently determines if an estimator is fitted, ensuring availability for scikit-learn versions < 1.4.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_validation.py::test__is_fitted",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_is_fitted",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-VAL-IPD-001",
                "requirement_description": "The system shall provide a function `_is_pandas_df(X)` that consistently determines if X is a pandas DataFrame, ensuring availability for scikit-learn versions < 1.4.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_is_pandas_df",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-PVAL-VP-001",
                "requirement_description": "The system shall provide a decorator `validate_params(parameter_constraints, prefer_skip_nested_validation)` for use on functions. This decorator shall enable parameter validation according to the provided constraints, compatible with scikit-learn 1.2+.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_param_validation.py::test_validate_params",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::validate_params",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TAG-GT-001",
                "requirement_description": "The system shall provide a function `get_tags(estimator)` that retrieves metadata tags for a given scikit-learn estimator.\n    *   The function must return tags in a consistent object format (conforming to the structure of `sklearn_compat.utils.Tags` and its nested dataclasses) across all supported scikit-learn versions (1.2+).\n    *   For scikit-learn versions prior to 1.6 (which used a dictionary-based tag system), the function shall convert these dictionary tags into the consistent object format.\n    *   The function must correctly retrieve tags even if the estimator's `_more_tags` method has been dynamically patched (e.g., by `check_estimator` compatibility shims).",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_tags.py::test_get_tags",
                        "description": ""
                    },
                    {
                        "id": "tests/utils/test_tags.py::test_patched_more_tags",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::get_tags",
                        "description": ""
                    },
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_to_new_tags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TAG-DC-001",
                "requirement_description": "The system shall provide an `InputTags` dataclass for defining or representing input data (X) characteristic tags (e.g., `one_d_array`, `sparse`, `allow_nan`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.",
                "test_traceability": [
                    {
                        "id": "Implicitly via `tests/utils/test_tags.py::test_get_tags`",
                        "description": "(as part of the returned `Tags` object)"
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::InputTags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TAG-DC-002",
                "requirement_description": "The system shall provide a `TargetTags` dataclass for defining or representing target data (y) characteristic tags (e.g., `required`, `multi_output`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.",
                "test_traceability": [
                    {
                        "id": "Implicitly via `tests/utils/test_tags.py::test_get_tags`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::TargetTags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TAG-DC-003",
                "requirement_description": "The system shall provide a `TransformerTags` dataclass for defining or representing transformer-specific tags (e.g., `preserves_dtype`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.",
                "test_traceability": [
                    {
                        "id": "Implicitly via `tests/utils/test_tags.py::test_get_tags`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::TransformerTags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TAG-DC-004",
                "requirement_description": "The system shall provide a `ClassifierTags` dataclass for defining or representing classifier-specific tags (e.g., `poor_score`, `multi_class`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.",
                "test_traceability": [
                    {
                        "id": "Implicitly via `tests/utils/test_tags.py::test_get_tags`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::ClassifierTags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TAG-DC-005",
                "requirement_description": "The system shall provide a `RegressorTags` dataclass for defining or representing regressor-specific tags (e.g., `poor_score`, `multi_label`). This ensures a consistent way to work with these tags, especially for scikit-learn versions < 1.6.",
                "test_traceability": [
                    {
                        "id": "Implicitly via `tests/utils/test_tags.py::test_get_tags`",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::RegressorTags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TAG-DC-006",
                "requirement_description": "The system shall provide a `Tags` dataclass that aggregates all other tag dataclasses (`InputTags`, `TargetTags`, etc.) and common estimator tags (e.g., `estimator_type`, `requires_fit`). This ensures a consistent top-level tag object, especially for scikit-learn versions < 1.6.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_tags.py::test_get_tags",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::Tags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ESTC-CE-001",
                "requirement_description": "The system shall provide a function `check_estimator(estimator, generate_only=False, ..., expected_failed_checks=None, ...)` for testing scikit-learn compatibility of an estimator.\n    *   It must accept an `expected_failed_checks` parameter (a dictionary mapping check names to reasons for failure).\n    *   For scikit-learn versions < 1.6 (which used `_xfail_checks` tag), this function must dynamically patch the provided estimator's `_more_tags` method to include the `expected_failed_checks` as `_xfail_checks`, allowing tests to be skipped appropriately.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_estimator_checks.py::test_check_estimator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::check_estimator",
                        "description": ""
                    },
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_patched_more_tags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-ESTC-PC-001",
                "requirement_description": "The system shall provide a decorator `parametrize_with_checks(estimators, ..., expected_failed_checks=None, ...)` for use with Pytest to generate test cases from scikit-learn's common estimator checks.\n    *   It must accept an `expected_failed_checks` parameter (a callable that returns a dictionary mapping check names to reasons for failure for a given estimator).\n    *   For scikit-learn versions < 1.6, this function must dynamically patch each estimator's `_more_tags` method to include its `expected_failed_checks` as `_xfail_checks`.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::parametrize_with_checks",
                        "description": ""
                    },
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_patched_more_tags",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EXTM-SS-001",
                "requirement_description": "The system shall provide a function `safe_sqr(X)` that ensures access to the `safe_sqr` utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_extmath.py::test_safe_sqr",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::safe_sqr",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-EXTM-AM-001",
                "requirement_description": "The system shall provide a function `_approximate_mode(class_counts, n_draws, rng)` that ensures access to the `_approximate_mode` utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_extmath.py::test__approximate_mode",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_approximate_mode",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-MULT-TOT-001",
                "requirement_description": "The system shall provide a function `type_of_target(y, input_name=\"\", raise_unknown=False)` that determines the type of a target variable `y`.\n    *   It must support the `raise_unknown` parameter. If `True` and the target type is unknown, it should raise a `ValueError`. If `False` (or not provided) it should return 'unknown'. This behavior must be consistent for scikit-learn versions 1.2+ (parameter introduced later in sklearn).",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_multiclass.py::type_of_target",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::type_of_target",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-FIX-UOC-001",
                "requirement_description": "The system shall provide a function `_in_unstable_openblas_configuration()` that ensures access to the `_in_unstable_openblas_configuration` utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_fixes.py::test__in_unstable_openblas_configuration",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_in_unstable_openblas_configuration",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-FIX-I32-001",
                "requirement_description": "The system shall provide a constant `_IS_32BIT` that indicates if the Python interpreter is 32-bit. This ensures its availability from a consistent path, mirroring its move in scikit-learn 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_fixes.py::test__IS_32BIT",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_IS_32BIT",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-FIX-IWS-001",
                "requirement_description": "The system shall provide a constant `_IS_WASM` that indicates if the Python environment is WebAssembly. This ensures its availability from a consistent path, mirroring its introduction/move in scikit-learn 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_fixes.py::test__IS_WASM",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_IS_WASM",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CHNK-GB-001",
                "requirement_description": "The system shall provide a function `gen_batches(n, batch_size)` that generates slice objects for batch processing. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_chunking.py::test_gen_batches",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::gen_batches",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CHNK-GES-001",
                "requirement_description": "The system shall provide a function `gen_even_slices(n, n_packs)` that generates slice objects for splitting data into even packs. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_chunking.py::test_gen_even_slices",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::gen_even_slices",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CHNK-GCNR-001",
                "requirement_description": "The system shall provide a function `get_chunk_n_rows(row_bytes, max_n_rows=None, working_memory=None)` that calculates the number of rows per chunk for processing large datasets. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_chunking.py::test_get_chunk_n_rows",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::get_chunk_n_rows",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-CHNK-CG-001",
                "requirement_description": "The system shall provide a function `chunk_generator(iterator, chunk_size)` that yields chunks of data from an iterator. This provides a consistent interface to this utility, which was renamed (from `_chunk_generator`) and moved in scikit-learn 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_chunking.py::test_chunk_generator",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::chunk_generator",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INDX-DKT-001",
                "requirement_description": "The system shall provide a function `_determine_key_type(key)` that determines the type of a key for indexing. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_indexing.py::test__determine_key_type",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_determine_key_type",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INDX-SI-001",
                "requirement_description": "The system shall provide a function `_safe_indexing(X, indices, axis=0)` for safe indexing of array-like objects. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_indexing.py::test__safe_indexing",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_safe_indexing",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INDX-SA-001",
                "requirement_description": "The system shall provide a function `_safe_assign(X_dst, X_src, row_indexer, col_indexer)` for safe assignment to array-like objects. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_safe_assign",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INDX-GCI-001",
                "requirement_description": "The system shall provide a function `_get_column_indices(X, key)` that retrieves column indices from data structures like pandas DataFrames. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_indexing.py::test__get_column_indices",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_get_column_indices",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INDX-RS-001",
                "requirement_description": "The system shall provide a function `resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None)` for resampling arrays. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_indexing.py::test_resample",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::resample",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-INDX-SH-001",
                "requirement_description": "The system shall provide a function `shuffle(*arrays, random_state=None, n_samples=None)` for shuffling arrays consistently. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_indexing.py::test_shuffle",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::shuffle",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-MASK-SM-001",
                "requirement_description": "The system shall provide a function `safe_mask(data, mask)` for safely applying a boolean mask to data. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_mask.py::test_safe_mask",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::safe_mask",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-MASK-ASS-001",
                "requirement_description": "The system shall provide a function `axis0_safe_slice(X, mask, n_samples)` for safely slicing an array along axis 0 using a mask. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_mask.py::test_axis0_safe_slice",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::axis0_safe_slice",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-MASK-ITM-001",
                "requirement_description": "The system shall provide a function `indices_to_mask(indices, mask_length)` that converts a list of indices to a boolean mask. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_mask.py::test_indices_to_mask",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::indices_to_mask",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-MISS-ISN-001",
                "requirement_description": "The system shall provide a function `is_scalar_nan(x)` that checks if a scalar value is NaN. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_missing.py::test_is_scalar_nan",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::is_scalar_nan",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-MISS-IPN-001",
                "requirement_description": "The system shall provide a function `is_pandas_na(x)` that checks if a value is a pandas missing value indicator (e.g., `np.nan`, `pd.NA`). This provides a consistent interface to this utility, which was renamed (from `_is_pandas_na`) and moved in scikit-learn 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_missing.py::test_is_pandas_na",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::is_pandas_na",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-UI-PET-001",
                "requirement_description": "The system shall provide a context manager `_print_elapsed_time(title, process_name)` that prints the time elapsed for operations within its scope. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_user_interface.py::test_print_elapsed_time",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_print_elapsed_time",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-OPTD-CMS-001",
                "requirement_description": "The system shall provide a function `check_matplotlib_support(caller_name)` that checks for Matplotlib availability and raises an ImportError if not found. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_optional_dependencies.py::test_check_matplotlib_support",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::check_matplotlib_support",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-OPTD-CPS-001",
                "requirement_description": "The system shall provide a function `check_pandas_support(caller_name)` that checks for Pandas availability and raises an ImportError if not found. This ensures access to this utility, which was moved within scikit-learn modules in version 1.5.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_optional_dependencies.py::test_check_pandas_support",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::check_pandas_support",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-META-PR-001",
                "requirement_description": "The system shall provide a function `process_routing(obj, method, **kwargs)` for handling metadata routing in scikit-learn estimators.\n    *   This function must adapt to the signature changes of `process_routing` introduced in scikit-learn 1.4 (where `sample_weight` became part of `**kwargs`).\n    *   For scikit-learn versions < 1.3 (where metadata routing was not available), calls to this function should raise a `NotImplementedError`.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_metadata_routing.py::test_process_routing",
                        "description": ""
                    },
                    {
                        "id": "tests/utils/test_metadata_routing.py::test__raise_for_params_not_implemented",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::process_routing",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-META-RFP-001",
                "requirement_description": "The system shall provide a function `_raise_for_params(params, owner, method)` used in metadata routing to raise errors for unexpected parameters.\n    *   This function ensures availability for scikit-learn versions from 1.3 onwards (parameter validation behavior differs from 1.4).\n    *   For scikit-learn versions < 1.3, calls to this function should raise a `NotImplementedError`.",
                "test_traceability": [
                    {
                        "id": "tests/utils/test_metadata_routing.py::test_raise_for_params",
                        "description": ""
                    },
                    {
                        "id": "tests/utils/test_metadata_routing.py::test__raise_for_params_not_implemented",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_raise_for_params",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "FR-TEST-CI-001",
                "requirement_description": "The system shall provide a function `_construct_instances(Estimator)` that generates instances of an estimator class, typically for testing purposes. This function backports or provides a consistent interface to this utility for scikit-learn versions 1.2+.",
                "test_traceability": [
                    {
                        "id": "tests/utils/_test_common/test_instance_generator.py::test__construct_instances",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/_sklearn_compat.py::_construct_instances",
                        "description": ""
                    }
                ]
            },
            {
                "requirement_id": "NFR-GEN-VER-001",
                "requirement_description": "The system shall expose its own version information as a string attribute named `__version__` at the top level of the package.",
                "test_traceability": [
                    {
                        "id": "tests/test_version.py::test_version",
                        "description": ""
                    }
                ],
                "code_traceability": [
                    {
                        "id": "src/sklearn_compat/__init__.py",
                        "description": "(for `__version__` definition)"
                    }
                ]
            }
        ],
        "commit_sha": "f73eaa861d57276d167cc263a1ebfcb67feb57b4",
        "full_code_skeleton": "--- File: src/sklearn_compat/_sklearn_compat.py ---\n```python\ndef _dataclass_args():\n    pass\n\ndef get_tags(estimator):\n    \"\"\"Get estimator tags in a consistent format across different sklearn versions.\n\n    This function provides compatibility between sklearn versions before and after 1.6.\n    It returns either a Tags object (sklearn >= 1.6) or a converted Tags object from\n    the dictionary format (sklearn < 1.6) containing metadata about the estimator's\n    requirements and capabilities.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        A scikit-learn estimator instance.\n\n    Returns\n    -------\n    tags : Tags\n        An object containing metadata about the estimator's requirements and\n        capabilities (e.g., input types, fitting requirements, classifier/regressor\n        specific tags).\n    \"\"\"\n    pass\n\ndef _to_new_tags(old_tags, estimator=None):\n    \"\"\"Utility function convert old tags (dictionary) to new tags (dataclass).\"\"\"\n    pass\n\ndef _fit_context(*, prefer_skip_nested_validation):\n    \"\"\"Decorator to run the fit methods of estimators within context managers.\"\"\"\n    pass\n\ndef validate_params(parameter_constraints, *, prefer_skip_nested_validation):\n    \"\"\"Validate the parameters of an estimator.\"\"\"\n    pass\n\ndef _is_fitted(estimator, attributes=None, all_or_any=all):\n    \"\"\"Determine if an estimator is fitted\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Returns\n    -------\n    fitted : bool\n        Whether the estimator is fitted.\n    \"\"\"\n    pass\n\ndef process_routing(_obj, _method, /, **kwargs):\n    pass\n\ndef _raise_for_params(params, owner, method):\n    pass\n\ndef process_routing(_obj, _method, /, **kwargs):\n    \"\"\"Validate and route input parameters.\"\"\"\n    pass\n\ndef _raise_for_params(params, owner, method):\n    \"\"\"Raise an error if metadata routing is not enabled and params are passed.\"\"\"\n    pass\n\ndef _is_pandas_df(X):\n    \"\"\"Return True if the X is a pandas dataframe.\"\"\"\n    pass\n\ndef is_clusterer(estimator):\n    \"\"\"Return True if the given estimator is (probably) a clusterer.\"\"\"\n    pass\n\ndef type_of_target(y, input_name=\"\", *, raise_unknown=False):\n    pass\n\ndef _construct_instances(Estimator):\n    pass\n\ndef validate_data(_estimator, /, X=\"no_validation\", y=\"no_validation\", reset=True, validate_separately=False, skip_check_array=False, **kwargs):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    See the original scikit-learn documentation:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.validate_data.html#sklearn.utils.validation.validate_data\n    \"\"\"\n    pass\n\ndef _check_n_features(estimator, X, *, reset):\n    \"\"\"Set the `n_features_in_` attribute, or check against it on an estimator.\"\"\"\n    pass\n\ndef _check_feature_names(estimator, X, *, reset):\n    \"\"\"Check `input_features` and generate names if needed.\"\"\"\n    pass\n\ndef check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype=\"numeric\", order=None, copy=False, force_writeable=False, ensure_all_finite=None, ensure_non_negative=False, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None, input_name=\"\"):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    Check the original documentation for more details:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_array.html\n    \"\"\"\n    pass\n\ndef check_X_y(X, y, accept_sparse=False, *, accept_large_sparse=True, dtype=\"numeric\", order=None, copy=False, force_writeable=False, ensure_all_finite=None, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, estimator=None):\n    \"\"\"Input validation for standard estimators.\n\n    Check the original documentation for more details:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_X_y.html\n    \"\"\"\n    pass\n\n@dataclass(**_dataclass_args())\nclass InputTags:\n    \"\"\"Tags for the input data.\n\n    Parameters\n    ----------\n    one_d_array : bool, default=False\n        Whether the input can be a 1D array.\n\n    two_d_array : bool, default=True\n        Whether the input can be a 2D array. Note that most common\n        tests currently run only if this flag is set to ``True``.\n\n    three_d_array : bool, default=False\n        Whether the input can be a 3D array.\n\n    sparse : bool, default=False\n        Whether the input can be a sparse matrix.\n\n    categorical : bool, default=False\n        Whether the input can be categorical.\n\n    string : bool, default=False\n        Whether the input can be an array-like of strings.\n\n    dict : bool, default=False\n        Whether the input can be a dictionary.\n\n    positive_only : bool, default=False\n        Whether the estimator requires positive X.\n\n    allow_nan : bool, default=False\n        Whether the estimator supports data with missing values encoded as `np.nan`.\n\n    pairwise : bool, default=False\n        This boolean attribute indicates whether the data (`X`),\n        :term:`fit` and similar methods consists of pairwise measures\n        over samples rather than a feature representation for each\n        sample.  It is usually `True` where an estimator has a\n        `metric` or `affinity` or `kernel` parameter with value\n        'precomputed'. Its primary purpose is to support a\n        :term:`meta-estimator` or a cross validation procedure that\n        extracts a sub-sample of data intended for a pairwise\n        estimator, where the data needs to be indexed on both axes.\n        Specifically, this tag is used by\n        `sklearn.utils.metaestimators._safe_split` to slice rows and\n        columns.\n    \"\"\"\n    one_d_array: bool = False\n    two_d_array: bool = True\n    three_d_array: bool = False\n    sparse: bool = False\n    categorical: bool = False\n    string: bool = False\n    dict: bool = False\n    positive_only: bool = False\n    allow_nan: bool = False\n    pairwise: bool = False\n\n@dataclass(**_dataclass_args())\nclass TargetTags:\n    \"\"\"Tags for the target data.\n\n    Parameters\n    ----------\n    required : bool\n        Whether the estimator requires y to be passed to `fit`,\n        `fit_predict` or `fit_transform` methods. The tag is ``True``\n        for estimators inheriting from `~sklearn.base.RegressorMixin`\n        and `~sklearn.base.ClassifierMixin`.\n\n    one_d_labels : bool, default=False\n        Whether the input is a 1D labels (y).\n\n    two_d_labels : bool, default=False\n        Whether the input is a 2D labels (y).\n\n    positive_only : bool, default=False\n        Whether the estimator requires a positive y (only applicable\n        for regression).\n\n    multi_output : bool, default=False\n        Whether a regressor supports multi-target outputs or a classifier supports\n        multi-class multi-output.\n\n    single_output : bool, default=True\n        Whether the target can be single-output. This can be ``False`` if the\n        estimator supports only multi-output cases.\n    \"\"\"\n    required: bool\n    one_d_labels: bool = False\n    two_d_labels: bool = False\n    positive_only: bool = False\n    multi_output: bool = False\n    single_output: bool = True\n\n@dataclass(**_dataclass_args())\nclass TransformerTags:\n    \"\"\"Tags for the transformer.\n\n    Parameters\n    ----------\n    preserves_dtype : list[str], default=[\"float64\"]\n        Applies only on transformers. It corresponds to the data types\n        which will be preserved such that `X_trans.dtype` is the same\n        as `X.dtype` after calling `transformer.transform(X)`. If this\n        list is empty, then the transformer is not expected to\n        preserve the data type. The first value in the list is\n        considered as the default data type, corresponding to the data\n        type of the output when the input data type is not going to be\n        preserved.\n    \"\"\"\n    preserves_dtype: list[str] = field(default_factory=lambda: [\"float64\"])\n\n@dataclass(**_dataclass_args())\nclass ClassifierTags:\n    \"\"\"Tags for the classifier.\n\n    Parameters\n    ----------\n    poor_score : bool, default=False\n        Whether the estimator fails to provide a \"reasonable\" test-set\n        score, which currently for classification is an accuracy of\n        0.83 on ``make_blobs(n_samples=300, random_state=0)``. The\n        datasets and values are based on current estimators in scikit-learn\n        and might be replaced by something more systematic.\n\n    multi_class : bool, default=True\n        Whether the classifier can handle multi-class\n        classification. Note that all classifiers support binary\n        classification. Therefore this flag indicates whether the\n        classifier is a binary-classifier-only or not.\n\n    multi_label : bool, default=False\n        Whether the classifier supports multi-label output.\n    \"\"\"\n    poor_score: bool = False\n    multi_class: bool = True\n    multi_label: bool = False\n\n@dataclass(**_dataclass_args())\nclass RegressorTags:\n    \"\"\"Tags for the regressor.\n\n    Parameters\n    ----------\n    poor_score : bool, default=False\n        Whether the estimator fails to provide a \"reasonable\" test-set\n        score, which currently for regression is an R2 of 0.5 on\n        ``make_regression(n_samples=200, n_features=10,\n        n_informative=1, bias=5.0, noise=20, random_state=42)``. The\n        dataset and values are based on current estimators in scikit-learn\n        and might be replaced by something more systematic.\n\n    multi_label : bool, default=False\n        Whether the regressor supports multilabel output.\n    \"\"\"\n    poor_score: bool = False\n    multi_label: bool = False\n\n@dataclass(**_dataclass_args())\nclass Tags:\n    \"\"\"Tags for the estimator.\n\n    See :ref:`estimator_tags` for more information.\n\n    Parameters\n    ----------\n    estimator_type : str or None\n        The type of the estimator. Can be one of:\n        - \"classifier\"\n        - \"regressor\"\n        - \"transformer\"\n        - \"clusterer\"\n        - \"outlier_detector\"\n        - \"density_estimator\"\n\n    target_tags : :class:`TargetTags`\n        The target(y) tags.\n\n    transformer_tags : :class:`TransformerTags` or None\n        The transformer tags.\n\n    classifier_tags : :class:`ClassifierTags` or None\n        The classifier tags.\n\n    regressor_tags : :class:`RegressorTags` or None\n        The regressor tags.\n\n    array_api_support : bool, default=False\n        Whether the estimator supports Array API compatible inputs.\n\n    no_validation : bool, default=False\n        Whether the estimator skips input-validation. This is only meant for\n        stateless and dummy transformers!\n\n    non_deterministic : bool, default=False\n        Whether the estimator is not deterministic given a fixed ``random_state``.\n\n    requires_fit : bool, default=True\n        Whether the estimator requires to be fitted before calling one of\n        `transform`, `predict`, `predict_proba`, or `decision_function`.\n\n    _skip_test : bool, default=False\n        Whether to skip common tests entirely. Don't use this unless\n        you have a *very good* reason.\n\n    input_tags : :class:`InputTags`\n        The input data(X) tags.\n    \"\"\"\n    estimator_type: str | None\n    target_tags: TargetTags\n    transformer_tags: TransformerTags | None = None\n    classifier_tags: ClassifierTags | None = None\n    regressor_tags: RegressorTags | None = None\n    array_api_support: bool = False\n    no_validation: bool = False\n    non_deterministic: bool = False\n    requires_fit: bool = True\n    _skip_test: bool = False\n    input_tags: InputTags = field(default_factory=InputTags)\n\ndef _patched_more_tags(estimator, expected_failed_checks):\n    pass\n\ndef check_estimator(estimator=None, generate_only=False, *, legacy: bool = True, expected_failed_checks: dict[str, str] | None = None, on_skip: Literal[\"warn\"] | None = \"warn\", on_fail: Literal[\"raise\", \"warn\"] | None = \"raise\", callback: Callable | None = None):\n    pass\n\ndef parametrize_with_checks(estimators, *, legacy: bool = True, expected_failed_checks: Callable | None = None):\n    pass\n\n```",
        "full_code_skeleton_structured": [
            {
                "file_path": "src/sklearn_compat/_sklearn_compat.py",
                "code": "def _dataclass_args():\n    pass\n\ndef get_tags(estimator):\n    \"\"\"Get estimator tags in a consistent format across different sklearn versions.\n\n    This function provides compatibility between sklearn versions before and after 1.6.\n    It returns either a Tags object (sklearn >= 1.6) or a converted Tags object from\n    the dictionary format (sklearn < 1.6) containing metadata about the estimator's\n    requirements and capabilities.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        A scikit-learn estimator instance.\n\n    Returns\n    -------\n    tags : Tags\n        An object containing metadata about the estimator's requirements and\n        capabilities (e.g., input types, fitting requirements, classifier/regressor\n        specific tags).\n    \"\"\"\n    pass\n\ndef _to_new_tags(old_tags, estimator=None):\n    \"\"\"Utility function convert old tags (dictionary) to new tags (dataclass).\"\"\"\n    pass\n\ndef _fit_context(*, prefer_skip_nested_validation):\n    \"\"\"Decorator to run the fit methods of estimators within context managers.\"\"\"\n    pass\n\ndef validate_params(parameter_constraints, *, prefer_skip_nested_validation):\n    \"\"\"Validate the parameters of an estimator.\"\"\"\n    pass\n\ndef _is_fitted(estimator, attributes=None, all_or_any=all):\n    \"\"\"Determine if an estimator is fitted\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Returns\n    -------\n    fitted : bool\n        Whether the estimator is fitted.\n    \"\"\"\n    pass\n\ndef process_routing(_obj, _method, /, **kwargs):\n    pass\n\ndef _raise_for_params(params, owner, method):\n    pass\n\ndef process_routing(_obj, _method, /, **kwargs):\n    \"\"\"Validate and route input parameters.\"\"\"\n    pass\n\ndef _raise_for_params(params, owner, method):\n    \"\"\"Raise an error if metadata routing is not enabled and params are passed.\"\"\"\n    pass\n\ndef _is_pandas_df(X):\n    \"\"\"Return True if the X is a pandas dataframe.\"\"\"\n    pass\n\ndef is_clusterer(estimator):\n    \"\"\"Return True if the given estimator is (probably) a clusterer.\"\"\"\n    pass\n\ndef type_of_target(y, input_name=\"\", *, raise_unknown=False):\n    pass\n\ndef _construct_instances(Estimator):\n    pass\n\ndef validate_data(_estimator, /, X=\"no_validation\", y=\"no_validation\", reset=True, validate_separately=False, skip_check_array=False, **kwargs):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    See the original scikit-learn documentation:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.validate_data.html#sklearn.utils.validation.validate_data\n    \"\"\"\n    pass\n\ndef _check_n_features(estimator, X, *, reset):\n    \"\"\"Set the `n_features_in_` attribute, or check against it on an estimator.\"\"\"\n    pass\n\ndef _check_feature_names(estimator, X, *, reset):\n    \"\"\"Check `input_features` and generate names if needed.\"\"\"\n    pass\n\ndef check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype=\"numeric\", order=None, copy=False, force_writeable=False, ensure_all_finite=None, ensure_non_negative=False, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None, input_name=\"\"):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    Check the original documentation for more details:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_array.html\n    \"\"\"\n    pass\n\ndef check_X_y(X, y, accept_sparse=False, *, accept_large_sparse=True, dtype=\"numeric\", order=None, copy=False, force_writeable=False, ensure_all_finite=None, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, estimator=None):\n    \"\"\"Input validation for standard estimators.\n\n    Check the original documentation for more details:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_X_y.html\n    \"\"\"\n    pass\n\n@dataclass(**_dataclass_args())\nclass InputTags:\n    \"\"\"Tags for the input data.\n\n    Parameters\n    ----------\n    one_d_array : bool, default=False\n        Whether the input can be a 1D array.\n\n    two_d_array : bool, default=True\n        Whether the input can be a 2D array. Note that most common\n        tests currently run only if this flag is set to ``True``.\n\n    three_d_array : bool, default=False\n        Whether the input can be a 3D array.\n\n    sparse : bool, default=False\n        Whether the input can be a sparse matrix.\n\n    categorical : bool, default=False\n        Whether the input can be categorical.\n\n    string : bool, default=False\n        Whether the input can be an array-like of strings.\n\n    dict : bool, default=False\n        Whether the input can be a dictionary.\n\n    positive_only : bool, default=False\n        Whether the estimator requires positive X.\n\n    allow_nan : bool, default=False\n        Whether the estimator supports data with missing values encoded as `np.nan`.\n\n    pairwise : bool, default=False\n        This boolean attribute indicates whether the data (`X`),\n        :term:`fit` and similar methods consists of pairwise measures\n        over samples rather than a feature representation for each\n        sample.  It is usually `True` where an estimator has a\n        `metric` or `affinity` or `kernel` parameter with value\n        'precomputed'. Its primary purpose is to support a\n        :term:`meta-estimator` or a cross validation procedure that\n        extracts a sub-sample of data intended for a pairwise\n        estimator, where the data needs to be indexed on both axes.\n        Specifically, this tag is used by\n        `sklearn.utils.metaestimators._safe_split` to slice rows and\n        columns.\n    \"\"\"\n    one_d_array: bool = False\n    two_d_array: bool = True\n    three_d_array: bool = False\n    sparse: bool = False\n    categorical: bool = False\n    string: bool = False\n    dict: bool = False\n    positive_only: bool = False\n    allow_nan: bool = False\n    pairwise: bool = False\n\n@dataclass(**_dataclass_args())\nclass TargetTags:\n    \"\"\"Tags for the target data.\n\n    Parameters\n    ----------\n    required : bool\n        Whether the estimator requires y to be passed to `fit`,\n        `fit_predict` or `fit_transform` methods. The tag is ``True``\n        for estimators inheriting from `~sklearn.base.RegressorMixin`\n        and `~sklearn.base.ClassifierMixin`.\n\n    one_d_labels : bool, default=False\n        Whether the input is a 1D labels (y).\n\n    two_d_labels : bool, default=False\n        Whether the input is a 2D labels (y).\n\n    positive_only : bool, default=False\n        Whether the estimator requires a positive y (only applicable\n        for regression).\n\n    multi_output : bool, default=False\n        Whether a regressor supports multi-target outputs or a classifier supports\n        multi-class multi-output.\n\n    single_output : bool, default=True\n        Whether the target can be single-output. This can be ``False`` if the\n        estimator supports only multi-output cases.\n    \"\"\"\n    required: bool\n    one_d_labels: bool = False\n    two_d_labels: bool = False\n    positive_only: bool = False\n    multi_output: bool = False\n    single_output: bool = True\n\n@dataclass(**_dataclass_args())\nclass TransformerTags:\n    \"\"\"Tags for the transformer.\n\n    Parameters\n    ----------\n    preserves_dtype : list[str], default=[\"float64\"]\n        Applies only on transformers. It corresponds to the data types\n        which will be preserved such that `X_trans.dtype` is the same\n        as `X.dtype` after calling `transformer.transform(X)`. If this\n        list is empty, then the transformer is not expected to\n        preserve the data type. The first value in the list is\n        considered as the default data type, corresponding to the data\n        type of the output when the input data type is not going to be\n        preserved.\n    \"\"\"\n    preserves_dtype: list[str] = field(default_factory=lambda: [\"float64\"])\n\n@dataclass(**_dataclass_args())\nclass ClassifierTags:\n    \"\"\"Tags for the classifier.\n\n    Parameters\n    ----------\n    poor_score : bool, default=False\n        Whether the estimator fails to provide a \"reasonable\" test-set\n        score, which currently for classification is an accuracy of\n        0.83 on ``make_blobs(n_samples=300, random_state=0)``. The\n        datasets and values are based on current estimators in scikit-learn\n        and might be replaced by something more systematic.\n\n    multi_class : bool, default=True\n        Whether the classifier can handle multi-class\n        classification. Note that all classifiers support binary\n        classification. Therefore this flag indicates whether the\n        classifier is a binary-classifier-only or not.\n\n    multi_label : bool, default=False\n        Whether the classifier supports multi-label output.\n    \"\"\"\n    poor_score: bool = False\n    multi_class: bool = True\n    multi_label: bool = False\n\n@dataclass(**_dataclass_args())\nclass RegressorTags:\n    \"\"\"Tags for the regressor.\n\n    Parameters\n    ----------\n    poor_score : bool, default=False\n        Whether the estimator fails to provide a \"reasonable\" test-set\n        score, which currently for regression is an R2 of 0.5 on\n        ``make_regression(n_samples=200, n_features=10,\n        n_informative=1, bias=5.0, noise=20, random_state=42)``. The\n        dataset and values are based on current estimators in scikit-learn\n        and might be replaced by something more systematic.\n\n    multi_label : bool, default=False\n        Whether the regressor supports multilabel output.\n    \"\"\"\n    poor_score: bool = False\n    multi_label: bool = False\n\n@dataclass(**_dataclass_args())\nclass Tags:\n    \"\"\"Tags for the estimator.\n\n    See :ref:`estimator_tags` for more information.\n\n    Parameters\n    ----------\n    estimator_type : str or None\n        The type of the estimator. Can be one of:\n        - \"classifier\"\n        - \"regressor\"\n        - \"transformer\"\n        - \"clusterer\"\n        - \"outlier_detector\"\n        - \"density_estimator\"\n\n    target_tags : :class:`TargetTags`\n        The target(y) tags.\n\n    transformer_tags : :class:`TransformerTags` or None\n        The transformer tags.\n\n    classifier_tags : :class:`ClassifierTags` or None\n        The classifier tags.\n\n    regressor_tags : :class:`RegressorTags` or None\n        The regressor tags.\n\n    array_api_support : bool, default=False\n        Whether the estimator supports Array API compatible inputs.\n\n    no_validation : bool, default=False\n        Whether the estimator skips input-validation. This is only meant for\n        stateless and dummy transformers!\n\n    non_deterministic : bool, default=False\n        Whether the estimator is not deterministic given a fixed ``random_state``.\n\n    requires_fit : bool, default=True\n        Whether the estimator requires to be fitted before calling one of\n        `transform`, `predict`, `predict_proba`, or `decision_function`.\n\n    _skip_test : bool, default=False\n        Whether to skip common tests entirely. Don't use this unless\n        you have a *very good* reason.\n\n    input_tags : :class:`InputTags`\n        The input data(X) tags.\n    \"\"\"\n    estimator_type: str | None\n    target_tags: TargetTags\n    transformer_tags: TransformerTags | None = None\n    classifier_tags: ClassifierTags | None = None\n    regressor_tags: RegressorTags | None = None\n    array_api_support: bool = False\n    no_validation: bool = False\n    non_deterministic: bool = False\n    requires_fit: bool = True\n    _skip_test: bool = False\n    input_tags: InputTags = field(default_factory=InputTags)\n\ndef _patched_more_tags(estimator, expected_failed_checks):\n    pass\n\ndef check_estimator(estimator=None, generate_only=False, *, legacy: bool = True, expected_failed_checks: dict[str, str] | None = None, on_skip: Literal[\"warn\"] | None = \"warn\", on_fail: Literal[\"raise\", \"warn\"] | None = \"raise\", callback: Callable | None = None):\n    pass\n\ndef parametrize_with_checks(estimators, *, legacy: bool = True, expected_failed_checks: Callable | None = None):\n    pass\n\n"
            }
        ],
        "minimal_code_skeleton": "--- File: src/sklearn_compat/__init__.py ---\n```python\n__version__ = \"0.1.3\"\n```\n--- File: src/sklearn_compat/_sklearn_compat.py ---\n```python\nfrom typing import Callable, Literal # Note: This import is for context, actual skeleton should not have it. User will add it.\n\n# The actual skeleton starts below. Type hints like Literal and Callable are kept as per instructions.\n\ndef get_tags(estimator):\n    \"\"\"Get estimator tags in a consistent format across different sklearn versions.\n\n    This function provides compatibility between sklearn versions before and after 1.6.\n    It returns either a Tags object (sklearn >= 1.6) or a converted Tags object from\n    the dictionary format (sklearn < 1.6) containing metadata about the estimator's\n    requirements and capabilities.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        A scikit-learn estimator instance.\n\n    Returns\n    -------\n    tags : Tags\n        An object containing metadata about the estimator's requirements and\n        capabilities (e.g., input types, fitting requirements, classifier/regressor\n        specific tags).\n    \"\"\"\n    pass\n\ndef _fit_context(*, prefer_skip_nested_validation):\n    \"\"\"Decorator to run the fit methods of estimators within context managers.\"\"\"\n    pass\n\ndef _is_fitted(estimator, attributes=None, all_or_any=all):\n    \"\"\"Determine if an estimator is fitted\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Returns\n    -------\n    fitted : bool\n        Whether the estimator is fitted.\n    \"\"\"\n    pass\n\ndef process_routing(_obj, _method, /, **kwargs):\n    pass\n\ndef _raise_for_params(params, owner, method):\n    pass\n\ndef _is_pandas_df(X):\n    \"\"\"Return True if the X is a pandas dataframe.\"\"\"\n    pass\n\ndef is_clusterer(estimator):\n    \"\"\"Return True if the given estimator is (probably) a clusterer.\"\"\"\n    pass\n\ndef type_of_target(y, input_name=\"\", *, raise_unknown=False):\n    pass\n\ndef _construct_instances(Estimator):\n    pass\n\ndef validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **kwargs,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    See the original scikit-learn documentation:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.validate_data.html#sklearn.utils.validation.validate_data\n    \"\"\"\n    pass\n\ndef _check_n_features(estimator, X, *, reset):\n    \"\"\"Set the `n_features_in_` attribute, or check against it on an estimator.\"\"\"\n    pass\n\ndef _check_feature_names(estimator, X, *, reset):\n    \"\"\"Check `input_features` and generate names if needed.\"\"\"\n    pass\n\ndef check_array(\n    array,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_writeable=False,\n    ensure_all_finite=None,\n    ensure_non_negative=False,\n    ensure_2d=True,\n    allow_nd=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    estimator=None,\n    input_name=\"\",\n):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    Check the original documentation for more details:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_array.html\n    \"\"\"\n    pass\n\ndef check_X_y(\n    X,\n    y,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_writeable=False,\n    ensure_all_finite=None,\n    ensure_2d=True,\n    allow_nd=False,\n    multi_output=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    y_numeric=False,\n    estimator=None,\n):\n    \"\"\"Input validation for standard estimators.\n\n    Check the original documentation for more details:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_X_y.html\n    \"\"\"\n    pass\n\ndef _patched_more_tags(estimator, expected_failed_checks):\n    pass\n\ndef check_estimator(\n    estimator=None,\n    generate_only=False,\n    *,\n    legacy: bool = True,\n    expected_failed_checks: dict[str, str] | None = None,\n    on_skip: Literal[\"warn\"] | None = \"warn\",\n    on_fail: Literal[\"raise\", \"warn\"] | None = \"raise\",\n    callback: Callable | None = None,\n):\n    pass\n\ndef parametrize_with_checks(\n    estimators,\n    *,\n    legacy: bool = True,\n    expected_failed_checks: Callable | None = None,\n):\n    pass\n```",
        "minimal_code_skeleton_structured": [
            {
                "file_path": "src/sklearn_compat/__init__.py",
                "code": "__version__ = \"0.1.3\"\n"
            },
            {
                "file_path": "src/sklearn_compat/_sklearn_compat.py",
                "code": "from typing import Callable, Literal # Note: This import is for context, actual skeleton should not have it. User will add it.\n\n# The actual skeleton starts below. Type hints like Literal and Callable are kept as per instructions.\n\ndef get_tags(estimator):\n    \"\"\"Get estimator tags in a consistent format across different sklearn versions.\n\n    This function provides compatibility between sklearn versions before and after 1.6.\n    It returns either a Tags object (sklearn >= 1.6) or a converted Tags object from\n    the dictionary format (sklearn < 1.6) containing metadata about the estimator's\n    requirements and capabilities.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        A scikit-learn estimator instance.\n\n    Returns\n    -------\n    tags : Tags\n        An object containing metadata about the estimator's requirements and\n        capabilities (e.g., input types, fitting requirements, classifier/regressor\n        specific tags).\n    \"\"\"\n    pass\n\ndef _fit_context(*, prefer_skip_nested_validation):\n    \"\"\"Decorator to run the fit methods of estimators within context managers.\"\"\"\n    pass\n\ndef _is_fitted(estimator, attributes=None, all_or_any=all):\n    \"\"\"Determine if an estimator is fitted\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Returns\n    -------\n    fitted : bool\n        Whether the estimator is fitted.\n    \"\"\"\n    pass\n\ndef process_routing(_obj, _method, /, **kwargs):\n    pass\n\ndef _raise_for_params(params, owner, method):\n    pass\n\ndef _is_pandas_df(X):\n    \"\"\"Return True if the X is a pandas dataframe.\"\"\"\n    pass\n\ndef is_clusterer(estimator):\n    \"\"\"Return True if the given estimator is (probably) a clusterer.\"\"\"\n    pass\n\ndef type_of_target(y, input_name=\"\", *, raise_unknown=False):\n    pass\n\ndef _construct_instances(Estimator):\n    pass\n\ndef validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **kwargs,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    See the original scikit-learn documentation:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.validation.validate_data.html#sklearn.utils.validation.validate_data\n    \"\"\"\n    pass\n\ndef _check_n_features(estimator, X, *, reset):\n    \"\"\"Set the `n_features_in_` attribute, or check against it on an estimator.\"\"\"\n    pass\n\ndef _check_feature_names(estimator, X, *, reset):\n    \"\"\"Check `input_features` and generate names if needed.\"\"\"\n    pass\n\ndef check_array(\n    array,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_writeable=False,\n    ensure_all_finite=None,\n    ensure_non_negative=False,\n    ensure_2d=True,\n    allow_nd=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    estimator=None,\n    input_name=\"\",\n):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    Check the original documentation for more details:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_array.html\n    \"\"\"\n    pass\n\ndef check_X_y(\n    X,\n    y,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_writeable=False,\n    ensure_all_finite=None,\n    ensure_2d=True,\n    allow_nd=False,\n    multi_output=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    y_numeric=False,\n    estimator=None,\n):\n    \"\"\"Input validation for standard estimators.\n\n    Check the original documentation for more details:\n    https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_X_y.html\n    \"\"\"\n    pass\n\ndef _patched_more_tags(estimator, expected_failed_checks):\n    pass\n\ndef check_estimator(\n    estimator=None,\n    generate_only=False,\n    *,\n    legacy: bool = True,\n    expected_failed_checks: dict[str, str] | None = None,\n    on_skip: Literal[\"warn\"] | None = \"warn\",\n    on_fail: Literal[\"raise\", \"warn\"] | None = \"raise\",\n    callback: Callable | None = None,\n):\n    pass\n\ndef parametrize_with_checks(\n    estimators,\n    *,\n    legacy: bool = True,\n    expected_failed_checks: Callable | None = None,\n):\n    pass\n"
            }
        ],
        "minimal_test_cases": [
            {
                "test_id": "tests/test_version.py::test_version",
                "covers": [
                    "sklearn_compat.__version__ - access check"
                ]
            },
            {
                "test_id": "tests/test_base.py::test_is_clusterer",
                "covers": [
                    "sklearn_compat.base.is_clusterer - happy path with clusterer and non-clusterer"
                ]
            },
            {
                "test_id": "tests/test_common.py::test_basic_estimator[Classifier()-check_estimators_fit_returns_self]",
                "covers": [
                    "sklearn_compat.base._fit_context - decorator usage in estimator fit method",
                    "sklearn_compat.utils.validation.validate_data - basic usage in estimator fit method"
                ]
            },
            {
                "test_id": "tests/utils/test_tags.py::test_get_tags",
                "covers": [
                    "sklearn_compat.utils.get_tags - happy path",
                    "sklearn_compat.utils.Tags - implicit usage via get_tags",
                    "sklearn_compat.utils.InputTags - implicit usage via get_tags",
                    "sklearn_compat.utils.TargetTags - implicit usage via get_tags",
                    "sklearn_compat.utils.TransformerTags - implicit usage via get_tags",
                    "sklearn_compat.utils.ClassifierTags - implicit usage via get_tags",
                    "sklearn_compat.utils.RegressorTags - implicit usage via get_tags"
                ]
            },
            {
                "test_id": "tests/utils/test_estimator_checks.py::test_check_estimator",
                "covers": [
                    "sklearn_compat.utils.estimator_checks.check_estimator - happy path usage"
                ]
            },
            {
                "test_id": "tests/utils/test_estimator_checks.py::test_parametrize_with_checks[MyEstimator()-check_estimator_cloneable0]",
                "covers": [
                    "sklearn_compat.utils.estimator_checks.parametrize_with_checks - decorator usage"
                ]
            },
            {
                "test_id": "tests/utils/test_extmath.py::test__approximate_mode",
                "covers": [
                    "sklearn_compat.utils.extmath._approximate_mode - happy path"
                ]
            },
            {
                "test_id": "tests/utils/test_extmath.py::test_safe_sqr",
                "covers": [
                    "sklearn_compat.utils.extmath.safe_sqr - happy path"
                ]
            },
            {
                "test_id": "tests/utils/test_fixes.py::test__IS_32BIT",
                "covers": [
                    "sklearn_compat.utils.fixes._IS_32BIT - access check"
                ]
            },
            {
                "test_id": "tests/utils/test_fixes.py::test__IS_WASM",
                "covers": [
                    "sklearn_compat.utils.fixes._IS_WASM - access check"
                ]
            },
            {
                "test_id": "tests/utils/test_fixes.py::test__in_unstable_openblas_configuration",
                "covers": [
                    "sklearn_compat.utils.fixes._in_unstable_openblas_configuration - smoke test call"
                ]
            },
            {
                "test_id": "tests/utils/test_metadata_routing.py::test_raise_for_params",
                "covers": [
                    "sklearn_compat.utils.metadata_routing._raise_for_params - error case for extra params (sklearn >= 1.3)"
                ]
            },
            {
                "test_id": "tests/utils/test_metadata_routing.py::test_process_routing",
                "covers": [
                    "sklearn_compat.utils.metadata_routing.process_routing - happy path for routing (sklearn >= 1.3)"
                ]
            },
            {
                "test_id": "tests/utils/test_multiclass.py::type_of_target",
                "covers": [
                    "sklearn_compat.utils.multiclass.type_of_target - error case for unknown target type"
                ]
            },
            {
                "test_id": "tests/utils/test_validation.py::test_check_feature_names",
                "covers": [
                    "sklearn_compat.utils.validation._check_feature_names - happy path"
                ]
            },
            {
                "test_id": "tests/utils/test_validation.py::test_check_n_features",
                "covers": [
                    "sklearn_compat.utils.validation._check_n_features - happy path"
                ]
            },
            {
                "test_id": "tests/utils/test_validation.py::test__is_fitted",
                "covers": [
                    "sklearn_compat.utils.validation._is_fitted - happy path"
                ]
            },
            {
                "test_id": "tests/utils/test_validation.py::test__to_object_array",
                "covers": [
                    "sklearn_compat.utils.validation._to_object_array - happy path"
                ]
            },
            {
                "test_id": "tests/utils/test_validation.py::test_check_array_ensure_all_finite",
                "covers": [
                    "sklearn_compat.utils.validation.check_array - testing ensure_all_finite parameter"
                ]
            },
            {
                "test_id": "tests/utils/test_validation.py::test_check_X_y_ensure_all_finite",
                "covers": [
                    "sklearn_compat.utils.validation.check_X_y - testing ensure_all_finite parameter"
                ]
            },
            {
                "test_id": "tests/test_sklearn_compat.py::test__sklearn_compat",
                "covers": [
                    "sklearn_compat._sklearn_compat - module import smoke test"
                ]
            }
        ]
    }
]